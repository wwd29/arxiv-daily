<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-22</h1>
<h3>Title: Whose Side Are You On? Investigating the Political Stance of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pagnarasmey Pit, Xingjun Ma, Mike Conway, Qingyu Chen, James Bailey, Henry Pit, Putrasmey Keo, Watey Diep, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13840">https://arxiv.org/abs/2403.13840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13840">https://arxiv.org/pdf/2403.13840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13840]] Whose Side Are You On? Investigating the Political Stance of Large  Language Models(https://arxiv.org/abs/2403.13840)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have gained significant popularity for their application in various everyday tasks such as text generation, summarization, and information retrieval. As the widespread adoption of LLMs continues to surge, it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias. In this paper, we propose a quantitative framework and pipeline designed to systematically investigate the political orientation of LLMs. Our investigation delves into the political alignment of LLMs across a spectrum of eight polarizing topics, spanning from abortion to LGBTQ issues. Across topics, the results indicate that LLMs exhibit a tendency to provide responses that closely align with liberal or left-leaning perspectives rather than conservative or right-leaning ones when user queries include details pertaining to occupation, race, or political affiliation. The findings presented in this study not only reaffirm earlier observations regarding the left-leaning characteristics of LLMs but also surface particular attributes, such as occupation, that are particularly susceptible to such inclinations even when directly steered towards conservatism. As a recommendation to avoid these models providing politicised responses, users should be mindful when crafting queries, and exercise caution in selecting neutral prompt language.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其在文本生成、摘要和信息检索等各种日常任务中的应用而广受欢迎。随着法学硕士的广泛采用持续激增，确保这些模型产生政治公正的反应变得越来越重要，目的是防止信息泡沫，维护代表性的公平性并减轻确认偏差。在本文中，我们提出了一个定量框架和管道，旨在系统地调查法学硕士的政治倾向。我们的调查深入探讨了法学硕士在八个两极分化主题中的政治立场，从堕胎到 LGBTQ 问题。在各个主题中，结果表明，当用户查询包含有关职业、种族或政治派别的详细信息时，法学硕士倾向于提供与自由主义或左倾观点紧密一致的回答，而不是保守或右倾观点。这项研究中提出的结果不仅重申了早期关于法学硕士左倾特征的观察结果，而且还揭示了一些特殊属性，例如职业，即使直接转向保守主义，这些属性也特别容易受到这种倾向的影响。作为避免这些模型提供政治化响应的建议，用户在设计查询时应小心，并谨慎选择中立的提示语言。</li>
</ul>

<h3>Title: Train & Constrain: Phonologically Informed Tongue-Twister Generation  from Topics and Paraphrases</h3>
<ul>
<li><strong>Authors: </strong>Tyler Loakman, Chen Tang, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13901">https://arxiv.org/abs/2403.13901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13901">https://arxiv.org/pdf/2403.13901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13901]] Train & Constrain: Phonologically Informed Tongue-Twister Generation  from Topics and Paraphrases(https://arxiv.org/abs/2403.13901)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Previous work in phonologically and phonetically grounded language generation has mainly focused on domains such as puns and poetry. In this article, we present new work on the generation of tongue-twisters - a form of language that is required to be conditioned on a phoneme level to maximize sound overlap, whilst maintaining semantic consistency with an input topic and still being grammatically correct. We present TwisterLister, a pipeline for generating phonologically informed tongue-twisters from Large Language Models (LLMs) that we use to generate TwistList 2.0, the largest annotated dataset of tongue-twisters to date, consisting of 17K+ examples from a combination of human and LLM authors. Our generation pipeline involves the use of a phonologically constrained vocabulary alongside LLM prompting to generate novel, non-derivative tongue-twister examples. We additionally present the results of automatic and human evaluation of smaller models trained on our generated dataset to demonstrate the extent to which phonologically motivated language types can be generated without explicit injection of phonological knowledge. Additionally, we introduce a Phoneme-Aware Constrained Decoding module (PACD) that can be integrated into any causal language model and demonstrate that this method generates good quality tongue-twisters both with and without fine-tuning the underlying language model. We also design and implement a range of automatic metrics for the task of tongue-twister generation that is phonologically motivated and captures the unique essence of tongue-twisters based on Phonemic Edit Distance (PED).</li>
<li><strong>摘要：</strong>以前在语音和语音基础语言生成方面的工作主要集中在双关语和诗歌等领域。在本文中，我们提出了关于绕口令生成的新工作，绕口令是一种需要以音素级别为条件的语言形式，以最大化声音重叠，同时保持与输入主题的语义一致性，并且语法仍然正确。我们推出了 TwisterLister，这是一个从大型语言模型 (LLM) 生成语音绕口令的管道，我们用它来生成 TwistList 2.0，这是迄今为止最大的带注释绕口令数据集，由来自人类和 LLM 组合的 17K 多个示例组成作者。我们的生成流程涉及使用语音受限词汇以及法学硕士，提示生成新颖的、非派生的绕口令示例。我们还展示了对在我们生成的数据集上训练的较小模型的自动和人工评估的结果，以证明在不显式注入语音知识的情况下可以生成语音驱动的语言类型的程度。此外，我们引入了一个音素感知约束解码模块（PACD），它可以集成到任何因果语言模型中，并证明该方法无论是否微调底层语言模型都可以生成高质量的绕口令。我们还设计并实现了一系列用于绕口令生成任务的自动指标，该指标以语音为动机，并基于音位编辑距离（PED）捕捉绕口令的独特本质。</li>
</ul>

<h3>Title: Leveraging Linguistically Enhanced Embeddings for Open Information  Extraction</h3>
<ul>
<li><strong>Authors: </strong>Fauzan Farooqui, Thanmay Jayakumar, Pulkit Mathur, Mansi Radke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13903">https://arxiv.org/abs/2403.13903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13903">https://arxiv.org/pdf/2403.13903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13903]] Leveraging Linguistically Enhanced Embeddings for Open Information  Extraction(https://arxiv.org/abs/2403.13903)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Open Information Extraction (OIE) is a structured prediction (SP) task in Natural Language Processing (NLP) that aims to extract structured $n$-ary tuples - usually subject-relation-object triples - from free text. The word embeddings in the input text can be enhanced with linguistic features, usually Part-of-Speech (PoS) and Syntactic Dependency Parse (SynDP) labels. However, past enhancement techniques cannot leverage the power of pretrained language models (PLMs), which themselves have been hardly used for OIE. To bridge this gap, we are the first to leverage linguistic features with a Seq2Seq PLM for OIE. We do so by introducing two methods - Weighted Addition and Linearized Concatenation. Our work can give any neural OIE architecture the key performance boost from both PLMs and linguistic features in one go. In our settings, this shows wide improvements of up to 24.9%, 27.3% and 14.9% on Precision, Recall and F1 scores respectively over the baseline. Beyond this, we address other important challenges in the field: to reduce compute overheads with the features, we are the first ones to exploit Semantic Dependency Parse (SemDP) tags; to address flaws in current datasets, we create a clean synthetic dataset; finally, we contribute the first known study of OIE behaviour in SP models.</li>
<li><strong>摘要：</strong>开放信息提取 (OIE) 是自然语言处理 (NLP) 中的一项结构化预测 (SP) 任务，旨在从自由文本中提取结构化 $n$ 元组（通常是主语-关系-宾语三元组）。输入文本中的词嵌入可以通过语言特征来增强，通常是词性（PoS）和句法依存解析（SynDP）标签。然而，过去的增强技术无法利用预训练语言模型 (PLM) 的强大功能，而 PLM 本身几乎没有用于 OIE。为了弥补这一差距，我们率先利用 OIE 的 Seq2Seq PLM 的语言特征。我们通过引入两种方法来实现这一点 - 加权加法和线性化串联。我们的工作可以一次性为任何神经 OIE 架构提供 PLM 和语言特征的关键性能提升。在我们的设置中，精度、召回率和 F1 分数较基线分别提高了 24.9%、27.3% 和 14.9%。除此之外，我们还解决了该领域的其他重要挑战：为了减少功能的计算开销，我们是第一个利用语义依赖解析（SemDP）标签的人；为了解决当前数据集中的缺陷，我们创建了一个干净的合成数据集；最后，我们贡献了第一个已知的 SP 模型中 OIE 行为的研究。</li>
</ul>

<h3>Title: Reducing Large Language Model Bias with Emphasis on 'Restricted  Industries': Automated Dataset Augmentation and Prejudice Quantification</h3>
<ul>
<li><strong>Authors: </strong>Devam Mondal, Carlo Lipizzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13925">https://arxiv.org/abs/2403.13925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13925">https://arxiv.org/pdf/2403.13925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13925]] Reducing Large Language Model Bias with Emphasis on 'Restricted  Industries': Automated Dataset Augmentation and Prejudice Quantification(https://arxiv.org/abs/2403.13925)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite the growing capabilities of large language models, there exists concerns about the biases they develop. In this paper, we propose a novel, automated mechanism for debiasing through specified dataset augmentation in the lens of bias producers and in the context of 'restricted industries' with limited data. We additionally create two new additional metrics, the mb-index and db-index, to quantify bias, considering the idea that bias occurs due to both intrinsic model architecture and dataset.</li>
<li><strong>摘要：</strong>尽管大型语言模型的能力不断增强，但人们仍然担心它们会产生偏差。在本文中，我们提出了一种新颖的自动化机制，通过在偏差生产者的视角和数据有限的“受限行业”的背景下通过指定的数据集增强来消除偏差。考虑到偏差是由于内在模型架构和数据集造成的，我们还创建了两个新的附加指标，即 mb-index 和 db-index 来量化偏差。</li>
</ul>

<h3>Title: Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained  Sentence Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Gaifan Zhang, Yi Zhou, Danushka Bollegala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14001">https://arxiv.org/abs/2403.14001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14001">https://arxiv.org/pdf/2403.14001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14001]] Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained  Sentence Embeddings(https://arxiv.org/abs/2403.14001)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Sentence embeddings produced by Pretrained Language Models (PLMs) have received wide attention from the NLP community due to their superior performance when representing texts in numerous downstream applications. However, the high dimensionality of the sentence embeddings produced by PLMs is problematic when representing large numbers of sentences in memory- or compute-constrained devices. As a solution, we evaluate unsupervised dimensionality reduction methods to reduce the dimensionality of sentence embeddings produced by PLMs. Our experimental results show that simple methods such as Principal Component Analysis (PCA) can reduce the dimensionality of sentence embeddings by almost $50\%$, without incurring a significant loss in performance in multiple downstream tasks. Surprisingly, reducing the dimensionality further improves performance over the original high-dimensional versions for the sentence embeddings produced by some PLMs in some tasks.</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 生成的句子嵌入因其在众多下游应用程序中表示文本时的卓越性能而受到了 NLP 社区的广泛关注。然而，当在内存或计算受限的设备中表示大量句子时，PLM 生成的句子嵌入的高维性会出现问题。作为解决方案，我们评估无监督降维方法来降低 PLM 生成的句子嵌入的维度。我们的实验结果表明，主成分分析（PCA）等简单方法可以将句子嵌入的维数减少近 50\%$，而不会在多个下游任务中造成性能的显着损失。令人惊讶的是，对于某些 PLM 在某些任务中生成的句子嵌入，降低维数比原始高维版本进一步提高了性能。</li>
</ul>

<h3>Title: On Prompt Sensitivity of ChatGPT in Affective Computing</h3>
<ul>
<li><strong>Authors: </strong>Mostafa M. Amin, Björn W. Schuller</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14006">https://arxiv.org/abs/2403.14006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14006">https://arxiv.org/pdf/2403.14006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14006]] On Prompt Sensitivity of ChatGPT in Affective Computing(https://arxiv.org/abs/2403.14006)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated the emerging capabilities of foundation models like ChatGPT in several fields, including affective computing. However, accessing these emerging capabilities is facilitated through prompt engineering. Despite the existence of some prompting techniques, the field is still rapidly evolving and many prompting ideas still require investigation. In this work, we introduce a method to evaluate and investigate the sensitivity of the performance of foundation models based on different prompts or generation parameters. We perform our evaluation on ChatGPT within the scope of affective computing on three major problems, namely sentiment analysis, toxicity detection, and sarcasm detection. First, we carry out a sensitivity analysis on pivotal parameters in auto-regressive text generation, specifically the temperature parameter $T$ and the top-$p$ parameter in Nucleus sampling, dictating how conservative or creative the model should be during generation. Furthermore, we explore the efficacy of several prompting ideas, where we explore how giving different incentives or structures affect the performance. Our evaluation takes into consideration performance measures on the affective computing tasks, and the effectiveness of the model to follow the stated instructions, hence generating easy-to-parse responses to be smoothly used in downstream applications.</li>
<li><strong>摘要：</strong>最近的研究证明了 ChatGPT 等基础模型在多个领域（包括情感计算）的新兴功能。然而，通过快速工程可以促进获得这些新兴功能。尽管存在一些提示技术，但该领域仍在快速发展，许多提示想法仍然需要研究。在这项工作中，我们介绍了一种基于不同提示或生成参数来评估和研究基础模型性能敏感性的方法。我们在情感计算的范围内对 ChatGPT 进行了三个主要问题的评估，即情感分析、毒性检测和讽刺检测。首先，我们对自回归文本生成中的关键参数进行敏感性分析，特别是 Nucleus 采样中的温度参数 $T$ 和 top-$p$ 参数，指示模型在生成过程中应该有多保守或创造性。此外，我们探讨了几种激励想法的功效，其中我们探讨了给予不同的激励或结构如何影响绩效。我们的评估考虑了情感计算任务的性能指标，以及模型遵循规定指令的有效性，从而生成易于解析的响应，以便在下游应用程序中顺利使用。</li>
</ul>

<h3>Title: A New Massive Multilingual Dataset for High-Performance Language  Technologies</h3>
<ul>
<li><strong>Authors: </strong>Ona de Gibert, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema Ramírez-Sánchez, Andrey Kutuzov, Sampo Pyysalo, Stephan Oepen, Jörg Tiedemann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14009">https://arxiv.org/abs/2403.14009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14009">https://arxiv.org/pdf/2403.14009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14009]] A New Massive Multilingual Dataset for High-Performance Language  Technologies(https://arxiv.org/abs/2403.14009)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We present the HPLT (High Performance Language Technologies) language resources, a new massive multilingual dataset including both monolingual and bilingual corpora extracted from CommonCrawl and previously unused web crawls from the Internet Archive. We describe our methods for data acquisition, management and processing of large corpora, which rely on open-source software tools and high-performance computing. Our monolingual collection focuses on low- to medium-resourced languages and covers 75 languages and a total of ~5.6 trillion word tokens de-duplicated on the document level. Our English-centric parallel corpus is derived from its monolingual counterpart and covers 18 language pairs and more than 96 million aligned sentence pairs with roughly 1.4 billion English tokens. The HPLT language resources are one of the largest open text corpora ever released, providing a great resource for language modeling and machine translation training. We publicly release the corpora, the software, and the tools used in this work.</li>
<li><strong>摘要：</strong>我们展示了 HPLT（高性能语言技术）语言资源，这是一个新的大规模多语言数据集，包括从 CommonCrawl 中提取的单语和双语语料库以及从互联网档案馆中提取的以前未使用的网络爬虫。我们描述了大型语料库的数据采集、管理和处理方法，这些方法依赖于开源软件工具和高性能计算。我们的单语集合侧重于中低资源语言，涵盖 75 种语言，并在文档级别删除了总计约 5.6 万亿个单词标记。我们以英语为中心的平行语料库源自其单语对应语料库，涵盖 18 个语言对和超过 9600 万个对齐句子对，以及大约 14 亿个英语标记。 HPLT 语言资源是迄今为止发布的最大的开放文本语料库之一，为语言建模和机器翻译培训提供了丰富的资源。我们公开发布本工作中使用的语料库、软件和工具。</li>
</ul>

<h3>Title: A Taxonomy of Ambiguity Types for NLP</h3>
<ul>
<li><strong>Authors: </strong>Margaret Y. Li, Alisa Liu, Zhaofeng Wu, Noah A. Smith</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14072">https://arxiv.org/abs/2403.14072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14072">https://arxiv.org/pdf/2403.14072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14072]] A Taxonomy of Ambiguity Types for NLP(https://arxiv.org/abs/2403.14072)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Ambiguity is an critical component of language that allows for more effective communication between speakers, but is often ignored in NLP. Recent work suggests that NLP systems may struggle to grasp certain elements of human language understanding because they may not handle ambiguities at the level that humans naturally do in communication. Additionally, different types of ambiguity may serve different purposes and require different approaches for resolution, and we aim to investigate how language models' abilities vary across types. We propose a taxonomy of ambiguity types as seen in English to facilitate NLP analysis. Our taxonomy can help make meaningful splits in language ambiguity data, allowing for more fine-grained assessments of both datasets and model performance.</li>
<li><strong>摘要：</strong>歧义是语言的一个重要组成部分，它可以让说话者之间进行更有效的沟通，但在 NLP 中经常被忽视。最近的研究表明，NLP 系统可能难以掌握人类语言理解的某些要素，因为它们可能无法像人类在交流中自然处理的那样处理歧义。此外，不同类型的歧义可能有不同的目的，需要不同的解决方法，我们的目标是研究语言模型的能力如何因类型而异。我们提出了一种英语歧义类型的分类法，以促进 NLP 分析。我们的分类法可以帮助对语言歧义数据进行有意义的分割，从而可以对数据集和模型性能进行更细粒度的评估。</li>
</ul>

<h3>Title: Benchmarking Chinese Commonsense Reasoning of LLMs: From  Chinese-Specifics to Reasoning-Memorization Correlations</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Sun, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, Conghui He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14112">https://arxiv.org/abs/2403.14112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14112">https://arxiv.org/pdf/2403.14112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14112]] Benchmarking Chinese Commonsense Reasoning of LLMs: From  Chinese-Specifics to Reasoning-Memorization Correlations(https://arxiv.org/abs/2403.14112)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM .</li>
<li><strong>摘要：</strong>我们推出了CHARM，这是第一个全面深入评估中文大语言模型（LLM）常识推理能力的基准，它涵盖了全球已知的常识和中国特有的常识。我们在CHARM上对7位英语和12位汉语方向的法学硕士进行了评估，采用了Chain-of-Thought等5种有代表性的提示策略来提高法学硕士的推理能力。我们的研究结果表明，法学硕士的语言取向和任务领域会影响提示策略的有效性，这丰富了之前的研究成果。我们建立了紧密相连的推理和记忆任务，发现一些法学硕士在记忆中文常识方面存在困难，影响了他们的推理能力，而另一些法学硕士尽管记忆表现相似，但在推理上却表现出差异。我们还评估了法学硕士的独立记忆推理能力并分析了典型错误。我们的研究准确地识别了法学硕士的优势和劣势，为优化提供了明确的方向。也可为其他领域的研究提供参考。我们将在 https://github.com/opendatalab/CHARM 发布 CHARM。</li>
</ul>

<h3>Title: From Handcrafted Features to LLMs: A Brief Survey for Machine  Translation Quality Estimation</h3>
<ul>
<li><strong>Authors: </strong>Haofei Zhao, Yilun Liu, Shimin Tao, Weibin Meng, Yimeng Chen, Xiang Geng, Chang Su, Min Zhang, Hao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14118">https://arxiv.org/abs/2403.14118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14118">https://arxiv.org/pdf/2403.14118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14118]] From Handcrafted Features to LLMs: A Brief Survey for Machine  Translation Quality Estimation(https://arxiv.org/abs/2403.14118)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Machine Translation Quality Estimation (MTQE) is the task of estimating the quality of machine-translated text in real time without the need for reference translations, which is of great importance for the development of MT. After two decades of evolution, QE has yielded a wealth of results. This article provides a comprehensive overview of QE datasets, annotation methods, shared tasks, methodologies, challenges, and future research directions. It begins with an introduction to the background and significance of QE, followed by an explanation of the concepts and evaluation metrics for word-level QE, sentence-level QE, document-level QE, and explainable QE. The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and Large Language Models (LLMs), with a further division of deep learning-based methods into classic deep learning and those incorporating pre-trained language models (LMs). Additionally, the article details the advantages and limitations of each method and offers a straightforward comparison of different approaches. Finally, the paper discusses the current challenges in QE research and provides an outlook on future research directions.</li>
<li><strong>摘要：</strong>机器翻译质量评估（MTQE）是在不需要参考译文的情况下实时评估机器翻译文本的质量的任务，这对于机器翻译的发展非常重要。经过二十年的演变，量化宽松已经取得了丰硕的成果。本文全面概述了 QE 数据集、注释方法、共享任务、方法、挑战和未来研究方向。首先介绍了QE的背景和意义，然后解释了词级QE、句子级QE、文档级QE和可解释QE的概念和评估指标。该论文将 QE 历史上开发的方法分为基于手工特征、深度学习和大型语言模型（LLM）的方法，并将基于深度学习的方法进一步分为经典深度学习和结合预训练语言的方法模型（LM）。此外，本文还详细介绍了每种方法的优点和局限性，并提供了不同方法的直接比较。最后，本文讨论了当前量化宽松研究面临的挑战，并对未来的研究方向进行了展望。</li>
</ul>

<h3>Title: MMIDR: Teaching Large Language Model to Interpret Multimodal  Misinformation via Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Longzheng Wang, Xiaohan Xu, Lei Zhang, Jiarui Lu, Yongxiu Xu, Hongbo Xu, Chuang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14171">https://arxiv.org/abs/2403.14171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14171">https://arxiv.org/pdf/2403.14171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14171]] MMIDR: Teaching Large Language Model to Interpret Multimodal  Misinformation via Knowledge Distillation(https://arxiv.org/abs/2403.14171)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Automatic detection of multimodal misinformation has gained a widespread attention recently. However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored. Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question. To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation. To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module. Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation. Furthermore, we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs. To explore several research questions regarding the performance of LLMs in multimodal misinformation detection tasks, we construct an instruction-following multimodal misinformation dataset and conduct comprehensive experiments. The experimental findings reveal that our MMIDR exhibits sufficient detection performance and possesses the capacity to provide compelling rationales to support its assessments.</li>
<li><strong>摘要：</strong>多模态错误信息的自动检测最近引起了广泛的关注。然而，强大的大型语言模型（LLM）用于多模式错误信息检测的潜力仍未得到充分开发。此外，如何教导法学硕士以经济有效且易于理解的方式解释多模式错误信息仍然是一个悬而未决的问题。为了解决这个问题，我们提出了 MMIDR，这是一个旨在教导法学硕士为多模式错误信息决策过程提供流畅和高质量文本解释的框架。为了将多模态错误信息转换为适当的指令跟踪格式，我们提出了数据增强视角和管道。该管道由视觉信息处理模块和证据检索模块组成。随后，我们通过处理内容提示专有法学硕士提取解释多模式错误信息真实性的理由。此外，我们设计了一种有效的知识蒸馏方法，将专有法学硕士在解释多模式错误信息方面的能力提炼为开源法学硕士。为了探索有关法学硕士在多模态错误信息检测任务中性能的几个研究问题，我们构建了一个遵循指令的多模态错误信息数据集并进行了全面的实验。实验结果表明，我们的 MMIDR 表现出足够的检测性能，并有能力提供令人信服的理由来支持其评估。</li>
</ul>

<h3>Title: Context Quality Matters in Training Fusion-in-Decoder for Extractive  Open-Domain Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Kosuke Akimoto, Kunihiro Takeoka, Masafumi Oyamada</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14197">https://arxiv.org/abs/2403.14197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14197">https://arxiv.org/pdf/2403.14197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14197]] Context Quality Matters in Training Fusion-in-Decoder for Extractive  Open-Domain Question Answering(https://arxiv.org/abs/2403.14197)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation models augment knowledge encoded in a language model by providing additional relevant external knowledge (context) during generation. Although it has been shown that the quantity and quality of context impact the performance of retrieval-augmented generation models during inference, limited research explores how these characteristics affect model training. This paper explores how context quantity and quality during model training affect the performance of Fusion-in-Decoder (FiD), the state-of-the-art retrieval-augmented generation model, in extractive open-domain question answering tasks. Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality. Through the experimental results, we also reveal FiD models trained with different context quality have different cross-attention distribution patterns. Specifically, as context quality during training increases, FiD models tend to attend more uniformly to each passage in context. Finally, based on these observations, we propose a method to mitigate overfitting to specific context quality by introducing bias to the cross-attention distribution, which we demonstrate to be effective in improving the performance of FiD models on different context quality.</li>
<li><strong>摘要：</strong>检索增强生成模型通过在生成过程中提供额外的相关外部知识（上下文）来增强语言模型中编码的知识。尽管已经证明上下文的数量和质量会影响检索增强生成模型在推理过程中的性能，但有限的研究探讨了这些特征如何影响模型训练。本文探讨了模型训练期间的上下文数量和质量如何影响 Fusion-in-Decoder (FiD)（最先进的检索增强生成模型）在提取式开放域问答任务中的性能。实验结果表明，FiD 模型在训练期间过度拟合上下文质量，并且在不同上下文质量上进行评估时表现出次优性能。通过实验结果，我们还揭示了用不同上下文质量训练的 FiD 模型具有不同的交叉注意力分布模式。具体来说，随着训练期间上下文质量的提高，FiD 模型往往会更一致地关注上下文中的每个段落。最后，基于这些观察，我们提出了一种通过向交叉注意力分布引入偏差来减轻对特定上下文质量的过度拟合的方法，我们证明该方法可以有效提高 FiD 模型在不同上下文质量上的性能。</li>
</ul>

<h3>Title: Improving the Robustness of Large Language Models via Consistency  Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zhao Yukun, Yan Lingyong, Sun Weiwei, Xing Guoliang, Wang Shuaiqiang, Meng Chong, Cheng Zhicong, Ren Zhaochun, Yin Dawei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14221">https://arxiv.org/abs/2403.14221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14221">https://arxiv.org/pdf/2403.14221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14221]] Improving the Robustness of Large Language Models via Consistency  Alignment(https://arxiv.org/abs/2403.14221)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses. The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources. We conduct extensive experiments on recent publicly available LLMs on instruction-following tasks and demonstrate the effectiveness of our training framework.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在遵循用户指令和生成有用响应方面取得了巨大成功。然而，它们的鲁棒性仍然远未达到最佳状态，因为由于口头指令的微小变化，它们可能会产生明显不一致的响应。最近的文献探讨了这种不一致问题，强调了持续改进响应生成稳健性的重要性。但目前仍缺乏系统的分析和解决方案。在本文中，我们定量定义了不一致问题，并提出了一个由指令增强监督微调和一致性对齐训练组成的两阶段训练框架。第一阶段通过类似的指令增强帮助模型概括以下指令。在第二阶段，我们提高多样性，并通过区分相似反应中的细微差异，帮助模型了解哪些反应更符合人类的期望。训练过程是通过第一阶段训练模型推断的自我奖励来完成的，而不参考外部人类偏好资源。我们对最近公开的关于遵循指令任务的法学硕士进行了广泛的实验，并证明了我们培训框架的有效性。</li>
</ul>

<h3>Title: Reinforcement Learning from Reflective Feedback (RLRF): Aligning and  Improving LLMs via Fine-Grained Self-Reflection</h3>
<ul>
<li><strong>Authors: </strong>Kyungjae Lee, Dasol Hwang, Sunghyun Park, Youngsoo Jang, Moontae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14238">https://arxiv.org/abs/2403.14238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14238">https://arxiv.org/pdf/2403.14238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14238]] Reinforcement Learning from Reflective Feedback (RLRF): Aligning and  Improving LLMs via Fine-Grained Self-Reflection(https://arxiv.org/abs/2403.14238)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.</li>
<li><strong>摘要：</strong>尽管 RLHF 承诺使法学硕士与人类偏好保持一致，但它常常导致肤浅的一致性，优先考虑风格的改变而不是提高法学硕士的下游表现。未指定的偏好可能会模糊调整模型的方向。缺乏探索限制了对所需输出的识别以改进模型。为了克服这些挑战，我们提出了一个新颖的框架：反射反馈强化学习（RLRF），它利用基于详细标准的细粒度反馈来提高法学硕士的核心能力。 RLRF 采用自我反思机制来系统地探索和完善 LLM 响应，然后通过 RL 算法以及有希望的响应来微调模型。我们在 Just-Eval、事实性和数学推理方面的实验证明了 RLRF 超越表面水平调整的功效和变革潜力。</li>
</ul>

<h3>Title: Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large  Language Models with Machine Learning in tele-dermatology</h3>
<ul>
<li><strong>Authors: </strong>Dimitrios P. Panagoulias, Evridiki Tsoureli-Nikita, Maria Virvou, George A. Tsihrintzis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14243">https://arxiv.org/abs/2403.14243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14243">https://arxiv.org/pdf/2403.14243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14243]] Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large  Language Models with Machine Learning in tele-dermatology(https://arxiv.org/abs/2403.14243)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The rise of Artificial Intelligence creates great promise in the field of medical discovery, diagnostics and patient management. However, the vast complexity of all medical domains require a more complex approach that combines machine learning algorithms, classifiers, segmentation algorithms and, lately, large language models. In this paper, we describe, implement and assess an Artificial Intelligence-empowered system and methodology aimed at assisting the diagnosis process of skin lesions and other skin conditions within the field of dermatology that aims to holistically address the diagnostic process in this domain. The workflow integrates large language, transformer-based vision models and sophisticated machine learning tools. This holistic approach achieves a nuanced interpretation of dermatological conditions that simulates and facilitates a dermatologist's workflow. We assess our proposed methodology through a thorough cross-model validation technique embedded in an evaluation pipeline that utilizes publicly available medical case studies of skin conditions and relevant images. To quantitatively score the system performance, advanced machine learning and natural language processing tools are employed which focus on similarity comparison and natural language inference. Additionally, we incorporate a human expert evaluation process based on a structured checklist to further validate our results. We implemented the proposed methodology in a system which achieved approximate (weighted) scores of 0.87 for both contextual understanding and diagnostic accuracy, demonstrating the efficacy of our approach in enhancing dermatological analysis. The proposed methodology is expected to prove useful in the development of next-generation tele-dermatology applications, enhancing remote consultation capabilities and access to care, especially in underserved areas.</li>
<li><strong>摘要：</strong>人工智能的兴起在医学发现、诊断和患者管理领域创造了巨大的前景。然而，所有医学领域的巨大复杂性需要更复杂的方法，结合机器学习算法、分类器、分割算法和最近的大型语言模型。在本文中，我们描述、实施和评估人工智能支持的系统和方法，旨在协助皮肤病学领域内皮肤病变和其他皮肤状况的诊断过程，旨在全面解决该领域的诊断过程。该工作流程集成了大型语言、基于变压器的视觉模型和复杂的机器学习工具。这种整体方法可以对皮肤病状况进行细致入微的解释，模拟并促进皮肤科医生的工作流程。我们通过嵌入评估管道中的彻底的跨模型验证技术来评估我们提出的方法，该评估管道利用公开的皮肤状况和相关图像的医学案例研究。为了对系统性能进行定量评分，采用了先进的机器学习和自然语言处理工具，重点关注相似性比较和自然语言推理。此外，我们还结合了基于结构化检查表的人类专家评估流程，以进一步验证我们的结果。我们在一个系统中实施了所提出的方法，该系统在上下文理解和诊断准确性方面取得了 0.87 的近似（加权）分数，证明了我们的方法在增强皮肤病学分析方面的功效。所提出的方法预计将在开发下一代远程皮肤病学应用程序、增强远程咨询能力和获得护理的机会方面发挥作用，特别是在服务不足的地区。</li>
</ul>

<h3>Title: LayoutLLM: Large Language Model Instruction Tuning for Visually Rich  Document Understanding</h3>
<ul>
<li><strong>Authors: </strong>Masato Fujitake</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14252">https://arxiv.org/abs/2403.14252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14252">https://arxiv.org/pdf/2403.14252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14252]] LayoutLLM: Large Language Model Instruction Tuning for Visually Rich  Document Understanding(https://arxiv.org/abs/2403.14252)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. Our experiments demonstrate improvement over the baseline model in various document analysis tasks.</li>
<li><strong>摘要：</strong>本文提出了 LayoutLLM，一种更灵活的文档分析方法，用于理解图像文档。视觉丰富的文档理解任务，例如文档图像分类和信息提取，由于其重要性而受到广泛关注。现有的方法已经被开发出来，通过结合图像、文本和布局结构的预训练意识来增强文档理解。然而，这些方法需要针对每个任务和数据集进行微调，并且模型的训练和操作成本很高。为了克服这一限制，我们提出了一种新的 LayoutLLM，将它们与大规模语言模型 (LLM) 集成。通过利用文档图像理解方面的现有研究优势和法学硕士卓越的语言理解能力，所提出的模型通过多模态指令数据集进行微调，在单个模型中执行对文档图像的理解。我们的实验证明了在各种文档分析任务中相对于基线模型的改进。</li>
</ul>

<h3>Title: K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional  Expression</h3>
<ul>
<li><strong>Authors: </strong>Kyuhee Kim, Surin Lee, Sangah Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14253">https://arxiv.org/abs/2403.14253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14253">https://arxiv.org/pdf/2403.14253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14253]] K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional  Expression(https://arxiv.org/abs/2403.14253)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>In many literary texts, emotions are indirectly conveyed through descriptions of actions, facial expressions, and appearances, necessitating emotion inference for narrative understanding. In this paper, we introduce K-Act2Emo, a Korean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional expressions and the emotions inferable from them. We categorize reasoning types into inferences in positive situations, inferences in negative situations, and inferences when expressions do not serve as emotional cues. Unlike existing CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results validate its effectiveness for training emotion inference models. Significantly, the BART-based knowledge model fine-tuned with K-Act2Emo outperforms various existing Korean large language models, achieving performance levels comparable to GPT-4 Turbo.</li>
<li><strong>摘要：</strong>在许多文学文本中，情感是通过对动作、面部表情和外表的描述来间接传达的，因此需要对叙事理解进行情感推断。在本文中，我们介绍了 K-Act2Emo，这是一种韩国常识知识图谱 (CSKG)，包含 1,900 种间接情感表达以及从中推断出的情感。我们将推理类型分为积极情况下的推理、消极情况下的推理以及表情不作为情感线索时的推理。与现有的 CSKG 不同，K-Act2Emo 专门研究情感背景，实验结果验证了其训练情感推理模型的有效性。值得注意的是，使用 K-Act2Emo 进行微调的基于 BART 的知识模型优于各种现有的韩语大语言模型，达到了与 GPT-4 Turbo 相当的性能水平。</li>
</ul>

<h3>Title: ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion  Classification</h3>
<ul>
<li><strong>Authors: </strong>Sehee Lim, Yejin Kim, Chi-Hyun Choi, Jy-yong Sohn, Byung-Hoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14255">https://arxiv.org/abs/2403.14255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14255">https://arxiv.org/pdf/2403.14255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14255]] ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion  Classification(https://arxiv.org/abs/2403.14255)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Improving the accessibility of psychotherapy with the aid of Large Language Models (LLMs) is garnering a significant attention in recent years. Recognizing cognitive distortions from the interviewee's utterances can be an essential part of psychotherapy, especially for cognitive behavioral therapy. In this paper, we propose ERD, which improves LLM-based cognitive distortion classification performance with the aid of additional modules of (1) extracting the parts related to cognitive distortion, and (2) debating the reasoning steps by multiple agents. Our experimental results on a public dataset show that ERD improves the multi-class F1 score as well as binary specificity score. Regarding the latter score, it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to LLMs.</li>
<li><strong>摘要：</strong>近年来，借助大语言模型（LLM）提高心理治疗的可及性引起了人们的广泛关注。从受访者的话语中识别认知扭曲可能是心理治疗的重要组成部分，特别是对于认知行为治疗而言。在本文中，我们提出了 ERD，它借助以下附加模块提高了基于 LLM 的认知扭曲分类性能：（1）提取与认知扭曲相关的部分，以及（2）通过多个代理讨论推理步骤。我们在公共数据集上的实验结果表明，ERD 提高了多类 F1 分数以及二元特异性分数。关于后一个分数，事实证明，我们的方法可以有效地消除假阳性率较高的基线方法的偏差，特别是当向法学硕士提供多智能体辩论的摘要时。</li>
</ul>

<h3>Title: LLM-based Extraction of Contradictions from Patents</h3>
<ul>
<li><strong>Authors: </strong>Stefan Trapp, Joachim Warschat</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14258">https://arxiv.org/abs/2403.14258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14258">https://arxiv.org/pdf/2403.14258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14258]] LLM-based Extraction of Contradictions from Patents(https://arxiv.org/abs/2403.14258)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Already since the 1950s TRIZ shows that patents and the technical contradictions they solve are an important source of inspiration for the development of innovative products. However, TRIZ is a heuristic based on a historic patent analysis and does not make use of the ever-increasing number of latest technological solutions in current patents. Because of the huge number of patents, their length, and, last but not least, their complexity there is a need for modern patent retrieval and patent analysis to go beyond keyword-oriented methods. Recent advances in patent retrieval and analysis mainly focus on dense vectors based on neural AI Transformer language models like Google BERT. They are, for example, used for dense retrieval, question answering or summarization and key concept extraction. A research focus within the methods for patent summarization and key concept extraction are generic inventive concepts respectively TRIZ concepts like problems, solutions, advantage of invention, parameters, and contradictions. Succeeding rule-based approaches, finetuned BERT-like language models for sentence-wise classification represent the state-of-the-art of inventive concept extraction. While they work comparatively well for basic concepts like problems or solutions, contradictions - as a more complex abstraction - remain a challenge for these models. This paper goes one step further, as it presents a method to extract TRIZ contradictions from patent texts based on Prompt Engineering using a generative Large Language Model (LLM), namely OpenAI's GPT-4. Contradiction detection, sentence extraction, contradiction summarization, parameter extraction and assignment to the 39 abstract TRIZ engineering parameters are all performed in a single prompt using the LangChain framework. Our results show that "off-the-shelf" GPT-4 is a serious alternative to existing approaches.</li>
<li><strong>摘要：</strong>自 20 世纪 50 年代以来，TRIZ 就表明专利及其解决的技术矛盾是创新产品开发的重要灵感来源。然而，TRIZ 是一种基于历史专利分析的启发式方法，并没有利用当前专利中不断增加的最新技术解决方案。由于专利数量庞大、长度长，以及最后但并非最不重要的一点是其复杂性，现代专利检索和专利分析需要超越以关键词为导向的方法。专利检索和分析的最新进展主要集中在基于神经 AI Transformer 语言模型（如 Google BERT）的密集向量上。例如，它们用于密集检索、问题回答或总结以及关键概念提取。专利摘要和关键概念提取方法的研究重点分别是通用发明概念和 TRIZ 概念，例如问题、解决方案、发明优点、参数和矛盾。后续的基于规则的方法、用于句子分类的微调的类似 BERT 的语言模型代表了发明概念提取的最先进技术。虽然它们对于问题或解决方案等基本概念相对有效，但矛盾（作为更复杂的抽象）仍然是这些模型的挑战。本文更进一步，提出了一种基于 Prompt Engineering 使用生成式大语言模型（LLM）从专利文本中提取 TRIZ 矛盾的方法，即 OpenAI 的 GPT-4。矛盾检测、句子提取、矛盾总结、参数提取以及对 39 个抽象 TRIZ 工程参数的分配都使用 LangChain 框架在单个提示中完成。我们的结果表明，“现成的”GPT-4 是现有方法的重要替代方案。</li>
</ul>

<h3>Title: ChainLM: Empowering Large Language Models with Improved Chain-of-Thought  Prompting</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14312">https://arxiv.org/abs/2403.14312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14312">https://arxiv.org/pdf/2403.14312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14312]] ChainLM: Empowering Large Language Models with Improved Chain-of-Thought  Prompting(https://arxiv.org/abs/2403.14312)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method, wherein multiple debaters discuss each reasoning step to arrive at the correct answer. Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex reasoning problems compared to existing models. In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance. We release our dataset and code at https://github.com/RUCAIBox/ChainLM.</li>
<li><strong>摘要：</strong>思想链（CoT）提示可以增强大型语言模型（LLM）的推理能力，使其成为解决复杂推理任务的主要方法。现有的 CoT 综合方法通常专注于更简单的推理任务，从而导致低质量和不一致的 CoT 提示。为了应对这一挑战，我们对 CoT 提示进行了实证研究，并引入了 CoTGenius，这是一种专为自动生成高级 CoT 提示而设计的新颖框架。 CoTGenius基于复杂化、多样化、指定化三大进化策略，以及进化成功判断和正确性验证两种过滤机制。我们进一步使用 CoTGenius 创建一个广泛的 CoT 数据集，并随后在此数据集上微调 Llama 2-Chat 7B 和 13B 模型。我们将生成的模型称为 ChainLM。为了解决推理步骤中的累积错误问题，我们提出了一种步骤级辩论方法，其中多个辩手讨论每个推理步骤以得出正确答案。大量实验表明，与现有模型相比，我们的 ChainLM 模型在解决一系列复杂推理问题方面表现出更高的能力。此外，我们还深入分析了 CoTGenius 中数据类别对模型性能的影响。我们在 https://github.com/RUCAIBox/ChainLM 发布了我们的数据集和代码。</li>
</ul>

<h3>Title: Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial  Narratives</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Liu, Yi Yang, Kar Yan Tam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14341">https://arxiv.org/abs/2403.14341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14341">https://arxiv.org/pdf/2403.14341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14341]] Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial  Narratives(https://arxiv.org/abs/2403.14341)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce the Financial-STS task, a financial domain-specific NLP task designed to measure the nuanced semantic similarity between pairs of financial narratives. These narratives originate from the financial statements of the same company but correspond to different periods, such as year-over-year comparisons. Measuring the subtle semantic differences between these paired narratives enables market stakeholders to gauge changes over time in the company's financial and operational situations, which is critical for financial decision-making. We find that existing pretrained embedding models and LLM embeddings fall short in discerning these subtle financial narrative shifts. To address this gap, we propose an LLM-augmented pipeline specifically designed for the Financial-STS task. Evaluation on a human-annotated dataset demonstrates that our proposed method outperforms existing methods trained on classic STS tasks and generic LLM embeddings.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 Financial-STS 任务，这是一项特定于金融领域的 NLP 任务，旨在衡量金融叙述对之间细微的语义相似性。这些叙述源自同一家公司的财务报表，但对应于不同时期，例如同比比较。衡量这些配对叙述之间微妙的语义差异使市场利益相关者能够衡量公司财务和运营状况随时间的变化，这对于财务决策至关重要。我们发现现有的预训练嵌入模型和 LLM 嵌入在识别这些微妙的财务叙述变化方面存在不足。为了解决这一差距，我们提出了专门为 Financial-STS 任务设计的 LLM 增强管道。对人工注释数据集的评估表明，我们提出的方法优于在经典 STS 任务和通用 LLM 嵌入上训练的现有方法。</li>
</ul>

<h3>Title: WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for  Atomic Factual Knowledge Update in Causal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14364">https://arxiv.org/abs/2403.14364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14364">https://arxiv.org/pdf/2403.14364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14364]] WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for  Atomic Factual Knowledge Update in Causal Language Models(https://arxiv.org/abs/2403.14364)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are "unknown" to them. One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model. To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static. We describe several update scenarios arising from various combinations of these three types of basic update. The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023. Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics. Contrary to other datasets, such as zsRE and CounterFact, WikiFactDiff constitutes a realistic update setting that involves various update scenarios, including replacements, archival, and new entity insertions. We also present an evaluation of existing update algorithms on WikiFactDiff.</li>
<li><strong>摘要：</strong>大语言模型（LLM）的真实性往往会随着时间的推移而衰减，因为训练后的事件对他们来说是“未知的”。保持模型最新的一种方法可能是事实更新：在模型中插入、替换或删除某些简单（原子）事实的任务。为了研究这项任务，我们提出了 WikiFactDiff，这是一个数据集，它描述了两个日期之间事实知识的演变，作为简单事实的集合，分为三类：新的、过时的和静态的。我们描述了由这三种类型的基本更新的各种组合产生的几种更新场景。事实由主语-关系-客体三元组表示；事实上，WikiFactDiff 是通过比较 2021 年 1 月 4 日和 2023 年 2 月 27 日维基数据知识库的状态构建的。这些事实伴随着语言模板和完形填空测试，可以运行更新算法及其评估指标。与 zsRE 和 CounterFact 等其他数据集相反，WikiFactDiff 构成了一个现实的更新设置，涉及各种更新场景，包括替换、归档和新实体插入。我们还在 WikiFactDiff 上对现有更新算法进行了评估。</li>
</ul>

<h3>Title: FIT-RAG: Black-Box RAG with Factual Information and Token Reduction</h3>
<ul>
<li><strong>Authors: </strong>Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14374">https://arxiv.org/abs/2403.14374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14374">https://arxiv.org/pdf/2403.14374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14374]] FIT-RAG: Black-Box RAG with Factual Information and Token Reduction(https://arxiv.org/abs/2403.14374)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG. To address these issues, this paper proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.</li>
<li><strong>摘要：</strong>由于参数数量异常庞大，微调大型语言模型 (LLM) 来更新长尾或过时的知识在许多应用中是不切实际的。为了避免微调，我们也可以将 LLM 视为黑盒（即冻结 LLM 的参数），并使用检索增强生成（RAG）系统（即黑盒 RAG）对其进行增强。最近，黑盒RAG在知识密集型任务中取得了成功，受到了广泛关注。现有的黑盒 RAG 方法通常会对检索器进行微调，以满足法学硕士的偏好，并将所有检索到的文档连接起来作为输入，这存在两个问题：（1）忽略事实信息。 LLM首选文件可能不包含给定问题的事实信息，这可能会误导检索者并损害黑盒RAG的有效性； (2) 代币浪费。简单地连接所有检索到的文档会给 LLM 带来大量不必要的标记，从而降低了黑盒 RAG 的效率。为了解决这些问题，本文提出了一种新颖的黑盒 RAG 框架，该框架利用检索中的事实信息并减少用于增强的标记数量，称为 FIT-RAG。 FIT-RAG 通过构建双标签文档评分器来利用事实信息。此外，它通过引入自知识识别器和子文档级令牌缩减器来减少令牌。 FIT-RAG 实现了卓越的有效性和效率，这通过三个开放域问答数据集（TriviaQA、NQ 和 PopQA）的大量实验得到了验证。 FIT-RAG 可以将 Llama2-13B-Chat 在 TriviaQA 上的回答准确率提高 14.3%，在 NQ 上提高 19.9%，在 PopQA 上提高 27.5%。此外，它在三个数据集中平均可以节省大约一半的令牌。</li>
</ul>

<h3>Title: Editing Knowledge Representation of Language Lodel via Rephrased Prefix  Prompts</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14381">https://arxiv.org/abs/2403.14381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14381">https://arxiv.org/pdf/2403.14381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14381]] Editing Knowledge Representation of Language Lodel via Rephrased Prefix  Prompts(https://arxiv.org/abs/2403.14381)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific prompts to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, prompt engineering is opaque and requires significant effort to find suitable prompts. To address these issues, we introduce a new method called PSPEM (Prefix Soft Prompt Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of prompt engineering by automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a prompt encoder and an encoding converter to refine key information in prompts and uses prompt alignment techniques to guide model generation, ensuring text consistency and adherence to the intended structure and content, thereby maintaining an optimal balance between efficiency and accuracy. We have validated the effectiveness of PSPEM through knowledge editing and attribute inserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\% editing accuracy and demonstrated the highest level of fluency. We further analyzed the similarities between PSPEM and original prompts and their impact on the model's internals. The results indicate that PSPEM can serve as an alternative to original prompts, supporting the model in effective editing.</li>
<li><strong>摘要：</strong>神经语言模型（LM）已经在庞大的语料库上进行了广泛的训练，以存储文本中描述的世界各个方面的事实知识。当前的技术通常采用知识编辑方法或特定提示来修改 LM 输出。然而，现有的知识编辑方法成本高昂且效率低下，难以生成适当的文本。此外，提示工程是不透明的，需要付出巨大的努力才能找到合适的提示。为了解决这些问题，我们引入了一种称为 PSPEM（前缀软提示编辑方法）的新方法，只需一次训练即可终身使用。它解决了知识编辑方法中的低效率和泛化性问题，并通过自动寻找最佳软提示来克服提示工程的不透明性。具体来说，PSPEM利用提示编码器和编码转换器来提炼提示中的关键信息，并使用提示对齐技术来指导模型生成，确保文本的一致性以及对预期结构和内容的遵守，从而保持效率和准确性之间的最佳平衡。我们通过知识编辑和属性插入验证了PSPEM的有效性。在 COUNTERFACT 数据集上，PSPEM 实现了近 100% 的编辑准确率，并展现了最高水平的流畅度。我们进一步分析了 PSPEM 和原始提示之间的相似之处及其对模型内部的影响。结果表明，PSPEM可以作为原始提示的替代方案，支持模型的有效编辑。</li>
</ul>

<h3>Title: From Large to Tiny: Distilling and Refining Mathematical Expertise for  Math Word Problems with Weakly Supervision</h3>
<ul>
<li><strong>Authors: </strong>Qingwen Lin, Boyan Xu, Zhengting Huang, Ruichu Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14390">https://arxiv.org/abs/2403.14390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14390">https://arxiv.org/pdf/2403.14390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14390]] From Large to Tiny: Distilling and Refining Mathematical Expertise for  Math Word Problems with Weakly Supervision(https://arxiv.org/abs/2403.14390)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly supervised task settings that rely solely on the final answer as a supervised signal. Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions. The rise of Large Language Models (LLMs) like ChatGPT has opened up new possibilities for addressing MWPs directly. However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight. In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from large to tiny language models. In \emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to distill mathematical knowledge from LLMs to construct problem-equation pairs required for supervised training. In \emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee the full utilization of all data, we further utilize the unsuccessfully searched data effectively by Knowledge Refine method. Finally, We train a small model using distilled data generated through two-stage methods. As our method fully leverages the semantic understanding capabilities during the searching 'problem-equation' pair, it demonstrates significantly improved performance on the Math23K and Weak12K datasets compared to existing small model methods, while maintaining a much lower computational cost than ChatGPT.</li>
<li><strong>摘要：</strong>为了解决通过中间方程的全面监督解决数学应用题（MWP）时高注释成本的挑战，最近的工作提出了弱监督任务设置，仅依赖于最终答案作为监督信号。现有的领先方法通常采用各种搜索技术来推断中间方程，但无法确保其与自然语言描述的语义一致性。 ChatGPT 等大型语言模型 (LLM) 的兴起为直接解决 MWP 开辟了新的可能性。然而，法学硕士的计算要求使其不太适合在资源紧张的环境中使用。鉴于这些挑战，我们引入了一种创新的两阶段框架，可以熟练地将数学专业知识从大型语言模型转移到小型语言模型。在 \emph{Distillation Stage} 中，我们提出了一系列满足 MWP 属性的提取过程，从 LLM 中提取数学知识，构建监督训练所需的问题方程对。在\emph{Refinement阶段}，由于知识蒸馏方法不能保证所有数据的充分利用，我们通过知识精炼方法进一步有效地利用未成功搜索的数据。最后，我们使用通过两阶段方法生成的蒸馏数据训练一个小模型。由于我们的方法在搜索“问题方程”对期间充分利用了语义理解功能，因此与现有的小模型方法相比，它在 Math23K 和 Weak12K 数据集上的性能显着提高，同时保持比 ChatGPT 低得多的计算成本。</li>
</ul>

<h3>Title: Building Accurate Translation-Tailored LLMs with Language Aware  Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Changtong Zan, Liang Ding, Li Shen, Yibing Zhen, Weifeng Liu, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14399">https://arxiv.org/abs/2403.14399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14399">https://arxiv.org/pdf/2403.14399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14399]] Building Accurate Translation-Tailored LLMs with Language Aware  Instruction Tuning(https://arxiv.org/abs/2403.14399)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems. However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models. To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations. However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information. In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs. Specifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities. In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples. Experiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline -- translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3\%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. Analysis shows that our method could preserve the model's general task performance on AlpacaEval. Code and models will be released at \url{https://github.com/alphadl/LanguageAware_Tuning}.</li>
<li><strong>摘要：</strong>翻译定制的大型语言模型（LLM）表现出卓越的翻译能力，甚至可以与受监督训练的商业翻译系统竞争。然而，脱靶翻译仍然是一个尚未解决的问题，特别是对于资源匮乏的语言来说，这阻碍了我们开发准确的基于法学硕士的翻译模型。为了缓解脱靶翻译问题并提高法学硕士的翻译表现，最近的工作要么设计了先进的提示策略来突出翻译指令的功能，要么通过提供少量演示来利用法学硕士的上下文学习能力。然而，这些方法本质上并没有提高LLM遵循翻译指令的能力，尤其是语言方向信息。在这项工作中，我们设计了一种两阶段微调算法来提高LLM的指令跟踪能力（特别是翻译方向）。具体来说，我们首先在翻译数据集上使用最大似然估计损失来调整 LLM，以得出基本的翻译功能。在第二阶段，我们通过在指令中随机替换翻译方向来构建指令冲突样本，然后引入额外的似然性损失来学习这些样本。在跨 16 个零样本方向的 LLaMA 模型上进行的 IWSLT 和 WMT 基准实验表明，与竞争基线——翻译微调 LLama 相比，我们的方法可以有效降低脱靶翻译率（平均 -53.3%），从而提高翻译质量，平均为 +5.7 SacreBLEU 和 +16.4 BLEURT。分析表明，我们的方法可以在 AlpacaEval 上保留模型的一般任务性能。代码和模型将在 \url{https://github.com/alphadl/LanguageAware_Tuning} 发布。</li>
</ul>

<h3>Title: Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language  Models through Question Complexity</h3>
<ul>
<li><strong>Authors: </strong>Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14403">https://arxiv.org/abs/2403.14403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14403">https://arxiv.org/pdf/2403.14403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14403]] Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language  Models through Question Complexity(https://arxiv.org/abs/2403.14403)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG.</li>
<li><strong>摘要：</strong>检索增强大型语言模型 (LLM) 将外部知识库中的非参数知识纳入 LLM，已成为提高问答 (QA) 等多项任务响应准确性的有前途的方法。然而，尽管有多种方法可以处理不同复杂度的查询，但它们要么处理具有不必要计算开销的简单查询，要么无法充分解决复杂的多步骤查询；然而，并非所有用户请求都只属于简单或复杂类别之一。在这项工作中，我们提出了一种新颖的自适应 QA 框架，它可以根据查询复杂性从最简单到最复杂的策略动态选择最适合（检索增强）LLM 的策略。此外，这个选择过程是通过分类器来操作的，分类器是一个较小的 LM，经过训练，可以使用自动收集的标签来预测传入查询的复杂性级别，这些标签是从模型的实际预测结果和数据集中固有的归纳偏差获得的。这种方法提供了一种平衡的策略，在迭代和单步检索增强的 LLM 以及无检索方法之间无缝适应，以响应一系列查询复杂性。我们在一组开放域 QA 数据集上验证了我们的模型，涵盖多种查询复杂性，并表明与包括自适应检索方法在内的相关基线相比，我们的模型提高了 QA 系统的整体效率和准确性。代码位于：https://github.com/starsuzi/Adaptive-RAG。</li>
</ul>

<h3>Title: Locating and Mitigating Gender Bias in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14409">https://arxiv.org/abs/2403.14409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14409">https://arxiv.org/pdf/2403.14409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14409]] Locating and Mitigating Gender Bias in Large Language Models(https://arxiv.org/abs/2403.14409)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在广泛的语料库上进行预训练，以学习包含人类偏好的事实和人类认知。然而，这个过程可能会无意中导致这些模型获得社会上普遍存在的偏见和刻板印象。先前的研究通常通过一维视角来解决偏见问题，集中于定位或减轻偏见。这种有限的视角为促进偏见研究协同补充和逐步建立障碍造成了障碍。在这项研究中，我们将定位和减轻偏见的过程整合到一个统一的框架内。最初，我们使用因果中介分析来追踪大型语言模型中不同组件激活的因果效应。在此基础上，我们提出了 LSDM（最小二乘德偏差法），这是一种基于知识编辑的方法，用于减轻职业代词中的性别偏见，并将其与三个性别偏见数据集和七个知识能力测试数据集的两个基线进行比较。实验结果表明，性别偏见的主要贡献者是作用于职业代词最后一个标记的底部 MLP 模块和作用于句子中最后一个单词的顶部注意力模块。此外，LSDM 比其他基线更有效地减轻模型中的性别偏见，同时充分保留模型在所有其他方面的能力。</li>
</ul>

<h3>Title: Emergent communication and learning pressures in language models: a  language evolution perspective</h3>
<ul>
<li><strong>Authors: </strong>Lukas Galke, Limor Raviv</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14427">https://arxiv.org/abs/2403.14427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14427">https://arxiv.org/pdf/2403.14427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14427]] Emergent communication and learning pressures in language models: a  language evolution perspective(https://arxiv.org/abs/2403.14427)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Language models and humans are two types of learning systems. Finding or facilitating commonalities could enable major breakthroughs in our understanding of the acquisition and evolution of language. Many theories of language evolution rely heavily on learning biases and learning pressures. Yet due to substantial differences in learning pressures, it is questionable whether the similarity between humans and machines is sufficient for insights to carry over and to be worth testing with human participants. Here, we review the emergent communication literature, a subfield of multi-agent reinforcement learning, from a language evolution perspective. We find that the emergent communication literature excels at designing and adapting models to recover initially absent linguistic phenomena of natural languages. Based on a short literature review, we identify key pressures that have recovered initially absent human patterns in emergent communication models: communicative success, efficiency, learnability, and other psycho-/sociolinguistic factors. We argue that this may serve as inspiration for how to design language models for language acquisition and language evolution research.</li>
<li><strong>摘要：</strong>语言模型和人类是两种类型的学习系统。发现或促进共性可以使我们对语言习得和进化的理解取得重大突破。许多语言进化理论在很大程度上依赖于学习偏见和学习压力。然而，由于学习压力存在巨大差异，人类和机器之间的相似性是否足以让见解得以延续并值得人类参与者进行测试，这是值得怀疑的。在这里，我们从语言进化的角度回顾了新兴的通信文献，这是多智能体强化学习的一个子领域。我们发现新兴的传播文献擅长设计和调整模型来恢复自然语言中最初缺失的语言现象。基于简短的文献综述，我们确定了在新兴沟通模式中恢复了最初缺失的人类模式的关键压力：沟通成功、效率、可学习性和其他心理/社会语言因素。我们认为这可能为如何设计用于语言习得和语言进化研究的语言模型提供灵感。</li>
</ul>

<h3>Title: A Multimodal Approach to Device-Directed Speech Detection with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dominik Wager, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14438">https://arxiv.org/abs/2403.14438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14438">https://arxiv.org/pdf/2403.14438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14438]] A Multimodal Approach to Device-Directed Speech Detection with Large  Language Models(https://arxiv.org/abs/2403.14438)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset.</li>
<li><strong>摘要：</strong>与虚拟助手的交互通常以预定义的触发短语开始，然后是用户命令。为了使与助手的交互更加直观，我们探索是否可以放弃用户必须以触发短语开始每个命令的要求。我们通过三种方式探索这个任务：首先，我们仅使用从音频波形获得的声学信息来训练分类器。其次，我们将自动语音识别 (ASR) 系统的解码器输出（例如 1-最佳假设）作为大型语言模型 (LLM) 的输入特征。最后，我们探索了一个结合了声学和词汇特征以及法学硕士中的 ASR 解码器信号的多模态系统。与纯文本和纯音频模型相比，使用多模态信息可将相对等错误率提高高达 39% 和 61%。增加 LLM 的规模并使用低秩自适应进行训练，可以使我们的数据集上的相对 EER 进一步降低高达 18%。</li>
</ul>

<h3>Title: gTBLS: Generating Tables from Text by Conditional Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Anirudh Sundar, Christopher Richardson, Larry Heck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14457">https://arxiv.org/abs/2403.14457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14457">https://arxiv.org/pdf/2403.14457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14457]] gTBLS: Generating Tables from Text by Conditional Question Answering(https://arxiv.org/abs/2403.14457)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Distilling large, unstructured text into a structured, condensed form such as tables is an open research problem. One of the primary challenges in automatically generating tables is ensuring their syntactic validity. Prior approaches address this challenge by including additional parameters in the Transformer's attention mechanism to attend to specific rows and column headers. In contrast to this single-stage method, this paper presents a two-stage approach called Generative Tables (gTBLS). The first stage infers table structure (row and column headers) from the text. The second stage formulates questions using these headers and fine-tunes a causal language model to answer them. Furthermore, the gTBLS approach is amenable to the utilization of pre-trained Large Language Models in a zero-shot configuration, presenting a solution for table generation in situations where fine-tuning is not feasible. gTBLS improves prior approaches by up to 10% in BERTScore on the table construction task and up to 20% on the table content generation task of the E2E, WikiTableText, WikiBio, and RotoWire datasets.</li>
<li><strong>摘要：</strong>将大型非结构化文本提炼为结构化、简洁的形式（例如表格）是一个开放的研究问题。自动生成表的主要挑战之一是确保其语法有效性。先前的方法通过在 Transformer 的注意力机制中包含附加参数来关注特定的行和列标题来解决这一挑战。与这种单阶段方法相反，本文提出了一种称为生成表（gTBLS）的两阶段方法。第一阶段从文本推断表结构（行标题和列标题）。第二阶段使用这些标题制定问题，并微调因果语言模型来回答这些问题。此外，gTBLS 方法适合在零样本配置中使用预先训练的大型语言模型，为在微调不可行的情况下生成表提供了解决方案。 gTBLS 在 BERTScore 的表构建任务上将现有方法改进了高达 10%，在 E2E、WikiTableText、WikiBio 和 RotoWire 数据集的表内容生成任务上将现有方法改进了高达 20%。</li>
</ul>

<h3>Title: Multi-Level Explanations for Generative Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer, Soumya Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14459">https://arxiv.org/abs/2403.14459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14459">https://arxiv.org/pdf/2403.14459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14459]] Multi-Level Explanations for Generative Language Models(https://arxiv.org/abs/2403.14459)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Perturbation-based explanation methods such as LIME and SHAP are commonly applied to text classification. This work focuses on their extension to generative language models. To address the challenges of text as output and long text inputs, we propose a general framework called MExGen that can be instantiated with different attribution algorithms. To handle text output, we introduce the notion of scalarizers for mapping text to real numbers and investigate multiple possibilities. To handle long inputs, we take a multi-level approach, proceeding from coarser levels of granularity to finer ones, and focus on algorithms with linear scaling in model queries. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for summarization and context-grounded question answering. The results show that our framework can provide more locally faithful explanations of generated outputs.</li>
<li><strong>摘要：</strong>基于扰动的解释方法（例如 LIME 和 SHAP）通常应用于文本分类。这项工作的重点是它们对生成语言模型的扩展。为了解决文本作为输出和长文本输入的挑战，我们提出了一个名为 MExGen 的通用框架，可以使用不同的归因算法进行实例化。为了处理文本输出，我们引入了标量器的概念，用于将文本映射到实数并研究多种可能性。为了处理长输入，我们采用多级方法，从较粗的粒度到更细的粒度，并重点关注模型查询中具有线性缩放的算法。我们对基于扰动的归因方法进行了自动和人工的系统评估，以进行摘要和基于上下文的问答。结果表明，我们的框架可以对生成的输出提供更本地化的忠实解释。</li>
</ul>

<h3>Title: ChatGPT Alternative Solutions: Large Language Models Survey</h3>
<ul>
<li><strong>Authors: </strong>Hanieh Alipour, Nick Pendar, Kohinoor Roy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14469">https://arxiv.org/abs/2403.14469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14469">https://arxiv.org/pdf/2403.14469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14469]] ChatGPT Alternative Solutions: Large Language Models Survey(https://arxiv.org/abs/2403.14469)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications. This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics. These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more. Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights. A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention. The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms. Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of LLMs. Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature. By examining multiple LLM models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories. This survey furnishes a well-rounded perspective on the current state of generative AI, shedding light on opportunities for further exploration, enhancement, and innovation.</li>
<li><strong>摘要：</strong>近年来，大型语言模型（LLM）的辉煌不仅在自然语言处理领域大放异彩，还在众多应用领域大放异彩。法学硕士能力的卓越展示引发了该领域研究贡献的激增，涵盖了不同的主题。这些贡献包括神经网络架构、上下文长度增强、模型对齐、训练数据集、基准测试、效率改进等方面的进步。近年来，学术界和工业界之间的动态协同，将法学硕士研究领域推向了新的高度。这一历程中一个值得注意的里程碑是 ChatGPT 的推出，这是一个基于法学硕士的强大人工智能聊天机器人，引起了社会的广泛关注。法学硕士不断发展的技术已经开始重塑整个人工智能社区的格局，有望为我们创建和使用人工智能算法的方式带来革命性的转变。鉴于这种快速的技术发展，我们的调查踏上了总结法学硕士领域最近取得的进步的旅程。通过对背景、关键发现和流行方法的探索，我们对最新的文献进行了回顾。通过研究多个法学硕士模型，我们的论文不仅提供了全面的概述，而且还绘制了一个课程，确定了现有的挑战并指出了未来潜在的研究轨迹。这项调查为生成式人工智能的现状提供了全面的视角，揭示了进一步探索、增强和创新的机会。</li>
</ul>

<h3>Title: Detoxifying Large Language Models via Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14472">https://arxiv.org/abs/2403.14472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14472">https://arxiv.org/pdf/2403.14472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14472]] Detoxifying Large Language Models via Knowledge Editing(https://arxiv.org/abs/2403.14472)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at https://github.com/zjunlp/EasyEdit.</li>
<li><strong>摘要：</strong>本文研究了使用知识编辑技术来消除大型语言模型 (LLM) 的毒害。我们构建了一个基准测试SafeEdit，它涵盖了九个不安全类别，具有各种强大的攻击提示，并配备了系统评估的全面指标。我们进行了实验，将知识编辑方法与之前的基线进行比较，表明知识编辑有潜力有效地消除法学硕士的毒害，同时对总体表现的影响有限。然后，我们提出了一种简单而有效的基线，称为术中神经监测解毒（DINM），仅通过一个实例即可在几个调整步骤内减少 LLM 的毒性。我们进一步对各种解毒方法的内部机制进行了深入分析，证明以前的方法如SFT和DPO可能只是抑制毒性参数的激活，而DINM在一定程度上减轻了毒性参数的毒性，使毒性参数永久化。调整。我们希望这些见解能够为未来开发解毒方法和法学硕士的基础知识机制的工作带来启发。代码和基准可在 https://github.com/zjunlp/EasyEdit 获取。</li>
</ul>

<h3>Title: EDT: Improving Large Language Models' Generation by Entropy-based  Dynamic Temperature Sampling</h3>
<ul>
<li><strong>Authors: </strong>Shimao Zhang, Yu Bao, Shujian Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14541">https://arxiv.org/abs/2403.14541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14541">https://arxiv.org/pdf/2403.14541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14541]] EDT: Improving Large Language Models' Generation by Entropy-based  Dynamic Temperature Sampling(https://arxiv.org/abs/2403.14541)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks. Temperature sampling is a commonly used decoding strategy for LLMs' generation process. However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks. Our experiments show that EDT significantly outperforms the existing strategies across different tasks.</li>
<li><strong>摘要：</strong>最近，大型语言模型（LLM）在广泛的下游语言任务中表现出了出色的性能。温度采样是 LLM 生成过程中常用的解码策略。然而，在大多数情况下使用固定的温度参数，这可能并不总是平衡发电质量和多样性的最佳选择。在本文中，我们提出了一种有效的基于熵的动态温度（EDT）采样方法，通过动态选择温度参数来在生成质量和多样性方面实现更平衡的性能。此外，我们还展示了 4 个不同代基准的模型性能和综合分析。我们的实验表明，EDT 在不同任务中显着优于现有策略。</li>
</ul>

<h3>Title: Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14551">https://arxiv.org/abs/2403.14551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14551">https://arxiv.org/pdf/2403.14551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14551]] Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling(https://arxiv.org/abs/2403.14551)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Today's most accurate language models are trained on orders of magnitude more language data than human language learners receive - but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs' representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive Grounding (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive Grounding combines a next token prediction strategy with a contrastive visual grounding objective, focusing on early-layer representations that encode lexical information. Across multiple word-learning and sentence-understanding benchmarks, LexiContrastive Grounding not only outperforms standard language-only models in learning efficiency, but also improves upon vision-and-language learning procedures including CLIP, GIT, Flamingo, and Vokenization. Moreover, LexiContrastive Grounding improves perplexity by around 5% on multiple language modeling tasks. This work underscores the potential of incorporating visual grounding into language models, aligning more closely with the multimodal nature of human language acquisition.</li>
<li><strong>摘要：</strong>当今最准确的语言模型所训练的语言数据比人类语言学习者接收到的语言数据多几个数量级，但没有来自在人类学习中发挥关键作用的其他感官方式的监督。我们能否通过在生态上更合理的监督来使 LM 的表示和预测更加准确（并且更加人性化）？本文描述了 LexiContrastive Grounding (LCG)，这是一种利用视觉监督来改进文本表示的扎根语言学习程序。 LexiContrastive Grounding 将下一个标记预测策略与对比视觉基础目标相结合，重点关注编码词汇信息的早期层表示。在多个单词学习和句子理解基准测试中，LexiContrastive Grounding 不仅在学习效率方面优于标准的纯语言模型，而且还改进了包括 CLIP、GIT、Flamingo 和 Vokenization 在内的视觉和语言学习程序。此外，LexiContrastive Grounding 将多语言建模任务的困惑度提高了约 5%。这项工作强调了将视觉基础纳入语言模型的潜力，更符合人类语言习得的多模态性质。</li>
</ul>

<h3>Title: The Era of Semantic Decoding</h3>
<ul>
<li><strong>Authors: </strong>Maxime Peyrard, Martin Josifoski, Robert West</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14562">https://arxiv.org/abs/2403.14562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14562">https://arxiv.org/pdf/2403.14562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14562]] The Era of Semantic Decoding(https://arxiv.org/abs/2403.14562)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.</li>
<li><strong>摘要：</strong>最近的工作证明了在法学硕士、人力投入和各种工具之间协调合作以解决法学硕士固有局限性的想法的巨大前景。我们提出了一种称为语义解码的新颖视角，它将这些协作过程构建为语义空间中的优化程序。具体来说，我们将法学硕士概念化为语义处理器，可以操纵我们称为语义标记（已知思想）的有意义的信息。法学硕士是众多其他语义处理器之一，包括人类和工具，例如搜索引擎或代码执行器。总的来说，语义处理器参与语义标记的动态交换，以逐步构建高实用性输出。我们将语义处理器之间精心策划的交互（在语义空间中进行优化和搜索）称为语义解码算法。这个概念与经过充分研究的句法解码问题直接相似，其中涉及精心设计算法以最好地利用自回归语言模型来提取高实用性的句法标记序列。通过关注语义层面并忽略句法细节，我们对人工智能系统的工程获得了全新的视角，使我们能够想象出具有更大复杂性和功能的系统。在这篇立场文件中，我们形式化了从句法到语义标记的转变以及句法和语义解码之间的类比。随后，我们探索通过语义解码算法在语义标记空间内进行优化的可能性。最后，我们列出了这一新视角所产生的研究机会和问题。语义解码视角为直接在有意义的概念空间中的搜索和优化提供了强大的抽象，并将语义标记作为新型计算的基本单位。</li>
</ul>

<h3>Title: A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students'  Formative Assessment Responses in Science</h3>
<ul>
<li><strong>Authors: </strong>Clayton Cohn, Nicole Hutchins, Tuan Le, Gautam Biswas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14565">https://arxiv.org/abs/2403.14565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14565">https://arxiv.org/pdf/2403.14565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14565]] A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students'  Formative Assessment Responses in Science(https://arxiv.org/abs/2403.14565)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.</li>
<li><strong>摘要：</strong>本文探讨了使用大型语言模型 (LLM) 来评分和解释 K-12 科学中的简答评估。虽然现有方法可以对更结构化的数学和计算机科学评估进行评分，但它们通常不提供分数的解释。我们的研究重点是利用 GPT-4 在中学地球科学中进行自动化评估，将小样本和主动学习与思维链推理相结合。使用人机交互的方法，我们成功地对形成性评估反应进行评分并提供有意义的解释。对我们方法的优缺点的系统分析揭示了人机循环技术增强开放式科学评估自动评分的潜力。</li>
</ul>

<h3>Title: Large Language Models for Multi-Choice Question Classification of  Medical Subjects</h3>
<ul>
<li><strong>Authors: </strong>Víctor Ponce-López</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14582">https://arxiv.org/abs/2403.14582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14582">https://arxiv.org/pdf/2403.14582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14582]] Large Language Models for Multi-Choice Question Classification of  Medical Subjects(https://arxiv.org/abs/2403.14582)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The aim of this paper is to evaluate whether large language models trained on multi-choice question data can be used to discriminate between medical subjects. This is an important and challenging task for automatic question answering. To achieve this goal, we train deep neural networks for multi-class classification of questions into the inferred medical subjects. Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively. In this sense, we show the capability of AI and LLMs in particular for multi-classification tasks in the Healthcare domain.</li>
<li><strong>摘要：</strong>本文的目的是评估在多选问题数据上训练的大型语言模型是否可以用于区分医学科目。对于自动问答来说，这是一项重要且具有挑战性的任务。为了实现这一目标，我们训练深度神经网络，将问题多类分类为推断的医学科目。使用我们的多问题 (MQ) Sequence-BERT 方法，我们在 MedMCQA 数据集上的表现优于最先进的结果，其开发集和测试集的准确度分别为 0.68 和 0.60。从这个意义上说，我们展示了人工智能和法学硕士的能力，特别是在医疗保健领域的多分类任务上。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
