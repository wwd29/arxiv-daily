<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-21</h1>
<h3>Title: Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation</h3>
<ul>
<li><strong>Authors: </strong>Junhong Wu, Yang Zhao, Yangyifan Xu, Bing Liu, Chengqing Zong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13944">https://arxiv.org/abs/2410.13944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13944">https://arxiv.org/pdf/2410.13944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13944]] Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation(https://arxiv.org/abs/2410.13944)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved impressive results across numerous NLP tasks but still encounter difficulties in machine translation. Traditional methods to improve translation have typically involved fine-tuning LLMs using parallel corpora. However, vanilla fine-tuning often leads to catastrophic forgetting of the instruction-following capabilities and alignment with human preferences, compromising their broad general abilities and introducing potential security risks. These abilities, which are developed using proprietary and unavailable training data, make existing continual instruction tuning methods ineffective. To overcome this issue, we propose a novel approach called RaDis (Rationale Distillation). RaDis harnesses the strong generative capabilities of LLMs to create rationales for training data, which are then "replayed" to prevent forgetting. These rationales encapsulate general knowledge and safety principles, acting as self-distillation targets to regulate the training process. By jointly training on both reference translations and self-generated rationales, the model can learn new translation skills while preserving its overall general abilities. Extensive experiments demonstrate that our method enhances machine translation performance while maintaining the broader capabilities of LLMs across other tasks. This work presents a pathway for creating more versatile LLMs that excel in specialized tasks without compromising generality and safety.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在众多 NLP 任务中取得了令人瞩目的成果，但在机器翻译中仍然遇到困难。传统的改进翻译的方法通常涉及使用平行语料库对 LLM 进行微调。然而，普通的微调往往会导致灾难性地忘记指令遵循能力和与人类偏好的一致性，损害其广泛的一般能力并带来潜在的安全风险。这些能力是使用专有和不可用的训练数据开发的，使现有的持续指令调整方法无效。为了解决这个问题，我们提出了一种称为 RaDis（理性蒸馏）的新方法。RaDis 利用 LLM 强大的生成能力来为训练数据创建原理，然后“重放”这些原理以防止遗忘。这些原理包含一般知识和安全原则，作为自我蒸馏目标来规范训练过程。通过对参考翻译和自我生成的原理进行联合训练，该模型可以学习新的翻译技能，同时保留其整体一般能力。大量实验表明，我们的方法提高了机器翻译性能，同时保留了 LLM 在其他任务中的广泛功能。这项工作提出了一种创建更通用的 LLM 的方法，这些 LLM 在专业任务中表现出色，同时又不损害通用性和安全性。</li>
</ul>

<h3>Title: From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization</h3>
<ul>
<li><strong>Authors: </strong>Catarina G. Belem, Pouya Pezeskhpour, Hayate Iso, Seiji Maekawa, Nikita Bhutani, Estevam Hruschka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13961">https://arxiv.org/abs/2410.13961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13961">https://arxiv.org/pdf/2410.13961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13961]] From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization(https://arxiv.org/abs/2410.13961)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Although many studies have investigated and reduced hallucinations in large language models (LLMs) for single-document tasks, research on hallucination in multi-document summarization (MDS) tasks remains largely unexplored. Specifically, it is unclear how the challenges arising from handling multiple documents (e.g., repetition and diversity of information) affect models outputs. In this work, we investigate how hallucinations manifest in LLMs when summarizing topic-specific information from multiple documents. Since no benchmarks exist for investigating hallucinations in MDS, we use existing news and conversation datasets, annotated with topic-specific insights, to create two novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks, we observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries. Moreover, when summarizing non-existent topic-related information, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and 44% of the time, raising concerns about their tendency to fabricate content. To understand the characteristics of these hallucinations, we manually evaluate 700+ insights and find that most errors stem from either failing to follow instructions or producing overly generic insights. Motivated by these observations, we investigate the efficacy of simple post-hoc baselines in mitigating hallucinations but find them only moderately effective. Our results underscore the need for more effective approaches to systematically mitigate hallucinations in MDS. We release our dataset and code at this http URL.</li>
<li><strong>摘要：</strong>尽管许多研究已经调查并减少了单文档任务的大型语言模型 (LLM) 中的幻觉，但对多文档摘要 (MDS) 任务中的幻觉的研究仍然很大程度上尚未探索。具体而言，尚不清楚处理多个文档（例如重复和多样化的信息）所带来的挑战如何影响模型输出。在这项工作中，我们研究了在从多个文档中总结特定主题的信息时幻觉在 LLM 中的表现方式。由于没有用于调查 MDS 中幻觉的基准，我们使用现有的新闻和对话数据集（注释了特定主题的见解）来创建两个新的多文档基准。当在我们的基准上评估 5 个 LLM 时，我们观察到平均而言，LLM 生成的摘要中高达 75% 的内容是幻觉，幻觉更有可能出现在摘要的末尾。此外，在总结不存在的主题相关信息时，GPT-3.5-turbo 和 GPT-4o 仍然有大约 79.35% 和 44% 的时间会生成摘要，这引发了人们对它们捏造内容倾向的担忧。为了了解这些幻觉的特征，我们手动评估了 700 多个见解，发现大多数错误源于未能遵循说明或产生过于笼统的见解。受这些观察的启发，我们研究了简单的事后基线在缓解幻觉方面的有效性，但发现它们的效果一般。我们的结果强调需要更有效的方法来系统地缓解 MDS 中的幻觉。我们在此 http URL 上发布了我们的数据集和代码。</li>
</ul>

<h3>Title: Detecting AI-Generated Texts in Cross-Domains</h3>
<ul>
<li><strong>Authors: </strong>You Zhou, Jie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13966">https://arxiv.org/abs/2410.13966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13966">https://arxiv.org/pdf/2410.13966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13966]] Detecting AI-Generated Texts in Cross-Domains(https://arxiv.org/abs/2410.13966)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Existing tools to detect text generated by a large language model (LLM) have met with certain success, but their performance can drop when dealing with texts in new domains. To tackle this issue, we train a ranking classifier called RoBERTa-Ranker, a modified version of RoBERTa, as a baseline model using a dataset we constructed that includes a wider variety of texts written by humans and generated by various LLMs. We then present a method to fine-tune RoBERTa-Ranker that requires only a small amount of labeled data in a new domain. Experiments show that this fine-tuned domain-aware model outperforms the popular DetectGPT and GPTZero on both in-domain and cross-domain texts, where AI-generated texts may either be in a different domain or generated by a different LLM not used to generate the training datasets. This approach makes it feasible and economical to build a single system to detect AI-generated texts across various domains.</li>
<li><strong>摘要：</strong>现有的用于检测大型语言模型 (LLM) 生成的文本的工具取得了一定的成功，但在处理新领域的文本时，它们的性能可能会下降。为了解决这个问题，我们使用我们构建的数据集训练了一个名为 RoBERTa-Ranker 的排名分类器（RoBERTa 的修改版本），作为基线模型，该数据集包含由各种 LLM 生成的人类编写的更多种文本。然后，我们提出了一种微调 RoBERTa-Ranker 的方法，该方法只需要新领域中的少量标记数据。实验表明，这种微调的领域感知模型在领域内和跨领域文本上的表现都优于流行的 DetectGPT 和 GPTZero，其中 AI 生成的文本可能位于不同的领域，或者由未用于生成训练数据集的不同 LLM 生成。这种方法使得构建一个单一系统来检测跨各个领域的 AI 生成的文本变得可行且经济。</li>
</ul>

<h3>Title: Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers</h3>
<ul>
<li><strong>Authors: </strong>Zhang Enyan, Zewei Wang, Michael A. Lepori, Ellie Pavlick, Helena Aparicio</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13984">https://arxiv.org/abs/2410.13984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13984">https://arxiv.org/pdf/2410.13984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13984]] Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers(https://arxiv.org/abs/2410.13984)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Distributional semantics is the linguistic theory that a word's meaning can be derived from its distribution in natural language (i.e., its use). Language models are commonly viewed as an implementation of distributional semantics, as they are optimized to capture the statistical features of natural language. It is often argued that distributional semantics models should excel at capturing graded/vague meaning based on linguistic conventions, but struggle with truth-conditional reasoning and symbolic processing. We evaluate this claim with a case study on vague (e.g. "many") and exact (e.g. "more than half") quantifiers. Contrary to expectations, we find that, across a broad range of models of various types, LLMs align more closely with human judgements on exact quantifiers versus vague ones. These findings call for a re-evaluation of the assumptions underpinning what distributional semantics models are, as well as what they can capture.</li>
<li><strong>摘要：</strong>分布语义学是一种语言理论，认为单词的含义可以从其在自然语言中的分布（即其使用）中得出。语言模型通常被视为分布语义学的一种实现，因为它们经过优化，可以捕捉自然语言的统计特征。人们经常认为，分布语义学模型应该擅长根据语言惯例捕捉分级/模糊含义，但在真值条件推理和符号处理方面却举步维艰。我们通过对模糊（例如“许多”）和精确（例如“一半以上”）量词的案例研究来评估这一说法。与预期相反，我们发现，在各种类型的广泛模型中，LLM 与人类对精确量词的判断比对模糊量词的判断更接近。这些发现要求重新评估分布语义学模型的假设以及它们可以捕捉的内容。</li>
</ul>

<h3>Title: RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Jiatan Huang, Mingchen Li, Zonghai Yao, Zhichao Yang, Yongkang Xiao, Feiyun Ouyang, Xiaohan Li, Shuo Han, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13987">https://arxiv.org/abs/2410.13987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13987">https://arxiv.org/pdf/2410.13987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13987]] RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual Knowledge Graphs(https://arxiv.org/abs/2410.13987)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Answering complex real-world questions often requires accurate retrieval from textual knowledge graphs (TKGs). The scarcity of annotated data, along with intricate topological structures, makes this task particularly challenging. As the nature of relational path information could enhance the inference ability of Large Language Models (LLMs), efficiently retrieving more complex relational path information from TKGs presents another key challenge. To tackle these challenges, we first develop a Dataset for LLMs Complex Reasoning over Textual Knowledge Graphs (RiTeK) with a broad topological structure this http URL synthesize realistic user queries that integrate diverse topological structures, relational information, and complex textual descriptions. We conduct rigorous expert evaluation to validate the quality of our synthesized queries. And then, we introduce an enhanced Monte Carlo Tree Search (MCTS) method, Relational MCTS, to automatically extract relational path information from textual graphs for specific queries. Our dataset mainly covers the medical domain as the relation types and entity are complex and publicly available. Experimental results indicate that RiTeK poses significant challenges for current retrieval and LLM systems, while the proposed Relational MCTS method enhances LLM inference ability and achieves state-of-the-art performance on RiTeK.</li>
<li><strong>摘要：</strong>回答复杂的现实问题通常需要从文本知识图谱 (TKG) 中进行准确检索。注释数据的稀缺性以及复杂的拓扑结构使这项任务特别具有挑战性。由于关系路径信息的性质可以增强大型语言模型 (LLM) 的推理能力，因此从 TKG 中高效检索更复杂的关系路径信息是另一个关键挑战。为了应对这些挑战，我们首先开发了一个具有广泛拓扑结构的 LLM 复杂推理文本知识图谱 (RiTeK) 数据集，此 http URL 合成了集成各种拓扑结构、关系信息和复杂文本描述的真实用户查询。我们进行严格的专家评估以验证我们合成查询的质量。然后，我们引入了一种增强的蒙特卡洛树搜索 (MCTS) 方法，即关系 MCTS，用于自动从文本图中提取特定查询的关系路径信息。我们的数据集主要涵盖医学领域，因为关系类型和实体很复杂且公开可用。实验结果表明，RiTeK 对当前的检索和 LLM 系统提出了重大挑战，而提出的关系 MCTS 方法增强了 LLM 推理能力并在 RiTeK 上取得了最先进的性能。</li>
</ul>

<h3>Title: LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education</h3>
<ul>
<li><strong>Authors: </strong>Iain Weissburg, Sathvika Anand, Sharon Levy, Haewon Jeong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14012">https://arxiv.org/abs/2410.14012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14012">https://arxiv.org/pdf/2410.14012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14012]] LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education(https://arxiv.org/abs/2410.14012)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the increasing adoption of large language models (LLMs) in education, concerns about inherent biases in these models have gained prominence. We evaluate LLMs for bias in the personalized educational setting, specifically focusing on the models' roles as "teachers". We reveal significant biases in how models generate and select educational content tailored to different demographic groups, including race, ethnicity, sex, gender, disability status, income, and national origin. We introduce and apply two bias score metrics--Mean Absolute Bias (MAB) and Maximum Difference Bias (MDB)--to analyze 9 open and closed state-of-the-art LLMs. Our experiments, which utilize over 17,000 educational explanations across multiple difficulty levels and topics, uncover that models perpetuate both typical and inverted harmful stereotypes.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在教育领域的应用越来越广泛，人们对这些模型固有偏见的担忧也日益突出。我们在个性化教育环境中评估 LLM 的偏见，特别关注模型作为“教师”的角色。我们揭示了模型在生成和选择针对不同人口群体的教育内容方面存在显著偏见，包括种族、民族、性别、残疾状况、收入和国籍。我们引入并应用了两个偏见分数指标——平均绝对偏差 (MAB) 和最大差异偏差 (MDB)——来分析 9 个开放和封闭的最先进的 LLM。我们的实验利用了 17,000 多个教育解释，涵盖多个难度级别和主题，发现模型延续了典型和颠倒的有害刻板印象。</li>
</ul>

<h3>Title: Generating Signed Language Instructions in Large-Scale Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Mert İnan, Katherine Atwell, Anthony Sicilia, Lorna Quandt, Malihe Alikhani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14026">https://arxiv.org/abs/2410.14026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14026">https://arxiv.org/pdf/2410.14026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14026]] Generating Signed Language Instructions in Large-Scale Dialogue Systems(https://arxiv.org/abs/2410.14026)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce a goal-oriented conversational AI system enhanced with American Sign Language (ASL) instructions, presenting the first implementation of such a system on a worldwide multimodal conversational AI platform. Accessible through a touch-based interface, our system receives input from users and seamlessly generates ASL instructions by leveraging retrieval methods and cognitively based gloss translations. Central to our design is a sign translation module powered by Large Language Models, alongside a token-based video retrieval system for delivering instructional content from recipes and wikiHow guides. Our development process is deeply rooted in a commitment to community engagement, incorporating insights from the Deaf and Hard-of-Hearing community, as well as experts in cognitive and ASL learning sciences. The effectiveness of our signing instructions is validated by user feedback, achieving ratings on par with those of the system in its non-signing variant. Additionally, our system demonstrates exceptional performance in retrieval accuracy and text-generation quality, measured by metrics such as BERTScore. We have made our codebase and datasets publicly accessible at this https URL, and a demo of our signed instruction video retrieval system is available at this https URL.</li>
<li><strong>摘要：</strong>我们推出了一个以目标为导向的对话式 AI 系统，该系统通过美国手语 (ASL) 指令进行了增强，这是该系统在全球多模式对话式 AI 平台上的首次实现。我们的系统可通过触摸式界面访问，接收用户的输入，并通过利用检索方法和基于认知的注释翻译无缝生成 ASL 指令。我们设计的核心是一个由大型语言模型提供支持的手语翻译模块，以及一个基于 token 的视频检索系统，用于提供食谱和 wikiHow 指南中的教学内容。我们的开发过程深深植根于对社区参与的承诺，吸收了聋人和听力障碍社区以及认知和 ASL 学习科学专家的见解。我们的手语指令的有效性得到了用户反馈的验证，其评级与非手语版本的系统相当。此外，我们的系统在检索准确性和文本生成质量方面表现出色，以 BERTScore 等指标衡量。我们已通过此 https URL 公开我们的代码库和数据集，并且我们的签名指令视频检索系统的演示可在此 https URL 上找到。</li>
</ul>

<h3>Title: Measuring and Modifying the Readability of English Texts with GPT-4</h3>
<ul>
<li><strong>Authors: </strong>Sean Trott (1), Pamela D. Rivière (1) ((1) Department of Cognitive Science, University of California San Diego)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14028">https://arxiv.org/abs/2410.14028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14028">https://arxiv.org/pdf/2410.14028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14028]] Measuring and Modifying the Readability of English Texts with GPT-4(https://arxiv.org/abs/2410.14028)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The success of Large Language Models (LLMs) in other domains has raised the question of whether LLMs can reliably assess and manipulate the readability of text. We approach this question empirically. First, using a published corpus of 4,724 English text excerpts, we find that readability estimates produced ``zero-shot'' from GPT-4 Turbo and GPT-4o mini exhibit relatively high correlation with human judgments (r = 0.76 and r = 0.74, respectively), out-performing estimates derived from traditional readability formulas and various psycholinguistic indices. Then, in a pre-registered human experiment (N = 59), we ask whether Turbo can reliably make text easier or harder to read. We find evidence to support this hypothesis, though considerable variance in human judgments remains unexplained. We conclude by discussing the limitations of this approach, including limited scope, as well as the validity of the ``readability'' construct and its dependence on context, audience, and goal.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在其他领域的成功引发了一个问题：LLM 是否能够可靠地评估和操纵文本的可读性。我们从实证的角度来探讨这个问题。首先，使用已发布的 4,724 段英文文本摘录语料库，我们发现 GPT-4 Turbo 和 GPT-4o mini 产生的“零样本”可读性估计与人类判断具有相对较高的相关性（分别为 r = 0.76 和 r = 0.74），优于传统可读性公式和各种心理语言学指标得出的估计值。然后，在一项预先注册的人类实验（N = 59）中，我们询问 Turbo 是否可以可靠地使文本更易于阅读或更难阅读。我们找到了支持这一假设的证据，尽管人类判断的巨大差异仍无法解释。最后，我们讨论了这种方法的局限性，包括范围有限，以及“可读性”构造的有效性及其对上下文、受众和目标的依赖性。</li>
</ul>

<h3>Title: Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles</h3>
<ul>
<li><strong>Authors: </strong>Xiao Pu, Tianxing He, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14042">https://arxiv.org/abs/2410.14042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14042">https://arxiv.org/pdf/2410.14042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14042]] Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles(https://arxiv.org/abs/2410.14042)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Prompt compression condenses contexts while maintaining their informativeness for different usage scenarios. It not only shortens the inference time and reduces computational costs during the usage of large language models, but also lowers expenses when using closed-source models. In a preliminary study, we discover that when instructing language models to compress prompts, different compression styles (e.g., extractive or abstractive) impact performance of compressed prompts on downstream tasks. Building on this insight, we propose Style-Compress, a lightweight framework that adapts a smaller language model to compress prompts for a larger model on a new task without additional training. Our approach iteratively generates and selects effective compressed prompts as task-specific demonstrations through style variation and in-context learning, enabling smaller models to act as efficient compressors with task-specific examples. Style-Compress outperforms two baseline compression models in four tasks: original prompt reconstruction, text summarization, multi-hop QA, and CoT reasoning. In addition, with only 10 samples and 100 queries for adaptation, prompts compressed by Style-Compress achieve performance on par with or better than original prompts at a compression ratio of 0.25 or 0.5.</li>
<li><strong>摘要：</strong>提示压缩可以压缩上下文，同时保持上下文在不同使用场景下的信息性。它不仅可以缩短推理时间并降低使用大型语言模型时的计算成本，还可以降低使用闭源模型时的费用。在一项初步研究中，我们发现在指示语言模型压缩提示时，不同的压缩风格（例如，提取或抽象）会影响压缩提示在下游任务上的性能。基于这一见解，我们提出了 Style-Compress，这是一个轻量级框架，它调整较小的语言模型以压缩新任务上较大模型的提示，而无需额外训练。我们的方法通过风格变化和上下文学习迭代生成和选择有效的压缩提示作为特定于任务的演示，使较小的模型能够作为具有特定于任务的示例的高效压缩器。Style-Compress 在四个任务中的表现优于两个基线压缩模型：原始提示重建、文本摘要、多跳 QA 和 CoT 推理。此外，仅使用 10 个样本和 100 个查询进行适应，经过 Style-Compress 压缩的提示在压缩率为 0.25 或 0.5 的情况下达到与原始提示相当或更好的性能。</li>
</ul>

<h3>Title: Efficient Retrieval of Temporal Event Sequences from Textual Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Zefang Liu, Yinzhu Quan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14043">https://arxiv.org/abs/2410.14043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14043">https://arxiv.org/pdf/2410.14043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14043]] Efficient Retrieval of Temporal Event Sequences from Textual Descriptions(https://arxiv.org/abs/2410.14043)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Retrieving temporal event sequences from textual descriptions is essential for applications such as analyzing e-commerce behavior, monitoring social media activities, and tracking criminal incidents. In this paper, we introduce TPP-LLM-Embedding, a unified model for efficiently embedding and retrieving event sequences based on natural language descriptions. Built on the TPP-LLM framework, which integrates large language models with temporal point processes, our model encodes both event types and times, generating a sequence-level representation through pooling. Textual descriptions are embedded using the same architecture, ensuring a shared embedding space for both sequences and descriptions. We optimize a contrastive loss based on similarity between these embeddings, bringing matching pairs closer and separating non-matching ones. TPP-LLM-Embedding enables efficient retrieval and demonstrates superior performance compared to baseline models across diverse datasets.</li>
<li><strong>摘要：</strong>从文本描述中检索时间事件序列对于分析电子商务行为、监控社交媒体活动和追踪犯罪事件等应用至关重要。在本文中，我们介绍了 TPP-LLM-Embedding，这是一种基于自然语言描述高效嵌入和检索事件序列的统一模型。我们的模型基于 TPP-LLM 框架构建，该框架将大型语言模型与时间点过程相结合，对事件类型和时间进行编码，通过池化生成序列级表示。文本描述使用相同的架构嵌入，确保序列和描述共享嵌入空间。我们根据这些嵌入之间的相似性优化对比损失，使匹配对更接近并分离不匹配对。TPP-LLM-Embedding 可实现高效检索，并且与跨不同数据集的基线模型相比表现出卓越的性能。</li>
</ul>

<h3>Title: Learning Metadata-Agnostic Representations for Text-to-SQL In-Context Example Selection</h3>
<ul>
<li><strong>Authors: </strong>Chuhong Mai, Ro-ee Tal, Thahir Mohamed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14049">https://arxiv.org/abs/2410.14049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14049">https://arxiv.org/pdf/2410.14049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14049]] Learning Metadata-Agnostic Representations for Text-to-SQL In-Context Example Selection(https://arxiv.org/abs/2410.14049)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is a powerful paradigm where large language models (LLMs) benefit from task demonstrations added to the prompt. Yet, selecting optimal demonstrations is not trivial, especially for complex or multi-modal tasks where input and output distributions differ. We hypothesize that forming task-specific representations of the input is key. In this paper, we propose a method to align representations of natural language questions and those of SQL queries in a shared embedding space. Our technique, dubbed MARLO - Metadata-Agnostic Representation Learning for Text-tO-SQL - uses query structure to model querying intent without over-indexing on underlying database metadata (i.e. tables, columns, or domain-specific entities of a database referenced in the question or query). This allows MARLO to select examples that are structurally and semantically relevant for the task rather than examples that are spuriously related to a certain domain or question phrasing. When used to retrieve examples based on question similarity, MARLO shows superior performance compared to generic embedding models (on average +2.9\%pt. in execution accuracy) on the Spider benchmark. It also outperforms the next best method that masks metadata information by +0.8\%pt. in execution accuracy on average, while imposing a significantly lower inference latency.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 是一种强大的范例，其中大型语言模型 (LLM) 受益于添加到提示中的任务演示。然而，选择最佳演示并非易事，尤其是对于输入和输出分布不同的复杂或多模态任务。我们假设形成输入的任务特定表示是关键。在本文中，我们提出了一种在共享嵌入空间中对齐自然语言问题和 SQL 查询表示的方法。我们的技术称为 MARLO（与元数据无关的 Text-to-SQL 表示学习）使用查询结构来模拟查询意图，而无需对底层数据库元数据（即问题或查询中引用的数据库的表、列或域特定实体）进行过度索引。这使 MARLO 能够选择与任务在结构和语义上相关的示例，而不是与某个领域或问题措辞虚假相关的示例。当用于根据问题相似性检索示例时，MARLO 在 Spider 基准上表现出比通用嵌入模型（执行准确率平均高出 2.9%pt）更出色的性能。与屏蔽元数据信息的下一个最佳方法相比，MARLO 的执行准确率平均高出 0.8%pt，同时推理延迟显著降低。</li>
</ul>

<h3>Title: From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Alireza Rezazadeh, Zichao Li, Wei Wei, Yujia Bao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14052">https://arxiv.org/abs/2410.14052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14052">https://arxiv.org/pdf/2410.14052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14052]] From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs(https://arxiv.org/abs/2410.14052)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models have significantly improved their context windows, yet challenges in effective long-term memory management remain. We introduce MemTree, an algorithm that leverages a dynamic, tree-structured memory representation to optimize the organization, retrieval, and integration of information, akin to human cognitive schemas. MemTree organizes memory hierarchically, with each node encapsulating aggregated textual content, corresponding semantic embeddings, and varying abstraction levels across the tree's depths. Our algorithm dynamically adapts this memory structure by computing and comparing semantic embeddings of new and existing information to enrich the model's context-awareness. This approach allows MemTree to handle complex reasoning and extended interactions more effectively than traditional memory augmentation methods, which often rely on flat lookup tables. Evaluations on benchmarks for multi-turn dialogue understanding and document question answering show that MemTree significantly enhances performance in scenarios that demand structured memory management.</li>
<li><strong>摘要：</strong>大型语言模型的最新进展显著改善了它们的上下文窗口，但有效的长期记忆管理仍然存在挑战。我们引入了 MemTree，这是一种利用动态树状结构记忆表示来优化信息组织、检索和集成的算法，类似于人类的认知模式。MemTree 按层次结构组织记忆，每个节点都封装聚合的文本内容、相应的语义嵌入和树深度上不同的抽象级别。我们的算法通过计算和比较新信息和现有信息的语义嵌入来动态调整这种记忆结构，以丰富模型的上下文感知。这种方法使 MemTree 能够比传统的记忆增强方法更有效地处理复杂的推理和扩展交互，而传统的记忆增强方法通常依赖于平面查找表。对多轮对话理解和文档问答基准的评估表明，MemTree 在需要结构化内存管理的场景中显著提高了性能。</li>
</ul>

<h3>Title: Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Simone Conia, Daniel Lee, Min Li, Umar Farooq Minhas, Saloni Potdar, Yunyao Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14057">https://arxiv.org/abs/2410.14057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14057">https://arxiv.org/pdf/2410.14057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14057]] Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs(https://arxiv.org/abs/2410.14057)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Translating text that contains entity names is a challenging task, as cultural-related references can vary significantly across languages. These variations may also be caused by transcreation, an adaptation process that entails more than transliteration and word-for-word translation. In this paper, we address the problem of cross-cultural translation on two fronts: (i) we introduce XC-Translate, the first large-scale, manually-created benchmark for machine translation that focuses on text that contains potentially culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end method to integrate information from a multilingual knowledge graph into a neural machine translation model by leveraging a dense retrieval mechanism. Our experiments and analyses show that current machine translation systems and large language models still struggle to translate texts containing entity names, whereas KG-MT outperforms state-of-the-art approaches by a large margin, obtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4, respectively.</li>
<li><strong>摘要：</strong>翻译包含实体名称的文本是一项艰巨的任务，因为不同语言中的文化相关指称可能存在很大差异。这些差异也可能是由创译引起的，创译是一种适应过程，它涉及的不仅仅是音译和逐字翻译。在本文中，我们从两个方面解决跨文化翻译问题：（i）我们引入了 XC-Translate，这是第一个大规模、手动创建的机器翻译基准，专注于包含可能具有文化细微差别的实体名称的文本；（ii）我们提出了 KG-MT，这是一种新颖的端到端方法，通过利用密集检索机制将多语言知识图谱中的信息集成到神经机器翻译模型中。我们的实验和分析表明，当前的机器翻译系统和大型语言模型仍然难以翻译包含实体名称的文本，而 KG-MT 的表现则远远优于最先进的方法，与 NLLB-200 和 GPT-4 相比分别获得了 129% 和 62% 的相对改进。</li>
</ul>

<h3>Title: Be My Donor. Transfer the NLP Datasets Between the Languages Using LLM</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii Popov, Egor Terentev, Igor Buyanov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14074">https://arxiv.org/abs/2410.14074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14074">https://arxiv.org/pdf/2410.14074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14074]] Be My Donor. Transfer the NLP Datasets Between the Languages Using LLM(https://arxiv.org/abs/2410.14074)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>In this work, we investigated how one can use the LLM to transfer the dataset and its annotation from one language to another. This is crucial since sharing the knowledge between different languages could boost certain underresourced directions in the target language, saving lots of efforts in data annotation or quick prototyping. We experiment with English and Russian pairs translating the DEFT corpus. This corpus contains three layers of annotation dedicated to term-definition pair mining, which is a rare annotation type for Russian. We provide a pipeline for the annotation transferring using ChatGPT3.5-turbo and Llama-3.1-8b as core LLMs. In the end, we train the BERT-based models on the translated dataset to establish a baseline.</li>
<li><strong>摘要：</strong>在这项工作中，我们研究了如何使用 LLM 将数据集及其注释从一种语言转移到另一种语言。这至关重要，因为在不同语言之间共享知识可以促进目标语言中某些资源不足的方向，从而节省大量数据注释或快速原型设计的工作。我们尝试用英语和俄语对翻译 DEFT 语料库。该语料库包含三层注释，专门用于术语定义对挖掘，这是俄语中罕见的注释类型。我们使用 ChatGPT3.5-turbo 和 Llama-3.1-8b 作为核心 LLM 为注释传输提供了管道。最后，我们在翻译的数据集上训练基于 BERT 的模型以建立基线。</li>
</ul>

<h3>Title: A Lightweight Multi Aspect Controlled Text Generation Solution For Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Zhang, Jiayi Lin, Haibo Tong, Bingxuan Hou, Dongyu Zhang, Jialin Li, Junli Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14144">https://arxiv.org/abs/2410.14144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14144">https://arxiv.org/pdf/2410.14144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14144]] A Lightweight Multi Aspect Controlled Text Generation Solution For Large Language Models(https://arxiv.org/abs/2410.14144)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show remarkable abilities with instruction tuning. However, they fail to achieve ideal tasks when lacking high-quality instruction tuning data on target tasks. Multi-Aspect Controllable Text Generation (MCTG) is a representative task for this dilemma, where aspect datasets are usually biased and correlated. Existing work exploits additional model structures and strategies for solutions, limiting adaptability to LLMs. To activate MCTG ability of LLMs, we propose a lightweight MCTG pipeline based on data augmentation. We analyze bias and correlations in traditional datasets, and address these concerns with augmented control attributes and sentences. Augmented datasets are feasible for instruction tuning. In our experiments, LLMs perform better in MCTG after data augmentation, with a 20% accuracy rise and less aspect correlations.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 表现出卓越的指令调优能力。然而，当缺乏目标任务的高质量指令调优数据时，它们无法实现理想的任务。多方面可控文本生成 (MCTG) 是此困境的代表性任务，其中方面数据集通常存在偏差和相关性。现有工作利用额外的模型结构和策略来解决问题，限制了 LLM 的适应性。为了激活 LLM 的 MCTG 能力，我们提出了一种基于数据增强的轻量级 MCTG 管道。我们分析传统数据集中的偏差和相关性，并通过增强控制属性和句子解决这些问题。增强数据集对于指令调优是可行的。在我们的实验中，数据增强后的 LLM 在 MCTG 中表现更好，准确率提高了 20%，方面相关性更少。</li>
</ul>

<h3>Title: CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>June M. Liu, He Cao, Renliang Sun, Rui Wang, Yu Li, Jiaxing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14145">https://arxiv.org/abs/2410.14145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14145">https://arxiv.org/pdf/2410.14145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14145]] CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using Large Language Models(https://arxiv.org/abs/2410.14145)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Generating emotionally appropriate responses in conversations with large language models presents a significant challenge due to the complexities of human emotions and cognitive processes, which remain largely underexplored in their critical role in social interactions. In this study, we introduce a two-stage automatic data generation framework to create CAPE, a Chinese dataset named Cognitive Appraisal theory-based Emotional corpus. This corpus facilitates the generation of dialogues with contextually appropriate emotional responses by accounting for diverse personal and situational factors. We propose two tasks utilizing this dataset: emotion prediction and next utterance prediction. Both automated and human evaluations demonstrate that agents trained on our dataset can deliver responses that are more aligned with human emotional expressions. Our study shows the potential for advancing emotional expression in conversational agents, paving the way for more nuanced and meaningful human-computer interactions.</li>
<li><strong>摘要：</strong>由于人类情感和认知过程的复杂性，以及它们在社交互动中的重要作用，使用大型语言模型在对话中生成情绪上适当的回应是一项重大挑战。在这项研究中，我们引入了一个两阶段的自动数据生成框架来创建 CAPE，这是一个名为基于认知评估理论的情绪语料库的中文数据集。该语料库通过考虑各种个人和情境因素来促进生成具有适合上下文的情绪反应的对话。我们提出了两个利用该数据集的任务：情绪预测和下一句预测。自动评估和人工评估均表明，在我们的数据集上训练的代理可以提供更符合人类情绪表达的响应。我们的研究展示了在对话代理中推进情感表达的潜力，为更细致入微和更有意义的人机交互铺平了道路。</li>
</ul>

<h3>Title: SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Ji, Yang Li, Hongtao Liu, Zhicheng Du, Zhewei Wei, Weiran Shen, Qi Qi, Yankai Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14152">https://arxiv.org/abs/2410.14152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14152">https://arxiv.org/pdf/2410.14152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14152]] SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent(https://arxiv.org/abs/2410.14152)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Public scarce resource allocation plays a crucial role in economics as it directly influences the efficiency and equity in society. Traditional studies including theoretical model-based, empirical study-based and simulation-based methods encounter limitations due to the idealized assumption of complete information and individual rationality, as well as constraints posed by limited available data. In this work, we propose an innovative framework, SRAP-Agent (Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent), which integrates Large Language Models (LLMs) into economic simulations, aiming to bridge the gap between theoretical models and real-world dynamics. Using public housing allocation scenarios as a case study, we conduct extensive policy simulation experiments to verify the feasibility and effectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm with certain optimization objectives. The source code can be found in this https URL</li>
<li><strong>摘要：</strong>公共稀缺资源配置在经济学中起着至关重要的作用，因为它直接影响社会的效率和公平。传统的研究方法包括基于理论模型、基于实证研究和基于模拟的方法，由于对完整信息和个人理性的理想化假设以及有限的可用数据带来的限制而受到限制。在本文中，我们提出了一个创新框架 SRAP-Agent（使用基于 LLM 的代理模拟和优化稀缺资源配置政策），它将大型语言模型 (LLM) 集成到经济模拟中，旨在弥合理论模型与现实世界动态之间的差距。以公共住房分配场景为例，我们进行了广泛的政策模拟实验，以验证 SRAP-Agent 的可行性和有效性，并采用了具有某些优化目标的策略优化算法。源代码可以在这个 https URL 中找到</li>
</ul>

<h3>Title: Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wei Jie Yeo, Ranjan Satapthy, Erik Cambria</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14155">https://arxiv.org/abs/2410.14155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14155">https://arxiv.org/pdf/2410.14155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14155]] Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models(https://arxiv.org/abs/2410.14155)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are capable of generating persuasive Natural Language Explanations (NLEs) to justify their answers. However, the faithfulness of these explanations should not be readily trusted at face value. Recent studies have proposed various methods to measure the faithfulness of NLEs, typically by inserting perturbations at the explanation or feature level. We argue that these approaches are neither comprehensive nor correctly designed according to the established definition of faithfulness. Moreover, we highlight the risks of grounding faithfulness findings on out-of-distribution samples. In this work, we leverage a causal mediation technique called activation patching, to measure the faithfulness of an explanation towards supporting the explained answer. Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness. We experimented across models varying from 2B to 27B parameters and found that models that underwent alignment tuning tend to produce more faithful and plausible explanations. We find that Causal Faithfulness is a promising improvement over existing faithfulness tests by taking into account the model's internal computations and avoiding out of distribution concerns that could otherwise undermine the validity of faithfulness assessments. We release the code in \url{this https URL}</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 能够生成有说服力的自然语言解释 (NLE) 来证明其答案。然而，这些解释的忠实性不应该轻易被相信。最近的研究提出了各种方法来衡量 NLE 的忠实性，通常是通过在解释或特征级别插入扰动。我们认为这些方法既不全面，也没有根据既定的忠实性定义正确设计。此外，我们强调了将忠实性发现建立在分布外样本上的风险。在这项工作中，我们利用一种称为激活修补的因果中介技术来衡量解释对支持解释答案的忠实性。我们提出的指标因果忠实性量化了解释与相应模型输出之间的因果归因的一致性，作为忠实性的指标。我们在从 2B 到 27B 个参数的模型上进行了实验，发现经过对齐调整的模型往往会产生更忠实和合理的解释。我们发现，因果忠诚度通过考虑模型的内部计算并避免可能破坏忠诚度评估有效性的分布问题，是对现有忠诚度测试的一项有希望的改进。我们在 \url{此 https URL} 中发布了代码</li>
</ul>

<h3>Title: Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14157">https://arxiv.org/abs/2410.14157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14157">https://arxiv.org/pdf/2410.14157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14157]] Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning(https://arxiv.org/abs/2410.14157)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Autoregressive language models, despite their impressive capabilities, struggle with complex reasoning and long-term planning tasks. We introduce discrete diffusion models as a novel solution to these challenges. Through the lens of subgoal imbalance, we demonstrate how diffusion models effectively learn difficult subgoals that elude autoregressive approaches. We propose Multi-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on difficulty during learning. On complex tasks like Countdown, Sudoku, and Boolean Satisfiability Problems, MDM significantly outperforms autoregressive models without using search techniques. For instance, MDM achieves 91.5\% and 100\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\% and 20.7\% for autoregressive models. Our work highlights the potential of diffusion-based approaches in advancing AI capabilities for sophisticated language understanding and problem-solving tasks.</li>
<li><strong>摘要：</strong>自回归语言模型尽管功能强大，但在处理复杂的推理和长期规划任务时却举步维艰。我们引入了离散扩散模型作为这些挑战的新解决方案。通过子目标不平衡的视角，我们展示了扩散模型如何有效地学习自回归方法无法解决的困难子目标。我们提出了多粒度扩散模型 (MDM)，它根据学习过程中的难度对子目标进行优先排序。在倒计时、数独和布尔可满足性问题等复杂任务上，MDM 的表现明显优于不使用搜索技术的自回归模型。例如，MDM 在倒计时和数独上的准确率分别为 91.5% 和 100%，而自回归模型的准确率分别为 45.8% 和 20.7%。我们的工作凸显了基于扩散的方法在提高 AI 能力以完成复杂语言理解和解决问题任务方面的潜力。</li>
</ul>

<h3>Title: Automated Genre-Aware Article Scoring and Feedback Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chihang Wang, Yuxin Dong, Zhenhong Zhang, Ruotong Wang, Shuo Wang, Jiajing Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14165">https://arxiv.org/abs/2410.14165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14165">https://arxiv.org/pdf/2410.14165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14165]] Automated Genre-Aware Article Scoring and Feedback Using Large Language Models(https://arxiv.org/abs/2410.14165)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>This paper focuses on the development of an advanced intelligent article scoring system that not only assesses the overall quality of written work but also offers detailed feature-based scoring tailored to various article genres. By integrating the pre-trained BERT model with the large language model Chat-GPT, the system gains a deep understanding of both the content and structure of the text, enabling it to provide a thorough evaluation along with targeted suggestions for improvement. Experimental results demonstrate that this system outperforms traditional scoring methods across multiple public datasets, particularly in feature-based assessments, offering a more accurate reflection of the quality of different article types. Moreover, the system generates personalized feedback to assist users in enhancing their writing skills, underscoring the potential and practical value of automated scoring technologies in educational contexts.</li>
<li><strong>摘要：</strong>本文重点介绍一种先进的智能文章评分系统，该系统不仅可以评估写作的整体质量，还可以针对各种文章类型提供基于特征的详细评分。通过将预训练的 BERT 模型与大型语言模型 Chat-GPT 相结合，系统可以深入了解文本的内容和结构，从而提供全面的评估以及有针对性的改进建议。实验结果表明，该系统在多个公共数据集上的表现优于传统评分方法，特别是在基于特征的评估方面，可以更准确地反映不同文章类型的质量。此外，该系统还会生成个性化反馈，以帮助用户提高写作技巧，凸显了自动评分技术在教育环境中的潜力和实用价值。</li>
</ul>

<h3>Title: LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems</h3>
<ul>
<li><strong>Authors: </strong>Nan Xu, Xuezhe Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14166">https://arxiv.org/abs/2410.14166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14166">https://arxiv.org/pdf/2410.14166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14166]] LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems(https://arxiv.org/abs/2410.14166)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Interestingly, LLMs yet struggle with some basic tasks that humans find trivial to handle, e.g., counting the number of character r's in the word "strawberry". There are several popular conjectures (e.g., tokenization, architecture and training data) regarding the reason for deficiency of LLMs in simple word-based counting problems, sharing the similar belief that such failure stems from model pretraining hence probably inevitable during deployment. In this paper, we carefully design multiple evaluation settings to investigate validity of prevalent conjectures. Meanwhile, we measure transferability of advanced mathematical and coding reasoning capabilities from specialized LLMs to simple counting tasks. Although specialized LLMs suffer from counting problems as well, we find conjectures about inherent deficiency of LLMs invalid and further seek opportunities to elicit knowledge and capabilities from LLMs that are beneficial to counting tasks. Compared with strategies such as finetuning and in-context learning that are commonly adopted to enhance performance on new or challenging tasks, we show that engaging reasoning is the most robust and efficient way to help LLMs better perceive tasks with more accurate responses. We hope our conjecture validation design could provide insights into the study of future critical failure modes of LLMs. Based on challenges in transferring advanced capabilities to much simpler tasks, we call for more attention to model capability acquisition and evaluation. We also highlight the importance of cultivating consciousness of "reasoning before responding" during model pretraining.</li>
<li><strong>摘要：</strong>有趣的是，LLM 仍然难以处理一些人类认为很容易处理的基本任务，例如，计算单词“strawberry”中字符 r 的数量。关于 LLM 在简单的基于单词的计数问题中存在缺陷的原因，有几种流行的猜想（例如，标记化、架构和训练数据），它们都认为这种失败源于模型预训练，因此在部署过程中可能不可避免。在本文中，我们精心设计了多个评估设置来调查普遍猜想的有效性。同时，我们测量了高级数学和编码推理能力从专门的 LLM 到简单计数任务的可转移性。尽管专门的 LLM 也存在计数问题，但我们发现关于 LLM 固有缺陷的猜想是无效的，并进一步寻找机会从 LLM 中获取对计数任务有益的知识和能力。与通常用于提高新任务或具有挑战性的任务表现的微调和情境学习等策略相比，我们表明，参与式推理是帮助 LLM 更好地感知任务并做出更准确反应的最强大和最有效的方式。我们希望我们的猜想验证设计能够为研究 LLM 未来的关键故障模式提供参考。基于将高级能力转移到更简单的任务的挑战，我们呼吁更多地关注模型能力的获取和评估。我们还强调在模型预训练期间培养“先推理后反应”意识的重要性。</li>
</ul>

<h3>Title: MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems</h3>
<ul>
<li><strong>Authors: </strong>Zifeng Zhu, Mengzhao Jia, Zhihan Zhang, Lang Li, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14179">https://arxiv.org/abs/2410.14179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14179">https://arxiv.org/pdf/2410.14179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14179]] MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems(https://arxiv.org/abs/2410.14179)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated impressive abilities across various tasks, including visual question answering and chart comprehension, yet existing benchmarks for chart-related tasks fall short in capturing the complexity of real-world multi-chart scenarios. Current benchmarks primarily focus on single-chart tasks, neglecting the multi-hop reasoning required to extract and integrate information from multiple charts, which is essential in practical applications. To fill this gap, we introduce MultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas: direct question answering, parallel question answering, comparative reasoning, and sequential reasoning. Our evaluation of a wide range of MLLMs reveals significant performance gaps compared to humans. These results highlight the challenges in multi-chart comprehension and the potential of MultiChartQA to drive advancements in this field. Our code and data are available at this https URL</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLLM) 在各种任务中都表现出了令人印象深刻的能力，包括视觉问答和图表理解，但现有的图表相关任务基准测试无法捕捉现实世界多图表场景的复杂性。当前的基准测试主要侧重于单图表任务，忽略了从多个图表中提取和整合信息所需的多跳推理，而这在实际应用中至关重要。为了填补这一空白，我们引入了 MultiChartQA，这是一个基准测试，可评估 MLLM 在四个关键领域的能力：直接问答、并行问答、比较推理和顺序推理。我们对各种 MLLM 的评估表明，与人类相比，它们在性能上存在显著差距。这些结果突出了多图表理解方面的挑战以及 MultiChartQA 推动该领域进步的潜力。我们的代码和数据可在此 https URL 上找到</li>
</ul>

<h3>Title: XForecast: Evaluating Natural Language Explanations for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Taha Aksu, Chenghao Liu, Amrita Saha, Sarah Tan, Caiming Xiong, Doyen Sahoo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14180">https://arxiv.org/abs/2410.14180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14180">https://arxiv.org/pdf/2410.14180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14180]] XForecast: Evaluating Natural Language Explanations for Time Series Forecasting(https://arxiv.org/abs/2410.14180)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Time series forecasting aids decision-making, especially for stakeholders who rely on accurate predictions, making it very important to understand and explain these models to ensure informed decisions. Traditional explainable AI (XAI) methods, which underline feature or temporal importance, often require expert knowledge. In contrast, natural language explanations (NLEs) are more accessible to laypeople. However, evaluating forecast NLEs is difficult due to the complex causal relationships in time series data. To address this, we introduce two new performance metrics based on simulatability, assessing how well a human surrogate can predict model forecasts using the explanations. Experiments show these metrics differentiate good from poor explanations and align with human judgments. Utilizing these metrics, we further evaluate the ability of state-of-the-art large language models (LLMs) to generate explanations for time series data, finding that numerical reasoning, rather than model size, is the main factor influencing explanation quality.</li>
<li><strong>摘要：</strong>时间序列预测有助于决策，尤其是对于依赖准确预测的利益相关者而言，因此理解和解释这些模型对于确保做出明智的决策非常重要。传统的可解释人工智能 (XAI) 方法强调特征或时间重要性，通常需要专业知识。相比之下，自然语言解释 (NLE) 更容易被普通人理解。然而，由于时间序列数据中复杂的因果关系，评估预测 NLE 很困难。为了解决这个问题，我们引入了两个基于可模拟性的新性能指标，评估人类替代者使用解释预测模型预测的能力。实验表明，这些指标可以区分好的解释和差的解释，并与人类的判断一致。利用这些指标，我们进一步评估了最先进的大型语言模型 (LLM) 为时间序列数据生成解释的能力，发现数值推理，而不是模型大小，是影响解释质量的主要因素。</li>
</ul>

<h3>Title: LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs</h3>
<ul>
<li><strong>Authors: </strong>Yujun Zhou, Jingdong Yang, Kehan Guo, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14182">https://arxiv.org/abs/2410.14182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14182">https://arxiv.org/pdf/2410.14182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14182]] LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs(https://arxiv.org/abs/2410.14182)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Laboratory accidents pose significant risks to human life and property, underscoring the importance of robust safety protocols. Despite advancements in safety training, laboratory personnel may still unknowingly engage in unsafe practices. With the increasing reliance on large language models (LLMs) for guidance in various fields, including laboratory settings, there is a growing concern about their reliability in critical safety-related decision-making. Unlike trained human researchers, LLMs lack formal lab safety education, raising questions about their ability to provide safe and accurate guidance. Existing research on LLM trustworthiness primarily focuses on issues such as ethical compliance, truthfulness, and fairness but fails to fully cover safety-critical real-world applications, like lab safety. To address this gap, we propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive evaluation framework based on a new taxonomy aligned with Occupational Safety and Health Administration (OSHA) protocols. This benchmark includes 765 multiple-choice questions verified by human experts, assessing LLMs and vision language models (VLMs) performance in lab safety contexts. Our evaluations demonstrate that while GPT-4o outperforms human participants, it is still prone to critical errors, highlighting the risks of relying on LLMs in safety-critical environments. Our findings emphasize the need for specialized benchmarks to accurately assess the trustworthiness of LLMs in real-world safety applications.</li>
<li><strong>摘要：</strong>实验室事故对人类生命和财产构成重大风险，这凸显了健全安全协议的重要性。尽管安全培训取得了进步，但实验室人员仍可能在不知情的情况下从事不安全的行为。随着包括实验室环境在内的各个领域越来越依赖大型语言模型 (LLM) 来提供指导，人们越来越担心它们在关键安全相关决策中的可靠性。与受过训练的人类研究人员不同，LLM 缺乏正式的实验室安全教育，这引发了人们对它们提供安全和准确指导的能力的质疑。现有的 LLM 可信度研究主要关注道德合规性、真实性和公平性等问题，但未能完全涵盖实验室安全等安全关键的现实世界应用。为了解决这一差距，我们提出了实验室安全基准 (LabSafety Bench)，这是一个基于与职业安全与健康管理局 (OSHA) 协议一致的新分类法的综合评估框架。该基准包括 765 个由人类专家验证的多项选择题，用于评估 LLM 和视觉语言模型 (VLM) 在实验室安全环境中的表现。我们的评估表明，尽管 GPT-4o 的表现优于人类参与者，但它仍然容易出现严重错误，凸显了在安全关键环境中依赖 LLM 的风险。我们的研究结果强调需要专门的基准来准确评估 LLM 在现实世界安全应用中的可信度。</li>
</ul>

<h3>Title: MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time</h3>
<ul>
<li><strong>Authors: </strong>Mozhi Zhang, Pengyu Wang, Chenkun Tan, Mianqiu Huang, Dong Zhang, Yaqian Zhou, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14184">https://arxiv.org/abs/2410.14184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14184">https://arxiv.org/pdf/2410.14184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14184]] MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time(https://arxiv.org/abs/2410.14184)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) acquire extensive knowledge and remarkable abilities from extensive text corpora, making them powerful tools for various applications. To make LLMs more usable, aligning them with human preferences is essential. Existing alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed predefined preferences directly within the model's parameters. These methods, however, often result in a static alignment that can not account for the diversity of human preferences in practical applications. In response to this challenge, we propose an effective method, \textbf{MetaAlign}, which aims to help LLMs dynamically align with various explicit or implicit preferences specified at inference time. Experimental results show that LLMs optimized on our meticulously constructed MetaAlign Dataset can effectively align with any preferences specified at the inference stage, validating the feasibility of MetaAlign. We hope that our work can provide some insights into the alignment of language models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 从大量文本语料库中获取广泛的知识和卓越的能力，使其成为各种应用的强大工具。为了使 LLM 更易于使用，使其与人类偏好保持一致至关重要。现有的对齐技术，例如从人类反馈中强化学习 (RLHF) 和直​​接偏好优化 (DPO)，通常将预定义的偏好直接嵌入模型的参数中。然而，这些方法通常会导致静态对齐，无法解释实际应用中人类偏好的多样性。为了应对这一挑战，我们提出了一种有效的方法 \textbf{MetaAlign}，旨在帮助 LLM 动态地与推理时指定的各种显式或隐式偏好对齐。实验结果表明，在我们精心构建的 MetaAlign 数据集上优化的 LLM 可以有效地与推理阶段指定的任何偏好对齐，验证了 MetaAlign 的可行性。我们希望我们的工作可以为语言模型的对齐提供一些见解。</li>
</ul>

<h3>Title: Speciesism in Natural Language Processing Research</h3>
<ul>
<li><strong>Authors: </strong>Masashi Takeshita, Rafal Rzepka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14194">https://arxiv.org/abs/2410.14194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14194">https://arxiv.org/pdf/2410.14194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14194]] Speciesism in Natural Language Processing Research(https://arxiv.org/abs/2410.14194)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Natural Language Processing (NLP) research on AI Safety and social bias in AI has focused on safety for humans and social bias against human minorities. However, some AI ethicists have argued that the moral significance of nonhuman animals has been ignored in AI research. Therefore, the purpose of this study is to investigate whether there is speciesism, i.e., discrimination against nonhuman animals, in NLP research. First, we explain why nonhuman animals are relevant in NLP research. Next, we survey the findings of existing research on speciesism in NLP researchers, data, and models and further investigate this problem in this study. The findings of this study suggest that speciesism exists within researchers, data, and models, respectively. Specifically, our survey and experiments show that (a) among NLP researchers, even those who study social bias in AI, do not recognize speciesism or speciesist bias; (b) among NLP data, speciesist bias is inherent in the data annotated in the datasets used to evaluate NLP models; (c) OpenAI GPTs, recent NLP models, exhibit speciesist bias by default. Finally, we discuss how we can reduce speciesism in NLP research.</li>
<li><strong>摘要：</strong>自然语言处理 (NLP) 关于人工智能安全和社会偏见的研究主要集中在人类安全和针对人类少数群体的社会偏见上。然而，一些人工智能伦理学家认为，非人类动物的道德意义在人工智能研究中被忽视了。因此，本研究的目的是调查 NLP 研究中是否存在物种歧视，即对非人类动物的歧视。首先，我们解释为什么非人类动物与 NLP 研究相关。接下来，我们调查了现有关于 NLP 研究人员、数据和模型中物种歧视的研究结果，并在本研究中进一步调查这一问题。本研究的结果表明，物种歧视分别存在于研究人员、数据和模型中。具体来说，我们的调查和实验表明：(a) 在 NLP 研究人员中，即使是那些研究人工智能社会偏见的人，也不承认物种歧视或物种歧视偏见；(b) 在 NLP 数据中，物种歧视偏见是用于评估 NLP 模型的数据集中注释的数据所固有的；(c) OpenAI GPT，最近的 NLP 模型，默认表现出物种歧视偏见。最后，我们讨论如何在 NLP 研究中减少物种歧视。</li>
</ul>

<h3>Title: Supervised Chain of Thought</h3>
<ul>
<li><strong>Authors: </strong>Xiang Zhang, Dujian Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14198">https://arxiv.org/abs/2410.14198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14198">https://arxiv.org/pdf/2410.14198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14198]] Supervised Chain of Thought(https://arxiv.org/abs/2410.14198)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized natural language processing and hold immense potential for advancing Artificial Intelligence. However, the core architecture of most mainstream LLMs -- the Transformer -- has inherent limitations in computational depth, rendering them theoretically incapable of solving many reasoning tasks that demand increasingly deep computations. Chain of Thought (CoT) prompting has emerged as a technique to address these architectural limitations, as evidenced by several theoretical studies. It offers a promising approach to solving complex reasoning tasks that were previously beyond the capabilities of these models. Despite its successes, CoT and its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a "one-prompt-for-all" approach, using a single prompt structure (e.g., "think step by step") for a wide range of tasks -- from counting and sorting to solving mathematical and algorithmic problems. This approach poses significant challenges for models to generate the correct reasoning steps, as the model must navigate through a vast prompt template space to find the appropriate template for each task. In this work, we build upon previous theoretical analyses of CoT to demonstrate how the one-prompt-for-all approach can negatively affect the computability of LLMs. We partition the solution search space into two: the prompt space and the answer space. Our findings show that task-specific supervision is essential for navigating the prompt space accurately and achieving optimal performance. Through experiments with state-of-the-art LLMs, we reveal a gap in reasoning performance when supervision is applied versus when it is not.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了自然语言处理，并具有推动人工智能发展的巨大潜力。然而，大多数主流 LLM 的核心架构 Transformer 在计算深度方面存在固有限制，从理论上讲，它们无法解决许多需要越来越深度计算的推理任务。思路链 (CoT) 提示已成为一种解决这些架构限制的技术，多项理论研究证明了这一点。它提供了一种有前途的方法来解决以前超出这些模型能力的复杂推理任务。尽管取得了成功，但 CoT 及其变体（如思维树、思维图等）依赖于“一招鲜”的方法，使用单一提示结构（例如“逐步思考”）完成各种任务——从计数和排序到解决数学和算法问题。这种方法对模型生成正确的推理步骤提出了重大挑战，因为模型必须浏览庞大的提示模板空间才能找到适合每个任务的模板。在这项工作中，我们基于先前对 CoT 的理论分析，展示了“一刀切”方法如何对 LLM 的可计算性产生负面影响。我们将解决方案搜索空间分为两部分：提示空间和答案空间。我们的研究结果表明，针对特定任务的监督对于准确导航提示空间并实现最佳性能至关重要。通过对最先进的 LLM 进行实验，我们发现在应用监督时与未应用监督时推理性能存在差距。</li>
</ul>

<h3>Title: Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs</h3>
<ul>
<li><strong>Authors: </strong>SeongYeub Chu, JongWoo Kim, Bryan Wong, MunYong Yi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14202">https://arxiv.org/abs/2410.14202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14202">https://arxiv.org/pdf/2410.14202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14202]] Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs(https://arxiv.org/abs/2410.14202)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Existing automated essay scoring (AES) has solely relied on essay text without using explanatory rationales for the scores, thereby forgoing an opportunity to capture the specific aspects evaluated by rubric indicators in a fine-grained manner. This paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates prompt-engineering-based large language models (LLMs) with a fine-tuning-based essay scoring model using a smaller large language model (S-LLM). RMTS uses an LLM-based trait-wise rationale generation system where a separate LLM agent generates trait-specific rationales based on rubric guidelines, which the scoring model uses to accurately predict multi-trait scores. Extensive experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize, show that RMTS significantly outperforms state-of-the-art models and vanilla S-LLMs in trait-specific scoring. By assisting quantitative assessment with fine-grained qualitative rationales, RMTS enhances the trait-wise reliability, providing partial explanations about essays.</li>
<li><strong>摘要：</strong>现有的自动作文评分 (AES) 仅依赖于作文文本，而不使用分数的解释性原理，从而放弃了以细粒度的方式捕捉评分标准指标评估的具体方面的机会。本文介绍了基于原理的多特质作文评分 (RMTS)，这是一种新颖的多特质作文评分方法，它将基于提示工程的大型语言模型 (LLM) 与使用较小大型语言模型 (S-LLM) 的基于微调的作文评分模型相结合。RMTS 使用基于 LLM 的特质原理生成系统，其中单独的 LLM 代理根据评分标准指南生成特质特定原理，评分模型使用这些原理准确预测多特质分数。在基准数据集（包括 ASAP、ASAP++ 和 Feedback Prize）上进行的大量实验表明，RMTS 在特质特定评分方面明显优于最先进的模型和原始 S-LLM。通过使用细粒度的定性原理辅助定量评估，RMTS 提高了特征可靠性，并为论文提供了部分解释。</li>
</ul>

<h3>Title: Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiaochuan Li, Zichun Yu, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14208">https://arxiv.org/abs/2410.14208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14208">https://arxiv.org/pdf/2410.14208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14208]] Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning(https://arxiv.org/abs/2410.14208)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Synthetic data has been widely used to train large language models, but their generative nature inevitably introduces noisy, non-informative, and misleading learning signals. In this paper, we propose Montessori-Instruct, a novel data synthesis framework that tailors the data synthesis ability of the teacher language model toward the student language model's learning process. Specifically, we utilize local data influence of synthetic training data points on students to characterize students' learning preferences. Then, we train the teacher model with Direct Preference Optimization (DPO) to generate synthetic data tailored toward student learning preferences. Experiments with Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and MT-Bench demonstrate that Montessori-Instruct significantly outperforms standard synthesis methods by 18.35\% and 46.24\% relatively. Our method also beats data synthesized by a stronger teacher model, GPT-4o. Further analysis confirms the benefits of teacher's learning to generate more influential training data in the student's improved learning, the advantages of local data influence in accurately measuring student preferences, and the robustness of Montessori-Instruct across different student models. Our code and data are open-sourced at this https URL.</li>
<li><strong>摘要：</strong>合成数据已广泛用于训练大型语言模型，但其生成性质不可避免地会引入嘈杂、无信息和误导性的学习信号。在本文中，我们提出了 Montessori-Instruct，这是一种新颖的数据合成框架，可将教师语言模型的数据合成能力定制为学生语言模型的学习过程。具体来说，我们利用合成训练数据点对学生的局部数据影响来表征学生的学习偏好。然后，我们使用直接偏好优化 (DPO) 训练教师模型，以生成针对学生学习偏好定制的合成数据。在 Alpaca Eval 和 MT-Bench 上使用 Llama3-8B-Instruct（老师）和 Llama3-8B（学生）进行的实验表明，Montessori-Instruct 的表现明显优于标准合成方法，相对分别高出 18.35% 和 46.24%。我们的方法也胜过更强大的教师模型 GPT-4o 合成的数据。进一步的分析证实了教师学习在学生学习改进中产生更具影响力的训练数据的好处、本地数据影响力在准确衡量学生偏好方面的优势以及 Montessori-Instruct 在不同学生模型中的稳健性。我们的代码和数据在此 https URL 上开源。</li>
</ul>

<h3>Title: Paths-over-Graph: Knowledge Graph Enpowered Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Wenjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14211">https://arxiv.org/abs/2410.14211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14211">https://arxiv.org/pdf/2410.14211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14211]] Paths-over-Graph: Knowledge Graph Enpowered Large Language Model Reasoning(https://arxiv.org/abs/2410.14211)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved impressive results in various tasks but struggle with hallucination problems and lack of relevant knowledge, especially in deep complex reasoning and knowledge-intensive tasks. Knowledge Graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. However, existing KG-based LLM reasoning methods face challenges like handling multi-hop reasoning, multi-entity questions, and effectively utilizing graph structures. To address these issues, we propose Paths-over-Graph (PoG), a novel method that enhances LLM reasoning by integrating knowledge reasoning paths from KGs, improving the interpretability and faithfulness of LLM outputs. PoG tackles multi-hop and multi-entity questions through a three-phase dynamic multi-hop path exploration, which combines the inherent knowledge of LLMs with factual knowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant information from the graph exploration first and introduces efficient three-step pruning techniques that incorporate graph structures, LLM prompting, and a pre-trained language model (e.g., SBERT) to effectively narrow down the explored candidate paths. This ensures all reasoning paths contain highly relevant information captured from KGs, making the reasoning faithful and interpretable in problem-solving. PoG innovatively utilizes graph structure to prune the irrelevant noise and represents the first method to implement multi-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive experiments on five benchmark KGQA datasets demonstrate PoG outperforms the state-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an average accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo surpasses ToG with GPT-4 by up to 23.9%.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种任务中取得了令人瞩目的成果，但在幻觉问题和缺乏相关知识方面仍存在困难，尤其是在深度复杂推理和知识密集型任务中。知识图谱 (KG) 以结构化格式捕获大量事实，为推理提供了可靠的知识来源。然而，现有的基于 KG 的 LLM 推理方法面临着处理多跳推理、多实体问题以及有效利用图结构等挑战。为了解决这些问题，我们提出了 Paths-over-Graph (PoG)，这是一种通过整合来自 KG 的知识推理路径来增强 LLM 推理的新方法，提高了 LLM 输出的可解释性和忠实度。PoG 通过三阶段动态多跳路径探索来解决多跳和多实体问题，它将 LLM 的固有知识与来自 KG 的事实知识相结合。为了提高效率，PoG 首先从图探索中修剪不相关的信息，并引入了高效的三步修剪技术，该技术结合了图结构、LLM 提示和预训练语言模型（例如 SBERT），以有效缩小探索的候选路径。这确保了所有推理路径都包含从 KG 中捕获的高度相关信息，从而使推理在解决问题时忠实且可解释。PoG 创新地利用图结构来修剪不相关的噪音，并代表了第一种在 KG 上为 LLM 推理任务实现多实体深度路径检测的方法。在五个基准 KGQA 数据集上的综合实验表明，PoG 在 GPT-3.5-Turbo 和 GPT-4 上的表现优于最先进的方法 ToG，平均准确率提高了 18.9%。值得注意的是，使用 GPT-3.5-Turbo 的 PoG 比使用 GPT-4 的 ToG 高出 23.9%。</li>
</ul>

<h3>Title: Few-Shot Joint Multimodal Entity-Relation Extraction via Knowledge-Enhanced Cross-modal Prompt Model</h3>
<ul>
<li><strong>Authors: </strong>Li Yuan, Yi Cai, Junsheng Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14225">https://arxiv.org/abs/2410.14225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14225">https://arxiv.org/pdf/2410.14225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14225]] Few-Shot Joint Multimodal Entity-Relation Extraction via Knowledge-Enhanced Cross-modal Prompt Model(https://arxiv.org/abs/2410.14225)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task that aims to extract entities and their relations from text-image pairs in social media posts. Existing methods for JMERE require large amounts of labeled data. However, gathering and annotating fine-grained multimodal data for JMERE poses significant challenges. Initially, we construct diverse and comprehensive multimodal few-shot datasets fitted to the original data distribution. To address the insufficient information in the few-shot setting, we introduce the \textbf{K}nowledge-\textbf{E}nhanced \textbf{C}ross-modal \textbf{P}rompt \textbf{M}odel (KECPM) for JMERE. This method can effectively address the problem of insufficient information in the few-shot setting by guiding a large language model to generate supplementary background knowledge. Our proposed method comprises two stages: (1) a knowledge ingestion stage that dynamically formulates prompts based on semantic similarity guide ChatGPT generating relevant knowledge and employs self-reflection to refine the knowledge; (2) a knowledge-enhanced language model stage that merges the auxiliary knowledge with the original input and utilizes a transformer-based model to align with JMERE's required output format. We extensively evaluate our approach on a few-shot dataset derived from the JMERE dataset, demonstrating its superiority over strong baselines in terms of both micro and macro F$_1$ scores. Additionally, we present qualitative analyses and case studies to elucidate the effectiveness of our model.</li>
<li><strong>摘要：</strong>联合多模态实体关系提取 (JMERE) 是一项具有挑战性的任务，旨在从社交媒体帖子中的文本-图像对中提取实体及其关系。现有的 JMERE 方法需要大量标记数据。然而，为 JMERE 收集和注释细粒度多模态数据带来了重大挑战。最初，我们构建了与原始数据分布相适应的多样化、全面的多模态小样本数据集。为了解决小样本设置中的信息不足问题，我们为 JMERE 引入了 \textbf{K}nowledge-\textbf{E}nhanced \textbf{C}crosss-modal \textbf{P}rompt \textbf{M}odel (KECPM)。该方法通过引导大型语言模型生成补充背景知识，可以有效解决小样本设置中信息不足的问题。我们提出的方法包括两个阶段：（1）知识摄取阶段，该阶段基于语义相似性指南 ChatGPT 动态制定提示，生成相关知识并采用自我反思来完善知识；（2）知识增强语言模型阶段，该阶段将辅助知识与原始输入合并，并利用基于转换器的模型与 JMERE 所需的输出格式保持一致。我们在从 JMERE 数据集派生的少样本数据集上对我们的方法进行了广泛的评估，证明了它在微观和宏观 F$_1$ 分数方面优于强基线。此外，我们还提供了定性分析和案例研究，以阐明我们模型的有效性。</li>
</ul>

<h3>Title: Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework</h3>
<ul>
<li><strong>Authors: </strong>Zhen Tao, Zhiyu Li, Runyu Chen, Dinghao Xi, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14231">https://arxiv.org/abs/2410.14231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14231">https://arxiv.org/pdf/2410.14231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14231]] Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework(https://arxiv.org/abs/2410.14231)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have transformed human writing by enhancing grammar correction, content expansion, and stylistic refinement. However, their widespread use raises concerns about authorship, originality, and ethics, even potentially threatening scholarly integrity. Existing detection methods, which mainly rely on single-feature analysis and binary classification, often fail to effectively identify LLM-generated text in academic contexts. To address these challenges, we propose a novel Multi-level Fine-grained Detection (MFD) framework that detects LLM-generated text by integrating low-level structural, high-level semantic, and deep-level linguistic features, while conducting sentence-level evaluations of lexicon, grammar, and syntax for comprehensive analysis. To improve detection of subtle differences in LLM-generated text and enhance robustness against paraphrasing, we apply two mainstream evasion techniques to rewrite the text. These variations, along with original texts, are used to train a text encoder via contrastive learning, extracting high-level semantic features of sentence to boost detection generalization. Furthermore, we leverage advanced LLM to analyze the entire text and extract deep-level linguistic features, enhancing the model's ability to capture complex patterns and nuances while effectively incorporating contextual information. Extensive experiments on public datasets show that the MFD model outperforms existing methods, achieving an MAE of 0.1346 and an accuracy of 88.56%. Our research provides institutions and publishers with an effective mechanism to detect LLM-generated text, mitigating risks of compromised authorship. Educators and editors can use the model's predictions to refine verification and plagiarism prevention protocols, ensuring adherence to standards.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通过增强语法校正、内容扩展和文体细化，改变了人类的写作。然而，它们的广泛使用引发了人们对作者身份、原创性和道德的担忧，甚至可能威胁到学术诚信。现有的检测方法主要依赖于单特征分析和二元分类，通常无法在学术环境中有效识别 LLM 生成的文本。为了应对这些挑战，我们提出了一种新颖的多级细粒度检测 (MFD) 框架，该框架通过整合低级结构、高级语义和深层语言特征来检测 LLM 生成的文本，同时对词汇、语法和句法进行句子级评估以进行全面分析。为了提高对 LLM 生成的文本中细微差异的检测能力并增强对释义的鲁棒性，我们应用了两种主流的逃避技术来重写文本。这些变体与原始文本一起用于通过对比学习训练文本编码器，提取句子的高级语义特征以提高检测泛化能力。此外，我们利用先进的 LLM 分析整个文本并提取深层语言特征，增强模型捕捉复杂模式和细微差别的能力，同时有效地整合上下文信息。在公共数据集上进行的大量实验表明，MFD 模型优于现有方法，MAE 达到 0.1346，准确率达到 88.56%。我们的研究为机构和出版商提供了一种有效的机制来检测 LLM 生成的文本，从而降低作者身份受损的风险。教育工作者和编辑可以使用模型的预测来改进验证和剽窃预防协议，确保遵守标准。</li>
</ul>

<h3>Title: Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Arora, Srujana Merugu, Shreya Jain, Vaibhav Saxena</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14235">https://arxiv.org/abs/2410.14235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14235">https://arxiv.org/pdf/2410.14235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14235]] Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning(https://arxiv.org/abs/2410.14235)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reasoning and linguistic skills form the cornerstone of human intelligence, facilitating problem-solving and decision-making. Recent advances in Large Language Models (LLMs) have led to impressive linguistic capabilities and emergent reasoning behaviors, fueling widespread adoption across application domains. However, LLMs still struggle with complex reasoning tasks, highlighting their systemic limitations. In this work, we focus on evaluating whether LLMs have the requisite representations to reason using two foundational relationships: "equivalence" and "inheritance". We introduce novel tasks and benchmarks spanning six languages and observe that current SOTA LLMs often produce conflicting answers to the same questions across languages in 17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases. To enhance consistency across languages, we propose novel "Compositional Representations" where tokens are represented as composition of equivalent tokens across languages, with resulting conflict reduction (up to -4.7%) indicating benefits of shared LLM representations.</li>
<li><strong>摘要：</strong>推理和语言技能是人类智能的基石，有助于解决问题和决策。大型语言模型 (LLM) 的最新进展带来了令人印象深刻的语言能力和新兴的推理行为，推动了应用领域的广泛采用。然而，LLM 仍然难以完成复杂的推理任务，这凸显了它们的系统局限性。在这项工作中，我们专注于评估 LLM 是否具有使用两个基本关系进行推理的必要表示：“等价”和“继承”。我们引入了涵盖六种语言的新任务和基准，并观察到当前的 SOTA LLM 在 17.3-57.5% 的情况下经常在不同语言中对同一问题产生相互冲突的答案，并且在高达 37.2% 的情况下违反继承约束。为了增强跨语言的一致性，我们提出了新颖的“组合表示”，其中标记表示为跨语言等效标记的组合，从而减少了冲突（高达 -4.7%），这表明共享 LLM 表示的好处。</li>
</ul>

<h3>Title: Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models</h3>
<ul>
<li><strong>Authors: </strong>Olga Loginova, Oleksandr Bezrukov, Alexey Kravets</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14248">https://arxiv.org/abs/2410.14248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14248">https://arxiv.org/pdf/2410.14248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14248]] Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models(https://arxiv.org/abs/2410.14248)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Evaluating Video Language Models (VLMs) is a challenging task. Due to its transparency, Multiple-Choice Question Answering (MCQA) is widely used to measure the performance of these models through accuracy. However, existing MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to selection bias, when models disproportionately favor certain answer options based on positional patterns observed during training. In this work, we conduct a comprehensive empirical analysis of several VLM architectures across major datasets designed to assess complex video-focused reasoning. We identify where the bias is most pronounced and demonstrate to what extent model responses reflect genuine understanding of video content and related questions, as opposed to reliance on arbitrary patterns or superficial cues, such as answer position. By decomposing the MCQA task and adapting fairness bias metrics to VLMs, we introduce a post-processing calibration technique BOLD to balance this bias. Our results show that reducing selection bias improves not only debiasing metrics but also overall model performance, including Accuracy and F1 Mean score. Our method, by suppressing "blind guessing", offers a more cost- and time-effective approach to mitigating selection bias compared to existing techniques. This study represents the first focused investigation of selection bias in video-to-text LLM-powered models.</li>
<li><strong>摘要：</strong>评估视频语言模型 (VLM) 是一项具有挑战性的任务。由于其透明度，多项选择题问答 (MCQA) 被广泛用于通过准确性来衡量这些模型的性能。然而，现有的 MCQA 基准测试无法捕捉 VLM 的全部推理能力，因为选择偏差会导致模型根据训练期间观察到的位置模式不成比例地偏向某些答案选项。在这项工作中，我们对几个 VLM 架构进行了全面的实证分析，这些架构涵盖了旨在评估复杂的以视频为中心的推理的主要数据集。我们确定了偏差最明显的地方，并展示了模型响应在多大程度上反映了对视频内容和相关问题的真正理解，而不是依赖于任意模式或表面线索，例如答案位置。通过分解 MCQA 任务并将公平偏差指标调整到 VLM，我们引入了一种后处理校准技术 BOLD 来平衡这种偏差。我们的结果表明，减少选择偏差不仅可以提高去偏差指标，还可以提高整体模型性能，包括准确度和 F1 平均分数。我们的方法通过抑制“盲目猜测”，提供了一种比现有技术更经济、更省时的方法来减轻选择偏差。这项研究是首次针对视频转文本 LLM 模型中的选择偏差进行重点研究。</li>
</ul>

<h3>Title: Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via Role Recognition and Involvement Measurement</h3>
<ul>
<li><strong>Authors: </strong>Zihao Cheng, Li Zhou, Feng Jiang, Benyou Wang, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14259">https://arxiv.org/abs/2410.14259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14259">https://arxiv.org/pdf/2410.14259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14259]] Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via Role Recognition and Involvement Measurement(https://arxiv.org/abs/2410.14259)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The rapid development of large language models (LLMs), like ChatGPT, has resulted in the widespread presence of LLM-generated content on social media platforms, raising concerns about misinformation, data biases, and privacy violations, which can undermine trust in online discourse. While detecting LLM-generated content is crucial for mitigating these risks, current methods often focus on binary classification, failing to address the complexities of real-world scenarios like human-AI collaboration. To move beyond binary classification and address these challenges, we propose a new paradigm for detecting LLM-generated content. This approach introduces two novel tasks: LLM Role Recognition (LLM-RR), a multi-class classification task that identifies specific roles of LLM in content generation, and LLM Influence Measurement (LLM-IM), a regression task that quantifies the extent of LLM involvement in content creation. To support these tasks, we propose LLMDetect, a benchmark designed to evaluate detectors' performance on these new tasks. LLMDetect includes the Hybrid News Detection Corpus (HNDC) for training detectors, as well as DetectEval, a comprehensive evaluation suite that considers five distinct cross-context variations and multi-intensity variations within the same LLM role. This allows for a thorough assessment of detectors' generalization and robustness across diverse contexts. Our empirical validation of 10 baseline detection methods demonstrates that fine-tuned PLM-based models consistently outperform others on both tasks, while advanced LLMs face challenges in accurately detecting their own generated content. Our experimental results and analysis offer insights for developing more effective detection models for LLM-generated content. This research enhances the understanding of LLM-generated content and establishes a foundation for more nuanced detection methodologies.</li>
<li><strong>摘要：</strong>ChatGPT 等大型语言模型 (LLM) 的快速发展导致 LLM 生成的内容在社交媒体平台上广泛存在，引发了人们对错误信息、数据偏见和隐私侵犯的担忧，这些可能会破坏人们对在线话语的信任。虽然检测 LLM 生成的内容对于减轻这些风险至关重要，但当前的方法通常侧重于二元分类，无法解决人机协作等现实场景的复杂性。为了超越二元分类并应对这些挑战，我们提出了一种用于检测 LLM 生成内容的新范式。这种方法引入了两个新任务：LLM 角色识别 (LLM-RR)，这是一项多类分类任务，可识别 LLM 在内容生成中的特定角色；LLM 影响力测量 (LLM-IM)，这是一项回归任务，可量化 LLM 在内容创建中的参与程度。为了支持这些任务，我们提出了 LLMDetect，这是一个旨在评估检测器在这些新任务上的表现的基准。 LLMDetect 包括用于训练检测器的混合新闻检测语料库 (HNDC)，以及 DetectEval，这是一个综合评估套件，它考虑了同一 LLM 角色中的五种不同的跨上下文变化和多强度变化。这可以全面评估检测器在不同上下文中的泛化和稳健性。我们对 10 种基线检测方法的实证验证表明，经过微调的基于 PLM 的模型在两个任务上始终优于其他模型，而高级 LLM 在准确检测自己生成的内容方面面临挑战。我们的实验结果和分析为开发更有效的 LLM 生成内容检测模型提供了见解。这项研究增强了对 LLM 生成内容的理解，并为更细致入微的检测方法奠定了基础。</li>
</ul>

<h3>Title: MoDification: Mixture of Depths Made Easy</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhang, Meizhi Zhong, Qimeng Wang, Xuantao Lu, Zheyu Ye, Chengqiang Lu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang, Dawei Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14268">https://arxiv.org/abs/2410.14268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14268">https://arxiv.org/pdf/2410.14268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14268]] MoDification: Mixture of Depths Made Easy(https://arxiv.org/abs/2410.14268)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Long-context efficiency has recently become a trending topic in serving large language models (LLMs). And mixture of depths (MoD) is proposed as a perfect fit to bring down both latency and memory. In this paper, however, we discover that MoD can barely transform existing LLMs without costly training over an extensive number of tokens. To enable the transformations from any LLMs to MoD ones, we showcase top-k operator in MoD should be promoted to threshold-p operator, and refinement to architecture and data should also be crafted along. All these designs form our method termed MoDification. Through a comprehensive set of experiments covering model scales from 3B to 70B, we exhibit MoDification strikes an excellent balance between efficiency and effectiveness. MoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in memory compared to original LLMs especially in long-context applications.</li>
<li><strong>摘要：</strong>长上下文效率最近成为服务大型语言模型 (LLM) 的热门话题。而混合深度 (MoD) 被提出作为降低延迟和内存的完美选择。然而，在本文中，我们发现 MoD 几乎无法转换现有的 LLM，除非对大量 token 进行昂贵的训练。为了实现从任何 LLM 到 MoD 的转换，我们展示了 MoD 中的 top-k 运算符应该提升为阈值-p 运算符，并且还应该对架构和数据进行细化。所有这些设计形成了我们的方法，称为 MoDification。通过一套全面的实验，涵盖从 3B 到 70B 的模型规模，我们表明 MoDification 在效率和有效性之间取得了极好的平衡。与原始 LLM 相比，MoDification 可以实现高达约 1.2 倍的延迟加速和约 1.8 倍的内存减少，尤其是在长上下文应用中。</li>
</ul>

<h3>Title: REEF: Representation Encoding Fingerprints for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14273">https://arxiv.org/abs/2410.14273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14273">https://arxiv.org/pdf/2410.14273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14273]] REEF: Representation Encoding Fingerprints for Large Language Models(https://arxiv.org/abs/2410.14273)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Protecting the intellectual property of open-source Large Language Models (LLMs) is very important, because training LLMs costs extensive computational resources and data. Therefore, model owners and third parties need to identify whether a suspect model is a subsequent development of the victim model. To this end, we propose a training-free REEF to identify the relationship between the suspect and victim models from the perspective of LLMs' feature representations. Specifically, REEF computes and compares the centered kernel alignment similarity between the representations of a suspect model and a victim model on the same samples. This training-free REEF does not impair the model's general capabilities and is robust to sequential fine-tuning, pruning, model merging, and permutations. In this way, REEF provides a simple and effective way for third parties and models' owners to protect LLMs' intellectual property together. The code is available at this https URL.</li>
<li><strong>摘要：</strong>保护开源大型语言模型 (LLM) 的知识产权非常重要，因为训练 LLM 需要大量的计算资源和数据。因此，模型所有者和第三方需要识别可疑模型是否是受害者模型的后续开发。为此，我们提出了一种无需训练的 REEF，从 LLM 的特征表示角度识别可疑模型和受害者模型之间的关系。具体来说，REEF 计算并比较同一样本上可疑模型和受害者模型的表示之间的中心核对齐相似度。这种无需训练的 REEF 不会损害模型的一般能力，并且对顺序微调、修剪、模型合并和排列具有鲁棒性。通过这种方式，REEF 为第三方和模型所有者提供了一种简单有效的方法来共同保护 LLM 的知识产权。代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: EcomEdit: An Automated E-commerce Knowledge Editing Framework for Enhanced Product and Purchase Intention Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ching Ming Samuel Lau, Weiqi Wang, Haochen Shi, Baixuan Xu, Jiaxin Bai, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14276">https://arxiv.org/abs/2410.14276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14276">https://arxiv.org/pdf/2410.14276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14276]] EcomEdit: An Automated E-commerce Knowledge Editing Framework for Enhanced Product and Purchase Intention Understanding(https://arxiv.org/abs/2410.14276)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge Editing (KE) aims to correct and update factual information in Large Language Models (LLMs) to ensure accuracy and relevance without computationally expensive fine-tuning. Though it has been proven effective in several domains, limited work has focused on its application within the e-commerce sector. However, there are naturally occurring scenarios that make KE necessary in this domain, such as the timely updating of product features and trending purchase intentions by customers, which necessitate further exploration. In this paper, we pioneer the application of KE in the e-commerce domain by presenting ECOMEDIT, an automated e-commerce knowledge editing framework tailored for e-commerce-related knowledge and tasks. Our framework leverages more powerful LLMs as judges to enable automatic knowledge conflict detection and incorporates conceptualization to enhance the semantic coverage of the knowledge to be edited. Through extensive experiments, we demonstrate the effectiveness of ECOMEDIT in improving LLMs' understanding of product descriptions and purchase intentions. We also show that LLMs, after our editing, can achieve stronger performance on downstream e-commerce tasks.</li>
<li><strong>摘要：</strong>知识编辑 (KE) 旨在纠正和更新大型语言模型 (LLM) 中的事实信息，以确保准确性和相关性，而无需进行计算成本高昂的微调。尽管它已被证明在多个领域有效，但有限的工作集中在其在电子商务领域的应用上。然而，在这个领域，有一些自然发生的场景使得 KE 成为必要，例如及时更新产品功能和客户趋势购买意向，这需要进一步探索。在本文中，我们通过介绍 ECOMEDIT 开创了 KE 在电子商务领域的应用，这是一个针对电子商务相关知识和任务量身定制的自动化电子商务知识编辑框架。我们的框架利用更强大的 LLM 作为判断者来实现自动知识冲突检测，并结合概念化来增强要编辑的知识的语义覆盖范围。通过大量实验，我们证明了 ECOMEDIT 在提高 LLM 对产品描述和购买意向的理解方面的有效性。我们还表明，经过我们的编辑，LLM 可以在下游电子商务任务上实现更强大的性能。</li>
</ul>

<h3>Title: SwaQuAD-24: QA Benchmark Dataset in Swahili</h3>
<ul>
<li><strong>Authors: </strong>Alfred Malengo Kondoro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14289">https://arxiv.org/abs/2410.14289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14289">https://arxiv.org/pdf/2410.14289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14289]] SwaQuAD-24: QA Benchmark Dataset in Swahili(https://arxiv.org/abs/2410.14289)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>This paper proposes the creation of a Swahili Question Answering (QA) benchmark dataset, aimed at addressing the underrepresentation of Swahili in natural language processing (NLP). Drawing from established benchmarks like SQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing high-quality, annotated question-answer pairs that capture the linguistic diversity and complexity of Swahili. The dataset is designed to support a variety of applications, including machine translation, information retrieval, and social services like healthcare chatbots. Ethical considerations, such as data privacy, bias mitigation, and inclusivity, are central to the dataset development. Additionally, the paper outlines future expansion plans to include domain-specific content, multimodal integration, and broader crowdsourcing efforts. The Swahili QA dataset aims to foster technological innovation in East Africa and provide an essential resource for NLP research and applications in low-resource languages.</li>
<li><strong>摘要：</strong>本文提议创建一个斯瓦希里语问答 (QA) 基准数据集，旨在解决斯瓦希里语在自然语言处理 (NLP) 中代表性不足的问题。该数据集借鉴了 SQuAD、GLUE、KenSwQuAD 和 KLUE 等成熟基准，专注于提供高质量、带注释的问答对，以捕捉斯瓦希里语的语言多样性和复杂性。该数据集旨在支持各种应用，包括机器翻译、信息检索和医疗聊天机器人等社会服务。数据隐私、偏见缓解和包容性等道德考虑是数据集开发的核心。此外，本文概述了未来的扩展计划，包括领域特定内容、多模式集成和更广泛的众包工作。斯瓦希里语 QA 数据集旨在促进东非的技术创新，并为资源匮乏语言的 NLP 研究和应用提供重要资源。</li>
</ul>

<h3>Title: LoGU: Long-form Generation with Uncertainty Expressions</h3>
<ul>
<li><strong>Authors: </strong>Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Sen Yang, Nigel Collier, Dong Yu, Deqing Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14309">https://arxiv.org/abs/2410.14309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14309">https://arxiv.org/pdf/2410.14309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14309]] LoGU: Long-form Generation with Uncertainty Expressions(https://arxiv.org/abs/2410.14309)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) demonstrate impressive capabilities, they still struggle with generating factually incorrect content (i.e., hallucinations). A promising approach to mitigate this issue is enabling models to express uncertainty when unsure. Previous research on uncertainty modeling has primarily focused on short-form QA, but realworld applications often require much longer responses. In this work, we introduce the task of Long-form Generation with Uncertainty(LoGU). We identify two key challenges: Uncertainty Suppression, where models hesitate to express uncertainty, and Uncertainty Misalignment, where models convey uncertainty inaccurately. To tackle these challenges, we propose a refinement-based data collection framework and a two-stage training pipeline. Our framework adopts a divide-and-conquer strategy, refining uncertainty based on atomic claims. The collected data are then used in training through supervised fine-tuning (SFT) and direct preference optimization (DPO) to enhance uncertainty expression. Extensive experiments on three long-form instruction following datasets show that our method significantly improves accuracy, reduces hallucinations, and maintains the comprehensiveness of responses.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 表现出令人印象深刻的能力，但它们仍然难以生成事实上不正确的内容（即幻觉）。缓解此问题的一个有希望的方法是让模型在不确定时表达不确定性。以前对不确定性建模的研究主要集中在短格式问答上，但现实世界的应用通常需要更长的响应。在这项工作中，我们介绍了具有不确定性的长格式生成 (LoGU) 任务。我们确定了两个关键挑战：不确定性抑制，其中模型不愿表达不确定性，以及不确定性错位，其中模型不准确地传达不确定性。为了应对这些挑战，我们提出了一个基于细化的数据收集框架和一个两阶段训练管道。我们的框架采用分而治之的策略，根据原子声明细化不确定性。然后通过监督微调 (SFT) 和直接偏好优化 (DPO) 将收集的数据用于训练，以增强不确定性表达。对三个长篇指导跟踪数据集进行的大量实验表明，我们的方法显著提高了准确性，减少了幻觉，并保持了反应的全面性。</li>
</ul>

<h3>Title: Critical Questions Generation: Motivation and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Blanca Calvo Figueras, Rodrigo Agerri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14335">https://arxiv.org/abs/2410.14335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14335">https://arxiv.org/pdf/2410.14335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14335]] Critical Questions Generation: Motivation and Challenges(https://arxiv.org/abs/2410.14335)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The development of Large Language Models (LLMs) has brought impressive performances on mitigation strategies against misinformation, such as counterargument generation. However, LLMs are still seriously hindered by outdated knowledge and by their tendency to generate hallucinated content. In order to circumvent these issues, we propose a new task, namely, Critical Questions Generation, consisting of processing an argumentative text to generate the critical questions (CQs) raised by it. In argumentation theory CQs are tools designed to lay bare the blind spots of an argument by pointing at the information it could be missing. Thus, instead of trying to deploy LLMs to produce knowledgeable and relevant counterarguments, we use them to question arguments, without requiring any external knowledge. Research on CQs Generation using LLMs requires a reference dataset for large scale experimentation. Thus, in this work we investigate two complementary methods to create such a resource: (i) instantiating CQs templates as defined by Walton's argumentation theory and (ii), using LLMs as CQs generators. By doing so, we contribute with a procedure to establish what is a valid CQ and conclude that, while LLMs are reasonable CQ generators, they still have a wide margin for improvement in this task.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的发展在缓解错误信息的策略（例如反驳生成）方面取得了令人印象深刻的表现。然而，过时的知识和产生幻觉内容的倾向仍然严重阻碍了 LLM 的发展。为了规避这些问题，我们提出了一项新任务，即关键问题生成，包括处理论证文本以生成由其提出的关键问题 (CQ)。在论证理论中，CQ 是一种旨在通过指出论证可能缺失的信息来揭示论证盲点的工具。因此，我们不是试图部署 LLM 来产生知识渊博且相关的反驳，而是使用它们来质疑论证，而无需任何外部知识。使用 LLM 进行 CQ 生成的研究需要参考数据集来进行大规模实验。因此，在这项工作中，我们研究了两种互补的方法来创建这样的资源：(i) 实例化 Walton 论证理论定义的 CQ 模板和 (ii) 使用 LLM 作为 CQ 生成器。通过这样做，我们提供了一个程序来确定什么是有效的 CQ，并得出结论：虽然 LLM 是合理的 CQ 生成器，但它们在这个任务上仍然有很大的改进空间。</li>
</ul>

<h3>Title: Efficiently Computing Susceptibility to Context in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Liu, Kevin Du, Mrinmaya Sachan, Ryan Cotterell</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14361">https://arxiv.org/abs/2410.14361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14361">https://arxiv.org/pdf/2410.14361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14361]] Efficiently Computing Susceptibility to Context in Language Models(https://arxiv.org/abs/2410.14361)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>One strength of modern language models is their ability to incorporate information from a user-input context when answering queries. However, they are not equally sensitive to the subtle changes to that context. To quantify this, Du et al. (2024) gives an information-theoretic metric to measure such sensitivity. Their metric, susceptibility, is defined as the degree to which contexts can influence a model's response to a query at a distributional level. However, exactly computing susceptibility is difficult and, thus, Du et al. (2024) falls back on a Monte Carlo approximation. Due to the large number of samples required, the Monte Carlo approximation is inefficient in practice. As a faster alternative, we propose Fisher susceptibility, an efficient method to estimate the susceptibility based on Fisher information. Empirically, we validate that Fisher susceptibility is comparable to Monte Carlo estimated susceptibility across a diverse set of query domains despite its being $70\times$ faster. Exploiting the improved efficiency, we apply Fisher susceptibility to analyze factors affecting the susceptibility of language models. We observe that larger models are as susceptible as smaller ones.</li>
<li><strong>摘要：</strong>现代语言模型的优势之一是它们能够在回答查询时整合来自用户输入上下文的信息。但是，它们对上下文的细微变化并不同样敏感。为了量化这一点，Du 等人 (2024) 给出了一个信息论指标来衡量这种敏感性。他们的指标，敏感度，被定义为上下文在分布层面上影响模型对查询的响应的程度。然而，准确计算敏感度很困难，因此，Du 等人 (2024) 回归到蒙特卡洛近似。由于需要大量样本，蒙特卡洛近似在实践中效率低下。作为一种更快的替代方案，我们提出了 Fisher 敏感度，这是一种基于 Fisher 信息估计敏感度的有效方法。从经验上讲，我们验证了 Fisher 敏感度与蒙特卡洛估计的敏感度在一系列不同的查询域中相当，尽管它快了 70 倍。利用改进的效率，我们应用 Fisher 敏感度来分析影响语言模型敏感度的因素。我们观察到较大的模型与较小的模型一样容易受到影响。</li>
</ul>

<h3>Title: How Do Multilingual Models Remember? Investigating Multilingual Factual Recall Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Constanza Fierro, Negar Foroutan, Desmond Elliott, Anders Søgaard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14387">https://arxiv.org/abs/2410.14387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14387">https://arxiv.org/pdf/2410.14387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14387]] How Do Multilingual Models Remember? Investigating Multilingual Factual Recall Mechanisms(https://arxiv.org/abs/2410.14387)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) store and retrieve vast amounts of factual knowledge acquired during pre-training. Prior research has localized and identified mechanisms behind knowledge recall; however, it has primarily focused on English monolingual models. The question of how these processes generalize to other languages and multilingual LLMs remains unexplored. In this paper, we address this gap by conducting a comprehensive analysis of two highly multilingual LLMs. We assess the extent to which previously identified components and mechanisms of factual recall in English apply to a multilingual context. Then, we examine when language plays a role in the recall process, uncovering evidence of language-independent and language-dependent mechanisms.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 存储和检索在预训练期间获得的大量事实知识。先前的研究已经定位和确定了知识回忆背后的机制；然而，它主要集中在英语单语模型上。这些过程如何推广到其他语言和多语言 LLM 的问题仍未得到探索。在本文中，我们通过对两个高度多语言的 LLM 进行全面分析来解决这一空白。我们评估了先前确定的英语事实回忆的组成部分和机制在多语言环境中的适用程度。然后，我们研究语言在回忆过程中发挥作用的时间，揭示了语言独立和语言相关机制的证据。</li>
</ul>

<h3>Title: Analyzing Context Utilization of LLMs in Document-Level Translation</h3>
<ul>
<li><strong>Authors: </strong>Wafaa Mohammed, Vlad Niculae</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14391">https://arxiv.org/abs/2410.14391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14391">https://arxiv.org/pdf/2410.14391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14391]] Analyzing Context Utilization of LLMs in Document-Level Translation(https://arxiv.org/abs/2410.14391)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLM) are increasingly strong contenders in machine translation. We study document-level translation, where some words cannot be translated without context from outside the sentence. We investigate the ability of prominent LLMs to utilize context by analyzing models' robustness to perturbed and randomized document context. We find that LLMs' improved document-translation performance is not always reflected in pronoun translation performance. We highlight the need for context-aware finetuning of LLMs with a focus on relevant parts of the context to improve their reliability for document-level translation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是机器翻译领域日益强大的竞争者。我们研究文档级翻译，其中有些单词如果没有句子外部的上下文就无法翻译。我们通过分析模型对受干扰和随机文档上下文的鲁棒性来研究著名 LLM 利用上下文的能力。我们发现 LLM 改进的文档翻译性能并不总是反映在代词翻译性能上。我们强调需要对 LLM 进行上下文感知微调，重点关注上下文的相关部分，以提高其文档级翻译的可靠性。</li>
</ul>

<h3>Title: Generative AI, Pragmatics, and Authenticity in Second Language Learning</h3>
<ul>
<li><strong>Authors: </strong>Robert Godwin-Jones`</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14395">https://arxiv.org/abs/2410.14395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14395">https://arxiv.org/pdf/2410.14395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14395]] Generative AI, Pragmatics, and Authenticity in Second Language Learning(https://arxiv.org/abs/2410.14395)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>There are obvious benefits to integrating generative AI (artificial intelligence) into language learning and teaching. Those include using AI as a language tutor, creating learning materials, or assessing learner output. However, due to how AI systems under-stand human language, based on a mathematical model using statistical probability, they lack the lived experience to be able to use language with the same social aware-ness as humans. Additionally, there are built-in linguistic and cultural biases based on their training data which is mostly in English and predominantly from Western sources. Those facts limit AI suitability for some language learning interactions. Stud-ies have clearly shown that systems such as ChatGPT often do not produce language that is pragmatically appropriate. The lack of linguistic and cultural authenticity has important implications for how AI is integrated into second language acquisition as well as in instruction targeting development of intercultural communication compe-tence.</li>
<li><strong>摘要：</strong>将生成式人工智能 (AI) 融入语言学习和教学有明显的好处。这些好处包括使用 AI 担任语言导师、创建学习材料或评估学习者的输出。然而，由于 AI 系统基于使用统计概率的数学模型来理解人类语言，它们缺乏生活经验，无法像人类一样使用具有相同社会意识的语言。此外，基于它们的训练数据（主要是英语且主要来自西方来源），它们存在内在的语言和文化偏见。这些事实限制了 AI 在某些语言学习互动中的适用性。研究清楚地表明，像 ChatGPT 这样的系统通常不会产生实用上合适的语言。缺乏语言和文化真实性对于如何将 AI 融入第二语言习得以及针对跨文化交流能力发展的教学具有重要意义。</li>
</ul>

<h3>Title: SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Magdalena Wysocka, Danilo S. Carvalho, Oskar Wysocki, Marco Valentino, Andre Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14399">https://arxiv.org/abs/2410.14399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14399">https://arxiv.org/pdf/2410.14399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14399]] SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning(https://arxiv.org/abs/2410.14399)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Syllogistic reasoning is crucial for Natural Language Inference (NLI). This capability is particularly significant in specialized domains such as biomedicine, where it can support automatic evidence interpretation and scientific discovery. This paper presents SylloBio-NLI, a novel framework that leverages external ontologies to systematically instantiate diverse syllogistic arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language Models (LLMs) on identifying valid conclusions and extracting supporting evidence across 28 syllogistic schemes instantiated with human genome pathways. Extensive experiments reveal that biomedical syllogistic reasoning is particularly challenging for zero-shot LLMs, which achieve an average accuracy between 70% on generalized modus ponens and 23% on disjunctive syllogism. At the same time, we found that few-shot prompting can boost the performance of different LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper analysis shows that both techniques exhibit high sensitivity to superficial lexical variations, highlighting a dependency between reliability, models' architecture, and pre-training regime. Overall, our results indicate that, while in-context examples have the potential to elicit syllogistic reasoning in LLMs, existing models are still far from achieving the robustness and consistency required for safe biomedical NLI applications.</li>
<li><strong>摘要：</strong>三段论推理对于自然语言推理 (NLI) 至关重要。这种能力在生物医学等专业领域尤其重要，它可以支持自动证据解释和科学发现。本文介绍了 SylloBio-NLI，这是一个新颖的框架，它利用外部本体系统地实例化生物医学 NLI 的各种三段论论证。我们使用 SylloBio-NLI 来评估大型语言模型 (LLM) 在 28 种以人类基因组路径为实例的三段论方案中识别有效结论和提取支持证据的能力。大量实验表明，生物医学三段论推理对于零样本 LLM 尤其具有挑战性，其在广义肯定前件式上的平均准确率为 70%，在析取三段论上的平均准确率为 23%。同时，我们发现少量提示可以提高不同 LLM 的性能，包括 Gemma (+14%) 和 LLama-3 (+43%)。然而，更深入的分析表明，这两种技术都对表面词汇变化表现出很高的敏感性，突出了可靠性、模型架构和预训练机制之间的依赖关系。总体而言，我们的结果表明，虽然上下文示例有可能在 LLM 中引发三段论推理，但现有模型仍远未达到安全生物医学 NLI 应用所需的稳健性和一致性。</li>
</ul>

<h3>Title: Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion</h3>
<ul>
<li><strong>Authors: </strong>Denitsa Saynova, Lovisa Hagström, Moa Johansson, Richard Johansson, Marco Kuhlmann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14405">https://arxiv.org/abs/2410.14405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14405">https://arxiv.org/pdf/2410.14405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14405]] Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion(https://arxiv.org/abs/2410.14405)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Previous interpretations of language models (LMs) miss important distinctions in how these models process factual information. For example, given the query "Astrid Lindgren was born in" with the corresponding completion "Sweden", no difference is made between whether the prediction was based on having the exact knowledge of the birthplace of the Swedish author or assuming that a person with a Swedish-sounding name was born in Sweden. In this paper, we investigate four different prediction scenarios for which the LM can be expected to show distinct behaviors. These scenarios correspond to different levels of model reliability and types of information being processed - some being less desirable for factual predictions. To facilitate precise interpretations of LMs for fact completion, we propose a model-specific recipe called PrISM for constructing datasets with examples of each scenario based on a set of diagnostic criteria. We apply a popular interpretability method, causal tracing (CT), to the four prediction scenarios and find that while CT produces different results for each scenario, aggregations over a set of mixed examples may only represent the results from the scenario with the strongest measured signal. In summary, we contribute tools for a more granular study of fact completion in language models and analyses that provide a more nuanced understanding of how LMs process fact-related queries.</li>
<li><strong>摘要：</strong>以前对语言模型 (LM) 的解释忽略了这些模型处理事实信息的重要区别。例如，给定查询“Astrid Lindgren 出生于”和相应的补全“瑞典”，预测是基于对瑞典作家出生地的准确了解，还是假设一个名字听起来像瑞典语的人出生在瑞典，两者之间没有区别。在本文中，我们研究了四种不同的预测场景，在这些场景中，LM 可能会表现出不同的行为。这些场景对应于不同级别的模型可靠性和正在处理的信息类型 -​​ 有些场景对于事实预测不太理想。为了便于对 LM 进行精确解释以完成事实，我们提出了一种特定于模型的方法 PrISM，用于根据一组诊断标准构建包含每个场景示例的数据集。我们将一种流行的可解释性方法因果追踪 (CT) 应用于这四种预测场景，发现虽然 CT 会为每种场景产生不同的结果，但对一组混合示例的聚合可能仅表示来自具有最强测量信号的场景的结果。总之，我们提供了工具来更细致地研究语言模型和分析中的事实完成，从而提供了对 LM 如何处理与事实相关的查询的更细致的理解。</li>
</ul>

<h3>Title: Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Meihuizi Jia, Yichao Feng, Luu Anh Tuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14425">https://arxiv.org/abs/2410.14425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14425">https://arxiv.org/pdf/2410.14425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14425]] Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation(https://arxiv.org/abs/2410.14425)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) can bridge the gap between large language models (LLMs) and downstream tasks. However, PEFT has been proven vulnerable to malicious attacks. Research indicates that poisoned LLMs, even after PEFT, retain the capability to activate internalized backdoors when input samples contain predefined triggers. In this paper, we introduce a novel weak-to-strong unlearning algorithm to defend against backdoor attacks based on feature alignment knowledge distillation, named W2SDefense. Specifically, we first train a small-scale language model through full-parameter fine-tuning to serve as the clean teacher model. Then, this teacher model guides the large-scale poisoned student model in unlearning the backdoor, leveraging PEFT. Theoretical analysis suggests that W2SDefense has the potential to enhance the student model's ability to unlearn backdoor features, preventing the activation of the backdoor. We conduct experiments on text classification tasks involving three state-of-the-art language models and three different backdoor attack algorithms. Our empirical results demonstrate the outstanding performance of W2SDefense in defending against backdoor attacks without compromising model performance.</li>
<li><strong>摘要：</strong>参数高效微调 (PEFT) 可以弥补大型语言模型 (LLM) 与下游任务之间的差距。然而，PEFT 已被证明容易受到恶意攻击。研究表明，即使在 PEFT 之后，中毒的 LLM 在输入样本包含预定义触发器时仍保留激活内部后门的能力。在本文中，我们介绍了一种基于特征对齐知识蒸馏的新型弱到强反学习算法，以防御后门攻击，称为 W2SDefense。具体来说，我们首先通过全参数微调训练一个小规模的语言模型作为干净的教师模型。然后，这个教师模型利用 PEFT 指导大规模中毒的学生模型反学习后门。理论分析表明，W2SDefense 有可能增强学生模型反学习后门特征的能力，从而防止后门被激活。我们对涉及三种最先进的语言模型和三种不同的后门攻击算法的文本分类任务进行了实验。我们的实证结果证明了 W2SDefense 在防御后门攻击方面表现出色，同时又不影响模型性能。</li>
</ul>

<h3>Title: A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>You Wu, Haoyi Wu, Kewei Tu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14442">https://arxiv.org/abs/2410.14442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14442">https://arxiv.org/pdf/2410.14442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14442]] A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference(https://arxiv.org/abs/2410.14442)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, sharing key-value (KV) cache across layers has been found effective in efficient inference of large language models (LLMs). To systematically investigate different techniques of cross-layer KV sharing, we propose a unified framework that covers several recent methods and their novel variants. We conduct comprehensive experiments on all the configurations of the framework, evaluating their generation throughput and performance in language modeling and downstream tasks. We find that when reducing the size of the KV cache by 2x, most configurations can achieve competitive performance to and higher throughput than standard transformers, but when further reducing the size of the KV cache, pairing queries of all layers with KVs of upper layers can better maintain performance, although it also introduces additional training cost and prefilling latency. We hope that this work will help users choose the appropriate approach according to their requirements and facilitate research on the acceleration of LLM inference.</li>
<li><strong>摘要：</strong>最近，跨层共享键值 (KV) 缓存已被发现可有效提高大型语言模型 (LLM) 的推理效率。为了系统地研究跨层 KV 共享的不同技术，我们提出了一个统一的框架，该框架涵盖了几种近期方法及其新变体。我们对框架的所有配置进行了全面的实验，评估了它们在语言建模和下游任务中的生成吞吐量和性能。我们发现，当将 KV 缓存的大小减少 2 倍时，大多数配置都可以实现与标准 Transformer 相媲美的性能和更高的吞吐量，但是当进一步减小 KV 缓存的大小时，将所有层的查询与上层的 KV 配对可以更好地保持性能，尽管这也会引入额外的训练成本和预填充延迟。我们希望这项工作能够帮助用户根据他们的需求选择合适的方法，并促进对 LLM 推理加速的研究。</li>
</ul>

<h3>Title: Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of Language Models</h3>
<ul>
<li><strong>Authors: </strong>James Vo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14480">https://arxiv.org/abs/2410.14480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14480">https://arxiv.org/pdf/2410.14480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14480]] Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of Language Models(https://arxiv.org/abs/2410.14480)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to advance, the need for precise and efficient evaluation metrics becomes more pressing. Traditional approaches, while informative, often face limitations in computational demands and interpretability. In this paper, we introduce a novel hybrid evaluation method that integrates two established techniques: entropy derived from covariance matrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing hidden states from LLMs, then computes the covariance matrix and MNN from these representations. We further calculate the entropy of the covariance matrix to capture uncertainty and redundancy in the model's outputs. By combining these metrics into a composite score, we offer a comprehensive evaluation framework that balances accuracy with computational efficiency. Additionally, our approach allows for flexibility in adjusting the weightings between entropy and MNN, tailoring the evaluation for different objectives. Through a series of experiments on various LLMs, we demonstrate the robustness and efficacy of our method, offering deeper insights into model performance. This work contributes to the ongoing development of LLM evaluation and opens avenues for future innovations in model assessment techniques.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的不断发展，对精确、高效的评估指标的需求变得更加迫切。传统方法虽然信息量大，但往往在计算需求和可解释性方面受到限制。在本文中，我们介绍了一种新型混合评估方法，该方法集成了两种成熟的技术：从协方差矩阵导出的熵和矩阵核范数 (MNN)。我们的方法首先对 LLM 中的隐藏状态进行归一化，然后根据这些表示计算协方差矩阵和 MNN。我们进一步计算协方差矩阵的熵，以捕获模型输出中的不确定性和冗余。通过将这些指标组合成一个综合分数，我们提供了一个全面的评估框架，在准确性和计算效率之间取得平衡。此外，我们的方法允许灵活地调整熵和 MNN 之间的权重，从而针对不同的目标定制评估。通过对各种 LLM 进行的一系列实验，我们证明了我们方法的稳健性和有效性，从而更深入地了解了模型性能。这项工作有助于 LLM 评估的持续发展，并为模型评估技术的未来创新开辟了道路。</li>
</ul>

<h3>Title: Tell me what I need to know: Exploring LLM-based (Personalized) Abstractive Multi-Source Meeting Summarization</h3>
<ul>
<li><strong>Authors: </strong>Frederic Kirstein, Terry Ruas, Robert Kratel, Bela Gipp</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14545">https://arxiv.org/abs/2410.14545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14545">https://arxiv.org/pdf/2410.14545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14545]] Tell me what I need to know: Exploring LLM-based (Personalized) Abstractive Multi-Source Meeting Summarization(https://arxiv.org/abs/2410.14545)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Meeting summarization is crucial in digital communication, but existing solutions struggle with salience identification to generate personalized, workable summaries, and context understanding to fully comprehend the meetings' content. Previous attempts to address these issues by considering related supplementary resources (e.g., presentation slides) alongside transcripts are hindered by models' limited context sizes and handling the additional complexities of the multi-source tasks, such as identifying relevant information in additional files and seamlessly aligning it with the meeting content. This work explores multi-source meeting summarization considering supplementary materials through a three-stage large language model approach: identifying transcript passages needing additional context, inferring relevant details from supplementary materials and inserting them into the transcript, and generating a summary from this enriched transcript. Our multi-source approach enhances model understanding, increasing summary relevance by ~9% and producing more content-rich outputs. We introduce a personalization protocol that extracts participant characteristics and tailors summaries accordingly, improving informativeness by ~10%. This work further provides insights on performance-cost trade-offs across four leading model families, including edge-device capable options. Our approach can be extended to similar complex generative tasks benefitting from additional resources and personalization, such as dialogue systems and action planning.</li>
<li><strong>摘要：</strong>会议摘要在数字通信中至关重要，但现有的解决方案难以识别显著性以生成个性化、可行的摘要，也难以理解上下文以充分理解会议内容。以前尝试通过考虑相关补充资源（例如，演示文稿幻灯片）和记录来解决这些问题，但由于模型的上下文大小有限，并且需要处理多源任务的额外复杂性，例如识别附加文件中的相关信息并将其与会议内容无缝对齐，因此这些尝试受到了阻碍。这项工作通过三阶段大型语言模型方法探索了考虑补充材料的多源会议摘要：识别需要额外上下文的记录段落，从补充材料中推断相关细节并将其插入记录中，并从这个丰富的记录中生成摘要。我们的多源方法增强了模型理解，将摘要相关性提高了约 9%，并产生了更多内容丰富的输出。我们引入了一种个性化协议，可以提取参与者的特征并相应地定制摘要，从而将信息量提高约 10%。这项研究进一步提供了有关四种领先模型系列（包括支持边缘设备的选项）的性能成本权衡的见解。我们的方法可以扩展到类似的复杂生成任务，这些任务受益于额外的资源和个性化，例如对话系统和行动规划。</li>
</ul>

<h3>Title: RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Peng, Jinming Nian, Alexandre Evfimievski, Yi Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14567">https://arxiv.org/abs/2410.14567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14567">https://arxiv.org/pdf/2410.14567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14567]] RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions(https://arxiv.org/abs/2410.14567)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Conversational AI agents use Retrieval Augmented Generation (RAG) to provide verifiable document-grounded responses to user inquiries. However, many natural questions do not have good answers: about 25\% contain false assumptions~\cite{Yu2023:CREPE}, and over 50\% are ambiguous~\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve their responses to confusing questions. This paper presents a novel synthetic data generation method to efficiently create a diverse set of context-grounded confusing questions from a given document corpus. We conduct an empirical comparative evaluation of several large language models as RAG agents to measure the accuracy of confusion detection and appropriate response generation. We contribute a benchmark dataset to the public domain.</li>
<li><strong>摘要：</strong>对话式 AI 代理使用检索增强生成 (RAG) 为用户查询提供可验证的基于文档的响应。但是，许多自然问题没有好的答案：大约 25\% 包含错误假设~\cite{Yu2023:CREPE}，超过 50\% 是模棱两可的~\cite{Min2020:AmbigQA}。RAG 代理需要高质量的数据来改善对混淆问题的回答。本文提出了一种新颖的合成数据生成方法，可从给定的文档语料库中有效地创建一组多样化的基于上下文的混淆问题。我们对作为 RAG 代理的几种大型语言模型进行了实证比较评估，以衡量混淆检测和适当响应生成的准确性。我们向公共领域贡献了一个基准数据集。</li>
</ul>

<h3>Title: Large Language Models Are Overparameterized Text Encoders</h3>
<ul>
<li><strong>Authors: </strong>Thennal D K, Tim Fischer, Chris Biemann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14578">https://arxiv.org/abs/2410.14578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14578">https://arxiv.org/pdf/2410.14578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14578]] Large Language Models Are Overparameterized Text Encoders(https://arxiv.org/abs/2410.14578)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate strong performance as text embedding models when finetuned with supervised contrastive training. However, their large size balloons inference time and memory requirements. In this paper, we show that by pruning the last $p\%$ layers of an LLM before supervised training for only 1000 steps, we can achieve a proportional reduction in memory and inference time. We evaluate four different state-of-the-art LLMs on text embedding tasks and find that our method can prune up to 30\% of layers with negligible impact on performance and up to 80\% with only a modest drop. With only three lines of code, our method is easily implemented in any pipeline for transforming LLMs to text encoders. We also propose $\text{L}^3 \text{Prune}$, a novel layer-pruning strategy based on the model's initial loss that provides two optimal pruning configurations: a large variant with negligible performance loss and a small variant for resource-constrained settings. On average, the large variant prunes 21\% of the parameters with a $-0.3$ performance drop, and the small variant only suffers from a $-5.1$ decrease while pruning 74\% of the model. We consider these results strong evidence that LLMs are overparameterized for text embedding tasks, and can be easily pruned.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在经过监督对比训练微调后，作为文本嵌入模型表现出强大的性能。然而，它们的大尺寸会导致推理时间和内存需求激增。在本文中，我们表明，通过在监督训练之前仅 1000 步修剪 LLM 的最后 $p\%$ 层，我们可以实现内存和推理时间的成比例减少。我们在文本嵌入任务上评估了四种不同的最先进的 LLM，发现我们的方法可以修剪高达 30\% 的层，对性能的影响可以忽略不计，最多可以修剪 80\% 的层，而性能只会略有下降。只需三行代码，我们的方法就可以轻松地在任何将 LLM 转换为文本编码器的管道中实现。我们还提出了 $\text{L}^3 \text{Prune}$，这是一种基于模型初始损失的新型层修剪策略，它提供了两种最佳修剪配置：性能损失可忽略不计的大型变体和适用于资源受限设置的小变体。平均而言，大型变体会修剪 21\% 的参数，性能下降 $-0.3$，而小变体只会遭受 $-5.1$ 的损失，同时修剪 74\% 的模型。我们认为这些结果有力地证明了 LLM 对于文本嵌入任务而言参数化过度，并且可以轻松修剪。</li>
</ul>

<h3>Title: Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool Knowledge Bases</h3>
<ul>
<li><strong>Authors: </strong>Elias Lumer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14594">https://arxiv.org/abs/2410.14594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14594">https://arxiv.org/pdf/2410.14594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14594]] Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool Knowledge Bases(https://arxiv.org/abs/2410.14594)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks like secure database interactions and multi-agent code development. However, scaling tool capacity beyond agent reasoning or model limits remains a challenge. In this paper, we address these challenges by introducing Toolshed Knowledge Bases, a tool knowledge base (vector database) designed to store enhanced tool representations and optimize tool selection for large-scale tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a novel ensemble of tool-applied advanced retrieval-augmented generation (RAG) techniques across the pre-retrieval, intra-retrieval, and post-retrieval phases, without requiring model fine-tuning. During pre-retrieval, tool documents are enhanced with key information and stored in the Toolshed Knowledge Base. Intra-retrieval focuses on query planning and transformation to increase retrieval accuracy. Post-retrieval refines the retrieved tool documents and enables self-reflection. Furthermore, by varying both the total number of tools (tool-M) an Agent has access to and the tool selection threshold (top-k), we address trade-offs between retrieval accuracy, agent performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools benchmark datasets, respectively (Recall@5).</li>
<li><strong>摘要：</strong>工具配备代理 (LLM) 的最新进展使安全数据库交互和多代理代码开发等复杂任务成为可能。然而，将工具容量扩展到代理推理或模型限制之外仍然是一个挑战。在本文中，我们通过引入 Toolshed 知识库来应对这些挑战，这是一个工具知识库（矢量数据库），旨在存储增强的工具表示并优化大规模工具配备代理的工具选择。此外，我们提出了高级 RAG-工具融合，这是一套新颖的工具应用高级检索增强生成 (RAG) 技术，涵盖预检索、内检索和后检索阶段，无需模型微调。在预检索期间，工具文档会通过关键信息得到增强，并存储在 Toolshed 知识库中。内检索侧重于查询规划和转换，以提高检索准确性。后检索会细化检索到的工具文档并实现自我反思。此外，通过改变代理可以使用的工具总数 (tool-M) 和工具选择阈值 (top-k)，我们解决了检索准确性、代理性能和 token 成本之间的权衡。我们的方法分别在 ToolE 单工具、ToolE 多工具和 Seal-Tools 基准数据集上实现了 46%、56% 和 47% 的绝对改进 (Recall@5)。</li>
</ul>

<h3>Title: Teaching Models to Balance Resisting and Accepting Persuasion</h3>
<ul>
<li><strong>Authors: </strong>Elias Stengel-Eskin, Peter Hase, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14596">https://arxiv.org/abs/2410.14596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14596">https://arxiv.org/pdf/2410.14596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14596]] Teaching Models to Balance Resisting and Accepting Persuasion(https://arxiv.org/abs/2410.14596)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. negative) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. positive) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on the other. In order to balance positive and negative persuasion, we introduce Persuasion-Balanced Training (or PBT), which leverages multi-agent recursive dialogue trees to create data and trains models via preference optimization to accept persuasion when appropriate. PBT consistently improves resistance to misinformation and resilience to being challenged while also resulting in the best overall performance on holistic data containing both positive and negative persuasion. Crucially, we show that PBT models are better teammates in multi-agent debates. We find that without PBT, pairs of stronger and weaker models have unstable performance, with the order in which the models present their answers determining whether the team obtains the stronger or weaker model's performance. PBT leads to better and more stable results and less order dependence, with the stronger model consistently pulling the weaker one up.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 容易受到说服，当模型面对对抗性对话者时，这可能会带来风险。我们迈出了保护模型免受说服的第一步，同时也认为防御对抗性（即消极）说服只是等式的一半：模型还应该能够接受有益的（即积极的）说服来改进他们的答案。我们表明，只针对一方优化模型会导致另一方表现不佳。为了平衡积极和消极的说服，我们引入了说服平衡训练 (PBT)，它利用多智能体递归对话树来创建数据，并通过偏好优化训练模型以在适当的时候接受说服。PBT 持续提高对错误信息的抵抗力和对挑战的适应力，同时在包含积极和消极说服的整体数据上实现最佳整体性能。至关重要的是，我们表明 PBT 模型是多智能体辩论中更好的队友。我们发现，如果没有 PBT，较强和较弱模型的组合表现不稳定，模型呈现答案的顺序决定了团队获得较强还是较弱模型的表现。PBT 可带来更好、更稳定的结果，顺序依赖性更小，较强的模型会持续拉动较弱的模型。</li>
</ul>

<h3>Title: Diverging Preferences: When do Annotators Disagree and do Models Know?</h3>
<ul>
<li><strong>Authors: </strong>Michael JQ Zhang, Zhilin Wang, Jena D. Hwang, Yi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi, Xiang Ren, Valentina Pyatkin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14632">https://arxiv.org/abs/2410.14632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14632">https://arxiv.org/pdf/2410.14632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14632]] Diverging Preferences: When do Annotators Disagree and do Models Know?(https://arxiv.org/abs/2410.14632)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We examine diverging preferences in human-labeled preference datasets. We develop a taxonomy of disagreement sources spanning 10 categories across four high-level classes -- task underspecification, response style, refusals, and annotation errors. We find that the majority of disagreements are in opposition with standard reward modeling approaches, which are designed with the assumption that annotator disagreement is noise. We then explore how these findings impact two areas of LLM development: reward modeling and evaluation. In our experiments, we demonstrate how standard reward modeling methods, like the Bradley-Terry model, fail to differentiate whether a given preference judgment is the result of unanimous agreement among annotators or the majority opinion among diverging user preferences. We also find that these tendencies are also echoed by popular LLM-as-Judge evaluation methods, which consistently identify a winning response in cases of diverging preferences. These findings highlight remaining challenges in LLM evaluations, which are greatly influenced by divisive features like response style, and in developing pluralistically aligned LLMs. To address these issues, we develop methods for identifying diverging preferences to mitigate their influence on evaluation and training.</li>
<li><strong>摘要：</strong>我们研究了人工标记的偏好数据集中的不同偏好。我们开发了一个分歧来源分类法，涵盖四个高级类别的 10 个类别——任务未指定、响应风格、拒绝和注释错误。我们发现大多数分歧与标准奖励建模方法相反，这些方法的设计假设注释者的分歧是噪音。然后，我们探讨这些发现如何影响 LLM 开发的两个领域：奖励建模和评估。在我们的实验中，我们展示了标准奖励建模方法（如 Bradley-Terry 模型）如何无法区分给定的偏好判断是注释者一致同意的结果还是不同用户偏好中的多数意见。我们还发现，这些趋势也得到了流行的 LLM-as-Judge 评估方法的呼应，这些方法在偏好分歧的情况下始终能确定获胜的回应。这些发现凸显了 LLM 评估中仍然存在的挑战，这些挑战受到响应风格等分歧特征的极大影响，以及在开发多元化的 LLM 方面。为了解决这些问题，我们开发了识别不同偏好的方法，以减轻它们对评估和培训的影响。</li>
</ul>

<h3>Title: GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Raghuveer Thirukovalluru, Bhuwan Dhingra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14635">https://arxiv.org/abs/2410.14635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14635">https://arxiv.org/pdf/2410.14635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14635]] GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings(https://arxiv.org/abs/2410.14635)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Training-free embedding methods directly leverage pretrained large language models (LLMs) to embed text, bypassing the costly and complex procedure of contrastive learning. Previous training-free embedding methods have mainly focused on optimizing embedding prompts and have overlooked the benefits of utilizing the generative abilities of LLMs. We propose a novel method, GenEOL, which uses LLMs to generate diverse transformations of a sentence that preserve its meaning, and aggregates the resulting embeddings of these transformations to enhance the overall sentence embedding. GenEOL significantly outperforms the existing training-free embedding methods by an average of 2.85 points across several LLMs on the sentence semantic text similarity (STS) benchmark. Our analysis shows that GenEOL stabilizes representation quality across LLM layers and is robust to perturbations of embedding prompts. GenEOL also achieves notable gains on multiple clustering, reranking and pair-classification tasks from the MTEB benchmark.</li>
<li><strong>摘要：</strong>无需训练的嵌入方法直接利用预训练的大型语言模型 (LLM) 来嵌入文本，从而绕过了昂贵而复杂的对比学习过程。以前的无需训练的嵌入方法主要侧重于优化嵌入提示，而忽略了利用 LLM 生成能力的好处。我们提出了一种新方法 GenEOL，它使用 LLM 生成保留其含义的句子的多种转换，并聚合这些转换的结果嵌入以增强整体句子嵌入。在句子语义文本相似性 (STS) 基准上，GenEOL 在多个 LLM 上的表现显著优于现有的无需训练的嵌入方法，平均高出 2.85 分。我们的分析表明，GenEOL 可以稳定 LLM 层之间的表示质量，并且对嵌入提示的扰动具有鲁棒性。GenEOL 还在 MTEB 基准的多个聚类、重新排名和配对分类任务中取得了显着的进步。</li>
</ul>

<h3>Title: Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs</h3>
<ul>
<li><strong>Authors: </strong>Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Huadong Wang, Xiaojiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14641">https://arxiv.org/abs/2410.14641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14641">https://arxiv.org/pdf/2410.14641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14641]] Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs(https://arxiv.org/abs/2410.14641)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Positional bias in large language models (LLMs) hinders their ability to effectively process long inputs. A prominent example is the "lost in the middle" phenomenon, where LLMs struggle to utilize relevant information situated in the middle of the input. While prior research primarily focuses on single pieces of relevant information, real-world applications often involve multiple relevant information pieces. To bridge this gap, we present LongPiBench, a benchmark designed to assess positional bias involving multiple pieces of relevant information. Thorough experiments are conducted with five commercial and six open-source models. These experiments reveal that while most current models are robust against the "lost in the middle" issue, there exist significant biases related to the spacing of relevant information pieces. These findings highlight the importance of evaluating and reducing positional biases to advance LLM's capabilities.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 中的位置偏差阻碍了它们有效处理长输入的能力。一个突出的例子是“迷失在中间”现象，其中 LLM 难以利用位于输入中间的相关信息。虽然先前的研究主要关注单个相关信息，但现实世界的应用通常涉及多个相关信息。为了弥补这一差距，我们提出了 LongPiBench，这是一个旨在评估涉及多个相关信息的位置偏差的基准。对五个商业模型和六个开源模型进行了彻底的实验。这些实验表明，虽然大多数当前模型对“迷失在中间”问题具有鲁棒性，但存在与相关信息片段间距相关的显著偏差。这些发现强调了评估和减少位置偏差以提高 LLM 能力的重要性。</li>
</ul>

<h3>Title: Real-time Fake News from Adversarial Feedback</h3>
<ul>
<li><strong>Authors: </strong>Sanxing Chen, Yukun Huang, Bhuwan Dhingra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14651">https://arxiv.org/abs/2410.14651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14651">https://arxiv.org/pdf/2410.14651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14651]] Real-time Fake News from Adversarial Feedback(https://arxiv.org/abs/2410.14651)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>We show that existing evaluations for fake news detection based on conventional sources, such as claims on fact-checking websites, result in an increasing accuracy over time for LLM-based detectors -- even after their knowledge cutoffs. This suggests that recent popular political claims, which form the majority of fake news on such sources, are easily classified using surface-level shallow patterns. Instead, we argue that a proper fake news detection dataset should test a model's ability to reason factually about the current world by retrieving and reading related evidence. To this end, we develop a novel pipeline that leverages natural language feedback from a RAG-based detector to iteratively modify real-time news into deceptive fake news that challenges LLMs. Our iterative rewrite decreases the binary classification AUC by an absolute 17.5 percent for a strong RAG GPT-4o detector. Our experiments reveal the important role of RAG in both detecting and generating fake news, as retrieval-free LLM detectors are vulnerable to unseen events and adversarial attacks, while feedback from RAG detection helps discover more deceitful patterns in fake news.</li>
<li><strong>摘要：</strong>我们表明，基于传统来源（例如事实核查网站上的声明）的假新闻检测现有评估表明，基于 LLM 的检测器的准确性会随着时间的推移而提高——即使在知识截止之后也是如此。这表明，近期流行的政治主张（构成此类来源上的大多数假新闻）很容易使用表面浅层模式进行分类。相反，我们认为，合适的假新闻检测数据集应该通过检索和阅读相关证据来测试模型对当前世界进行事实推理的能力。为此，我们开发了一种新颖的流程，该流程利用基于 RAG 的检测器的自然语言反馈，将实时新闻迭代修改为挑战 LLM 的欺骗性假新闻。对于强大的 RAG GPT-4o 检测器，我们的迭代重写将二元分类 AUC 降低了 17.5%。我们的实验揭示了 RAG 在检测和生成虚假新闻方面的重要作用，因为无检索 LLM 检测器容易受到看不见的事件和对抗性攻击，而 RAG 检测的反馈有助于发现虚假新闻中更多欺骗性模式。</li>
</ul>

<h3>Title: MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps</h3>
<ul>
<li><strong>Authors: </strong>Xiongtao Zhou, Jie He, Lanyu Chen, jingyu li, Haojing Chen, Victor Gutierrez Basulto, Jeff Z. Pan, Hanjie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14668">https://arxiv.org/abs/2410.14668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14668">https://arxiv.org/pdf/2410.14668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14668]] MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps(https://arxiv.org/abs/2410.14668)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Multimodal Chain of Thought (MCoT) is a popular prompting strategy for improving the performance of multimodal large language models (MLLMs) across a range of complex reasoning tasks. Despite its popularity, there is a notable absence of automated methods for evaluating the quality of reasoning steps in MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation (MiCEval), a framework designed to assess the correctness of reasoning chains by evaluating the quality of both the description and each reasoning step. The evaluation of the description component focuses on the accuracy of the image descriptions, while the reasoning step evaluates the quality of each step as it is conditionally generated based on the preceding steps. MiCEval is built upon a fine-grained dataset with annotations that rate each step according to correctness, relevance, and informativeness. Extensive experiments on four state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more closely with human judgments compared to existing methods based on cosine similarity or fine-tuning approaches. MiCEval datasets and code can be found in this https URL.</li>
<li><strong>摘要：</strong>多模态思维链 (MCoT) 是一种流行的提示策略，用于提高多模态大型语言模型 (MLLM) 在一系列复杂推理任务中的性能。尽管它很受欢迎，但显然缺乏用于评估 MCoT 中推理步骤质量的自动化方法。为了解决这一差距，我们提出了多模态思维链评估 (MiCEval)，这是一个旨在通过评估描述和每个推理步骤的质量来评估推理链正确性的框架。描述组件的评估侧重于图像描述的准确性，而推理步骤则评估每个步骤的质量，因为它是基于前面的步骤有条件生成的。MiCEval 建立在一个细粒度数据集上，其中包含注释，可根据正确性、相关性和信息量对每个步骤进行评级。对四个最先进的 MLLM 进行的大量实验表明，与基于余弦相似性或微调方法的现有方法相比，使用 MiCEval 的分步评估更接近人类判断。 MiCEval 数据集和代码可在此 https URL 中找到。</li>
</ul>

<h3>Title: Enhancing Large Language Models' Situated Faithfulness to External Contexts</h3>
<ul>
<li><strong>Authors: </strong>Yukun Huang, Sanxing Chen, Hongyi Cai, Bhuwan Dhingra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14675">https://arxiv.org/abs/2410.14675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14675">https://arxiv.org/pdf/2410.14675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14675]] Enhancing Large Language Models' Situated Faithfulness to External Contexts(https://arxiv.org/abs/2410.14675)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are often augmented with external information as contexts, but this external information can sometimes be inaccurate or even intentionally misleading. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset called RedditQA featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. Our results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs. The data and code are released.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常会使用外部信息作为上下文进行增强，但这些外部信息有时可能不准确，甚至故意误导。我们认为，强大的 LLM 应该表现出情境忠诚度，根据对内部知识和外部上下文的信心动态校准对外部信息的信任。为了对这种能力进行基准测试，我们在多个 QA 数据集上评估了 LLM，其中包括一个新创建的数据集 RedditQA，其中包含来自 Reddit 帖子的野外不正确上下文。我们表明，当提供正确和不正确的上下文时，开源和专有模型都倾向于过度依赖外部信息，而不管其事实准确性如何。为了增强情境忠诚度，我们提出了两种方法：自引导置信度推理 (SCR) 和基于规则的置信度推理 (RCR)。SCR 使模型能够自我访问相对于其自身内部知识的外部信息的置信度，以产生最准确的答案。相比之下，RCR 从 LLM 中提取明确的置信度信号并使用预定义规则确定最终答案。我们的结果表明，对于具有强大推理能力的 LLM，例如 GPT-4o 和 GPT-4o mini，SCR 的表现优于 RCR，与直接输入增强基线相比，其性能提高了 24.2%。相反，对于像 Llama-3-8B 这样的较小模型，RCR 的表现优于 SCR。使用我们提出的置信推理直接偏好优化 (CR-DPO) 方法对 SCR 进行微调可提高可见和不可见数据集上的性能，在 Llama-3-8B 上的平均性能提高了 8.9%。除了定量结果外，我们还提供了有关 SCR 和 RCR 相对优势的见解。我们的研究结果强调了改善 LLM 中情境忠诚度的有希望的途径。数据和代码已发布。</li>
</ul>

<h3>Title: SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment</h3>
<ul>
<li><strong>Authors: </strong>Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14676">https://arxiv.org/abs/2410.14676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14676">https://arxiv.org/pdf/2410.14676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14676]] SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment(https://arxiv.org/abs/2410.14676)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Existing preference alignment is a one-size-fits-all alignment mechanism, where the part of the large language model (LLM) parametric knowledge with non-preferred features is uniformly blocked to all the users. However, this part of knowledge can be useful to advanced users whose expertise qualifies them to handle these information. The one-size-fits-all alignment mechanism undermines LLM's utility for these qualified users. To address this problem, we propose SudoLM, a framework that lets LLMs learn access control over specific parametric knowledge for users with different credentials via authorization alignment. SudoLM allows authorized users to unlock their access to all the parametric knowledge with an assigned SUDO key while blocking access to non-qualified users. Experiments on two application scenarios demonstrate that SudoLM effectively controls the user's access to the parametric knowledge and maintains its general utility.</li>
<li><strong>摘要：</strong>现有的偏好对齐是一种一刀切的对齐机制，其中大型语言模型 (LLM) 参数知识中具有非偏好特征的部分被统一阻止给所有用户。然而，这部分知识对高级用户是有用的，他们的专业知识使他们有资格处理这些信息。一刀切的对齐机制削弱了 LLM 对这些合格用户的实用性。为了解决这个问题，我们提出了 SudoLM，这是一个框架，它让 LLM 通过授权对齐来学习具有不同凭证的用户对特定参数知识的访问控制。SudoLM 允许授权用户使用分配的 SUDO 密钥解锁对所有参数知识的访问权限，同时阻止不合格用户的访问。在两个应用场景上的实验表明，SudoLM 有效地控制了用户对参数知识的访问并保持了其普遍实用性。</li>
</ul>

<h3>Title: Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts</h3>
<ul>
<li><strong>Authors: </strong>German Gritsai, Anastasia Voznyuk, Andrey Grabovoy, Yury Chekhovich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14677">https://arxiv.org/abs/2410.14677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14677">https://arxiv.org/pdf/2410.14677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14677]] Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts(https://arxiv.org/abs/2410.14677)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid development of autoregressive Large Language Models (LLMs) has significantly improved the quality of generated texts, necessitating reliable machine-generated text detectors. A huge number of detectors and collections with AI fragments have emerged, and several detection methods even showed recognition quality up to 99.9% according to the target metrics in such collections. However, the quality of such detectors tends to drop dramatically in the wild, posing a question: Are detectors actually highly trustworthy or do their high benchmark scores come from the poor quality of evaluation datasets? In this paper, we emphasise the need for robust and qualitative methods for evaluating generated data to be secure against bias and low generalising ability of future model. We present a systematic review of datasets from competitions dedicated to AI-generated content detection and propose methods for evaluating the quality of datasets containing AI-generated fragments. In addition, we discuss the possibility of using high-quality generated data to achieve two goals: improving the training of detection models and improving the training datasets themselves. Our contribution aims to facilitate a better understanding of the dynamics between human and machine text, which will ultimately support the integrity of information in an increasingly automated world.</li>
<li><strong>摘要：</strong>自回归大型语言模型 (LLM) 的快速发展显著提高了生成文本的质量，因此需要可靠的机器生成文本检测器。大量带有 AI 片段的检测器和集合已经出现，并且根据这些集合中的目标指标，几种检测方法甚至显示出高达 99.9% 的识别质量。然而，这种检测器的质量往往会在野外急剧下降，这提出了一个问题：检测器是否真的值得信赖，还是它们的高基准分数来自评估数据集的质量差？在本文中，我们强调需要稳健和定性的方法来评估生成的数据，以防止未来模型出现偏见和低泛化能力。我们对专门用于 AI 生成内容检测的竞赛数据集进行了系统回顾，并提出了评估包含 AI 生成片段的数据集质量的方法。此外，我们讨论了使用高质量生成的数据实现两个目标的可能性：改进检测模型的训练和改进训练数据集本身。我们的贡献旨在促进更好地理解人与机器文本之间的动态，这最终将支持日益自动化的世界中的信息完整性。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
