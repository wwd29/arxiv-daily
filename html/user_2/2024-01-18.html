<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-18</h1>
<h3>Title: MATE-Pred: Multimodal Attention-based TCR-Epitope interaction Predictor</h3>
<ul>
<li><strong>Authors: </strong>Etienne Goffinet, Raghvendra Mall, Ankita Singh, Rahul Kaushik, Filippo Castiglione</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08619">https://arxiv.org/abs/2401.08619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08619">https://arxiv.org/pdf/2401.08619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08619]] MATE-Pred: Multimodal Attention-based TCR-Epitope interaction Predictor(https://arxiv.org/abs/2401.08619)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, rag</a></li>
<li><strong>Abstract: </strong>An accurate binding affinity prediction between T-cell receptors and epitopes contributes decisively to develop successful immunotherapy strategies. Some state-of-the-art computational methods implement deep learning techniques by integrating evolutionary features to convert the amino acid residues of cell receptors and epitope sequences into numerical values, while some other methods employ pre-trained language models to summarize the embedding vectors at the amino acid residue level to obtain sequence-wise representations. Here, we propose a highly reliable novel method, MATE-Pred, that performs multi-modal attention-based prediction of T-cell receptors and epitopes binding affinity. The MATE-Pred is compared and benchmarked with other deep learning models that leverage multi-modal representations of T-cell receptors and epitopes. In the proposed method, the textual representation of proteins is embedded with a pre-trained bi-directional encoder model and combined with two additional modalities: a) a comprehensive set of selected physicochemical properties; b) predicted contact maps that estimate the 3D distances between amino acid residues in the sequences. The MATE-Pred demonstrates the potential of multi-modal model in achieving state-of-the-art performance (+8.4\% MCC, +5.5\% AUC compared to baselines) and efficiently capturing contextual, physicochemical, and structural information from amino acid residues. The performance of MATE-Pred projects its potential application in various drug discovery regimes.</li>
<li><strong>摘要：</strong>T 细胞受体和表位之间准确的结合亲和力预测对于开发成功的免疫治疗策略具有决定性作用。一些最先进的计算方法通过整合进化特征来实现深度学习技术，将细胞受体的氨基酸残基和表位序列转换为数值，而其他一些方法则采用预先训练的语言模型来总结嵌入向量氨基酸残基水平以获得序列表示。在这里，我们提出了一种高度可靠的新方法 MATE-Pred，该方法对 T 细胞受体和表位结合亲和力进行基于多模式注意的预测。 MATE-Pred 与其他利用 T 细胞受体和表位多模式表示的深度学习模型进行了比较和基准测试。在所提出的方法中，蛋白质的文本表示嵌入了预先训练的双向编码器模型，并与两种附加模式相结合：a）一组全面的选定的物理化学特性； b) 预测接触图，用于估计序列中氨基酸残基之间的 3D 距离。 MATE-Pred 展示了多模态模型在实现最先进性能（与基线相比 +8.4\% MCC、+5.5\% AUC）和有效捕获氨基酸的背景、物理化学和结构信息方面的潜力酸残留物。 MATE-Pred 的性能预测了其在各种药物发现体系中的潜在应用。</li>
</ul>

<h3>Title: Gemini Pro Defeated by GPT-4V: Evidence from Education</h3>
<ul>
<li><strong>Authors: </strong>Gyeong-Geon Lee, Ehsan Latif, Lehong Shi, Xiaoming Zhai</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08660">https://arxiv.org/abs/2401.08660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08660">https://arxiv.org/pdf/2401.08660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08660]] Gemini Pro Defeated by GPT-4V: Evidence from Education(https://arxiv.org/abs/2401.08660)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt</a></li>
<li><strong>Abstract: </strong>This study compared the classification performance of Gemini Pro and GPT-4V in educational settings. Employing visual question answering (VQA) techniques, the study examined both models' abilities to read text-based rubrics and then automatically score student-drawn models in science education. We employed both quantitative and qualitative analyses using a dataset derived from student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics for Image Feedback) prompting methods. The findings reveal that GPT-4V significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic Weighted Kappa. The qualitative analysis reveals that the differences may be due to the models' ability to process fine-grained texts in images and overall image classification performance. Even adapting the NERIF approach by further de-sizing the input images, Gemini Pro seems not able to perform as well as GPT-4V. The findings suggest GPT-4V's superior capability in handling complex multimodal educational tasks. The study concludes that while both models represent advancements in AI, GPT-4V's higher performance makes it a more suitable tool for educational applications involving multimodal data interpretation.</li>
<li><strong>摘要：</strong>本研究比较了 Gemini Pro 和 GPT-4V 在教育环境中的分类性能。该研究采用视觉问答（VQA）技术，检查了两种模型阅读基于文本的规则的能力，然后自动对学生在科学教育中绘制的模型进行评分。我们使用源自学生绘制的科学模型的数据集并采用 NERIF（图像反馈符号增强评分标准）提示方法进行定量和定性分析。研究结果表明，GPT-4V 在评分准确性和二次加权 Kappa 方面显着优于 Gemini Pro。定性分析表明，差异可能是由于模型处理图像中细粒度文本的能力以及整体图像分类性能造成的。即使通过进一步缩小输入图像的大小来适应 NERIF 方法，Gemini Pro 的性能似乎也无法与 GPT-4V 一样好。研究结果表明，GPT-4V 在处理复杂的多模式教育任务方面具有卓越的能力。研究得出的结论是，虽然这两种模型都代表了人工智能的进步，但 GPT-4V 的更高性能使其成为更适合涉及多模式数据解释的教育应用的工具。</li>
</ul>

<h3>Title: An Integrated Imitation and Reinforcement Learning Methodology for  Robust Agile Aircraft Control with Limited Pilot Demonstration Data</h3>
<ul>
<li><strong>Authors: </strong>Gulay Goktas Sever, Umut Demir, Abdullah Sadik Satir, Mustafa Cagatay Sahin, Nazim Kemal Ure</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, cs.RO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08663">https://arxiv.org/abs/2401.08663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08663">https://arxiv.org/pdf/2401.08663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08663]] An Integrated Imitation and Reinforcement Learning Methodology for  Robust Agile Aircraft Control with Limited Pilot Demonstration Data(https://arxiv.org/abs/2401.08663)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this paper, we present a methodology for constructing data-driven maneuver generation models for agile aircraft that can generalize across a wide range of trim conditions and aircraft model parameters. Maneuver generation models play a crucial role in the testing and evaluation of aircraft prototypes, providing insights into the maneuverability and agility of the aircraft. However, constructing the models typically requires extensive amounts of real pilot data, which can be time-consuming and costly to obtain. Moreover, models built with limited data often struggle to generalize beyond the specific flight conditions covered in the original dataset. To address these challenges, we propose a hybrid architecture that leverages a simulation model, referred to as the source model. This open-source agile aircraft simulator shares similar dynamics with the target aircraft and allows us to generate unlimited data for building a proxy maneuver generation model. We then fine-tune this model to the target aircraft using a limited amount of real pilot data. Our approach combines techniques from imitation learning, transfer learning, and reinforcement learning to achieve this objective. To validate our methodology, we utilize real agile pilot data provided by Turkish Aerospace Industries (TAI). By employing the F-16 as the source model, we demonstrate that it is possible to construct a maneuver generation model that generalizes across various trim conditions and aircraft parameters without requiring any additional real pilot data. Our results showcase the effectiveness of our approach in developing robust and adaptable models for agile aircraft.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种为敏捷飞机构建数据驱动的机动生成模型的方法，该模型可以概括各种配平条件和飞机模型参数。机动生成模型在飞机原型的测试和评估中发挥着至关重要的作用，可以深入了解飞机的机动性和敏捷性。然而，构建模型通常需要大量的真实试点数据，获取这些数据可能非常耗时且成本高昂。此外，用有限的数据构建的模型通常很难推广到原始数据集中涵盖的特定飞行条件之外。为了应对这些挑战，我们提出了一种利用模拟模型（称为源模型）的混合架构。这种开源敏捷飞机模拟器与目标飞机具有相似的动力学，使我们能够生成无限的数据来构建代理机动生成模型。然后，我们使用有限数量的真实飞行员数据对该模型进行微调以适应目标飞机。我们的方法结合了模仿学习、迁移学习和强化学习的技术来实现这一目标。为了验证我们的方法，我们利用土耳其航空航天工业公司 (TAI) 提供的真实敏捷试点数据。通过使用 F-16 作为源模型，我们证明可以构建一个机动生成模型，该模型可以概括各种配平条件和飞机参数，而无需任何额外的真实飞行员数据。我们的结果展示了我们的方法在为敏捷飞机开发稳健且适应性强的模型方面的有效性。</li>
</ul>

<h3>Title: Adapting Large Language Models for Education: Foundational Capabilities,  Potentials, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Qingyao Li, Lingyue Fu, Weiming Zhang, Xianyu Chen, Jingwei Yu, Wei Xia, Weinan Zhang, Ruiming Tang, Yong Yu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08664">https://arxiv.org/abs/2401.08664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08664">https://arxiv.org/pdf/2401.08664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08664]] Adapting Large Language Models for Education: Foundational Capabilities,  Potentials, and Challenges(https://arxiv.org/abs/2401.08664)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, rag</a></li>
<li><strong>Abstract: </strong>Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to offer personalized education resources due to the challenge of addressing the diverse obstacles students encounter throughout their learning journey. Recently, the emergence of large language models (LLMs), such as ChatGPT, offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM researches related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Based on the current development status, we further outline two approaches for an LLM-based education system: a unified approach and a mixture-of-expert (MoE) approach. Finally, we explore the challenges and future directions, providing new research opportunities and perspectives on adapting LLMs for education.</li>
<li><strong>摘要：</strong>在线教育平台利用互联网分发教育资源，力求提供便捷的教育，但往往无法与学生进行实时交流。由于解决学生在学习过程中遇到的各种障碍的挑战，他们经常难以提供个性化的教育资源。最近，大型语言模型（LLM）的出现，例如 ChatGPT，为通过理解个人请求来解决这个问题提供了可能性。尽管法学硕士在各个领域都取得了成功，但创建基于法学硕士的教育体系对于所需的广泛教育技能来说仍然具有挑战性。本文回顾了近年来兴起的与教育能力相关的法学硕士研究，包括数学、写作、编程、推理、知识问答等，旨在探讨其在构建下一代智能教育体系中的潜力。根据目前的发展现状，我们进一步概述了基于LLM的教育体系的两种途径：统一途径和混合专家（MoE）途径。最后，我们探讨了挑战和未来方向，为法学硕士适应教育提供了新的研究机会和观点。</li>
</ul>

<h3>Title: Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems  with Multi-Leg Demand Routes</h3>
<ul>
<li><strong>Authors: </strong>Joshua Levin, Randall Correll, Takanori Ide, Takafumi Suzuki, Takaho Saito, Alan Arai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08669">https://arxiv.org/abs/2401.08669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08669">https://arxiv.org/pdf/2401.08669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08669]] Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems  with Multi-Leg Demand Routes(https://arxiv.org/abs/2401.08669)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms. While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated. In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements. In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node. With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements. Our models have the advantage that they can be trained for a small number of trucks and nodes, and then embedded into a large supply chain to yield solutions for larger numbers of trucks and nodes. We test our approach on a real supply chain environment arising in the operations of Japanese automotive parts manufacturer Aisin Corporation, and find that our algorithm outperforms Aisin's previous best solution.</li>
<li><strong>摘要：</strong>深度强化学习 (RL) 已被证明可以有效地生成某些车辆路径问题 (VRP) 的近似解决方案，特别是在使用编码器-解码器注意机制生成的策略时。虽然这些技术对于相对简单的问题实例非常成功，但仍然存在研究不足且高度复杂的 VRP 变体，尚未证明有效的 RL 方法。在这项工作中，我们重点关注这样一种 VRP 变体，它包含多辆卡车和多段路由要求。在这些问题中，需求需要沿着节点序列移动，而不仅仅是从起始节点到结束节点。为了使深度强化学习成为现实世界工业规模供应链物流的可行策略，我们开发了现有编码器-解码器注意力模型的新扩展，使其能够处理多辆卡车和多支路路线要求。我们的模型的优点是可以针对少量卡车和节点进行训练，然后嵌入到大型供应链中，为大量卡车和节点提供解决方案。我们在日本汽车零部件制造商爱信公司运营中的真实供应链环境中测试了我们的方法，发现我们的算法优于爱信之前的最佳解决方案。</li>
</ul>

<h3>Title: Concept Alignment</h3>
<ul>
<li><strong>Authors: </strong>Sunayana Rane, Polyphony J. Bruna, Ilia Sucholutsky, Christopher Kello, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08672">https://arxiv.org/abs/2401.08672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08672">https://arxiv.org/pdf/2401.08672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08672]] Concept Alignment(https://arxiv.org/abs/2401.08672)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Discussion of AI alignment (alignment between humans and AI systems) has focused on value alignment, broadly referring to creating AI systems that share human values. We argue that before we can even attempt to align values, it is imperative that AI systems and humans align the concepts they use to understand the world. We integrate ideas from philosophy, cognitive science, and deep learning to explain the need for concept alignment, not just value alignment, between humans and machines. We summarize existing accounts of how humans and machines currently learn concepts, and we outline opportunities and challenges in the path towards shared concepts. Finally, we explain how we can leverage the tools already being developed in cognitive science and AI research to accelerate progress towards concept alignment.</li>
<li><strong>摘要：</strong>关于人工智能一致性（人类与人工智能系统之间的一致性）的讨论主要集中在价值一致性上，广义上是指创建共享人类价值观的人工智能系统。我们认为，在我们尝试调整价值观之前，人工智能系统和人类必须调整他们用来理解世界的概念。我们整合哲学、认知科学和深度学习的思想来解释人与机器之间概念对齐的必要性，而不仅仅是价值对齐。我们总结了人类和机器目前如何学习概念的现有描述，并概述了实现共享概念的道路上的机遇和挑战。最后，我们解释了如何利用认知科学和人工智能研究中已经开发的工具来加速概念协调的进展。</li>
</ul>

<h3>Title: Contrastive Learning with Negative Sampling Correction</h3>
<ul>
<li><strong>Authors: </strong>Lu Wang, Chao Du, Pu Zhao, Chuan Luo, Zhangchi Zhu, Bo Qiao, Wei Zhang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08690">https://arxiv.org/abs/2401.08690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08690">https://arxiv.org/pdf/2401.08690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08690]] Contrastive Learning with Negative Sampling Correction(https://arxiv.org/abs/2401.08690)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>As one of the most effective self-supervised representation learning methods, contrastive learning (CL) relies on multiple negative pairs to contrast against each positive pair. In the standard practice of contrastive learning, data augmentation methods are utilized to generate both positive and negative pairs. While existing works have been focusing on improving the positive sampling, the negative sampling process is often overlooked. In fact, the generated negative samples are often polluted by positive samples, which leads to a biased loss and performance degradation. To correct the negative sampling bias, we propose a novel contrastive learning method named Positive-Unlabeled Contrastive Learning (PUCL). PUCL treats the generated negative samples as unlabeled samples and uses information from positive samples to correct bias in contrastive loss. We prove that the corrected loss used in PUCL only incurs a negligible bias compared to the unbiased contrastive loss. PUCL can be applied to general contrastive learning problems and outperforms state-of-the-art methods on various image and graph classification tasks. The code of PUCL is in the supplementary file.</li>
<li><strong>摘要：</strong>作为最有效的自监督表示学习方法之一，对比学习（CL）依赖于多个负对来与每个正对进行对比。在对比学习的标准实践中，利用数据增强方法来生成正对和负对。虽然现有的工作一直专注于改进正采样，但负采样过程经常被忽视。事实上，生成的负样本经常受到正样本的污染，从而导致有偏损失和性能下降。为了纠正负采样偏差，我们提出了一种新颖的对比学习方法，称为正向无标记对比学习（PUCL）。 PUCL 将生成的负样本视为未标记样本，并使用来自正样本的信息来纠正对比损失中的偏差。我们证明，与无偏对比损失相比，PUCL 中使用的校正损失仅产生可忽略不计的偏差。 PUCL 可以应用于一般的对比学习问题，并且在各种图像和图形分类任务上优于最先进的方法。 PUCL的代码在补充文件中。</li>
</ul>

<h3>Title: Combining Confidence Elicitation and Sample-based Methods for  Uncertainty Quantification in Misinformation Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Mauricio Rivera, Jean-François Godbout, Reihaneh Rabbany, Kellin Pelrine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08694">https://arxiv.org/abs/2401.08694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08694">https://arxiv.org/pdf/2401.08694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08694]] Combining Confidence Elicitation and Sample-based Methods for  Uncertainty Quantification in Misinformation Mitigation(https://arxiv.org/abs/2401.08694)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, hallucination, prompt, rag</a></li>
<li><strong>Abstract: </strong>Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overall, our work proposes novel uncertainty quantification methods that will improve the reliability of Large Language Models in misinformation mitigation applications.</li>
<li><strong>摘要：</strong>大型语言模型已成为解决错误信息缓解问题的主要候选者。然而，现有的方法难以应对幻觉和过度自信的预测。我们提出了一个不确定性量化框架，该框架利用直接置信度启发和基于采样的一致性方法，为 NLP 错误信息缓解解决方案提供更好的校准。我们首先研究基于样本的一致性方法的校准，该方法利用跨样本大小和随机水平的一致性的独特特征。接下来，我们评估单步与两步置信度启发过程中稳健的数字语言提示的性能和分布变化。我们还比较了相同提示符与不同版本的 GPT 和不同数字比例的性能。最后，我们结合基于样本的一致性和语言化方法提出了一个混合框架，为 GPT 模型提供更好的不确定性估计。总的来说，我们的工作提出了新颖的不确定性量化方法，将提高大型语言模型在错误信息缓解应用中的可靠性。</li>
</ul>

<h3>Title: Enabling Collaborative Clinical Diagnosis of Infectious Keratitis by  Integrating Expert Knowledge and Interpretable Data-driven Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Zhengqing Fang, Shuowen Zhou, Zhouhang Yuan, Yuxuan Si, Mengze Li, Jinxu Li, Yesheng Xu, Wenjia Xie, Kun Kuang, Yingming Li, Fei Wu, Yu-Feng Yao</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08695">https://arxiv.org/abs/2401.08695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08695">https://arxiv.org/pdf/2401.08695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08695]] Enabling Collaborative Clinical Diagnosis of Infectious Keratitis by  Integrating Expert Knowledge and Interpretable Data-driven Intelligence(https://arxiv.org/abs/2401.08695)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Although data-driven artificial intelligence (AI) in medical image diagnosis has shown impressive performance in silico, the lack of interpretability makes it difficult to incorporate the "black box" into clinicians' workflows. To make the diagnostic patterns learned from data understandable by clinicians, we develop an interpretable model, knowledge-guided diagnosis model (KGDM), that provides a visualized reasoning process containing AI-based biomarkers and retrieved cases that with the same diagnostic patterns. It embraces clinicians' prompts into the interpreted reasoning through human-AI interaction, leading to potentially enhanced safety and more accurate predictions. This study investigates the performance, interpretability, and clinical utility of KGDM in the diagnosis of infectious keratitis (IK), which is the leading cause of corneal blindness. The classification performance of KGDM is evaluated on a prospective validation dataset, an external testing dataset, and an publicly available testing dataset. The diagnostic odds ratios (DOR) of the interpreted AI-based biomarkers are effective, ranging from 3.011 to 35.233 and exhibit consistent diagnostic patterns with clinic experience. Moreover, a human-AI collaborative diagnosis test is conducted and the participants with collaboration achieved a performance exceeding that of both humans and AI. By synergistically integrating interpretability and interaction, this study facilitates the convergence of clinicians' expertise and data-driven intelligence. The promotion of inexperienced ophthalmologists with the aid of AI-based biomarkers, as well as increased AI prediction by intervention from experienced ones, demonstrate a promising diagnostic paradigm for infectious keratitis using KGDM, which holds the potential for extension to other diseases where experienced medical practitioners are limited and the safety of AI is concerned.</li>
<li><strong>摘要：</strong>尽管数据驱动的人工智能（AI）在医学图像诊断中已经显示出令人印象深刻的性能，但缺乏可解释性使得很难将“黑匣子”纳入临床医生的工作流程。为了使临床医生能够理解从数据中学习到的诊断模式，我们开发了一种可解释的模型，即知识引导诊断模型（KGDM），它提供了包含基于人工智能的生物标志物的可视化推理过程，并检索具有相同诊断模式的病例。它通过人机交互将临床医生的提示纳入解释推理中，从而潜在地提高安全性和更准确的预测。本研究调查了 KGDM 在诊断感染性角膜炎 (IK) 中的性能、可解释性和临床实用性，感染性角膜炎是导致角膜失明的主要原因。 KGDM 的分类性能是在预期验证数据集、外部测试数据集和公开可用的测试数据集上进行评估的。基于 AI 的生物标志物解释的诊断比值比 (DOR) 是有效的，范围为 3.011 至 35.233，并且表现出与临床经验一致的诊断模式。此外，还进行了人机协作诊断测试，协作的参与者取得了超过人类和人工智能的性能。通过协同整合可解释性和交互性，这项研究促进了临床医生的专业知识和数据驱动的情报的融合。借助基于人工智能的生物标志物来提升缺乏经验的眼科医生，以及通过经验丰富的眼科医生的干预来增加人工智能预测，证明了使用 KGDM 的传染性角膜炎的一种有前途的诊断范式，该范式具有扩展到经验丰富的医疗从业者需要的其他疾病的潜力其局限性和人工智能的安全性令人担忧。</li>
</ul>

<h3>Title: Decoupled Prototype Learning for Reliable Test-Time Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Guowei Wang, Changxing Ding, Wentao Tan, Mingkui Tan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08703">https://arxiv.org/abs/2401.08703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08703">https://arxiv.org/pdf/2401.08703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08703]] Decoupled Prototype Learning for Reliable Test-Time Adaptation(https://arxiv.org/abs/2401.08703)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Test-time adaptation (TTA) is a task that continually adapts a pre-trained source model to the target domain during inference. One popular approach involves fine-tuning model with cross-entropy loss according to estimated pseudo-labels. However, its performance is significantly affected by noisy pseudo-labels. This study reveals that minimizing the classification error of each sample causes the cross-entropy loss's vulnerability to label noise. To address this issue, we propose a novel Decoupled Prototype Learning (DPL) method that features prototype-centric loss computation. First, we decouple the optimization of class prototypes. For each class prototype, we reduce its distance with positive samples and enlarge its distance with negative samples in a contrastive manner. This strategy prevents the model from overfitting to noisy pseudo-labels. Second, we propose a memory-based strategy to enhance DPL's robustness for the small batch sizes often encountered in TTA. We update each class's pseudo-feature from a memory in a momentum manner and insert an additional DPL loss. Finally, we introduce a consistency regularization-based approach to leverage samples with unconfident pseudo-labels. This approach transfers feature styles of samples with unconfident pseudo-labels to those with confident pseudo-labels. Thus, more reliable samples for TTA are created. The experimental results demonstrate that our methods achieve state-of-the-art performance on domain generalization benchmarks, and reliably improve the performance of self-training-based methods on image corruption benchmarks. The code will be released.</li>
<li><strong>摘要：</strong>测试时适应 (TTA) 是一项在推理过程中不断使预训练源模型适应目标域的任务。一种流行的方法涉及根据估计的伪标签微调具有交叉熵损失的模型。然而，它的性能受到噪声伪标签的显着影响。这项研究表明，最小化每个样本的分类误差会导致交叉熵损失容易受到标签噪声的影响。为了解决这个问题，我们提出了一种新颖的解耦原型学习（DPL）方法，其特点是以原型为中心的损失计算。首先，我们解耦类原型的优化。对于每个类原型，我们以对比的方式减小其与正样本的距离并扩大其与负样本的距离。这种策略可以防止模型过度拟合嘈杂的伪标签。其次，我们提出了一种基于内存的策略，以增强 DPL 对于 TTA 中经常遇到的小批量大小的鲁棒性。我们以动量方式从内存中更新每个类的伪特征，并插入额外的 DPL 损失。最后，我们引入了一种基于一致性正则化的方法来利用具有不置信伪标签的样本。这种方法将具有不置信伪标签的样本的特征样式转移到具有置信伪标签的样本的特征样式。因此，创建了更可靠的 TTA 样本。实验结果表明，我们的方法在领域泛化基准上实现了最先进的性能，并且可靠地提高了基于自训练的方法在图像损坏基准上的性能。代码将被发布。</li>
</ul>

<h3>Title: MA2GCN: Multi Adjacency relationship Attention Graph Convolutional  Networks for Traffic Prediction using Trajectory data</h3>
<ul>
<li><strong>Authors: </strong>Zhengke Sun, Yuliang Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08727">https://arxiv.org/abs/2401.08727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08727">https://arxiv.org/pdf/2401.08727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08727]] MA2GCN: Multi Adjacency relationship Attention Graph Convolutional  Networks for Traffic Prediction using Trajectory data(https://arxiv.org/abs/2401.08727)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>The problem of traffic congestion not only causes a large amount of economic losses, but also seriously endangers the urban environment. Predicting traffic congestion has important practical significance. So far, most studies have been based on historical data from sensors placed on different roads to predict future traffic flow and speed, to analyze the traffic congestion conditions of a certain road segment. However, due to the fixed position of sensors, it is difficult to mine new information. On the other hand, vehicle trajectory data is more flexible and can extract traffic information as needed. Therefore, we proposed a new traffic congestion prediction model - Multi Adjacency relationship Attention Graph Convolutional Networks(MA2GCN). This model transformed vehicle trajectory data into graph structured data in grid form, and proposed a vehicle entry and exit matrix based on the mobility between different grids. At the same time, in order to improve the performance of the model, this paper also built a new adaptive adjacency matrix generation method and adjacency matrix attention module. This model mainly used gated temporal convolution and graph convolution to extract temporal and spatial information, respectively. Compared with multiple baselines, our model achieved the best performance on Shanghai taxi GPS trajectory dataset. The code is available at https://github.com/zachysun/Taxi Traffic Benchmark.</li>
<li><strong>摘要：</strong>交通拥堵问题不仅造成大量经济损失，还严重危害城市环境。预测交通拥堵具有重要的现实意义。迄今为止，大多数研究都是基于放置在不同道路上的传感器的历史数据来预测未来的交通流量和速度，以分析某个路段的交通拥堵状况。然而，由于传感器位置固定，很难挖掘新的信息。另一方面，车辆轨迹数据更加灵活，可以根据需要提取交通信息。因此，我们提出了一种新的交通拥堵预测模型——多邻接关系注意力图卷积网络（MA2GCN）。该模型将车辆轨迹数据转化为网格形式的图结构数据，并根据不同网格之间的移动性提出了车辆进出矩阵。同时，为了提高模型的性能，本文还构建了新的自适应邻接矩阵生成方法和邻接矩阵注意模块。该模型主要使用门控时间卷积和图卷积分别提取时间和空间信息。与多个基线相比，我们的模型在上海出租车 GPS 轨迹数据集上取得了最佳性能。该代码可在 https://github.com/zachysun/Taxi Traffic Benchmark 获取。</li>
</ul>

<h3>Title: Bayes Conditional Distribution Estimation for Knowledge Distillation  Based on Conditional Mutual Information</h3>
<ul>
<li><strong>Authors: </strong>Linfeng Ye, Shayan Mohajer Hamidi, Renhao Tan, En-Hui Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08732">https://arxiv.org/abs/2401.08732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08732">https://arxiv.org/pdf/2401.08732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08732]] Bayes Conditional Distribution Estimation for Knowledge Distillation  Based on Conditional Mutual Information(https://arxiv.org/abs/2401.08732)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>It is believed that in knowledge distillation (KD), the role of the teacher is to provide an estimate for the unknown Bayes conditional probability distribution (BCPD) to be used in the student training process. Conventionally, this estimate is obtained by training the teacher using maximum log-likelihood (MLL) method. To improve this estimate for KD, in this paper we introduce the concept of conditional mutual information (CMI) into the estimation of BCPD and propose a novel estimator called the maximum CMI (MCMI) method. Specifically, in MCMI estimation, both the log-likelihood and CMI of the teacher are simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is further shown that maximizing the teacher's CMI value allows the teacher to capture more contextual information in an image cluster. Via conducting a thorough set of experiments, we show that by employing a teacher trained via MCMI estimation rather than one trained via MLL estimation in various state-of-the-art KD frameworks, the student's classification accuracy consistently increases, with the gain of up to 3.32\%. This suggests that the teacher's BCPD estimate provided by MCMI method is more accurate than that provided by MLL method. In addition, we show that such improvements in the student's accuracy are more drastic in zero-shot and few-shot settings. Notably, the student's accuracy increases with the gain of up to 5.72\% when 5\% of the training samples are available to the student (few-shot), and increases from 0\% to as high as 84\% for an omitted class (zero-shot). The code is available at \url{https://github.com/iclr2024mcmi/ICLRMCMI}.</li>
<li><strong>摘要：</strong>人们认为，在知识蒸馏（KD）中，教师的作用是提供对学生训练过程中使用的未知贝叶斯条件概率分布（BCPD）的估计。传统上，该估计是通过使用最大对数似然（MLL）方法训练教师来获得的。为了改进 KD 的估计，本文将条件互信息（CMI）的概念引入到 BCPD 的估计中，并提出了一种称为最大 CMI（MCMI）方法的新型估计器。具体来说，在 MCMI 估计中，当教师接受训练时，教师的对数似然和 CMI 同时最大化。通过 Eigen-CAM，进一步表明，最大化教师的 CMI 值可以让教师在图像簇中捕获更多的上下文信息。通过进行一系列彻底的实验，我们表明，在各种最先进的 KD 框架中，通过使用通过 MCMI 估计训练的教师而不是通过 MLL 估计训练的教师，学生的分类准确度持续提高，增益高达至 3.32%。这表明MCMI方法提供的教师BCPD估计比MLL方法提供的更准确。此外，我们还表明，在零样本和少样本设置中，学生准确性的提高更为显着。值得注意的是，当学生有 5\% 的训练样本（少数样本）可用时，学生的准确率提高了高达 5.72\%，并且对于省略的训练样本，准确率从 0\% 增加到高达 84\%类（零样本）。该代码位于 \url{https://github.com/iclr2024mcmi/ICLRMCMI}。</li>
</ul>

<h3>Title: MMToM-QA: Multimodal Theory of Mind Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Chuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio Torralba, Joshua B. Tenenbaum, Tianmin Shu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08743">https://arxiv.org/abs/2401.08743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08743">https://arxiv.org/pdf/2401.08743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08743]] MMToM-QA: Multimodal Theory of Mind Question Answering(https://arxiv.org/abs/2401.08743)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, rag</a></li>
<li><strong>Abstract: </strong>Theory of Mind (ToM), the ability to understand people's minds, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data, which can include visual cues, linguistic narratives, or both. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models.</li>
<li><strong>摘要：</strong>心智理论 (ToM) 是理解人们思想的能力，是开发具有人类水平社交智能的机器的重要组成部分。最近的机器学习模型，特别是大型语言模型，似乎显示了 ToM 理解的某些方面。然而，现有的 ToM 基准使用单峰数据集 - 视频或文本。另一方面，人类 ToM 不仅仅是视频或文本理解。人们可以根据从任何可用数据中提取的概念表征（例如目标、信念、计划）灵活地推断他人的想法，这些数据可以包括视觉线索、语言叙述或两者兼而有之。为了解决这个问题，我们引入了多模态心理理论问答 (MMToM-QA) 基准。 MMToM-QA 根据多模态数据和有关家庭环境中人的活动的不同类型的单模态数据来全面评估机器 ToM。为了设计多模态 ToM 能力，我们提出了一种新方法：BIP-ALM（语言模型加速的贝叶斯逆向规划）。 BIP-ALM 从多模态数据中提取统一表示，并利用语言模型进行可扩展的贝叶斯逆向规划。我们对人类表现、BIP-ALM 和最先进的模型（包括 GPT-4）进行了系统比较。实验表明，大型语言模型和大型多模态模型仍然缺乏强大的 ToM 能力。另一方面，BIP-ALM 通过利用基于模型的心理推理和语言模型的力量，显示出了有希望的结果。</li>
</ul>

<h3>Title: HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical  Assistance</h3>
<ul>
<li><strong>Authors: </strong>Huanjun Kong, Songyang Zhang, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08772">https://arxiv.org/abs/2401.08772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08772">https://arxiv.org/pdf/2401.08772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08772]] HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical  Assistance(https://arxiv.org/abs/2401.08772)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, code, chat</a></li>
<li><strong>Abstract: </strong>In this work, we present HuixiangDou, a technical assistant powered by Large Language Models (LLM). This system is designed to assist algorithm developers by providing insightful responses to questions related to open-source algorithm projects, such as computer vision and deep learning projects from OpenMMLab. We further explore the integration of this assistant into the group chats of instant messaging (IM) tools such as WeChat and Lark. Through several iterative improvements and trials, we have developed a sophisticated technical chat assistant capable of effectively answering users' technical questions without causing message flooding. This paper's contributions include: 1) Designing an algorithm pipeline specifically for group chat scenarios; 2) Verifying the reliable performance of text2vec in task rejection; 3) Identifying three critical requirements for LLMs in technical-assistant-like products, namely scoring ability, In-Context Learning (ICL), and Long Context. We have made the software and source code available at https://github.com/internlm/huixiangdou to aid in future research and application. HuixiangDou is applicable to any group chat within IM tools.</li>
<li><strong>摘要：</strong>在这项工作中，我们介绍了 HuiyangDou，一个由大型语言模型 (LLM) 提供支持的技术助理。该系统旨在通过对与开源算法项目（例如 OpenMMLab 的计算机视觉和深度学习项目）相关的问题提供富有洞察力的回答来帮助算法开发人员。我们进一步探索将该助手集成到微信、飞书等即时通讯工具的群聊中。经过多次迭代改进和试验，我们开发出了一款成熟的技术聊天助手，能够有效回答用户的技术问题，同时不会造成消息泛滥。本文的贡献包括：1）专门针对群聊场景设计了算法管道； 2）验证text2vec在任务拒绝方面的可靠性能； 3）确定法学硕士在技术助理类产品中的三个关键要求，即评分能力、情境学习（ICL）和长情境。我们已在 https://github.com/internlm/huixiangdou 提供软件和源代码，以帮助将来的研究和应用。回享豆适用于任何IM工具内的群聊。</li>
</ul>

<h3>Title: AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant  Reviews and Images on Social Media</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Gambetti, Qiwei Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08825">https://arxiv.org/abs/2401.08825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08825">https://arxiv.org/pdf/2401.08825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08825]] AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant  Reviews and Images on Social Media(https://arxiv.org/abs/2401.08825)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag</a></li>
<li><strong>Abstract: </strong>Online reviews in the form of user-generated content (UGC) significantly impact consumer decision-making. However, the pervasive issue of not only human fake content but also machine-generated content challenges UGC's reliability. Recent advances in Large Language Models (LLMs) may pave the way to fabricate indistinguishable fake generated content at a much lower cost. Leveraging OpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a multi-modal dataset of 20,144 restaurant review-image pairs divided into authentic and machine-generated. We explore unimodal and multimodal detection models, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from readability and photographic theories to score reviews and images, respectively, demonstrating their utility as hand-crafted features in scalable and interpretable detection models, with comparable performance. The paper contributes by open-sourcing the dataset and releasing fake review detectors, recommending its use in unimodal and multimodal fake review detection tasks, and evaluating linguistic and visual features in synthetic versus authentic data.</li>
<li><strong>摘要：</strong>用户生成内容 (UGC) 形式的在线评论显着影响消费者的决策。然而，普遍存在的人类虚假内容和机器生成内容的问题都对 UGC 的可靠性提出了挑战。大型语言模型（LLM）的最新进展可能为以低得多的成本制造难以区分的虚假生成内容铺平道路。利用 OpenAI 的 GPT-4-Turbo 和 DALL-E-2 模型，我们制作了 AiGen-FoodReview，这是一个包含 20,144 个餐厅评论-图像对的多模态数据集，分为真实的和机器生成的。我们探索单模态和多模态检测模型，利用 FLAVA 实现了 99.80% 的多模态准确率。我们使用可读性和摄影理论的属性分别对评论和图像进行评分，证明它们在可扩展和可解释的检测模型中作为手工制作的特征的实用性，并且具有可比的性能。该论文通过开源数据集并发布虚假评论检测器、推荐其在单模态和多模态虚假评论检测任务中的使用以及评估合成数据与真实数据中的语言和视觉特征来做出贡献。</li>
</ul>

<h3>Title: REValueD: Regularised Ensemble Value-Decomposition for Factorisable  Markov Decision Processes</h3>
<ul>
<li><strong>Authors: </strong>David Ireland, Giovanni Montana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08850">https://arxiv.org/abs/2401.08850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08850">https://arxiv.org/pdf/2401.08850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08850]] REValueD: Regularised Ensemble Value-Decomposition for Factorisable  Markov Decision Processes(https://arxiv.org/abs/2401.08850)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag, agent</a></li>
<li><strong>Abstract: </strong>Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance, evaluating the significance of the regularisation loss and the scalability of REValueD with increasing sub-actions per dimension.</li>
<li><strong>摘要：</strong>由于存在大量可能的动作，离散动作强化学习算法在具有高维离散动作空间的任务中通常会出现问题。最近的一项进展利用价值分解（多智能体强化学习的概念）来应对这一挑战。这项研究深入研究了这种价值分解的影响，揭示了虽然它减少了 Q 学习算法固有的高估偏差，但它放大了目标方差。为了解决这个问题，我们提出了一组批评家来减轻目标方差。此外，我们引入了正则化损失，有助于减轻一个维度中的探索性行为对其他维度中最优行为的价值的影响。我们的新颖算法 REValueD 在 DeepMind Control Suite 任务的离散版本上进行了测试，展示了卓越的性能，特别是在具有挑战性的人形和狗任务中。我们进一步剖析影响 REValueD 性能的因素，评估正则化损失的重要性以及随着每个维度的子操作的增加 REValueD 的可扩展性。</li>
</ul>

<h3>Title: MambaTab: A Simple Yet Effective Approach for Handling Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Md Atik Ahamed, Qiang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08867">https://arxiv.org/abs/2401.08867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08867">https://arxiv.org/pdf/2401.08867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08867]] MambaTab: A Simple Yet Effective Approach for Handling Tabular Data(https://arxiv.org/abs/2401.08867)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Tabular data remains ubiquitous across domains despite growing use of images and texts for machine learning. While deep learning models like convolutional neural networks and transformers achieve strong performance on tabular data, they require extensive data preprocessing, tuning, and resources, limiting accessibility and scalability. This work develops an innovative approach based on a structured state-space model (SSM), MambaTab, for tabular data. SSMs have strong capabilities for efficiently extracting effective representations from data with long-range dependencies. MambaTab leverages Mamba, an emerging SSM variant, for end-to-end supervised learning on tables. Compared to state-of-the-art baselines, MambaTab delivers superior performance while requiring significantly fewer parameters and minimal preprocessing, as empirically validated on diverse benchmark datasets. MambaTab's efficiency, scalability, generalizability, and predictive gains signify it as a lightweight, "out-of-the-box" solution for diverse tabular data with promise for enabling wider practical applications.</li>
<li><strong>摘要：</strong>尽管越来越多地使用图像和文本进行机器学习，但表格数据在各个领域仍然普遍存在。虽然卷积神经网络和 Transformer 等深度学习模型在表格数据上实现了强大的性能，但它们需要大量的数据预处理、调整和资源，限制了可访问性和可扩展性。这项工作针对表格数据开发了一种基于结构化状态空间模型 (SSM) MambaTab 的创新方法。 SSM 具有从具有远程依赖性的数据中有效提取有效表示的强大功能。 MambaTab 利用 Mamba（一种新兴的 SSM 变体）在表上进行端到端监督学习。与最先进的基准相比，MambaTab 提供了卓越的性能，同时需要显着更少的参数和最少的预处理，正如在不同基准数据集上的经验验证。 MambaTab 的效率、可扩展性、通用性和预测收益表明它是一种轻量级、“开箱即用”的解决方案，适用于各种表格数据，有望实现更广泛的实际应用。</li>
</ul>

<h3>Title: MADA: Meta-Adaptive Optimizers through hyper-gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Kaan Ozkara, Can Karakus, Parameswaran Raman, Mingyi Hong, Shoham Sabach, Branislav Kveton, Volkan Cevher</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08893">https://arxiv.org/abs/2401.08893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08893">https://arxiv.org/pdf/2401.08893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08893]] MADA: Meta-Adaptive Optimizers through hyper-gradient Descent(https://arxiv.org/abs/2401.08893)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Since Adam was introduced, several novel adaptive optimizers for deep learning have been proposed. These optimizers typically excel in some tasks but may not outperform Adam uniformly across all tasks. In this work, we introduce Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can generalize several known optimizers and dynamically learn the most suitable one during training. The key idea in MADA is to parameterize the space of optimizers and search through it using hyper-gradient descent. Numerical results suggest that MADA is robust against sub-optimally tuned hyper-parameters, and outperforms Adam, Lion, and Adan with their default hyper-parameters, often even with optimized hyper-parameters. We also propose AVGrad, a variant of AMSGrad where the maximum operator is replaced with averaging, and observe that it performs better within MADA. Finally, we provide a convergence analysis to show that interpolation of optimizers (specifically, AVGrad and Adam) can improve their error bounds (up to constants), hinting at an advantage for meta-optimizers.</li>
<li><strong>摘要：</strong>自从 Adam 被引入以来，已经提出了几种新颖的深度学习自适应优化器。这些优化器通常在某些任务中表现出色，但在所有任务中可能无法统一地优于 Adam。在这项工作中，我们引入了元自适应优化器（MADA），这是一种统一的优化器框架，可以概括几个已知的优化器并在训练过程中动态学习最合适的优化器。 MADA 的关键思想是参数化优化器的空间并使用超梯度下降对其进行搜索。数值结果表明，MADA 对于次优调整的超参数具有鲁棒性，并且在默认超参数下（通常甚至在优化超参数下）优于 Adam、Lion 和 Adan。我们还提出了 AVGrad，这是 AMSGrad 的一个变体，其中最大值运算符被平均替换，并观察到它在 MADA 中表现更好。最后，我们提供了收敛分析，以表明优化器的插值（特别是 AVGrad 和 Adam）可以改善其误差范围（最多为常数），这暗示了元优化器的优势。</li>
</ul>

<h3>Title: cedar: Composable and Optimized Machine Learning Input Data Pipelines</h3>
<ul>
<li><strong>Authors: </strong>Mark Zhao, Emanuel Adamiak, Christos Kozyrakis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08895">https://arxiv.org/abs/2401.08895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08895">https://arxiv.org/pdf/2401.08895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08895]] cedar: Composable and Optimized Machine Learning Input Data Pipelines(https://arxiv.org/abs/2401.08895)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex of transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources -- or worse -- underutilize expensive accelerators. To address these demands, we present cedar, a programming model and framework that allows users to easily build, optimize, and execute input data pipelines. cedar presents an easy-to-use programming interface, allowing users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. Meanwhile, cedar transparently applies a complex and extensible set of optimization techniques (e.g., offloading, caching, prefetching, fusion, and reordering). It then orchestrates processing across a customizable set of local and distributed compute resources in order to maximize processing performance and efficiency, all without user input. On average across six diverse input data pipelines, cedar achieves a 2.49x, 1.87x, 2.18x, and 2.74x higher performance compared to tf.data, tf.data service, Ray Data, and PyTorch's DataLoader, respectively.</li>
<li><strong>摘要：</strong>输入数据管道是每个机器学习 (ML) 训练作业的重要组成部分。它负责读取大量训练数据，使用复杂的转换处理批量样本，并以低延迟和高吞吐量将它们加载到训练节点上。在数据量和训练吞吐量需求激增的推动下，高性能输入数据系统变得越来越重要。不幸的是，当前的输入数据系统无法充分利用关键性能优化，导致基础设施效率极低，需要大量资源，或者更糟的是，昂贵的加速器没有得到充分利用。为了满足这些需求，我们推出了 cedar，这是一种编程模型和框架，允许用户轻松构建、优化和执行输入数据管道。 cedar 提供了一个易于使用的编程接口，允许用户使用支持任意 ML 框架和库的可组合运算符来定义输入数据管道。同时，cedar 透明地应用了一组复杂且可扩展的优化技术（例如卸载、缓存、预取、融合和重新排序）。然后，它会跨一组可定制的本地和分布式计算资源协调处理，以最大限度地提高处理性能和效率，而所有这些都无需用户输入。平均而言，在六个不同的输入数据管道中，与 tf.data、tf.data 服务、Ray Data 和 PyTorch 的 DataLoader 相比，cedar 的性能分别提高了 2.49 倍、1.87 倍、2.18 倍和 2.74 倍。</li>
</ul>

<h3>Title: CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in  Variational AutoEncoder</h3>
<ul>
<li><strong>Authors: </strong>Hee-Jun Jung, Jaehyoung Jeong, Kangil Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08897">https://arxiv.org/abs/2401.08897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08897">https://arxiv.org/pdf/2401.08897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08897]] CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in  Variational AutoEncoder(https://arxiv.org/abs/2401.08897)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metric for multi-factor changes in comparison to disentanglement evaluation in VAEs. In quantitative and in-depth qualitative analysis, CFASL demonstrates a significant improvement of disentanglement in single-factor change, and multi-factor change conditions compared to state-of-the-art methods.</li>
<li><strong>摘要：</strong>输入和潜在向量的对称性为 VAE 中的解缠结学习提供了宝贵的见解。然而，只有少数作品被提出作为无监督方法，即使这些作品也需要训练数据中的已知因子信息。我们提出了一种新方法，复合因子对齐对称学习（CFASL），它被集成到 VAE 中，用于在无监督学习中学习基于对称的解缠，而无需了解数据集因子信息。CFASL 结合了三个用于学习基于对称的解缠的新功能： 1) 注入归纳偏差，以将潜在向量维度与显式可学习对称码本中的因子对齐对称性对齐 2) 通过学习码本中的因子对齐对称性，学习复合对称性以表达两个随机样本之间的未知因子变化 3) 诱导组具有两个条件的训练 VAE 的等变编码器和解码器。此外，与 VAE 中的解开评估相比，我们提出了一种针对多因素变化的扩展评估指标。在定量和深入的定性分析中，与最先进的方法相比，CFASL 证明了单因素变化和多因素变化条件下解缠结的显着改善。</li>
</ul>

<h3>Title: DeLF: Designing Learning Environments with Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Aida Afshar, Wenchao Li</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08936">https://arxiv.org/abs/2401.08936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08936">https://arxiv.org/pdf/2401.08936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08936]] DeLF: Designing Learning Environments with Foundation Models(https://arxiv.org/abs/2401.08936)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) offers a capable and intuitive structure for the fundamental sequential decision-making problem. Despite impressive breakthroughs, it can still be difficult to employ RL in practice in many simple applications. In this paper, we try to address this issue by introducing a method for designing the components of the RL environment for a given, user-intended application. We provide an initial formalization for the problem of RL component design, that concentrates on designing a good representation for observation and action space. We propose a method named DeLF: Designing Learning Environments with Foundation Models, that employs large language models to design and codify the user's intended learning scenario. By testing our method on four different learning environments, we demonstrate that DeLF can obtain executable environment codes for the corresponding RL problems.</li>
<li><strong>摘要：</strong>强化学习 (RL) 为基本的顺序决策问题提供了一种强大而直观的结构。尽管取得了令人印象深刻的突破，但在许多简单应用中实际应用强化学习仍然很困难。在本文中，我们尝试通过介绍一种为给定的用户预期应用程序设计 RL 环境组件的方法来解决这个问题。我们为 RL 组件设计问题提供了初始形式化，重点是为观察和动作空间设计良好的表示。我们提出了一种名为 DeLF：使用基础模型设计学习环境的方法，该方法采用大型语言模型来设计和编码用户预期的学习场景。通过在四种不同的学习环境上测试我们的方法，我们证明 DeLF 可以获得相应 RL 问题的可执行环境代码。</li>
</ul>

<h3>Title: CEL: A Continual Learning Model for Disease Outbreak Prediction by  Leveraging Domain Adaptation via Elastic Weight Consolidation</h3>
<ul>
<li><strong>Authors: </strong>Saba Aslam, Abdur Rasool, Hongyan Wu, Xiaoli Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08940">https://arxiv.org/abs/2401.08940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08940">https://arxiv.org/pdf/2401.08940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08940]] CEL: A Continual Learning Model for Disease Outbreak Prediction by  Leveraging Domain Adaptation via Elastic Weight Consolidation(https://arxiv.org/abs/2401.08940)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Continual learning, the ability of a model to learn over time without forgetting previous knowledge and, therefore, be adaptive to new data, is paramount in dynamic fields such as disease outbreak prediction. Deep neural networks, i.e., LSTM, are prone to error due to catastrophic forgetting. This study introduces a novel CEL model for continual learning by leveraging domain adaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate the catastrophic forgetting phenomenon in a domain incremental setting. The Fisher Information Matrix (FIM) is constructed with EWC to develop a regularization term that penalizes changes to important parameters, namely, the important previous knowledge. CEL's performance is evaluated on three distinct diseases, Influenza, Mpox, and Measles, with different metrics. The high R-squared values during evaluation and reevaluation outperform the other state-of-the-art models in several contexts, indicating that CEL adapts to incremental data well. CEL's robustness and reliability are underscored by its minimal 65% forgetting rate and 18% higher memory stability compared to existing benchmark studies. This study highlights CEL's versatility in disease outbreak prediction, addressing evolving data with temporal patterns. It offers a valuable model for proactive disease control with accurate, timely predictions.</li>
<li><strong>摘要：</strong>持续学习是模型随着时间的推移学习而不忘记以前的知识并因此适应新数据的能力，在疾病爆发预测等动态领域中至关重要。深度神经网络，即 LSTM，很容易因灾难性遗忘而出错。本研究引入了一种新颖的 CEL 模型，通过弹性权重合并 (EWC) 利用领域适应来进行持续学习。该模型旨在减轻领域增量环境中的灾难性遗忘现象。 Fisher 信息矩阵（FIM）是用 EWC 构建的，以开发一个正则化项，该项对重要参数（即重要的先前知识）的变化进行惩罚。 CEL 的表现是根据三种不同的疾病（流感、Mpox 和麻疹）使用不同的指标进行评估的。评估和重新评估过程中的高 R 平方值在多种情况下都优于其他最先进的模型，这表明 CEL 能够很好地适应增量数据。与现有基准研究相比，CEL 的遗忘率最低为 65%，记忆稳定性高出 18%，这凸显了 CEL 的稳健性和可靠性。这项研究强调了 CEL 在疾病爆发预测方面的多功能性，可处理具有时间模式的不断变化的数据。它为主动疾病控制和准确、及时的预测提供了一个有价值的模型。</li>
</ul>

<h3>Title: Cascading Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yihan Du, R. Srikant, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08961">https://arxiv.org/abs/2401.08961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08961">https://arxiv.org/pdf/2401.08961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08961]] Cascading Reinforcement Learning(https://arxiv.org/abs/2401.08961)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with large attraction probabilities but also leading to good successor states. This imposes a huge computational challenge due to the combinatorial action space. To tackle this challenge, we delve into the properties of value functions, and design an oracle BestPerm to efficiently find the optimal item list. Equipped with BestPerm, we develop two algorithms CascadingVI and CascadingBPI, which are both computationally-efficient and sample-efficient, and provide near-optimal regret and sample complexity guarantees. Furthermore, we present experiments to show the improved computational and sample efficiencies of our algorithms compared to straightforward adaptations of existing RL algorithms in practice.</li>
<li><strong>摘要：</strong>近年来，级联老虎机由于适用于推荐系统和在线广告而受到欢迎。在级联老虎机模型中，在每个时间步长，代理都会从项目池中推荐一个有序的项目子集（称为项目列表），每个项目都与未知的吸引概率相关联。然后，用户检查列表，并单击第一个有吸引力的项目（如果有），之后代理会收到奖励。代理的目标是最大化预期累积奖励。然而，有关级联老虎机的现有文献忽略了用户状态（例如历史行为）对推荐的影响以及会话进行时状态的变化。受此事实的启发，我们提出了一个通用的级联强化学习框架，该框架考虑了用户状态和状态转换对决策的影响。在级联强化学习中，我们不仅需要选择具有较大吸引概率的项目，而且还需要选择具有良好后继状态的项目。由于组合动作空间，这带来了巨大的计算挑战。为了应对这一挑战，我们深入研究了价值函数的属性，并设计了一个预言机 BestPerm 来有效地找到最佳项目列表。配备BestPerm，我们开发了两种算法CascadingVI和CascadingBPI，它们都具有计算效率和样本效率，并提供接近最优的遗憾和样本复杂性保证。此外，我们还通过实验来展示我们的算法与实践中现有 RL 算法的直接改编相比，计算和样本效率的提高。</li>
</ul>

<h3>Title: ReFT: Reasoning with Reinforced Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08967">https://arxiv.org/abs/2401.08967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08967">https://arxiv.org/pdf/2401.08967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08967]] ReFT: Reasoning with Reinforced Fine-Tuning(https://arxiv.org/abs/2401.08967)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.</li>
<li><strong>摘要：</strong>增强大型语言模型（LLM）推理能力的一种方法是使用思想链（CoT）注释进行监督微调（SFT）。然而，这种方法没有表现出足够强的泛化能力，因为训练仅依赖于给定的 CoT 数据。例如，在数学问题解决中，训练数据中的每个问题通常只有一个带注释的推理路径。直观上，算法最好从给定问题的多个带注释的推理路径中学习。为了解决这个问题，我们提出了一种简单而有效的方法，称为强化微调（ReFT），以增强学习 LLM 推理的通用性，以解决数学问题为例。 ReFT首先用SFT预热模型，然后采用在线强化学习，特别是本文中的PPO算法，进一步微调模型，在给定问题的情况下自动采样大量推理路径，自然而然地获得奖励从真实答案中得出。对 GSM8K、MathQA 和 SVAMP 数据集的大量实验表明，ReFT 的性能明显优于 SFT，并且通过结合多数投票和重新排序等推理时间策略，可以进一步提高性能。请注意，ReFT 通过从与 SFT 相同的训练问题中学习来获得改进，而不依赖于额外或增强的训练问题。这表明 ReFT 具有卓越的泛化能力。</li>
</ul>

<h3>Title: ACT-GAN: Radio map construction based on generative adversarial networks  with ACT blocks</h3>
<ul>
<li><strong>Authors: </strong>Chen Qi, Yang Jingjing, Huang Ming, Zhou Qiang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08976">https://arxiv.org/abs/2401.08976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08976">https://arxiv.org/pdf/2401.08976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08976]] ACT-GAN: Radio map construction based on generative adversarial networks  with ACT blocks(https://arxiv.org/abs/2401.08976)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The radio map, serving as a visual representation of electromagnetic spatial characteristics, plays a pivotal role in assessment of wireless communication networks and radio monitoring coverage. Addressing the issue of low accuracy existing in the current radio map construction, this paper presents a novel radio map construction method based on generative adversarial network (GAN) in which the Aggregated Contextual-Transformation (AOT) block, Convolutional Block Attention Module (CBAM), and Transposed Convolution (T-Conv) block are applied to the generator, and we name it as ACT-GAN. It significantly improves the reconstruction accuracy and local texture of the radio maps. The performance of ACT-GAN across three different scenarios is demonstrated. Experiment results reveal that in the scenario without sparse discrete observations, the proposed method reduces the root mean square error (RMSE) by 14.6% in comparison to the state-of-the-art models. In the scenario with sparse discrete observations, the RMSE is diminished by 13.2%. Furthermore, the predictive results of the proposed model show a more lucid representation of electromagnetic spatial field distribution. To verify the universality of this model in radio map construction tasks, the scenario of unknown radio emission source is investigated. The results indicate that the proposed model is robust radio map construction and accurate in predicting the location of the emission source.</li>
<li><strong>摘要：</strong>无线电地图作为电磁空间特征的直观表示，在无线通信网络评估和无线电监测覆盖范围方面发挥着关键作用。针对当前无线电地图构建中存在的精度不高的问题，提出一种基于生成对抗网络（GAN）的新型无线电地图构建方法，其中聚合上下文变换（AOT）块、卷积块注意力模块（CBAM） ，和转置卷积（T-Conv）块应用于生成器，我们将其命名为 ACT-GAN。它显着提高了无线电地图的重建精度和局部纹理。展示了 ACT-GAN 在三种不同场景中的性能。实验结果表明，在没有稀疏离散观测值的情况下，与最先进的模型相比，该方法将均方根误差（RMSE）降低了 14.6%。在离散观测值稀疏的情况下，RMSE 减小了 13.2%。此外，该模型的预测结果显示了电磁空间场分布的更清晰的表示。为了验证该模型在无线电地图构建任务中的普适性，对未知无线电发射源的场景进行了研究。结果表明，所提出的模型是稳健的无线电地图构建，并且可以准确地预测排放源的位置。</li>
</ul>

<h3>Title: FedLoGe: Joint Local and Generic Federated Learning under Long-tailed  Data</h3>
<ul>
<li><strong>Authors: </strong>Zikai Xiao, Zihan Chen, Liyinglan Liu, Yang Feng, Jian Wu, Wanlu Liu, Joey Tianyi Zhou, Howard Hao Yang, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08977">https://arxiv.org/abs/2401.08977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08977">https://arxiv.org/pdf/2401.08977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08977]] FedLoGe: Joint Local and Generic Federated Learning under Long-tailed  Data(https://arxiv.org/abs/2401.08977)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected from decentralized local clients manifests a globally prevalent long-tailed distribution, has garnered considerable attention in recent times. In the context of Fed-LT, existing works have predominantly centered on addressing the data imbalance issue to enhance the efficacy of the generic global model while neglecting the performance at the local level. In contrast, conventional Personalized Federated Learning (pFL) techniques are primarily devised to optimize personalized local models under the presumption of a balanced global data distribution. This paper introduces an approach termed Federated Local and Generic Model Training in Fed-LT (FedLoGe), which enhances both local and generic model performance through the integration of representation learning and classifier alignment within a neural collapse framework. Our investigation reveals the feasibility of employing a shared backbone as a foundational framework for capturing overarching global trends, while concurrently employing individualized classifiers to encapsulate distinct refinements stemming from each client's local features. Building upon this discovery, we establish the Static Sparse Equiangular Tight Frame Classifier (SSE-C), inspired by neural collapse principles that naturally prune extraneous noisy features and foster the acquisition of potent data representations. Furthermore, leveraging insights from imbalance neural collapse's classifier norm patterns, we develop Global and Local Adaptive Feature Realignment (GLA-FR) via an auxiliary global classifier and personalized Euclidean norm transfer to align global features with client preferences. Extensive experimental results on CIFAR-10/100-LT, ImageNet, and iNaturalist demonstrate the advantage of our method over state-of-the-art pFL and Fed-LT approaches.</li>
<li><strong>摘要：</strong>联合长尾学习（Fed-LT）是一种范式，其中从分散的本地客户端收集的数据表现出全球普遍的长尾分布，近年来引起了相当大的关注。在 Fed-LT 的背景下，现有的工作主要集中在解决数据不平衡问题，以提高通用全球模型的有效性，而忽略了地方层面的表现。相比之下，传统的个性化联合学习（pFL）技术主要是为了在平衡的全局数据分布的假设下优化个性化本地模型。本文介绍了一种名为 Fed-LT 联合局部和通用模型训练 (FedLoGe) 的方法，该方法通过在神经崩溃框架内集成表示学习和分类器对齐来增强局部和通用模型性能。我们的调查揭示了采用共享主干网作为捕捉总体全球趋势的基础框架的可行性，同时采用个性化分类器来封装源自每个客户本地特征的独特改进。基于这一发现，我们建立了静态稀疏等角紧框架分类器（SSE-C），其灵感来自于神经崩溃原理，该原理自然地修剪无关的噪声特征并促进有效数据表示的获取。此外，利用不平衡神经崩溃的分类器范数模式的见解，我们通过辅助全局分类器和个性化欧几里德范数迁移开发全局和局部自适应特征重新对齐（GLA-FR），以使全局特征与客户偏好保持一致。 CIFAR-10/100-LT、ImageNet 和 iNaturalist 上的大量实验结果证明了我们的方法相对于最先进的 pFL 和 Fed-LT 方法的优势。</li>
</ul>

<h3>Title: A GAN-based data poisoning framework against anomaly detection in  vertical federated learning</h3>
<ul>
<li><strong>Authors: </strong>Xiaolin Chen, Daoguang Zan, Wei Li, Bei Guan, Yongji Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08984">https://arxiv.org/abs/2401.08984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08984">https://arxiv.org/pdf/2401.08984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08984]] A GAN-based data poisoning framework against anomaly detection in  vertical federated learning(https://arxiv.org/abs/2401.08984)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>In vertical federated learning (VFL), commercial entities collaboratively train a model while preserving data privacy. However, a malicious participant's poisoning attack may degrade the performance of this collaborative model. The main challenge in achieving the poisoning attack is the absence of access to the server-side top model, leaving the malicious participant without a clear target model. To address this challenge, we introduce an innovative end-to-end poisoning framework P-GAN. Specifically, the malicious participant initially employs semi-supervised learning to train a surrogate target model. Subsequently, this participant employs a GAN-based method to produce adversarial perturbations to degrade the surrogate target model's performance. Finally, the generator is obtained and tailored for VFL poisoning. Besides, we develop an anomaly detection algorithm based on a deep auto-encoder (DAE), offering a robust defense mechanism to VFL scenarios. Through extensive experiments, we evaluate the efficacy of P-GAN and DAE, and further analyze the factors that influence their performance.</li>
<li><strong>摘要：</strong>在垂直联合学习（VFL）中，商业实体协作训练模型，同时保护数据隐私。然而，恶意参与者的投毒攻击可能会降低这种协作模型的性能。实现投毒攻击的主要挑战是无法访问服务器端顶层模型，导致恶意参与者没有明确的目标模型。为了应对这一挑战，我们引入了一种创新的端到端中毒框架 P-GAN。具体来说，恶意参与者最初采用半监督学习来训练代理目标模型。随后，该参与者采用基于 GAN 的方法来产生对抗性扰动，从而降低替代目标模型的性能。最后，获得了生成器并针对 VFL 中毒进行了定制。此外，我们开发了一种基于深度自动编码器（DAE）的异常检测算法，为VFL场景提供了强大的防御机制。通过大量的实验，我们评估了 P-GAN 和 DAE 的功效，并进一步分析了影响其性能的因素。</li>
</ul>

<h3>Title: Efficient Adapter Finetuning for Tail Languages in Streaming  Multilingual ASR</h3>
<ul>
<li><strong>Authors: </strong>Junwen Bai, Bo Li, Qiujia Li, Tara N. Sainath, Trevor Strohman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08992">https://arxiv.org/abs/2401.08992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08992">https://arxiv.org/pdf/2401.08992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08992]] Efficient Adapter Finetuning for Tail Languages in Streaming  Multilingual ASR(https://arxiv.org/abs/2401.08992)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>The end-to-end ASR model is often desired in the streaming multilingual scenario since it is easier to deploy and can benefit from pre-trained speech models such as powerful foundation models. Meanwhile, the heterogeneous nature and imbalanced data abundance of different languages may cause performance degradation, leading to asynchronous peak performance for different languages during training, especially on tail ones. Sometimes even the data itself may become unavailable as a result of the enhanced privacy protection. Existing work tend to significantly increase the model size or learn language-specific decoders to accommodate each language separately. In this study, we explore simple yet effective Language-Dependent Adapter (LDA) finetuning under a cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for tail languages in the streaming multilingual ASR. The adapter only accounts for 0.4% of the full model per language. It is plugged into the frozen foundation model and is the only trainable module during the finetuning process with noisy student training. The final model merges the adapter parameters from different checkpoints for different languages. The model performance is validated on a challenging multilingual dictation dataset, which includes 39 tail languages across Latin, Greek, Arabic, etc. Our proposed method brings 12.2% word error rate reduction on average and up to 37.5% on a single locale. Furthermore, we show that our parameter-efficient LDA can match the quality of the full model finetuning, thus greatly alleviating the asynchronous peak performance issue.</li>
<li><strong>摘要：</strong>在流式多语言场景中通常需要端到端 ASR 模型，因为它更容易部署并且可以受益于预训练的语音模型（例如强大的基础模型）。同时，不同语言的异构性和数据丰度不平衡可能会导致性能下降，导致不同语言在训练过程中出现异步峰值性能，尤其是尾部语言。有时，由于隐私保护的增强，甚至数据本身也可能变得不可用。现有的工作往往会显着增加模型大小或学习特定于语言的解码器以分别适应每种语言。在这项研究中，我们探索了级联 Conformer 传感器框架下简单而有效的语言相关适配器 (LDA) 微调，该框架通过流式多语言 ASR 中尾部语言的教师伪标记得到增强。每种语言的适配器仅占完整模型的 0.4%。它被插入到冻结的基础模型中，并且是在噪声学生训练的微调过程中唯一可训练的模块。最终模型合并了来自不同语言的不同检查点的适配器参数。该模型的性能在具有挑战性的多语言听写数据集上进行了验证，该数据集包括拉丁语、希腊语、阿拉伯语等 39 种尾部语言。我们提出的方法平均单词错误率降低了 12.2%，在单个语言环境中单词错误率最高降低了 37.5%。此外，我们还表明，我们的参数高效 LDA 可以与完整模型微调的质量相匹配，从而大大缓解异步峰值性能问题。</li>
</ul>

<h3>Title: Attack and Reset for Unlearning: Exploiting Adversarial Noise toward  Machine Unlearning through Parameter Re-initialization</h3>
<ul>
<li><strong>Authors: </strong>Yoonhwa Jung, Ikhyun Cho, Shun-Hsiang Hsu, Julia Hockenmaier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08998">https://arxiv.org/abs/2401.08998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08998">https://arxiv.org/pdf/2401.08998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08998]] Attack and Reset for Unlearning: Exploiting Adversarial Noise toward  Machine Unlearning through Parameter Re-initialization(https://arxiv.org/abs/2401.08998)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>With growing concerns surrounding privacy and regulatory compliance, the concept of machine unlearning has gained prominence, aiming to selectively forget or erase specific learned information from a trained model. In response to this critical need, we introduce a novel approach called Attack-and-Reset for Unlearning (ARU). This algorithm leverages meticulously crafted adversarial noise to generate a parameter mask, effectively resetting certain parameters and rendering them unlearnable. ARU outperforms current state-of-the-art results on two facial machine-unlearning benchmark datasets, MUFAC and MUCAC. In particular, we present the steps involved in attacking and masking that strategically filter and re-initialize network parameters biased towards the forget set. Our work represents a significant advancement in rendering data unexploitable to deep learning models through parameter re-initialization, achieved by harnessing adversarial noise to craft a mask.</li>
<li><strong>摘要：</strong>随着人们对隐私和监管合规性的担忧日益增加，机器遗忘的概念越来越受到重视，旨在有选择地忘记或删除训练模型中特定的学习信息。为了满足这一关键需求，我们引入了一种称为攻击和重置遗忘（ARU）的新颖方法。该算法利用精心设计的对抗性噪声来生成参数掩码，有效地重置某些参数并使它们无法学习。 ARU 在两个面部机器学习基准数据集 MUFAC 和 MUCAC 上的表现优于当前最先进的结果。特别是，我们提出了攻击和屏蔽所涉及的步骤，这些步骤战略性地过滤和重新初始化偏向遗忘集的网络参数。我们的工作代表了通过参数重新初始化使数据无法被深度学习模型利用的重大进步，这是通过利用对抗性噪声来制作掩模来实现的。</li>
</ul>

<h3>Title: Continuous Time Continuous Space Homeostatic Reinforcement Learning  (CTCS-HRRL) : Towards Biological Self-Autonomous Agent</h3>
<ul>
<li><strong>Authors: </strong>Hugo Laurencon, Yesoda Bhargava, Riddhi Zantye, Charbel-Raphaël Ségerie, Johann Lussange, Veeky Baths, Boris Gutkin</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08999">https://arxiv.org/abs/2401.08999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08999">https://arxiv.org/pdf/2401.08999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08999]] Continuous Time Continuous Space Homeostatic Reinforcement Learning  (CTCS-HRRL) : Towards Biological Self-Autonomous Agent(https://arxiv.org/abs/2401.08999)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Homeostasis is a biological process by which living beings maintain their internal balance. Previous research suggests that homeostasis is a learned behaviour. Recently introduced Homeostatic Regulated Reinforcement Learning (HRRL) framework attempts to explain this learned homeostatic behavior by linking Drive Reduction Theory and Reinforcement Learning. This linkage has been proven in the discrete time-space, but not in the continuous time-space. In this work, we advance the HRRL framework to a continuous time-space environment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL) framework. We achieve this by designing a model that mimics the homeostatic mechanisms in a real-world biological agent. This model uses the Hamilton-Jacobian Bellman Equation, and function approximation based on neural networks and Reinforcement Learning. Through a simulation-based experiment we demonstrate the efficacy of this model and uncover the evidence linked to the agent's ability to dynamically choose policies that favor homeostasis in a continuously changing internal-state milieu. Results of our experiments demonstrate that agent learns homeostatic behaviour in a CTCS environment, making CTCS-HRRL a promising framework for modellng animal dynamics and decision-making.</li>
<li><strong>摘要：</strong>体内平衡是生物体维持内部平衡的生物过程。先前的研究表明，体内平衡是一种习得的行为。最近推出的稳态调节强化学习（HRRL）框架试图通过将内驱力降低理论和强化学习联系起来来解释这种习得的稳态行为。这种联系在离散时空得到了证明，但在连续时空却没有得到证明。在这项工作中，我们将 HRRL 框架推进到连续时空环境，并验证 CTCS-HRRL（连续时间连续空间 HRRL）框架。我们通过设计一个模拟现实生物制剂中稳态机制的模型来实现这一目标。该模型使用 Hamilton-Jacobian Bellman 方程以及基于神经网络和强化学习的函数逼近。通过基于模拟的实验，我们证明了该模型的有效性，并揭示了与智能体在不断变化的内部状态环境中动态选择有利于稳态的政策的能力相关的证据。我们的实验结果表明，智能体在 CTCS 环境中学习稳态行为，使 CTCS-HRRL 成为动物动力学和决策建模的有前景的框架。</li>
</ul>

<h3>Title: AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dong shu, Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, Yongfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09002">https://arxiv.org/abs/2401.09002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09002">https://arxiv.org/pdf/2401.09002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09002]] AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on  Large Language Models(https://arxiv.org/abs/2401.09002)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>In our research, we pioneer a novel approach to evaluate the effectiveness of jailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2, diverging from traditional robustness-focused binary evaluations. Our study introduces two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework, using a scoring range from 0 to 1, offers a unique perspective, enabling a more comprehensive and nuanced evaluation of attack effectiveness and empowering attackers to refine their attack prompts with greater understanding. Furthermore, we have developed a comprehensive ground truth dataset specifically tailored for jailbreak tasks. This dataset not only serves as a crucial benchmark for our current study but also establishes a foundational resource for future research, enabling consistent and comparative analyses in this evolving field. Upon meticulous comparison with traditional evaluation methods, we discovered that our evaluation aligns with the baseline's trend while offering a more profound and detailed assessment. We believe that by accurately evaluating the effectiveness of attack prompts in the Jailbreak task, our work lays a solid foundation for assessing a wider array of similar or even more complex tasks in the realm of prompt injection, potentially revolutionizing this field.</li>
<li><strong>摘要：</strong>在我们的研究中，我们开创了一种新颖的方法来评估大型语言模型（LLM）（例如 GPT-4 和 LLaMa2）越狱攻击的有效性，这与传统的注重稳健性的二进制评估不同。我们的研究引入了两种不同的评估框架：粗粒度评估和细粒度评估。每个框架都使用 0 到 1 的评分范围，提供独特的视角，从而能够对攻击有效性进行更全面、更细致的评估，并使攻击者能够更好地理解并完善其攻击提示。此外，我们还开发了专门为越狱任务定制的综合地面实况数据集。该数据集不仅可以作为我们当前研究的重要基准，而且还为未来的研究奠定了基础资源，从而能够在这个不断发展的领域进行一致的比较分析。通过与传统评估方法的细致比较，我们发现我们的评估符合基线趋势，同时提供了更深刻和详细的评估。我们相信，通过准确评估越狱任务中攻击提示的有效性，我们的工作为评估提示注入领域中更广泛的类似甚至更复杂的任务奠定了坚实的基础，有可能彻底改变这一领域。</li>
</ul>

<h3>Title: Augmenting Math Word Problems via Iterative Question Composing</h3>
<ul>
<li><strong>Authors: </strong>Haoxiong Liu, Andrew Chi-Chih Yao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09003">https://arxiv.org/abs/2401.09003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09003">https://arxiv.org/pdf/2401.09003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09003]] Augmenting Math Word Problems via Iterative Question Composing(https://arxiv.org/abs/2401.09003)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.</li>
<li><strong>摘要：</strong>尽管最近在提高大型语言模型 (LLM) 的数学推理能力方面取得了进展，但对于开源 LLM 来说，在不使用外部工具的情况下解决竞赛级别的数学问题仍然具有挑战性。在这项工作中，我们引入了 MMIQC 数据集，它是经过处理的网络数据和合成问题-响应对的混合体，旨在为基础模型配备更好的数学推理技能。 Mistral-7B-MMIQC，在 MMIQC 上微调 Mistral-7B(arXiv:2310.06825) 得到的模型，在 MATH(arXiv:2103.03874) 上达到 36.0\% 的准确率，比之前提高了 5.8\%（模型大小 $\sim $7B）SOTA。我们的实验还表明，很大一部分改进归功于我们新颖的增强方法 IQC（迭代问题编写），其中我们迭代地要求法学硕士根据给定的种子问题组成新问题，并从另一个法学硕士进行拒绝抽样。 MMIQC 现已在 https://huggingface.co/datasets/Vivacem/MMIQC 上发布。</li>
</ul>

<h3>Title: Explain Thyself Bully: Sentiment Aided Cyberbullying Detection with  Explanation</h3>
<ul>
<li><strong>Authors: </strong>Krishanu Maity, Prince Jha, Raghav Jain, Sriparna Saha, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09023">https://arxiv.org/abs/2401.09023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09023">https://arxiv.org/pdf/2401.09023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09023]] Explain Thyself Bully: Sentiment Aided Cyberbullying Detection with  Explanation(https://arxiv.org/abs/2401.09023)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Cyberbullying has become a big issue with the popularity of different social media networks and online communication apps. While plenty of research is going on to develop better models for cyberbullying detection in monolingual language, there is very little research on the code-mixed languages and explainability aspect of cyberbullying. Recent laws like "right to explanations" of General Data Protection Regulation, have spurred research in developing interpretable models rather than focusing on performance. Motivated by this we develop the first interpretable multi-task model called {\em mExCB} for automatic cyberbullying detection from code-mixed languages which can simultaneously solve several tasks, cyberbullying detection, explanation/rationale identification, target group detection and sentiment analysis. We have introduced {\em BullyExplain}, the first benchmark dataset for explainable cyberbullying detection in code-mixed language. Each post in {\em BullyExplain} dataset is annotated with four labels, i.e., {\em bully label, sentiment label, target and rationales (explainability)}, i.e., which phrases are being responsible for annotating the post as a bully. The proposed multitask framework (mExCB) based on CNN and GRU with word and sub-sentence (SS) level attention is able to outperform several baselines and state of the art models when applied on {\em BullyExplain} dataset.</li>
<li><strong>摘要：</strong>随着不同社交媒体网络和在线通信应用程序的流行，网络欺凌已成为一个大问题。尽管正在进行大量研究来开发更好的单语言网络欺凌检测模型，但对代码混合语言和网络欺凌的可解释性方面的研究却很少。最近的法律，例如《通用数据保护条例》的“解释权”，刺激了开发可解释模型的研究，而不是关注性能。受此启发，我们开发了第一个可解释的多任务模型，称为 {\em mExCB}，用于从代码混合语言中自动检测网络欺凌，它可以同时解决多个任务：网络欺凌检测、解释/理由识别、目标群体检测和情绪分析。我们推出了 {\em BullyExplain}，这是第一个用代码混合语言进行可解释的网络欺凌检测的基准数据集。 BullyExplain 数据集中的每个帖子都用四个标签进行注释，即欺凌标签、情感标签、目标和基本原理（可解释性），即哪些短语负责将帖子注释为欺凌者。所提出的基于 CNN 和 GRU 的多任务框架（mExCB）具有单词和子句子（SS）级别的注意力，当应用于 {\em BullyExplain} 数据集时，能够超越多个基线和最先进的模型。</li>
</ul>

<h3>Title: LLMs for Relational Reasoning: How Far are We?</h3>
<ul>
<li><strong>Authors: </strong>Zhiming Li, Yushi Cao, Xiufeng Xu, Junzhe Jiang, Xu Liu, Yon Shin Teo, Shang-wei Lin, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09042">https://arxiv.org/abs/2401.09042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09042">https://arxiv.org/pdf/2401.09042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09042]] LLMs for Relational Reasoning: How Far are We?(https://arxiv.org/abs/2401.09042)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized many areas (e.g. natural language processing, software engineering, etc.) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs' reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representative and challenging measurement for evaluating logic program induction/synthesis systems as it requires inducing strict cause-effect logic to achieve robust deduction on independent and identically distributed (IID) and out-of-distribution (OOD) test samples. Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通过在广泛的下游任务上实现最先进的性能，彻底改变了许多领域（例如自然语言处理、软件工程等）。为了实现稳健和通用的人工智能，人们对研究法学硕士的推理能力产生了浓厚的兴趣。尽管之前的作品采用的文本和数字推理基准相当肤浅和简单，但很难仅仅通过在这些基准上取得积极的结果来断定法学硕士拥有强大的推理能力。最近的努力表明，法学硕士在解决顺序决策问题方面表现不佳，这些问题需要通过评估强化学习基准的表现来进行常识性规划。在这项工作中，我们基于归纳逻辑编程（ILP）基准对几位最先进的法学硕士的推理能力进行了深入评估，该基准被广泛认为是评估逻辑程序归纳的代表性和挑战性的衡量标准/综合系统，因为它需要引入严格的因果逻辑，以实现对独立同分布（IID）和分布外（OOD）测试样本的稳健推论。我们的评估表明，与模型大小小得多的神经程序归纳系统相比，最先进的法学硕士在推理能力方面要差得多，因为使用自然语言提示或真理实现的性能和泛化能力要低得多- 值矩阵提示。</li>
</ul>

<h3>Title: DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning</h3>
<ul>
<li><strong>Authors: </strong>Lixiang Han, Zhen Xiao, Zhenjiang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09068">https://arxiv.org/abs/2401.09068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09068">https://arxiv.org/pdf/2401.09068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09068]] DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning(https://arxiv.org/abs/2401.09068)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>DTMM is a library designed for efficient deployment and execution of machine learning models on weak IoT devices such as microcontroller units (MCUs). The motivation for designing DTMM comes from the emerging field of tiny machine learning (TinyML), which explores extending the reach of machine learning to many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak capability of embedded devices, it is necessary to compress models by pruning enough weights before deploying. Although pruning has been studied extensively on many computing platforms, two key issues with pruning methods are exacerbated on MCUs: models need to be deeply compressed without significantly compromising accuracy, and they should perform efficiently after pruning. Current solutions only achieve one of these objectives, but not both. In this paper, we find that pruned models have great potential for efficient deployment and execution on MCUs. Therefore, we propose DTMM with pruning unit selection, pre-execution pruning optimizations, runtime acceleration, and post-execution low-cost storage to fill the gap for efficient deployment and execution of pruned models. It can be integrated into commercial ML frameworks for practical deployment, and a prototype system has been developed. Extensive experiments on various models show promising gains compared to state-of-the-art methods.</li>
<li><strong>摘要：</strong>DTMM 是一个库，旨在在微控制器单元 (MCU) 等弱物联网设备上高效部署和执行机器学习模型。设计 DTMM 的动机来自新兴的微型机器学习 (TinyML) 领域，该领域探索将机器学习的范围扩展到许多低端物联网设备，以实现无处不在的智能。由于嵌入式设备的能力较弱，在部署之前需要通过修剪足够的权重来压缩模型。尽管剪枝已经在许多计算平台上得到了广泛的研究，但剪枝方法的两个关键问题在 MCU 上更加严重：模型需要在不显着影响准确性的情况下进行深度压缩，并且剪枝后它们应该高效执行。当前的解决方案只能实现这些目标之一，但不能同时实现这两个目标。在本文中，我们发现剪枝模型在 MCU 上高效部署和执行方面具有巨大潜力。因此，我们提出了具有剪枝单元选择、执行前剪枝优化、运行时加速和执行后低成本存储的DTMM，以填补剪枝模型高效部署和执行的空白。它可以集成到商业ML框架中进行实际部署，并且已经开发了原型系统。与最先进的方法相比，对各种模型的广泛实验显示出有希望的收益。</li>
</ul>

<h3>Title: Rethinking Spectral Graph Neural Networks with Spatially Adaptive  Filtering</h3>
<ul>
<li><strong>Authors: </strong>Jingwei Guo, Kaizhu Huang, Xinping Yi, Zixian Su, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09071">https://arxiv.org/abs/2401.09071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09071">https://arxiv.org/pdf/2401.09071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09071]] Rethinking Spectral Graph Neural Networks with Spatially Adaptive  Filtering(https://arxiv.org/abs/2401.09071)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial domain and inspire us to rethink graph spectral filters beyond the fixed-order polynomials, which neglect global information. Built upon the theoretical findings, we revisit the state-of-the-art spectral GNNs and propose a novel Spatially Adaptive Filtering (SAF) framework, which leverages the adapted new graph by spectral filtering for an auxiliary non-local aggregation. Notably, our proposed SAF comprehensively models both node similarity and dissimilarity from a global perspective, therefore alleviating persistent deficiencies of GNNs related to long-range dependencies and graph heterophily. Extensive experiments over 13 node classification benchmarks demonstrate the superiority of our proposed framework to the state-of-the-art models.</li>
<li><strong>摘要：</strong>虽然谱图神经网络（GNN）在理论上在谱域中具有良好的基础，但它们对多项式近似的实际依赖意味着与空间域的深刻联系。由于之前的研究很少从空间角度研究谱 GNN，因此它们的空间域可解释性仍然难以捉摸，例如，空间域中的谱 GNN 本质上编码了哪些信息？在本文中，为了回答这个问题，我们在光谱过滤和空间聚合之间建立了理论联系，揭示了一种内在的相互作用，即光谱过滤隐式地将原始图引导到一个适应的新图，并显式计算空间聚合。理论和实证研究都表明，改编后的新图不仅表现出非局部性，而且还适应有符号边权重以反映节点之间的标签一致性。因此，这些发现凸显了谱 GNN 在空间域中的可解释作用，并启发我们重新思考超越固定阶多项式的图谱滤波器，因为固定阶多项式忽略了全局信息。基于理论发现，我们重新审视了最先进的光谱 GNN，并提出了一种新颖的空间自适应过滤（SAF）框架，该框架通过光谱过滤来利用适应的新图来进行辅助非局部聚合。值得注意的是，我们提出的 SAF 从全局角度对节点相似性和不相似性进行了全面建模，从而缓解了 GNN 与远程依赖性和图异质性相关的持续缺陷。超过 13 个节点分类基准的广泛实验证明了我们提出的框架相对于最先进模型的优越性。</li>
</ul>

<h3>Title: Code Simulation Challenges for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin, Anthony Cohn, Nigel Shadbolt, Michael Wooldridge</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09074">https://arxiv.org/abs/2401.09074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09074">https://arxiv.org/pdf/2401.09074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09074]] Code Simulation Challenges for Large Language Models(https://arxiv.org/abs/2401.09074)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirically, our new Chain of Simulation (CoSm) method improves on the standard Chain of Thought prompting approach by avoiding the pitfalls of memorisation.</li>
<li><strong>摘要：</strong>我们研究大型语言模型 (LLM) 可以在多大程度上模拟计算机代码和算法的执行。我们首先查看直线程序，并表明当前的法学硕士即使使用如此简单的程序也表现出较差的性能——性能随着代码长度的增加而迅速下降。然后，我们研究法学硕士模拟包含关键路径和冗余指令的程序的能力。我们还超越了排序算法和嵌套循环的直线程序模拟，并且我们展示了例程的计算复杂性直接影响法学硕士模拟其执行的能力。我们观察到法学硕士仅在短程序或标准程序中按顺序执行指令并且误差范围较低。法学硕士的代码模拟与其模式识别和记忆能力存在紧张关系：对于记忆不利的任务，我们提出了一种新颖的提示方法来逐行模拟代码执行。根据经验，我们的新模拟链（CoSm）方法通过避免记忆陷阱，改进了标准思维链提示方法。</li>
</ul>

<h3>Title: What makes for a 'good' social actor? Using respect as a lens to  evaluate interactions with language agents</h3>
<ul>
<li><strong>Authors: </strong>Lize Alberts, Geoff Keeling, Amanda McCroskery</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09082">https://arxiv.org/abs/2401.09082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09082">https://arxiv.org/pdf/2401.09082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09082]] What makes for a 'good' social actor? Using respect as a lens to  evaluate interactions with language agents(https://arxiv.org/abs/2401.09082)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>With the growing popularity of dialogue agents based on large language models (LLMs), urgent attention has been drawn to finding ways to ensure their behaviour is ethical and appropriate. These are largely interpreted in terms of the 'HHH' criteria: making outputs more helpful and honest, and avoiding harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus is useful from the perspective of viewing LLM agents as mere mediums for information, it fails to account for pragmatic factors that can make the same utterance seem more or less offensive or tactless in different social situations. We propose an approach to ethics that is more centred on relational and situational factors, exploring what it means for a system, as a social actor, to treat an individual respectfully in a (series of) interaction(s). Our work anticipates a set of largely unexplored risks at the level of situated interaction, and offers practical suggestions to help LLM technologies behave as 'good' social actors and treat people respectfully.</li>
<li><strong>摘要：</strong>随着基于大语言模型（LLM）的对话代理越来越受欢迎，人们迫切关注如何确保其行为符合道德且适当。这些主要根据“HHH”标准进行解释：使输出更有帮助和诚实，并避免有害（有偏见、有毒或不准确）的陈述。虽然从将法学硕士代理人视为纯粹的信息媒介的角度来看，这种语义焦点是有用的，但它未能考虑到实用因素，这些因素可能使相同的话语在不同的社交场合显得或多或少具有攻击性或不得体。我们提出了一种更注重关系和情境因素的道德方法，探索作为社会参与者的系统在（一系列）互动中尊重个人意味着什么。我们的工作预见到了情境互动层面上一系列尚未探索的风险，并提供了实用的建议，帮助法学硕士技术充当“良好”的社会参与者并尊重他人。</li>
</ul>

<h3>Title: RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series  Tasks</h3>
<ul>
<li><strong>Authors: </strong>Haowen Hou, F. Richard Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09093">https://arxiv.org/abs/2401.09093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09093">https://arxiv.org/pdf/2401.09093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09093]] RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series  Tasks(https://arxiv.org/abs/2401.09093)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, code, rag</a></li>
<li><strong>Abstract: </strong>Traditional Recurrent Neural Network (RNN) architectures, such as LSTM and GRU, have historically held prominence in time series tasks. However, they have recently seen a decline in their dominant position across various time series tasks. As a result, recent advancements in time series forecasting have seen a notable shift away from RNNs towards alternative architectures such as Transformers, MLPs, and CNNs. To go beyond the limitations of traditional RNNs, we design an efficient RNN-based model for time series tasks, named RWKV-TS, with three distinctive features: (i) A novel RNN architecture characterized by $O(L)$ time complexity and memory usage. (ii) An enhanced ability to capture long-term sequence information compared to traditional RNNs. (iii) High computational efficiency coupled with the capacity to scale up effectively. Through extensive experimentation, our proposed RWKV-TS model demonstrates competitive performance when compared to state-of-the-art Transformer-based or CNN-based models. Notably, RWKV-TS exhibits not only comparable performance but also demonstrates reduced latency and memory utilization. The success of RWKV-TS encourages further exploration and innovation in leveraging RNN-based approaches within the domain of Time Series. The combination of competitive performance, low latency, and efficient memory usage positions RWKV-TS as a promising avenue for future research in time series tasks. Code is available at:\href{https://github.com/howard-hou/RWKV-TS}{ https://github.com/howard-hou/RWKV-TS}</li>
<li><strong>摘要：</strong>传统的循环神经网络 (RNN) 架构，例如 LSTM 和 GRU，历来在时间序列任务中占据着重要地位。然而，他们最近在各种时间序列任务中的主导地位有所下降。因此，时间序列预测的最新进展已经从 RNN 转向 Transformer、MLP 和 CNN 等替代架构。为了超越传统 RNN 的局限性，我们设计了一种高效的基于 RNN 的时间序列任务模型，名为 RWKV-TS，具有三个显着特征：（i）一种新颖的 RNN 架构，其特点是 $O(L)$ 时间复杂度和内存使用情况。 (ii) 与传统 RNN 相比，捕获长期序列信息的能力得到增强。 (iii) 高计算效率以及有效扩展的能力。通过广泛的实验，我们提出的 RWKV-TS 模型与最先进的基于 Transformer 或基于 CNN 的模型相比，表现出具有竞争力的性能。值得注意的是，RWKV-TS 不仅具有可比的性能，而且还降低了延迟和内存利用率。 RWKV-TS 的成功鼓励在时间序列领域利用基于 RNN 的方法进行进一步的探索和创新。具有竞争力的性能、低延迟和高效的内存使用相结合，使 RWKV-TS 成为未来时间序列任务研究的一个有前途的途径。代码位于：\href{https://github.com/howard-hou/RWKV-TS}{ https://github.com/howard-hou/RWKV-TS}</li>
</ul>

<h3>Title: Understanding Heterophily for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Junfu Wang, Yuanfang Guo, Liang Yang, Yunhong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09125">https://arxiv.org/abs/2401.09125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09125">https://arxiv.org/pdf/2401.09125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09125]] Understanding Heterophily for Graph Neural Networks(https://arxiv.org/abs/2401.09125)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of the impacts of different heterophily patterns for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Firstly, we show that by applying a GC operation, the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$, where $\mathbb{E}\left[\operatorname{deg}\right]$ is the averaged node degree. It reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. Secondly, we show that the topological noise has a detrimental impact on separability, which is equivalent to degrading $\mathbb{E}\left[\operatorname{deg}\right]$. Finally, when applying multiple GC operations, we show that the separability gains are determined by the normalized distance of the $l$-powered neighborhood distributions. It indicates that the nodes still possess separability as $l$ goes to infinity in a wide range of regimes. Extensive experiments on both synthetic and real-world data verify the effectiveness of our theory.</li>
<li><strong>摘要：</strong>具有异质性的图被认为是图神经网络（GNN）的挑战性场景，其中节点通过各种模式与不同的邻居连接。在本文中，我们通过提出的异质随机块模型（HSBM）将图卷积（GC）操作合并到完全连接的网络中，提出了不同异质模式对 GNN 影响的理论理解，HSBM 是一种通用随机图模型，可以适应不同的异质性模式。首先，我们表明，通过应用 GC 运算，可分离性增益由两个因素决定，即邻域分布的欧几里得距离和 $\sqrt{\mathbb{E}\left[\operatorname{deg}\right] }$，其中$\mathbb{E}\left[\operatorname{deg}\right]$是平均节点度。它表明，异质性对分类的影响需要与平均节点度一起评估。其次，我们证明拓扑噪声对可分离性有不利影响，这相当于降低$\mathbb{E}\left[\operatorname{deg}\right]$。最后，当应用多个 GC 操作时，我们表明可分离性增益是由 $l$ 驱动的邻域分布的归一化距离决定的。这表明当 $l$ 在各种情况下趋于无穷大时，节点仍然具有可分离性。对合成数据和真实数据的大量实验验证了我们理论的有效性。</li>
</ul>

<h3>Title: Asynchronous Local-SGD Training for Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Bo Liu, Rachita Chhaparia, Arthur Douillard, Satyen Kale, Andrei A. Rusu, Jiajun Shen, Arthur Szlam, Marc'Aurelio Ranzato</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09135">https://arxiv.org/abs/2401.09135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09135">https://arxiv.org/pdf/2401.09135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09135]] Asynchronous Local-SGD Training for Language Modeling(https://arxiv.org/abs/2401.09135)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of {\it asynchronous} Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based on their computation speed. This approach, evaluated with models up to 150M parameters on the C4 dataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and significantly surpasses it in terms of wall clock time.</li>
<li><strong>摘要：</strong>局部随机梯度下降 (Local-SGD)，也称为联合平均，是一种分布式优化方法，其中每个设备每次通信执行多个 SGD 更新。这项工作提出了用于训练语言模型的{\it asynchronous} Local-SGD 的实证研究；也就是说，每个工作线程在完成其 SGD 步骤后立即更新全局参数。我们通过检查工作器硬件异构性、模型大小、工作器数量和优化器如何影响学习性能来进行全面调查。我们发现，在简单的实现中，异步 Local-SGD 比同步 Local-SGD 需要更多的迭代才能收敛，尽管更频繁地更新（全局）模型参数。我们将当工作梯度过时时全局参数的动量加速视为一个关键挑战。我们提出了一种新颖的方法，该方法利用延迟的 Nesterov 动量更新，并根据工人的计算速度调整他们的本地训练步骤。这种方法在 C4 数据集上使用多达 150M 参数的模型进行评估，在每个更新步骤的困惑度方面与同步 Local-SGD 的性能相匹配，并且在挂钟时间方面显着超过它。</li>
</ul>

<h3>Title: Bridging Research and Readers: A Multi-Modal Automated Academic Papers  Interpretation System</h3>
<ul>
<li><strong>Authors: </strong>Feng Jiang, Kuang Wang, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09150">https://arxiv.org/abs/2401.09150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09150">https://arxiv.org/pdf/2401.09150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09150]] Bridging Research and Readers: A Multi-Modal Automated Academic Papers  Interpretation System(https://arxiv.org/abs/2401.09150)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>In the contemporary information era, significantly accelerated by the advent of Large-scale Language Models, the proliferation of scientific literature is reaching unprecedented levels. Researchers urgently require efficient tools for reading and summarizing academic papers, uncovering significant scientific literature, and employing diverse interpretative methodologies. To address this burgeoning demand, the role of automated scientific literature interpretation systems has become paramount. However, prevailing models, both commercial and open-source, confront notable challenges: they often overlook multimodal data, grapple with summarizing over-length texts, and lack diverse user interfaces. In response, we introduce an open-source multi-modal automated academic paper interpretation system (MMAPIS) with three-step process stages, incorporating LLMs to augment its functionality. Our system first employs the hybrid modality preprocessing and alignment module to extract plain text, and tables or figures from documents separately. It then aligns this information based on the section names they belong to, ensuring that data with identical section names are categorized under the same section. Following this, we introduce a hierarchical discourse-aware summarization method. It utilizes the extracted section names to divide the article into shorter text segments, facilitating specific summarizations both within and between sections via LLMs with specific prompts. Finally, we have designed four types of diversified user interfaces, including paper recommendation, multimodal Q\&A, audio broadcasting, and interpretation blog, which can be widely applied across various scenarios. Our qualitative and quantitative evaluations underscore the system's superiority, especially in scientific summarization, where it outperforms solutions relying solely on GPT-4.</li>
<li><strong>摘要：</strong>在当代信息时代，大规模语言模型的出现大大加速了科学文献的激增，达到了前所未有的水平。研究人员迫切需要有效的工具来阅读和总结学术论文、发现重要的科学文献以及采用不同的解释方法。为了满足这一不断增长的需求，自动化科学文献解释系统的作用变得至关重要。然而，流行的模型，无论是商业模型还是开源模型，都面临着显着的挑战：它们经常忽视多模态数据，难以总结超长的文本，并且缺乏多样化的用户界面。为此，我们引入了一个开源多模式自动化学术论文解释系统（MMAPIS），该系统具有三个步骤的流程阶段，并结合法学硕士来增强其功能。我们的系统首先采用混合模态预处理和对齐模块来分别从文档中提取纯文本、表格或图形。然后，它根据这些信息所属的部分名称来对齐这些信息，确保具有相同部分名称的数据被分类在同一部分下。接下来，我们介绍一种分层的话语感知摘要方法。它利用提取的章节名称将文章分为较短的文本段，通过具有特定提示的法学硕士促进章节内和章节之间的具体总结。最后，我们设计了论文推荐、多模态问答、音频广播、解读博客四种多样化的用户界面，可广泛应用于各种场景。我们的定性和定量评估强调了该系统的优越性，特别是在科学总结方面，它优于仅依赖 GPT-4 的解决方案。</li>
</ul>

<h3>Title: Fine-tuning Strategies for Domain Specific Question Answering under Low  Annotation Budget Constraints</h3>
<ul>
<li><strong>Authors: </strong>Kunpeng Guo, Dennis Diefenbach, Antoine Gourru, Christophe Gravier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09168">https://arxiv.org/abs/2401.09168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09168">https://arxiv.org/pdf/2401.09168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09168]] Fine-tuning Strategies for Domain Specific Question Answering under Low  Annotation Budget Constraints(https://arxiv.org/abs/2401.09168)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The progress introduced by pre-trained language models and their fine-tuning has resulted in significant improvements in most downstream NLP tasks. The unsupervised training of a language model combined with further target task fine-tuning has become the standard QA fine-tuning procedure. In this work, we demonstrate that this strategy is sub-optimal for fine-tuning QA models, especially under a low QA annotation budget, which is a usual setting in practice due to the extractive QA labeling cost. We draw our conclusions by conducting an exhaustive analysis of the performance of the alternatives of the sequential fine-tuning strategy on different QA datasets. Based on the experiments performed, we observed that the best strategy to fine-tune the QA model in low-budget settings is taking a pre-trained language model (PLM) and then fine-tuning PLM with a dataset composed of the target dataset and SQuAD dataset. With zero extra annotation effort, the best strategy outperforms the standard strategy by 2.28% to 6.48%. Our experiments provide one of the first investigations on how to best fine-tune a QA system under a low budget and are therefore of the utmost practical interest to the QA practitioners.</li>
<li><strong>摘要：</strong>预训练语言模型及其微调所带来的进步导致大多数下游 NLP 任务的显着改进。语言模型的无监督训练与进一步的目标任务微调相结合已成为标准的 QA 微调程序。在这项工作中，我们证明了这种策略对于微调 QA 模型来说并不是最优的，特别是在 QA 注释预算较低的情况下，由于提取 QA 标签成本，这是实践中的常见设置。我们通过对不同 QA 数据集上的顺序微调策略的替代方案的性能进行详尽的分析来得出结论。根据所进行的实验，我们观察到，在低预算环境中微调 QA 模型的最佳策略是采用预先训练的语言模型 (PLM)，然后使用由目标数据集和SQuAD 数据集。在零额外注释工作的情况下，最佳策略比标准策略高出 2.28% 到 6.48%。我们的实验提供了关于如何在低预算下最好地微调 QA 系统的首批研究之一，因此对 QA 从业者来说具有最大的实际意义。</li>
</ul>

<h3>Title: ADCNet: a unified framework for predicting the activity of antibody-drug  conjugates</h3>
<ul>
<li><strong>Authors: </strong>Liye Chen, Biaoshun Li, Yihao Chen, Mujie Lin, Shipeng Zhang, Chenxin Li, Yu Pang, Ling Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09176">https://arxiv.org/abs/2401.09176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09176">https://arxiv.org/pdf/2401.09176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09176]] ADCNet: a unified framework for predicting the activity of antibody-drug  conjugates(https://arxiv.org/abs/2401.09176)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, rag</a></li>
<li><strong>Abstract: </strong>Antibody-drug conjugate (ADC) has revolutionized the field of cancer treatment in the era of precision medicine due to their ability to precisely target cancer cells and release highly effective drug. Nevertheless, the realization of rational design of ADC is very difficult because the relationship between their structures and activities is difficult to understand. In the present study, we introduce a unified deep learning framework called ADCNet to help design potential ADCs. The ADCNet highly integrates the protein representation learning language model ESM-2 and small-molecule representation learning language model FG-BERT models to achieve activity prediction through learning meaningful features from antigen and antibody protein sequences of ADC, SMILES strings of linker and payload, and drug-antibody ratio (DAR) value. Based on a carefully designed and manually tailored ADC data set, extensive evaluation results reveal that ADCNet performs best on the test set compared to baseline machine learning models across all evaluation metrics. For example, it achieves an average prediction accuracy of 87.12%, a balanced accuracy of 0.8689, and an area under receiver operating characteristic curve of 0.9293 on the test set. In addition, cross-validation, ablation experiments, and external independent testing results further prove the stability, advancement, and robustness of the ADCNet architecture. For the convenience of the community, we develop the first online platform (https://ADCNet.idruglab.cn) for the prediction of ADCs activity based on the optimal ADCNet model, and the source code is publicly available at https://github.com/idrugLab/ADCNet.</li>
<li><strong>摘要：</strong>抗体药物偶联物（ADC）因其能够精确靶向癌细胞并释放高效药物，在精准医学时代彻底改变了癌症治疗领域。然而，实现ADC的合理设计是非常困难的，因为它们的结构和活动之间的关系很难理解。在本研究中，我们引入了一个名为 ADCNet 的统一深度学习框架来帮助设计潜在的 ADC。 ADCNet高度集成蛋白质表示学习语言模型ESM-2和小分子表示学习语言模型FG-BERT模型，通过从ADC的抗原和抗体蛋白质序列、连接子和有效负载的SMILES字符串以及学习到有意义的特征来实现活性预测。药物抗体比（DAR）值。基于精心设计和手动定制的 ADC 数据集，广泛的评估结果表明，与所有评估指标的基线机器学习模型相比，ADCNet 在测试集上表现最佳。例如，它在测试集上实现了 87.12% 的平均预测精度、0.8689 的平衡精度和 0.9293 的受试者工作特征曲线下面积。此外，交叉验证、消融实验和外部独立测试结果进一步证明了ADCNet架构的稳定性、先进性和鲁棒性。为了方便社区，我们开发了第一个基于最优ADCNet模型预测ADC活性的在线平台（https://ADCNet.idruglab.cn），源代码公开在https://github .com/idrugLab/ADCNet。</li>
</ul>

<h3>Title: Unsupervised Multiple Domain Translation through Controlled  Disentanglement in Variational Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Almudévar Antonio, Mariotte Théo, Ortega Alfonso, Tahon Marie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09180">https://arxiv.org/abs/2401.09180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09180">https://arxiv.org/pdf/2401.09180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09180]] Unsupervised Multiple Domain Translation through Controlled  Disentanglement in Variational Autoencoder(https://arxiv.org/abs/2401.09180)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Unsupervised Multiple Domain Translation is the task of transforming data from one domain to other domains without having paired data to train the systems. Typically, methods based on Generative Adversarial Networks (GANs) are used to address this task. However, our proposal exclusively relies on a modified version of a Variational Autoencoder. This modification consists of the use of two latent variables disentangled in a controlled way by design. One of this latent variables is imposed to depend exclusively on the domain, while the other one must depend on the rest of the variability factors of the data. Additionally, the conditions imposed over the domain latent variable allow for better control and understanding of the latent space. We empirically demonstrate that our approach works on different vision datasets improving the performance of other well known methods. Finally, we prove that, indeed, one of the latent variables stores all the information related to the domain and the other one hardly contains any domain information.</li>
<li><strong>摘要：</strong>无监督多域翻译是一种将数据从一个域转换到其他域的任务，无需配对数据来训练系统。通常，基于生成对抗网络 (GAN) 的方法用于解决此任务。然而，我们的建议完全依赖于变分自动编码器的修改版本。这种修改包括使用通过设计以受控方式解开的两个潜在变量。这些潜在变量之一被强加为完全依赖于域，而另一个必须依赖于数据的其余可变性因素。此外，对域潜在变量施加的条件可以更好地控制和理解潜在空间。我们凭经验证明我们的方法适用于不同的视觉数据集，提高了其他众所周知方法的性能。最后，我们证明，实际上，其中一个潜在变量存储了与领域相关的所有信息，而另一个潜在变量几乎不包含任何领域信息。</li>
</ul>

<h3>Title: Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with  Positive Forward Transfer</h3>
<ul>
<li><strong>Authors: </strong>Junhao Zheng, Qianli Ma, Zhen Liu, Binquan Wu, Huawen Feng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09181">https://arxiv.org/abs/2401.09181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09181">https://arxiv.org/pdf/2401.09181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09181]] Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with  Positive Forward Transfer(https://arxiv.org/abs/2401.09181)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code, rag</a></li>
<li><strong>Abstract: </strong>Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. By performing singular value decomposition (SVD) on input embeddings, we discover a large discrepancy in different input embeddings. The discrepancy results in the model learning irrelevant information for old and pre-trained tasks, which leads to catastrophic forgetting and negative forward transfer. To address these issues, we propose Fwd-Prompt, a prompt-based method projecting prompt gradient to the residual space to minimize the interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. Our experiments demonstrate that Fwd-Prompt achieves state-of-the-art performance while updating fewer parameters and requiring no old samples. Our research sheds light on the potential of continuously adapting MLLMs to new tasks under the instruction tuning paradigm and encourages future studies to explore MCIT. The code will soon be publicly available.</li>
<li><strong>摘要：</strong>多模态连续指令调优 (MCIT) 使多模态大型语言模型 (MLLM) 能够满足不断出现的需求，而无需昂贵的再培训。 MCIT 面临两个主要障碍：灾难性遗忘（旧知识被遗忘）和负前向迁移（未来任务的性能下降）。尽管现有方法极大地减轻了灾难性遗忘，但它们仍然遭受负前向转移的困扰。通过对输入嵌入执行奇异值分解（SVD），我们发现不同输入嵌入之间存在很大差异。这种差异导致模型学习旧任务和预训练任务的不相关信息，从而导致灾难性遗忘和负前向迁移。为了解决这些问题，我们提出了 Fwd-Prompt，这是一种基于提示的方法，将提示梯度投影到剩余空间，以最大限度地减少任务之间的干扰，并投影到预训练子空间以重用预训练知识。我们的实验表明，Fwd-Prompt 实现了最先进的性能，同时更新更少的参数并且不需要旧样本。我们的研究揭示了在指令调整范式下不断使 MLLM 适应新任务的潜力，并鼓励未来的研究探索 MCIT。该代码很快就会公开。</li>
</ul>

<h3>Title: An Optimal Transport Approach for Computing Adversarial Training Lower  Bounds in Multiclass Classification</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Garcia Trillos, Matt Jacobs, Jakwang Kim, Matthew Werenski</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09191">https://arxiv.org/abs/2401.09191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09191">https://arxiv.org/pdf/2401.09191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09191]] An Optimal Transport Approach for Computing Adversarial Training Lower  Bounds in Multiclass Classification(https://arxiv.org/abs/2401.09191)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Despite the success of deep learning-based algorithms, it is widely known that neural networks may fail to be robust. A popular paradigm to enforce robustness is adversarial training (AT), however, this introduces many computational and theoretical difficulties. Recent works have developed a connection between AT in the multiclass classification setting and multimarginal optimal transport (MOT), unlocking a new set of tools to study this problem. In this paper, we leverage the MOT connection to propose computationally tractable numerical algorithms for computing universal lower bounds on the optimal adversarial risk and identifying optimal classifiers. We propose two main algorithms based on linear programming (LP) and entropic regularization (Sinkhorn). Our key insight is that one can harmlessly truncate the higher order interactions between classes, preventing the combinatorial run times typically encountered in MOT problems. We validate these results with experiments on MNIST and CIFAR-$10$, which demonstrate the tractability of our approach.</li>
<li><strong>摘要：</strong>尽管基于深度学习的算法取得了成功，但众所周知，神经网络可能不够鲁棒。增强鲁棒性的流行范例是对抗训练（AT），然而，这引入了许多计算和理论困难。最近的工作在多类分类设置中的 AT 和多边缘最优传输（MOT）之间建立了联系，解锁了一组新的工具来研究这个问题。在本文中，我们利用 MOT 连接提出计算上易于处理的数值算法，用于计算最佳对抗风险的通用下界并识别最佳分类器。我们提出了两种基于线性规划（LP）和熵正则化（Sinkhorn）的主要算法。我们的主要见解是，我们可以无害地截断类之间的高阶交互，从而防止 MOT 问题中通常遇到的组合运行时间。我们通过 MNIST 和 CIFAR-$10$ 上的实验验证了这些结果，这证明了我们方法的易处理性。</li>
</ul>

<h3>Title: Preparing Lessons for Progressive Training on Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Pan, Ye Yuan, Yichun Yin, Jiaxin Shi, Zenglin Xu, Ming Zhang, Lifeng Shang, Xin Jiang, Qun Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09192">https://arxiv.org/abs/2401.09192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09192">https://arxiv.org/pdf/2401.09192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09192]] Preparing Lessons for Progressive Training on Language Models(https://arxiv.org/abs/2401.09192)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even rivaling methods using pretrained models, making it a universal and efficient solution for training deep models while reducing time, financial, and environmental costs.</li>
<li><strong>摘要：</strong>变形金刚在人工智能领域的快速进步是以模型规模不断增大而导致资源消耗和温室气体排放增加为代价的。先前的工作建议使用预训练的小模型来提高训练效率，但这种方法可能不适合新的模型结构。另一方面，从头开始训练可能会很慢，并且逐渐堆叠层通常无法实现显着的加速。为了应对这些挑战，我们提出了一种名为 Apollo 的新方法，该方法通过赚取高额收入来为 ex\textbf{p} 和 \textbf{o} 操作准备 \textbf{a}res 课程。低层训练期间的层功能\textbf{o}nality。我们的方法涉及低值优先采样（LVPS）来训练不同的深度和权重共享，以促进高效扩展。我们还引入了一种用于稳定模型深度扩展的插值方法。实验表明，Apollo 实现了最先进的加速比，甚至可以与使用预训练模型的方法相媲美，使其成为训练深度模型的通用且高效的解决方案，同时减少时间、财务和环境成本。</li>
</ul>

<h3>Title: UniVIE: A Unified Label Space Approach to Visual Information Extraction  from Form-like Documents</h3>
<ul>
<li><strong>Authors: </strong>Kai Hu, Jiawei Wang, Weihong Lin, Zhuoyao Zhong, Lei Sun, Qiang Huo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09220">https://arxiv.org/abs/2401.09220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09220">https://arxiv.org/pdf/2401.09220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09220]] UniVIE: A Unified Label Space Approach to Visual Information Extraction  from Form-like Documents(https://arxiv.org/abs/2401.09220)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Existing methods for Visual Information Extraction (VIE) from form-like documents typically fragment the process into separate subtasks, such as key information extraction, key-value pair extraction, and choice group extraction. However, these approaches often overlook the hierarchical structure of form documents, including hierarchical key-value pairs and hierarchical choice groups. To address these limitations, we present a new perspective, reframing VIE as a relation prediction problem and unifying labels of different tasks into a single label space. This unified approach allows for the definition of various relation types and effectively tackles hierarchical relationships in form-like documents. In line with this perspective, we present UniVIE, a unified model that addresses the VIE problem comprehensively. UniVIE functions using a coarse-to-fine strategy. It initially generates tree proposals through a tree proposal network, which are subsequently refined into hierarchical trees by a relation decoder module. To enhance the relation prediction capabilities of UniVIE, we incorporate two novel tree constraints into the relation decoder: a tree attention mask and a tree level embedding. Extensive experimental evaluations on both our in-house dataset HierForms and a publicly available dataset SIBR, substantiate that our method achieves state-of-the-art results, underscoring the effectiveness and potential of our unified approach in advancing the field of VIE.</li>
<li><strong>摘要：</strong>从类似表单的文档中进行视觉信息提取 (VIE) 的现有方法通常将流程分解为单独的子任务，例如关键信息提取、键值对提取和选择组提取。然而，这些方法常常忽略表单文档的层次结构，包括层次键值对和层次选择组。为了解决这些限制，我们提出了一个新的视角，将 VIE 重新定义为关系预测问题，并将不同任务的标签统一到单个标签空间中。这种统一的方法允许定义各种关系类型，并有效地处理类似表单的文档中的层次关系。基于这个视角，我们提出了UniVIE，一个全面解决VIE问题的统一模型。 UniVIE 使用从粗到细的策略运行。它最初通过树提案网络生成树提案，随后通过关系解码器模块将其细化为分层树。为了增强 UniVIE 的关系预测能力，我们将两个新颖的树约束合并到关系解码器中：树注意掩模和树级嵌入。对我们的内部数据集 HierForms 和公开数据集 SIBR 进行的广泛实验评估证实了我们的方法取得了最先进的结果，强调了我们统一方法在推进 VIE 领域的有效性和潜力。</li>
</ul>

<h3>Title: Classification and Reconstruction Processes in Deep Predictive Coding  Networks: Antagonists or Allies?</h3>
<ul>
<li><strong>Authors: </strong>Jan Rathjens, Laurenz Wiskott</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09237">https://arxiv.org/abs/2401.09237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09237">https://arxiv.org/pdf/2401.09237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09237]] Classification and Reconstruction Processes in Deep Predictive Coding  Networks: Antagonists or Allies?(https://arxiv.org/abs/2401.09237)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Predictive coding-inspired deep networks for visual computing integrate classification and reconstruction processes in shared intermediate layers. Although synergy between these processes is commonly assumed, it has yet to be convincingly demonstrated. In this study, we take a critical look at how classifying and reconstructing interact in deep learning architectures. Our approach utilizes a purposefully designed family of model architectures reminiscent of autoencoders, each equipped with an encoder, a decoder, and a classification head featuring varying modules and complexities. We meticulously analyze the extent to which classification- and reconstruction-driven information can seamlessly coexist within the shared latent layer of the model architectures. Our findings underscore a significant challenge: Classification-driven information diminishes reconstruction-driven information in intermediate layers' shared representations and vice versa. While expanding the shared representation's dimensions or increasing the network's complexity can alleviate this trade-off effect, our results challenge prevailing assumptions in predictive coding and offer guidance for future iterations of predictive coding concepts in deep networks.</li>
<li><strong>摘要：</strong>受预测编码启发的视觉计算深度网络在共享中间层中集成了分类和重建过程。尽管人们普遍认为这些过程之间存在协同作用，但尚未得到令人信服的证明。在这项研究中，我们批判性地研究了深度学习架构中分类和重建如何相互作用。我们的方法利用了一系列专门设计的模型架构，让人想起自动编码器，每个模型架构都配备了编码器、解码器和具有不同模块和复杂性的分类头。我们仔细分析了分类和重建驱动的信息在模型架构的共享潜在层中无缝共存的程度。我们的研究结果强调了一个重大挑战：分类驱动的信息减少了中间层共享表示中重建驱动的信息，反之亦然。虽然扩展共享表示的维度或增加网络的复杂性可以减轻这种权衡效应，但我们的结果挑战了预测编码中的普遍假设，并为深度网络中预测编码概念的未来迭代提供了指导。</li>
</ul>

<h3>Title: Cross-lingual Offensive Language Detection: A Systematic Review of  Datasets, Transfer Approaches and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Aiqi Jiang, Arkaitz Zubiaga</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09244">https://arxiv.org/abs/2401.09244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09244">https://arxiv.org/pdf/2401.09244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09244]] Cross-lingual Offensive Language Detection: A Systematic Review of  Datasets, Transfer Approaches and Challenges(https://arxiv.org/abs/2401.09244)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>The growing prevalence and rapid evolution of offensive language in social media amplify the complexities of detection, particularly highlighting the challenges in identifying such content across diverse languages. This survey presents a systematic and comprehensive exploration of Cross-Lingual Transfer Learning (CLTL) techniques in offensive language detection in social media. Our study stands as the first holistic overview to focus exclusively on the cross-lingual scenario in this domain. We analyse 67 relevant papers and categorise these studies across various dimensions, including the characteristics of multilingual datasets used, the cross-lingual resources employed, and the specific CLTL strategies implemented. According to "what to transfer", we also summarise three main CLTL transfer approaches: instance, feature, and parameter transfer. Additionally, we shed light on the current challenges and future research opportunities in this field. Furthermore, we have made our survey resources available online, including two comprehensive tables that provide accessible references to the multilingual datasets and CLTL methods used in the reviewed literature.</li>
<li><strong>摘要：</strong>社交媒体中攻击性语言的日益盛行和快速发展加剧了检测的复杂性，特别突出了在不同语言中识别此类内容的挑战。这项调查对社交媒体中的攻击性语言检测中的跨语言迁移学习（CLTL）技术进行了系统而全面的探索。我们的研究是第一个专门关注该领域跨语言场景的整体概述。我们分析了 67 篇相关论文，并从各个维度对这些研究进行了分类，包括所使用的多语言数据集的特征、所使用的跨语言资源以及实施的具体 CLTL 策略。根据“传输什么”，我们还总结了三种主要的 CLTL 传输方式：实例、特征和参数传输。此外，我们还阐明了该领域当前的挑战和未来的研究机会。此外，我们还在线提供了调查资源，包括两个综合表格，提供了所审查文献中使用的多语言数据集和 CLTL 方法的可访问参考。</li>
</ul>

<h3>Title: Learning from Emotions, Demographic Information and Implicit User  Feedback in Task-Oriented Document-Grounded Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Dominic Petrak, Thy Thy Tran, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09248">https://arxiv.org/abs/2401.09248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09248">https://arxiv.org/pdf/2401.09248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09248]] Learning from Emotions, Demographic Information and Implicit User  Feedback in Task-Oriented Document-Grounded Dialogues(https://arxiv.org/abs/2401.09248)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>The success of task-oriented and document-grounded dialogue systems depends on users accepting and enjoying using them. To achieve this, recently published work in the field of Human-Computer Interaction suggests that the combination of considering demographic information, user emotions and learning from the implicit feedback in their utterances, is particularly important. However, these findings have not yet been transferred to the field of Natural Language Processing, where these data are primarily studied separately. Accordingly, no sufficiently annotated dataset is available. To address this gap, we introduce FEDI, the first English dialogue dataset for task-oriented document-grounded dialogues annotated with demographic information, user emotions and implicit feedback. Our experiments with FLAN-T5, GPT-2 and LLaMA-2 show that these data have the potential to improve task completion and the factual consistency of the generated responses and user acceptance.</li>
<li><strong>摘要：</strong>面向任务和基于文档的对话系统的成功取决于用户接受并享受使用它们。为了实现这一目标，最近发表的人机交互领域的工作表明，考虑人口统计信息、用户情绪和从他们的话语中隐含的反馈中学习相结合尤为重要。然而，这些发现尚未转移到自然语言处理领域，这些数据主要是单独研究的。因此，没有足够注释的数据集可用。为了解决这一差距，我们引入了 FEDI，这是第一个用于以任务为导向、基于文档的对话的英语对话数据集，并用人口统计信息、用户情绪和隐式反馈进行注释。我们对 FLAN-T5、GPT-2 和 LLaMA-2 的实验表明，这些数据有可能提高任务完成度以及生成的响应和用户接受度的事实一致性。</li>
</ul>

<h3>Title: MSHyper: Multi-Scale Hypergraph Transformer for Long-Range Time Series  Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zongjiang Shang, Ling Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09261">https://arxiv.org/abs/2401.09261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09261">https://arxiv.org/pdf/2401.09261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09261]] MSHyper: Multi-Scale Hypergraph Transformer for Long-Range Time Series  Forecasting(https://arxiv.org/abs/2401.09261)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Demystifying interactions between temporal patterns of different scales is fundamental to precise long-range time series forecasting. However, previous works lack the ability to model high-order interactions. To promote more comprehensive pattern interaction modeling for long-range time series forecasting, we propose a Multi-Scale Hypergraph Transformer (MSHyper) framework. Specifically, a multi-scale hypergraph is introduced to provide foundations for modeling high-order pattern interactions. Then by treating hyperedges as nodes, we also build a hyperedge graph to enhance hypergraph modeling. In addition, a tri-stage message passing mechanism is introduced to aggregate pattern information and learn the interaction strength between temporal patterns of different scales. Extensive experiments on five real-world datasets demonstrate that MSHyper achieves state-of-the-art performance, reducing prediction errors by an average of 8.73% and 7.15% over the best baseline in MSE and MAE, respectively.</li>
<li><strong>摘要：</strong>揭开不同尺度的时间模式之间的相互作用是精确的长期时间序列预测的基础。然而，以前的工作缺乏对高阶交互进行建模的能力。为了促进长期时间序列预测的更全面的模式交互建模，我们提出了多尺度超图变换器（MSHyper）框架。具体来说，引入多尺度超图为高阶模式交互建模提供基础。然后，通过将超边视为节点，我们还构建了超边图来增强超图建模。此外，引入了三阶段消息传递机制来聚合模式信息并学习不同尺度的时间模式之间的交互强度。对五个真实世界数据集的大量实验表明，MSHyper 实现了最先进的性能，与 MSE 和 MAE 的最佳基线相比，预测误差平均分别减少了 8.73% 和 7.15%。</li>
</ul>

<h3>Title: Adaptive Regret for Bandits Made Possible: Two Queries Suffice</h3>
<ul>
<li><strong>Authors: </strong>Zhou Lu, Qiuyi Zhang, Xinyi Chen, Fred Zhang, David Woodruff, Elad Hazan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09278">https://arxiv.org/abs/2401.09278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09278">https://arxiv.org/pdf/2401.09278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09278]] Adaptive Regret for Bandits Made Possible: Two Queries Suffice(https://arxiv.org/abs/2401.09278)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Fast changing states or volatile environments pose a significant challenge to online optimization, which needs to perform rapid adaptation under limited observation. In this paper, we give query and regret optimal bandit algorithms under the strict notion of strongly adaptive regret, which measures the maximum regret over any contiguous interval $I$. Due to its worst-case nature, there is an almost-linear $\Omega(|I|^{1-\epsilon})$ regret lower bound, when only one query per round is allowed [Daniely el al, ICML 2015]. Surprisingly, with just two queries per round, we give Strongly Adaptive Bandit Learner (StABL) that achieves $\tilde{O}(\sqrt{n|I|})$ adaptive regret for multi-armed bandits with $n$ arms. The bound is tight and cannot be improved in general. Our algorithm leverages a multiplicative update scheme of varying stepsizes and a carefully chosen observation distribution to control the variance. Furthermore, we extend our results and provide optimal algorithms in the bandit convex optimization setting. Finally, we empirically demonstrate the superior performance of our algorithms under volatile environments and for downstream tasks, such as algorithm selection for hyperparameter optimization.</li>
<li><strong>摘要：</strong>快速变化的状态或不稳定的环境对在线优化提出了重大挑战，需要在有限的观察下进行快速适应。在本文中，我们在强自适应遗憾的严格概念下给出了查询和遗憾最佳老虎机算法，该算法测量任何连续区间 $I$ 上的最大遗憾。由于其最坏情况的性质，当每轮只允许一个查询时，存在几乎线性的 $\Omega(|I|^{1-\epsilon})$ 遗憾下限 [Daniely el al, ICML 2015] 。令人惊讶的是，每轮只需要两个查询，我们就给出了强自适应强盗学习器（StABL），它对具有 $n$ 臂的多臂强盗实现了 $\tilde{O}(\sqrt{n|I|})$ 自适应遗憾。界限很紧，一般无法改善。我们的算法利用不同步长的乘法更新方案和精心选择的观测分布来控制方差。此外，我们扩展了我们的结果并在强盗凸优化设置中提供了最佳算法。最后，我们凭经验证明了我们的算法在不稳定环境和下游任务（例如超参数优化的算法选择）下的优越性能。</li>
</ul>

<h3>Title: BENO: Boundary-embedded Neural Operators for Elliptic PDEs</h3>
<ul>
<li><strong>Authors: </strong>Haixin Wang, Jiaxin Li, Anubhav Dwivedi, Kentaro Hara, Tailin Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09323">https://arxiv.org/abs/2401.09323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09323">https://arxiv.org/pdf/2401.09323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09323]] BENO: Boundary-embedded Neural Operators for Elliptic PDEs(https://arxiv.org/abs/2401.09323)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic PDEs more efficiently by directly mapping the input to solutions. However, existing networks typically cannot handle complex geometries and inhomogeneous boundary values present in the real world. Here we introduce Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture that embeds the complex geometries and inhomogeneous boundary values into the solving of elliptic PDEs. Inspired by classical Green's function, BENO consists of two branches of Graph Neural Networks (GNNs) for interior source term and boundary values, respectively. Furthermore, a Transformer encoder maps the global boundary geometry into a latent vector which influences each message passing layer of the GNNs. We test our model extensively in elliptic PDEs with various boundary conditions. We show that all existing baseline methods fail to learn the solution operator. In contrast, our model, endowed with boundary-embedded architecture, outperforms state-of-the-art neural operators and strong baselines by an average of 60.96\%. Our source code can be found https://github.com/AI4Science-WestlakeU/beno.git.</li>
<li><strong>摘要：</strong>椭圆偏微分方程 (PDE) 是一类主要的与时间无关的偏微分方程，在流体动力学、等离子体物理学和固体力学等许多科学和工程领域中发挥着关键作用。最近，神经算子已经成为一种有前景的技术，可以通过直接将输入映射到解来更有效地求解椭圆偏微分方程。然而，现有网络通常无法处理现实世界中存在的复杂几何形状和不均匀边界值。在这里，我们介绍边界嵌入神经算子（BENO），这是一种新颖的神经算子架构，它将复杂的几何形状和非齐次边界值嵌入到椭圆偏微分方程的求解中。受经典格林函数的启发，BENO 由图神经网络 (GNN) 的两个分支组成，分别用于内部源项和边界值。此外，Transformer 编码器将全局边界几何形状映射为潜在向量，该向量影响 GNN 的每个消息传递层。我们在具有各种边界条件的椭圆偏微分方程中广泛测试我们的模型。我们证明所有现有的基线方法都无法学习解算子。相比之下，我们的模型具有边界嵌入架构，其性能比最先进的神经算子和强基线平均高出 60.96%。我们的源代码可以在 https://github.com/AI4Science-WestlakeU/beno.git 找到。</li>
</ul>

<h3>Title: Large Language Models Are Neurosymbolic Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Meng Fang, Shilong Deng, Yudi Zhang, Zijing Shi, Ling Chen, Mykola Pechenizkiy, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09334">https://arxiv.org/abs/2401.09334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09334">https://arxiv.org/pdf/2401.09334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09334]] Large Language Models Are Neurosymbolic Reasoners(https://arxiv.org/abs/2401.09334)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning. This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners. We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds. To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives. We begin by initializing the LLM agent and informing it of its role. The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module. With these inputs, the LLM agent chooses an action and interacts with the game environments. Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.</li>
<li><strong>摘要：</strong>现实世界中广泛的应用都具有符号性质，因此需要强大的符号推理能力。本文研究了大型语言模型 (LLM) 作为符号推理器的潜在应用。我们专注于基于文本的游戏，这是具有自然语言能力的智能体的重要基准，特别是在数学、地图阅读、排序和在基于文本的世界中应用常识等符号任务中。为了促进这些代理的发展，我们提出了一个 LLM 代理，旨在解决象征性挑战并实现游戏中的目标。我们首先初始化 LLM 代理并告知其角色。然后，代理从基于文本的游戏接收观察结果和一组有效动作，以及特定的符号模块。通过这些输入，LLM 代理选择一个操作并与游戏环境交互。我们的实验结果表明，我们的方法显着增强了 LLM 作为符号推理自动化代理的能力，并且我们的 LLM 代理在涉及符号任务的基于文本的游戏中非常有效，在所有任务中实现了 88% 的平均性能。</li>
</ul>

<h3>Title: Efficient slot labelling</h3>
<ul>
<li><strong>Authors: </strong>Vladimir Vlasov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09343">https://arxiv.org/abs/2401.09343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09343">https://arxiv.org/pdf/2401.09343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09343]] Efficient slot labelling(https://arxiv.org/abs/2401.09343)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Slot labelling is an essential component of any dialogue system, aiming to find important arguments in every user turn. Common approaches involve large pre-trained language models (PLMs) like BERT or RoBERTa, but they face challenges such as high computational requirements and dependence on pre-training data. In this work, we propose a lightweight method which performs on par or better than the state-of-the-art PLM-based methods, while having almost 10x less trainable parameters. This makes it especially applicable for real-life industry scenarios.</li>
<li><strong>摘要：</strong>槽位标签是任何对话系统的重要组成部分，旨在在每个用户回合中找到重要的论点。常见的方法涉及 BERT 或 RoBERTa 等大型预训练语言模型 (PLM)，但它们面临计算要求高和对预训练数据依赖等挑战。在这项工作中，我们提出了一种轻量级方法，其性能与最先进的基于 PLM 的方法相当或更好，同时可训练参数减少了近 10 倍。这使得它特别适用于现实生活中的行业场景。</li>
</ul>

<h3>Title: Stuck in the Quicksand of Numeracy, Far from AGI Summit: Evaluating  LLMs' Mathematical Competency through Ontology-guided Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Hong, Deepanway Ghosal, Navonil Majumder, Somak Aditya, Rada Mihalcea, Soujanya Poria</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09395">https://arxiv.org/abs/2401.09395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09395">https://arxiv.org/pdf/2401.09395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09395]] Stuck in the Quicksand of Numeracy, Far from AGI Summit: Evaluating  LLMs' Mathematical Competency through Ontology-guided Perturbations(https://arxiv.org/abs/2401.09395)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness, in mathematical reasoning tasks, remains an open question. In response, we develop (i) an ontology of perturbations of maths questions, (ii) a semi-automatic method of perturbation, and (iii) a dataset of perturbed maths questions to probe the limits of LLM capabilities in mathematical reasoning tasks. These controlled perturbations span across multiple fine dimensions of the structural and representational aspects of maths questions. Using GPT-4, we generated the MORE dataset by perturbing randomly selected five seed questions from GSM8K. This process was guided by our ontology and involved a thorough automatic and manual filtering process, yielding a set of 216 maths problems. We conducted comprehensive evaluation of both closed-source and open-source LLMs on MORE. The results show a significant performance drop across all the models against the perturbed questions. This strongly suggests that current LLMs lack robust mathematical skills and deep reasoning abilities. This research not only identifies multiple gaps in the capabilities of current models, but also highlights multiple potential directions for future development. Our dataset will be made publicly available at https://huggingface.co/datasets/declare-lab/GSM8k_MORE.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展在现有逻辑推理基准上展示了惊人的结果，其中一些模型甚至超越了人类的表现。然而，他们在数学推理任务中的能力和稳健性的真正深度仍然是一个悬而未决的问题。作为回应，我们开发了（i）数学问题扰动的本体论，（ii）半自动扰动方法，以及（iii）扰动数学问题的数据集，以探讨法学硕士在数学推理任务中能力的局限性。这些受控扰动跨越数学问题的结构和表征方面的多个精细维度。使用 GPT-4，我们通过扰动从 GSM8K 中随机选择的五个种子问题来生成 MORE 数据集。这个过程以我们的本体论为指导，涉及彻底的自动和手动过滤过程，产生了一组 216 个数学问题。我们对MORE上的闭源和开源LLM进行了综合评估。结果显示，针对扰动问题，所有模型的性能均显着下降。这强烈表明目前的法学硕士缺乏扎实的数学技能和深刻的推理能力。这项研究不仅确定了当前模型能力的多个差距，而且还强调了未来发展的多个潜在方向。我们的数据集将在 https://huggingface.co/datasets/declare-lab/GSM8k_MORE 上公开提供。</li>
</ul>

<h3>Title: Deciphering Textual Authenticity: A Generalized Strategy through the  Lens of Large Language Semantics for Detecting Human vs. Machine-Generated  Text</h3>
<ul>
<li><strong>Authors: </strong>Mazal Bethany, Brandon Wherry, Emet Bethany, Nishant Vishwamitra, Peyman Najafirad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09407">https://arxiv.org/abs/2401.09407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09407">https://arxiv.org/pdf/2401.09407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09407]] Deciphering Textual Authenticity: A Generalized Strategy through the  Lens of Large Language Semantics for Detecting Human vs. Machine-Generated  Text(https://arxiv.org/abs/2401.09407)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code, rag</a></li>
<li><strong>Abstract: </strong>With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore, t-SNE visualizations of the embeddings from a pretrained LLM's encoder show that they cannot reliably distinguish between human and machine-generated text. Based on our findings, we introduce a novel system, T5LLMCipher, for detecting machine-generated text using a pretrained T5 encoder combined with LLM embedding sub-clustering to address the text produced by diverse generators and domains in the real world. We evaluate our approach across 9 machine-generated text systems and 9 domains and find that our approach provides state-of-the-art generalization ability, with an average increase in F1 score on machine-generated text of 19.6\% on unseen generators and domains compared to the top performing existing approaches and correctly attributes the generator of text with an accuracy of 93.6\%.</li>
<li><strong>摘要：</strong>随着最近大型语言模型 (LLM) 的激增，对检测机器生成文本的工具的需求不断增加。机器生成文本的有效检测面临两个相关问题：首先，它们在泛化现实世界场景方面受到严重限制，其中机器生成文本是由各种生成器生成的，包括但不限于 GPT-4 和 Dolly ，并且跨越不同的领域，从学术手稿到社交媒体帖子。其次，现有的检测方法通过限制性的二元分类镜头来处理法学硕士生成的文本，忽略了不同法学硕士生成的工件的细微差别。在这项工作中，我们对现实场景中机器生成文本的检测进行了系统研究。我们首先研究最先进方法的有效性，发现它们对于现实世界中不同生成器和领域生成的文本受到严重限制。此外，来自预训练的 LLM 编码器的嵌入的 t-SNE 可视化表明，它们无法可靠地区分人类和机器生成的文本。基于我们的发现，我们引入了一种新颖的系统 T5LLMCipher，用于使用预训练的 T5 编码器结合 LLM 嵌入子聚类来检测机器生成的文本，以处理现实世界中不同生成器和域生成的文本。我们评估了 9 个机器生成文本系统和 9 个领域的方法，发现我们的方法提供了最先进的泛化能力，在未见过的生成器上，机器生成文本的 F1 分数平均提高了 19.6%，与表现最佳的现有方法相比，它能够准确地识别文本生成器的属性，准确度为 93.6\%。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
