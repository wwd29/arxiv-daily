<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-18</h1>
<h3>Title: Fewer Truncations Improve Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Hantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, Stefano Soatto</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.10830">https://arxiv.org/abs/2404.10830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.10830">https://arxiv.org/pdf/2404.10830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.10830]] Fewer Truncations Improve Language Modeling(https://arxiv.org/abs/2404.10830)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity -- it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., relatively +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.</li>
<li><strong>摘要：</strong>在大型语言模型训练中，输入文档通常连接在一起，然后分成相等长度的序列以避免填充标记。尽管串联方法效率很高，但它会损害数据完整性——它不可避免地将许多文档分成不完整的部分，导致过度截断，从而阻碍模型学习基于完整上下文构建逻辑连贯且事实上一致的内容。为了解决这个问题，我们提出了最佳拟合打包，这是一种可扩展且高效的方法，通过长度感知组合优化将文档打包到训练序列中。我们的方法完全消除了不必要的截断，同时保持与串联相同的训练效率。文本和代码预训练的实证结果表明，我们的方法实现了卓越的性能（例如，阅读理解相对+4.7%；上下文跟踪+16.8%；程序合成+9.2%），并减少了闭域幻觉有效提升高达 58.3%。</li>
</ul>

<h3>Title: Forcing Diffuse Distributions out of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zhang, Avi Schwarzschild, Nicholas Carlini, Zico Kolter, Daphne Ippolito</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.10859">https://arxiv.org/abs/2404.10859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.10859">https://arxiv.org/pdf/2404.10859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.10859]] Forcing Diffuse Distributions out of Language Models(https://arxiv.org/abs/2404.10859)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chat</a></li>
<li><strong>Abstract: </strong>Despite being trained specifically to follow user instructions, today's language models perform poorly when instructed to produce random outputs. For example, when prompted to pick a number uniformly between one and ten Llama-2-13B-chat disproportionately favors the number five, and when tasked with picking a first name at random, Mistral-7B-Instruct chooses Avery 40 times more often than we would expect based on the U.S. population. When these language models are used for real-world tasks where diversity of outputs is crucial, such as language model assisted dataset construction, their inability to produce diffuse distributions over valid choices is a major hurdle. In this work, we propose a fine-tuning method that encourages language models to output distributions that are diffuse over valid outcomes. The methods we introduce generalize across a variety of tasks and distributions and make large language models practical for synthetic dataset generation with little human intervention.</li>
<li><strong>摘要：</strong>尽管经过专门训练以遵循用户指令，但当今的语言模型在被指示生成随机输出时表现不佳。例如，当提示您统一选择 1 到 10 之间的数字时，Llama-2-13B-chat 不成比例地倾向于数字 5，并且当需要随机选择名字时，Mistral-7B-Instruct 选择 Avery 的频率是其选择 Avery 的 40 倍。我们根据美国人口进行预期。当这些语言模型用于输出多样性至关重要的现实世界任务时，例如语言模型辅助数据集构建，它们无法在有效选择上产生分散分布是一个主要障碍。在这项工作中，我们提出了一种微调方法，鼓励语言模型输出在有效结果上扩散的分布。我们介绍的方法可推广到各种任务和分布，并使大型语言模型可用于合成数据集生成，而无需人工干预。</li>
</ul>

<h3>Title: Incubating Text Classifiers Following User Instruction with Nothing but  LLM</h3>
<ul>
<li><strong>Authors: </strong>Letian Peng, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.10877">https://arxiv.org/abs/2404.10877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.10877">https://arxiv.org/pdf/2404.10877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.10877]] Incubating Text Classifiers Following User Instruction with Nothing but  LLM(https://arxiv.org/abs/2404.10877)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we aim to generate text classification data given arbitrary class definitions (i.e., user instruction), so one can train a small text classifier without any human annotation or raw corpus. Compared with pioneer attempts, our proposed Incubator is the first framework that can handle complicated and even mutually dependent classes (e.g., "TED Talk given by Educator" and "Other"). Specifically, Incubator is an LLM firstly tuned on the instruction-to-data mappings that we obtained from classification datasets and descriptions on HuggingFace together with in-context augmentation by GPT-4. We then refine Incubator by learning on the cluster centers of semantic textual embeddings to emphasize the uniformity and semantic diversity in generations. We compare Incubator on various classification tasks with strong baselines such as direct LLM-based inference and training data generation by prompt engineering. Experiments show Incubator is able to (1) perform well on traditional benchmarks, (2) take label dependency and user preference into consideration, and (3) enable logical text mining by incubating multiple classifiers.</li>
<li><strong>摘要：</strong>在本文中，我们的目标是在给定任意类定义（即用户指令）的情况下生成文本分类数据，因此可以在没有任何人工注释或原始语料库的情况下训练小型文本分类器。与先驱尝试相比，我们提出的孵化器是第一个可以处理复杂甚至相互依赖的类的框架（例如，“教育者给出的 TED 演讲”和“其他”）。具体来说，Incubator 是一个法学硕士，首先根据我们从 HuggingFace 上的分类数据集和描述以及 GPT-4 的上下文增强中获得的指令到数据映射进行调整。然后，我们通过学习语义文本嵌入的聚类中心来完善孵化器，以强调世代的一致性和语义多样性。我们将 Incubator 的各种分类任务与强大的基线进行比较，例如基于 LLM 的直接推理和通过即时工程生成训练数据。实验表明，Incubator 能够 (1) 在传统基准测试上表现良好，(2) 考虑标签依赖性和用户偏好，(3) 通过孵化多个分类器来实现逻辑文本挖掘。</li>
</ul>

<h3>Title: Search Beyond Queries: Training Smaller Language Models for Web  Interactions via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Moghis Fereidouni, A.B. Siddique</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.10887">https://arxiv.org/abs/2404.10887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.10887">https://arxiv.org/pdf/2404.10887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.10887]] Search Beyond Queries: Training Smaller Language Models for Web  Interactions via Reinforcement Learning(https://arxiv.org/abs/2404.10887)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, agent</a></li>
<li><strong>Abstract: </strong>Traditional search systems focus on query formulation for effective results but face challenges in scenarios such as product searches where crucial product details (e.g., size, color) remain concealed until users visit specific product pages. This highlights the need for intelligent web navigation agents capable of formulating queries and navigating web pages according to users' high-level intents. In response to this need, this work introduces a Grounded Language Agent for Intelligent Web Interactions, called GLAINTEL. Drawing upon advancements in language modeling and reinforcement learning, GLAINTEL investigates the efficacy of transformer-based models in enhancing the search capabilities of interactive web environments. Given the dynamic action space for each state in web navigation, GLAINTEL employs the Flan-T5 architecture and incorporates language modeling and value estimation heads. This work focuses on training smaller language models as agents across various scenarios, systematically evaluating the impact of human demonstrations on the training process. Specifically, we investigate scenarios where no human demonstrations are available and subsequently assess the effective utilization of such demonstrations. We also explore unsupervised domain adaptation for situations where demonstrations are confined to a specific domain. Experimental evaluations across diverse setups demonstrate the effectiveness of training agents in unsupervised settings, outperforming in-context learning-based approaches that employ larger models with up to 540 billion parameters. Surprisingly, behavioral cloning-based methods that straightforwardly use human demonstrations do not outperform unsupervised learning-based methods. Additionally, combining human demonstrations with Reinforcement Learning-based training yields results comparable to models utilizing GPT-4.</li>
<li><strong>摘要：</strong>传统的搜索系统专注于查询制定以获得有效的结果，但在产品搜索等场景中面临挑战，在用户访问特定产品页面之前，关键的产品详细信息（例如尺寸、颜色）仍然被隐藏。这凸显了对能够根据用户的高级意图制定查询和导航网页的智能网络导航代理的需求。为了满足这一需求，这项工作引入了一种用于智能 Web 交互的基础语言代理，称为 GLAINTEL。利用语言建模和强化学习方面的进步，GLAINTEL 研究了基于 Transformer 的模型在增强交互式 Web 环境的搜索能力方面的功效。考虑到网络导航中每个状态的动态动作空间，GLAINTEL 采用 Flan-T5 架构，并结合了语言建模和价值估计头。这项工作的重点是在各种场景中训练较小的语言模型作为代理，系统地评估人类演示对训练过程的影响。具体来说，我们调查无法进行人类演示的场景，并随后评估此类演示的有效利用。我们还探索了针对演示仅限于特定领域的情况的无监督领域适应。跨不同设置的实验评估证明了在无监督环境中训练代理的有效性，优于基于上下文学习的方法，这些方法采用具有多达 5400 亿个参数的更大模型。令人惊讶的是，直接使用人类演示的基于行为克隆的方法并不优于基于无监督学习的方法。此外，将人类演示与基于强化学习的训练相结合所产生的结果可与使用 GPT-4 的模型相媲美。</li>
</ul>

<h3>Title: Which questions should I answer? Salience Prediction of Inquisitive  Questions</h3>
<ul>
<li><strong>Authors: </strong>Yating Wu, Ritika Mangla, Alexandros G. Dimakis, Greg Durrett, Junyi Jessy Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.10917">https://arxiv.org/abs/2404.10917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.10917">https://arxiv.org/pdf/2404.10917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.10917]] Which questions should I answer? Salience Prediction of Inquisitive  Questions(https://arxiv.org/abs/2404.10917)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Inquisitive questions -- open-ended, curiosity-driven questions people ask as they read -- are an integral part of discourse processing (Kehler and Rohde, 2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has taken advantage of question generation capabilities of LLMs to enhance a wide range of applications. But the space of inquisitive questions is vast: many questions can be evoked from a given context. So which of those should be prioritized to find answers? Linguistic theories, unfortunately, have not yet provided an answer to this question. This paper presents QSALIENCE, a salience predictor of inquisitive questions. QSALIENCE is instruction-tuned over our dataset of linguist-annotated salience scores of 1,766 (context, question) pairs. A question scores high on salience if answering it would greatly enhance the understanding of the text (Van Rooy, 2003). We show that highly salient questions are empirically more likely to be answered in the same article, bridging potential questions (Onea, 2016) with Questions Under Discussion (Roberts, 2012). We further validate our findings by showing that answering salient questions is an indicator of summarization quality in news.</li>
<li><strong>摘要：</strong>好奇问题——人们在阅读时提出的开放式、好奇心驱动的问题——是话语处理（Kehler and Rohde，2017；Onea，2016）和理解（Prince，2004）不可或缺的一部分。 NLP 领域的最新工作利用了法学硕士的问题生成功能来增强广泛的应用。但好奇问题的空间是巨大的：许多问题可以从给定的背景中引发出来。那么应该优先考虑其中哪一个来寻找答案呢？不幸的是，语言学理论尚未提供这个问题的答案。本文提出了 QSALIENCE，一种好奇问题的显着性预测器。 QSALIENCE 是根据我们的语言学家注释的 1,766 个（上下文、问题）对的显着性分数数据集进行指令调整的。如果回答一个问题能够极大地增强对文本的理解，那么该问题的显着性得分就很高（Van Rooy，2003）。我们表明，根据经验，高度显着的问题更有可能在同一篇文章中得到回答，从而将潜在问题（Onea，2016）与正在讨论的问题（Roberts，2012）联系起来。我们进一步验证了我们的发现，表明回答突出问题是新闻摘要质量的指标。</li>
</ul>

<h3>Title: Teaching a Multilingual Large Language Model to Understand Multilingual  Speech via Multi-Instructional Training</h3>
<ul>
<li><strong>Authors: </strong>Pavel Denisov, Ngoc Thang Vu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.10922">https://arxiv.org/abs/2404.10922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.10922">https://arxiv.org/pdf/2404.10922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.10922]] Teaching a Multilingual Large Language Model to Understand Multilingual  Speech via Multi-Instructional Training(https://arxiv.org/abs/2404.10922)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in language modeling have led to the emergence of Large Language Models (LLMs) capable of various natural language processing tasks. Despite their success in text-based tasks, applying LLMs to the speech domain remains limited and challenging. This paper presents BLOOMZMMS, a novel model that integrates a multilingual LLM with a multilingual speech encoder, aiming to harness the capabilities of LLMs for speech recognition and beyond. Utilizing a multi-instructional training approach, we demonstrate the transferability of linguistic knowledge from the text to the speech modality. Our experiments, conducted on 1900 hours of transcribed data from 139 languages, establish that a multilingual speech representation can be effectively learned and aligned with a multilingual LLM. While this learned representation initially shows limitations in task generalization, we address this issue by generating synthetic targets in a multi-instructional style. Our zero-shot evaluation results confirm the robustness of our approach across multiple tasks, including speech translation and multilingual spoken language understanding, thereby opening new avenues for applying LLMs in the speech domain.</li>
<li><strong>摘要：</strong>语言建模的最新进展导致了能够执行各种自然语言处理任务的大型语言模型（LLM）的出现。尽管法学硕士在基于文本的任务中取得了成功，但将法学硕士应用于语音领域仍然有限且具有挑战性。本文提出了 BLOOMZMMS，这是一种将多语言 LLM 与多语言语音编码器集成在一起的新颖模型，旨在利用 LLM 进行语音识别及其他方面的功能。利用多教学训练方法，我们展示了语言知识从文本到语音模态的可迁移性。我们对来自 139 种语言的 1900 个小时的转录数据进行了实验，结果表明可以有效地学习多语言语音表示，并与多语言法学硕士保持一致。虽然这种学习的表示最初显示出任务泛化的局限性，但我们通过以多指令风格生成合成目标来解决这个问题。我们的零样本评估结果证实了我们的方法在多个任务中的稳健性，包括语音翻译和多语言口语理解，从而为在语音领域应用法学硕士开辟了新途径。</li>
</ul>

<h3>Title: More Room for Language: Investigating the Effect of Retrieval on  Language Models</h3>
<ul>
<li><strong>Authors: </strong>David Samuel, Lucas Georges Gabriel Charpentier, Sondre Wold</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.10939">https://arxiv.org/abs/2404.10939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.10939">https://arxiv.org/pdf/2404.10939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.10939]] More Room for Language: Investigating the Effect of Retrieval on  Language Models(https://arxiv.org/abs/2404.10939)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented language models pose a promising alternative to standard language modeling. During pretraining, these models search in a corpus of documents for contextually relevant information that could aid the language modeling objective. We introduce an 'ideal retrieval' methodology to study these models in a fully controllable setting. We conduct an extensive evaluation to examine how retrieval augmentation affects the behavior of the underlying language model. Among other things, we observe that these models: i) save substantially less world knowledge in their weights, ii) are better at understanding local context and inter-word dependencies, but iii) are worse at comprehending global context.</li>
<li><strong>摘要：</strong>检索增强语言模型为标准语言模型提供了一种有前景的替代方案。在预训练期间，这些模型在文档语料库中搜索有助于实现语言建模目标的上下文相关信息。我们引入了一种“理想检索”方法来在完全可控的环境中研究这些模型。我们进行了广泛的评估，以检查检索增强如何影响底层语言模型的行为。除此之外，我们观察到这些模型：i）在权重中保存的世界知识要少得多，ii）更好地理解本地上下文和词间依赖关系，但 iii）在理解全局上下文方面较差。</li>
</ul>

<h3>Title: Can Language Models Solve Olympiad Programming?</h3>
<ul>
<li><strong>Authors: </strong>Quan Shi, Michael Tang, Karthik Narasimhan, Shunyu Yao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.10952">https://arxiv.org/abs/2404.10952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.10952">https://arxiv.org/pdf/2404.10952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.10952]] Can Language Models Solve Olympiad Programming?(https://arxiv.org/abs/2404.10952)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Computing olympiads contain some of the most challenging problems for humans, requiring complex algorithmic reasoning, puzzle solving, in addition to generating efficient code. However, it has been understudied as a domain to evaluate language models (LMs). In this paper, we introduce the USACO benchmark with 307 problems from the USA Computing Olympiad, along with high-quality unit tests, reference code, and official analyses for each problem. These resources enable us to construct and test a range of LM inference methods for competitive programming for the first time. We find GPT-4 only achieves a 8.7% pass@1 accuracy with zero-shot chain-of-thought prompting, and our best inference method improves it to 20.2% using a combination of self-reflection and retrieval over episodic knowledge. However, this is far from solving the benchmark. To better understand the remaining challenges, we design a novel human-in-the-loop study and surprisingly find that a small number of targeted hints enable GPT-4 to solve 13 out of 15 problems previously unsolvable by any model and method. Our benchmark, baseline methods, quantitative results, and qualitative analysis serve as an initial step toward LMs with grounded, creative, and algorithmic reasoning.</li>
<li><strong>摘要：</strong>计算机奥林匹克竞赛包含一些对人类来说最具挑战性的问题，除了生成高效的代码之外，还需要复杂的算法推理、谜题解决。然而，它作为评估语言模型 (LM) 的领域尚未得到充分研究。在本文中，我们介绍了 USACO 基准测试，包含来自美国计算机奥林匹克竞赛的 307 个问题，以及每个问题的高质量单元测试、参考代码和官方分析。这些资源使我们能够首次构建和测试一系列用于竞争性编程的 LM 推理方法。我们发现 GPT-4 在零样本思维链提示下仅达到 8.7% 的 pass@1 准确率，而我们最好的推理方法通过结合自我反思和情景知识检索将其提高到 20.2%。然而，这还远远没有解决基准问题。为了更好地理解剩下的挑战，我们设计了一项新颖的人机循环研究，并令人惊讶地发现少量有针对性的提示使 GPT-4 能够解决以前任何模型和方法都无法解决的 15 个问题中的 13 个。我们的基准、基线方法、定量结果和定性分析是迈向具有基础、创造性和算法推理的 LM 的第一步。</li>
</ul>

<h3>Title: Uncertainty-Based Abstention in LLMs Improves Safety and Reduces  Hallucinations</h3>
<ul>
<li><strong>Authors: </strong>Christian Tomani, Kamalika Chaudhuri, Ivan Evtimov, Daniel Cremers, Mark Ibrahim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.10960">https://arxiv.org/abs/2404.10960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.10960">https://arxiv.org/pdf/2404.10960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.10960]] Uncertainty-Based Abstention in LLMs Improves Safety and Reduces  Hallucinations(https://arxiv.org/abs/2404.10960)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>A major barrier towards the practical deployment of large language models (LLMs) is their lack of reliability. Three situations where this is particularly apparent are correctness, hallucinations when given unanswerable questions, and safety. In all three cases, models should ideally abstain from responding, much like humans, whose ability to understand uncertainty makes us refrain from answering questions we don't know. Inspired by analogous approaches in classification, this study explores the feasibility and efficacy of abstaining while uncertain in the context of LLMs within the domain of question-answering. We investigate two kinds of uncertainties, statistical uncertainty metrics and a distinct verbalized measure, termed as In-Dialogue Uncertainty (InDU). Using these uncertainty measures combined with models with and without Reinforcement Learning with Human Feedback (RLHF), we show that in all three situations, abstention based on the right kind of uncertainty measure can boost the reliability of LLMs. By sacrificing only a few highly uncertain samples we can improve correctness by 2% to 8%, avoid 50% hallucinations via correctly identifying unanswerable questions and increase safety by 70% up to 99% with almost no additional computational overhead.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）实际部署的一个主要障碍是它们缺乏可靠性。这种情况特别明显的三种情况是正确性、给出无法回答的问题时的幻觉以及安全性。在这三种情况下，理想情况下模型应该避免做出回应，就像人类一样，人类理解不确定性的能力使我们避免回答我们不知道的问题。受类似分类方法的启发，本研究探讨了法学硕士在问答领域中不确定时弃权的可行性和有效性。我们研究了两种不确定性：统计不确定性指标和一种独特的言语测量，称为对话中不确定性（InDU）。将这些不确定性度量与带有或不带有人类反馈的强化学习（RLHF）的模型相结合，我们表明，在所有三种情况下，基于正确类型的不确定性度量的弃权可以提高法学硕士的可靠性。通过仅牺牲一些高度不确定的样本，我们可以将正确性提高 2% 至 8%，通过正确识别无法回答的问题来避免 50% 的幻觉，并将安全性提高 70% 至 99%，而几乎没有额外的计算开销。</li>
</ul>

<h3>Title: Procedural Dilemma Generation for Evaluating Moral Reasoning in Humans  and Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jan-Philipp Fränken, Kanishk Gandhi, Tori Qiu, Ayesha Khawaja, Noah D. Goodman, Tobias Gerstenberg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.10975">https://arxiv.org/abs/2404.10975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.10975">https://arxiv.org/pdf/2404.10975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.10975]] Procedural Dilemma Generation for Evaluating Moral Reasoning in Humans  and Language Models(https://arxiv.org/abs/2404.10975)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, agent</a></li>
<li><strong>Abstract: </strong>As AI systems like language models are increasingly integrated into decision-making processes affecting people's lives, it's critical to ensure that these systems have sound moral reasoning. To test whether they do, we need to develop systematic evaluations. We provide a framework that uses a language model to translate causal graphs that capture key aspects of moral dilemmas into prompt templates. With this framework, we procedurally generated a large and diverse set of moral dilemmas -- the OffTheRails benchmark -- consisting of 50 scenarios and 400 unique test items. We collected moral permissibility and intention judgments from human participants for a subset of our items and compared these judgments to those from two language models (GPT-4 and Claude-2) across eight conditions. We find that moral dilemmas in which the harm is a necessary means (as compared to a side effect) resulted in lower permissibility and higher intention ratings for both participants and language models. The same pattern was observed for evitable versus inevitable harmful outcomes. However, there was no clear effect of whether the harm resulted from an agent's action versus from having omitted to act. We discuss limitations of our prompt generation pipeline and opportunities for improving scenarios to increase the strength of experimental effects.</li>
<li><strong>摘要：</strong>随着语言模型等人工智能系统越来越多地融入到影响人们生活的决策过程中，确保这些系统具有健全的道德推理至关重要。为了检验它们是否有效，我们需要进行系统评估。我们提供了一个框架，使用语言模型将捕获道德困境关键方面的因果图转换为提示模板。通过这个框架，我们在程序上生成了一组大量且多样化的道德困境——OffTheRails 基准——由 50 个场景和 400 个独特的测试项目组成。我们收集了人类参与者对我们项目子集的道德许可性和意图判断，并将这些判断与八种条件下的两种语言模型（GPT-4 和 Claude-2）的判断进行了比较。我们发现，在道德困境中，伤害是一种必要手段（与副作用相比），导致参与者和语言模型的允许性较低，意图评级较高。对于可避免的有害结果与不可避免的有害结果也观察到相同的模式。然而，对于伤害是由代理人的行为还是由于不采取行动造成的，尚无明确的影响。我们讨论了即时生成管道的局限性以及改进场景以增加实验效果强度的机会。</li>
</ul>

<h3>Title: Offset Unlearning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>James Y. Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11045">https://arxiv.org/abs/2404.11045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11045">https://arxiv.org/pdf/2404.11045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11045]] Offset Unlearning for Large Language Models(https://arxiv.org/abs/2404.11045)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose $\delta$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, $\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that $\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. $\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLM）从训练语料库中获取知识的能力很强，但对语料库中敏感信息（例如受版权保护的、有害的和私人内容）的记忆引起了道德和法律方面的担忧。为了应对这些挑战，忘记学习已成为受有问题的培训数据影响的法学硕士的一种潜在补救措施。然而，以前的遗忘技术要么由于需要访问模型内​​部权重而不适用于黑盒法学硕士，要么因保留敏感数据以进行推理时间校正而违反了数据保护原则。我们提出$\delta$-unlearning，一种用于黑盒法学硕士的抵消学习框架。 $\delta$-unlearning 不是调整黑盒 LLM 本身，而是通过对比一对较小模型的 logit 来学习 unlearning 所需的 logit 偏移量。实验表明，$\delta$-unlearning 可以有效地忘却目标数据，同时在一般的遗忘范围任务上保持相似甚至更强的性能。 $\delta$-unlearning还有效地结合了不同的unlearning算法，使我们的方法成为一种通用的解决方案，可以将各种现有的unlearning算法适应黑盒LLM。</li>
</ul>

<h3>Title: On the Causal Nature of Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhiheng Lyu, Zhijing Jin, Fernando Gonzalez, Rada Mihalcea, Bernhard Schoelkopf, Mrinmaya Sachan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11055">https://arxiv.org/abs/2404.11055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11055">https://arxiv.org/pdf/2404.11055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11055]] On the Causal Nature of Sentiment Analysis(https://arxiv.org/abs/2404.11055)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Sentiment analysis (SA) aims to identify the sentiment expressed in a text, such as a product review. Given a review and the sentiment associated with it, this paper formulates SA as a combination of two tasks: (1) a causal discovery task that distinguishes whether a review "primes" the sentiment (Causal Hypothesis C1), or the sentiment "primes" the review (Causal Hypothesis C2); and (2) the traditional prediction task to model the sentiment using the review as input. Using the peak-end rule in psychology, we classify a sample as C1 if its overall sentiment score approximates an average of all the sentence-level sentiments in the review, and C2 if the overall sentiment score approximates an average of the peak and end sentiments. For the prediction task, we use the discovered causal mechanisms behind the samples to improve the performance of LLMs by proposing causal prompts that give the models an inductive bias of the underlying causal graph, leading to substantial improvements by up to 32.13 F1 points on zero-shot five-class SA. Our code is at https://github.com/cogito233/causal-sa</li>
<li><strong>摘要：</strong>情感分析 (SA) 旨在识别文本中表达的情感，例如产品评论。给定评论和与之相关的情绪，本文将 SA 表述为两个任务的组合：（1）因果发现任务，区分评论是“启动”情绪（因果假设 C1）还是情绪“启动”审查（因果假设 C2）； （2）传统的预测任务，使用评论作为输入来建模情绪。使用心理学中的峰值-结束规则，如果样本的总体情感得分接近评论中所有句子级情感的平均值，则将样本分类为 C1；如果总体情感得分接近峰值和最终情感的平均值，则将样本分类为 C2 。对于预测任务，我们使用样本背后发现的因果机制来提高 LLM 的性能，方法是提出因果提示，为模型提供底层因果图的归纳偏差，从而在零上实现高达 32.13 F1 点的实质性改进。射五级SA。我们的代码位于 https://github.com/cogito233/causal-sa</li>
</ul>

<h3>Title: ViLLM-Eval: A Comprehensive Evaluation Suite for Vietnamese Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Trong-Hieu Nguyen, Anh-Cuong Le, Viet-Cuong Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11086">https://arxiv.org/abs/2404.11086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11086">https://arxiv.org/pdf/2404.11086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11086]] ViLLM-Eval: A Comprehensive Evaluation Suite for Vietnamese Large  Language Models(https://arxiv.org/abs/2404.11086)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) necessitates the development of new benchmarks to accurately assess their capabilities. To address this need for Vietnamese, this work aims to introduce ViLLM-Eval, the comprehensive evaluation suite designed to measure the advanced knowledge and reasoning abilities of foundation models within a Vietnamese context. ViLLM-Eval consists of multiple-choice questions and predict next word tasks spanning various difficulty levels and diverse disciplines, ranging from humanities to science and engineering. A thorough evaluation of the most advanced LLMs on ViLLM-Eval revealed that even the best performing models have significant room for improvement in understanding and responding to Vietnamese language tasks. ViLLM-Eval is believed to be instrumental in identifying key strengths and weaknesses of foundation models, ultimately promoting their development and enhancing their performance for Vietnamese users.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的快速发展需要开发新的基准来准确评估其能力。为了满足越南语的这一需求，这项工作旨在引入 ViLLM-Eval，这是一个综合评估套件，旨在衡量越南语背景下基础模型的高级知识和推理能力。 ViLLM-Eval 包含多项选择题和预测下一个单词任务，涵盖各种难度级别和不同学科，从人文到科学和工程。对 ViLLM-Eval 上最先进的法学硕士的全面评估表明，即使是性能最好的模型，在理解和响应越南语任务方面也有很大的改进空间。 ViLLM-Eval 被认为有助于确定基础模型的主要优势和劣势，最终促进其发展并提高越南用户的性能。</li>
</ul>

<h3>Title: Inductive-Deductive Strategy Reuse for Multi-Turn Instructional  Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Jiao Ou, Jiayu Wu, Che Liu, Fuzheng Zhang, Di Zhang, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11095">https://arxiv.org/abs/2404.11095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11095">https://arxiv.org/pdf/2404.11095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11095]] Inductive-Deductive Strategy Reuse for Multi-Turn Instructional  Dialogues(https://arxiv.org/abs/2404.11095)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human expectations requires high-quality instructional dialogues, which can be achieved by raising diverse, in-depth, and insightful instructions that deepen interactions. Existing methods target instructions from real instruction dialogues as a learning goal and fine-tune a user simulator for posing instructions. However, the user simulator struggles to implicitly model complex dialogue flows and pose high-quality instructions. In this paper, we take inspiration from the cognitive abilities inherent in human learning and propose the explicit modeling of complex dialogue flows through instructional strategy reuse. Specifically, we first induce high-level strategies from various real instruction dialogues. These strategies are applied to new dialogue scenarios deductively, where the instructional strategies facilitate high-quality instructions. Experimental results show that our method can generate diverse, in-depth, and insightful instructions for a given dialogue history. The constructed multi-turn instructional dialogues can outperform competitive baselines on the downstream chat model.</li>
<li><strong>摘要：</strong>使大语言模型 (LLM) 与人类期望保持一致需要高质量的教学对话，这可以通过提出多样化、深入且富有洞察力的指导来加深互动来实现。现有方法将来自真实指令对话的指令作为学习目标，并微调用户模拟器以提出指令。然而，用户模拟器很难隐式地模拟复杂的对话流并提出高质量的指令。在本文中，我们从人类学习固有的认知能力中汲取灵感，并提出通过教学策略重用对复杂对话流进行显式建模。具体来说，我们首先从各种真实的教学对话中归纳出高级策略。这些策略被演绎地应用于新的对话场景，其中教学策略促进高质量的教学。实验结果表明，我们的方法可以为给定的对话历史生成多样化、深入且富有洞察力的指令。构建的多轮教学对话可以超越下游聊天模型的竞争基线。</li>
</ul>

<h3>Title: FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out  Document</h3>
<ul>
<li><strong>Authors: </strong>Joonho Yang, Seunghyun Yoon, Byeongjeong Kim, Hwanhee Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11184">https://arxiv.org/abs/2404.11184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11184">https://arxiv.org/pdf/2404.11184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11184]] FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out  Document(https://arxiv.org/abs/2404.11184)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Through the advent of pre-trained language models, there have been notable advancements in abstractive summarization systems. Simultaneously, a considerable number of novel methods for evaluating factual consistency in abstractive summarization systems has been developed. But these evaluation approaches incorporate substantial limitations, especially on refinement and interpretability. In this work, we propose highly effective and interpretable factual inconsistency detection method metric Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document for abstractive summarization systems that is based on fine-grained atomic facts decomposition. Moreover, we align atomic facts decomposed from the summary with the source document through adaptive granularity expansion. These atomic facts represent a more fine-grained unit of information, facilitating detailed understanding and interpretability of the summary's factual inconsistency. Experimental results demonstrate that our proposed factual consistency checking system significantly outperforms existing systems. We release the code at https://github.com/plm3332/FIZZ.</li>
<li><strong>摘要：</strong>通过预训练语言模型的出现，抽象摘要系统取得了显着的进步。同时，已经开发了相当多用于评估抽象摘要系统中事实一致性的新方法。但这些评估方法存在很大的局限性，特别是在细化和可解释性方面。在这项工作中，我们为基于细粒度原子事实分解的抽象摘要系统提出了高效且可解释的事实不一致检测方法，通过放大摘要和缩小文档来度量事实不一致检测。此外，我们通过自适应粒度扩展将摘要分解的原子事实与源文档对齐。这些原子事实代表了更细粒度的信息单元，有助于详细理解和解释摘要中的事实不一致之处。实验结果表明，我们提出的事实一致性检查系统显着优于现有系统。我们在 https://github.com/plm3332/FIZZ 发布了代码。</li>
</ul>

<h3>Title: Prompt-tuning for Clickbait Detection via Text Summarization</h3>
<ul>
<li><strong>Authors: </strong>Haoxiang Deng, Yi Zhu, Ye Wang, Jipeng Qiang, Yunhao Yuan, Yun Li, Runmei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11206">https://arxiv.org/abs/2404.11206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11206">https://arxiv.org/pdf/2404.11206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11206]] Prompt-tuning for Clickbait Detection via Text Summarization(https://arxiv.org/abs/2404.11206)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Clickbaits are surprising social posts or deceptive news headlines that attempt to lure users for more clicks, which have posted at unprecedented rates for more profit or commercial revenue. The spread of clickbait has significant negative impacts on the users, which brings users misleading or even click-jacking attacks. Different from fake news, the crucial problem in clickbait detection is determining whether the headline matches the corresponding content. Most existing methods compute the semantic similarity between the headlines and contents for detecting clickbait. However, due to significant differences in length and semantic features between headlines and contents, directly calculating semantic similarity is often difficult to summarize the relationship between them. To address this problem, we propose a prompt-tuning method for clickbait detection via text summarization in this paper, text summarization is introduced to summarize the contents, and clickbait detection is performed based on the similarity between the generated summary and the contents. Specifically, we first introduce a two-stage text summarization model to produce high-quality news summaries based on pre-trained language models, and then both the headlines and new generated summaries are incorporated as the inputs for prompt-tuning. Additionally, a variety of strategies are conducted to incorporate external knowledge for improving the performance of clickbait detection. The extensive experiments on well-known clickbait detection datasets demonstrate that our method achieved state-of-the-art performance.</li>
<li><strong>摘要：</strong>点击诱饵是令人惊讶的社交帖子或欺骗性新闻标题，试图引诱用户获得更多点击，这些点击以前所未有的速度发布以获取更多利润或商业收入。点击诱饵的传播对用户产生了显着的负面影响，给用户带来误导甚至点击劫持攻击。与假新闻不同，点击诱饵检测的关键问题是确定标题是否与相应内容匹配。大多数现有方法都会计算标题和内容之间的语义相似度来检测标题诱饵。然而，由于标题和内容之间的长度和语义特征存在显着差异，直接计算语义相似度往往难以概括它们之间的关系。为了解决这个问题，我们在本文中提出了一种通过文本摘要进行点击诱饵检测的提示调整方法，引入文本摘要来概括内容，并根据生成的摘要与内容之间的相似性进行点击诱饵检测。具体来说，我们首先引入两阶段文本摘要模型，基于预训练的语言模型生成高质量的新闻摘要，然后将标题和新生成的摘要合并为提示调整的输入。此外，还采取了多种策略来整合外部知识，以提高点击诱饵检测的性能。对著名的点击诱饵检测数据集进行的广泛实验表明，我们的方法实现了最先进的性能。</li>
</ul>

<h3>Title: Position Engineering: Boosting Large Language Models through Positional  Information Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan He, Huiqiang Jiang, Zilong Wang, Yuqing Yang, Luna Qiu, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11216">https://arxiv.org/abs/2404.11216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11216">https://arxiv.org/pdf/2404.11216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11216]] Position Engineering: Boosting Large Language Models through Positional  Information Manipulation(https://arxiv.org/abs/2404.11216)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The performance of large language models (LLMs) is significantly influenced by the quality of the prompts provided. In response, researchers have developed enormous prompt engineering strategies aimed at modifying the prompt text to enhance task performance. In this paper, we introduce a novel technique termed position engineering, which offers a more efficient way to guide large language models. Unlike prompt engineering, which requires substantial effort to modify the text provided to LLMs, position engineering merely involves altering the positional information in the prompt without modifying the text itself. We have evaluated position engineering in two widely-used LLM scenarios: retrieval-augmented generation (RAG) and in-context learning (ICL). Our findings show that position engineering substantially improves upon the baseline in both cases. Position engineering thus represents a promising new strategy for exploiting the capabilities of large language models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的性能很大程度上受到所提供提示的质量的影响。作为回应，研究人员开发了巨大的提示工程策略，旨在修改提示文本以提高任务性能。在本文中，我们介绍了一种称为位置工程的新技术，它提供了一种更有效的方法来指导大型语言模型。与提示工程需要付出大量努力来修改提供给法学硕士的文本不同，位置工程仅涉及更改提示中的位置信息，而不修改文本本身。我们在两种广泛使用的法学硕士场景中评估了位置工程：检索增强生成（RAG）和上下文学习（ICL）。我们的研究结果表明，位置工程在这两种情况下都大大改善了基线。因此，位置工程代表了一种有前途的利用大型语言模型功能的新策略。</li>
</ul>

<h3>Title: In-Context Learning State Vector with Inner and Momentum Optimization</h3>
<ul>
<li><strong>Authors: </strong>Dongfang Li, Zhenyu Liu, Xinshuo Hu, Zetian Sun, Baotian Hu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11225">https://arxiv.org/abs/2404.11225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11225">https://arxiv.org/pdf/2404.11225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11225]] In-Context Learning State Vector with Inner and Momentum Optimization(https://arxiv.org/abs/2404.11225)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples. Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer. However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored. In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introduce the concept of state vector. Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation. Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge. We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks. Code is available at https://github.com/HITsz-TMG/ICL-State-Vector</li>
<li><strong>摘要：</strong>大型语言模型（LLM）仅通过几个例子就表现出了令人印象深刻的执行上下文学习（ICL）的能力。最近的研究表明，ICL 学习到的函数可以通过从 Transformer 导出的压缩向量来表示。然而，这些载体的工作机制和优化仍有待深入探索。在本文中，我们通过对这些压缩向量进行全面分析来解决这一差距，与梯度下降训练的参数进行比较，并引入状态向量的概念。受到模型汤和基于动量的梯度下降工作的启发，我们提出了内部和动量优化方法，这些方法用于在测试时适应中逐步细化状态向量。此外，我们在多示例设置中模拟状态向量聚合，其中包含大量示例的演示对于常规 ICL 来说通常太长，并进一步提出了一种分而治之的聚合方法来解决这一挑战。我们使用 Llama-2 和 GPT-J 在零样本设置和少样本设置中进行了广泛的实验。实验结果表明，我们的优化方法有效增强了状态向量，并在各种任务上实现了最先进的性能。代码可在 https://github.com/HITsz-TMG/ICL-State-Vector 获取</li>
</ul>

<h3>Title: Sampling-based Pseudo-Likelihood for Membership Inference Attacks</h3>
<ul>
<li><strong>Authors: </strong>Masahiro Kaneko, Youmi Ma, Yuki Wata, Naoaki Okazaki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11262">https://arxiv.org/abs/2404.11262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11262">https://arxiv.org/pdf/2404.11262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11262]] Sampling-based Pseudo-Likelihood for Membership Inference Attacks(https://arxiv.org/abs/2404.11262)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are trained on large-scale web data, which makes it difficult to grasp the contribution of each text. This poses the risk of leaking inappropriate data such as benchmarks, personal information, and copyrighted texts in the training data. Membership Inference Attacks (MIA), which determine whether a given text is included in the model's training data, have been attracting attention. Previous studies of MIAs revealed that likelihood-based classification is effective for detecting leaks in LLMs. However, the existing methods cannot be applied to some proprietary models like ChatGPT or Claude 3 because the likelihood is unavailable to the user. In this study, we propose a Sampling-based Pseudo-Likelihood (\textbf{SPL}) method for MIA (\textbf{SaMIA}) that calculates SPL using only the text generated by an LLM to detect leaks. The SaMIA treats the target text as the reference text and multiple outputs from the LLM as text samples, calculates the degree of $n$-gram match as SPL, and determines the membership of the text in the training data. Even without likelihoods, SaMIA performed on par with existing likelihood-based methods.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）是在大规模网络数据上进行训练的，这使得很难掌握每个文本的贡献。这带来了泄露不适当数据的风险，例如训练数据中的基准、个人信息和受版权保护的文本。成员推理攻击（MIA），它确定给定的文本是否包含在模型的训练数据中，一直引起人们的关注。之前对 MIA 的研究表明，基于可能性的分类对于检测法学硕士的泄漏是有效的。然而，现有方法不能应用于某些专有模型，如 ChatGPT 或 Claude 3，因为用户无法获得可能性。在本研究中，我们提出了一种用于 MIA (\textbf{SaMIA}) 的基于采样的伪似然 (\textbf{SPL}) 方法，该方法仅使用 LLM 生成的文本来计算 SPL，以检测泄漏。 SaMIA将目标文本视为参考文本，将LLM的多个输出视为文本样本，计算$n$-gram匹配度作为SPL，并确定文本在训练数据中的隶属度。即使没有似然性，SaMIA 的表现也与现有的基于似然性的方法相当。</li>
</ul>

<h3>Title: A Preference-driven Paradigm for Enhanced Translation with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dawei Zhu, Sony Trenous, Xiaoyu Shen, Dietrich Klakow, Bill Byrne, Eva Hasler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11288">https://arxiv.org/abs/2404.11288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11288">https://arxiv.org/pdf/2404.11288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11288]] A Preference-driven Paradigm for Enhanced Translation with Large  Language Models(https://arxiv.org/abs/2404.11288)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent research has shown that large language models (LLMs) can achieve remarkable translation performance through supervised fine-tuning (SFT) using only a small amount of parallel data. However, SFT simply instructs the model to imitate the reference translations at the token level, making it vulnerable to the noise present in the references. Hence, the assistance from SFT often reaches a plateau once the LLMs have achieved a certain level of translation capability, and further increasing the size of parallel data does not provide additional benefits. To overcome this plateau associated with imitation-based SFT, we propose a preference-based approach built upon the Plackett-Luce model. The objective is to steer LLMs towards a more nuanced understanding of translation preferences from a holistic view, while also being more resilient in the absence of gold translations. We further build a dataset named MAPLE to verify the effectiveness of our approach, which includes multiple translations of varying quality for each source sentence. Extensive experiments demonstrate the superiority of our approach in "breaking the plateau" across diverse LLMs and test settings. Our in-depth analysis underscores the pivotal role of diverse translations and accurate preference scores in the success of our approach.</li>
<li><strong>摘要：</strong>最近的研究表明，大型语言模型（LLM）可以通过仅使用少量并行数据的监督微调（SFT）来实现出色的翻译性能。然而，SFT 只是指示模型在标记级别模仿参考翻译，使其容易受到参考中存在的噪声的影响。因此，一旦法学硕士达到一定水平的翻译能力，SFT 的帮助往往会达到一个平台期，并且进一步增加并行数据的大小并不能带来额外的好处。为了克服与基于模仿的 SFT 相关的这一平台，我们提出了一种基于 Plackett-Luce 模型的基于偏好的方法。目的是引导法学硕士从整体角度更细致地了解翻译偏好，同时在缺乏黄金翻译的情况下更具弹性。我们进一步构建了一个名为 MAPLE 的数据集来验证我们方法的有效性，其中包括每个源句子的不同质量的多个翻译。大量的实验证明了我们的方法在跨不同法学硕士和测试环境“打破平台”方面的优越性。我们的深入分析强调了多样化翻译和准确的偏好评分对于我们方法的成功的关键作用。</li>
</ul>

<h3>Title: To Drop or Not to Drop? Predicting Argument Ellipsis Judgments: A Case  Study in Japanese</h3>
<ul>
<li><strong>Authors: </strong>Yukiko Ishizuki, Tatsuki Kuribayashi, Yuichiroh Matsubayashi, Ryohei Sasano, Kentaro Inui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11315">https://arxiv.org/abs/2404.11315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11315">https://arxiv.org/pdf/2404.11315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11315]] To Drop or Not to Drop? Predicting Argument Ellipsis Judgments: A Case  Study in Japanese(https://arxiv.org/abs/2404.11315)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Speakers sometimes omit certain arguments of a predicate in a sentence; such omission is especially frequent in pro-drop languages. This study addresses a question about ellipsis -- what can explain the native speakers' ellipsis decisions? -- motivated by the interest in human discourse processing and writing assistance for this choice. To this end, we first collect large-scale human annotations of whether and why a particular argument should be omitted across over 2,000 data points in the balanced corpus of Japanese, a prototypical pro-drop language. The data indicate that native speakers overall share common criteria for such judgments and further clarify their quantitative characteristics, e.g., the distribution of related linguistic factors in the balanced corpus. Furthermore, the performance of the language model-based argument ellipsis judgment model is examined, and the gap between the systems' prediction and human judgments in specific linguistic aspects is revealed. We hope our fundamental resource encourages further studies on natural human ellipsis judgment.</li>
<li><strong>摘要：</strong>说话者有时会省略句子中谓语的某些论元；这种遗漏在支持放弃的语言中尤其常见。这项研究解决了一个有关省略的问题——什么可以解释母语人士的省略决定？ ——出于对人类话语处理和写作帮助的兴趣而做出的选择。为此，我们首先在日语（一种典型的赞成放弃语言）的平衡语料库中收集了关于是否以及为什么应该省略某个特定论点的大规模人类注释。数据表明，母语人士总体上具有共同的判断标准，并进一步明确了其定量特征，例如相关语言因素在平衡语料库中的分布。此外，还检查了基于语言模型的论元省略判断模型的性能，并揭示了系统预测与人类判断在特定语言方面的差距。我们希望我们的基础资源能够鼓励对自然人类省略判断的进一步研究。</li>
</ul>

<h3>Title: Open-Ended Wargames with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daniel P. Hogan, Andrea Brennen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11446">https://arxiv.org/abs/2404.11446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11446">https://arxiv.org/pdf/2404.11446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11446]] Open-Ended Wargames with Large Language Models(https://arxiv.org/abs/2404.11446)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Wargames are a powerful tool for understanding and rehearsing real-world decision making. Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes. There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses. Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames. We introduce "Snow Globe," an LLM-powered multi-agent system for playing qualitative wargames. With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof. We describe its software architecture conceptually and release an open-source implementation alongside this publication. As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis. We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem.</li>
<li><strong>摘要：</strong>兵棋推演是理解和演练现实世界决策的强大工具。使用人工智能 (AI) 自动进行兵棋推演可以实现超出人类进行的兵棋推演的可能性，例如多次玩游戏以查看一系列可能的结果。兵棋推演分为两类：定量博弈（具有离散类型的行动）和定性博弈（围绕开放式反应）。从历史上看，自动化工作主要集中在定量兵棋推演，但大型语言模型 (LLM) 使得定性兵棋推演自动化成为可能。我们推出“Snow Globe”，这是一个由 LLM 驱动的多智能体系统，用于进行定性兵棋推演。借助 Snow Globe，基于文本的定性兵棋推演的每个阶段（从场景准备到赛后分析）都可以选择由人工智能、人类或其组合来执行。我们从概念上描述了其软件架构，并与本出版物一起发布了开源实现。作为案例研究，我们模拟了有关人工智能事件响应的桌面演习和有关地缘政治危机的政治兵棋推演。我们讨论该方法的潜在应用以及它如何适应更广泛的兵棋推演生态系统。</li>
</ul>

<h3>Title: AI-Enhanced Cognitive Behavioral Therapy: Deep Learning and Large  Language Models for Extracting Cognitive Pathways from Social Media Texts</h3>
<ul>
<li><strong>Authors: </strong>Meng Jiang, Yi Jing Yu, Qing Zhao, Jianqiang Li, Changwei Song, Hongzhi Qi, Wei Zhai, Dan Luo, Xiaoqin Wang, Guanghui Fu, Bing Xiang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11449">https://arxiv.org/abs/2404.11449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11449">https://arxiv.org/pdf/2404.11449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11449]] AI-Enhanced Cognitive Behavioral Therapy: Deep Learning and Large  Language Models for Extracting Cognitive Pathways from Social Media Texts(https://arxiv.org/abs/2404.11449)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Cognitive Behavioral Therapy (CBT) is an effective technique for addressing the irrational thoughts stemming from mental illnesses, but it necessitates precise identification of cognitive pathways to be successfully implemented in patient care. In current society, individuals frequently express negative emotions on social media on specific topics, often exhibiting cognitive distortions, including suicidal behaviors in extreme cases. Yet, there is a notable absence of methodologies for analyzing cognitive pathways that could aid psychotherapists in conducting effective interventions online. In this study, we gathered data from social media and established the task of extracting cognitive pathways, annotating the data based on a cognitive theoretical framework. We initially categorized the task of extracting cognitive pathways as a hierarchical text classification with four main categories and nineteen subcategories. Following this, we structured a text summarization task to help psychotherapists quickly grasp the essential information. Our experiments evaluate the performance of deep learning and large language models (LLMs) on these tasks. The results demonstrate that our deep learning method achieved a micro-F1 score of 62.34% in the hierarchical text classification task. Meanwhile, in the text summarization task, GPT-4 attained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing the experimental deep learning model's performance. However, it may suffer from an issue of hallucination. We have made all models and codes publicly available to support further research in this field.</li>
<li><strong>摘要：</strong>认知行为疗法（CBT）是解决精神疾病引起的非理性想法的有效技术，但它需要精确识别认知路径才能在患者护理中成功实施。当今社会，个体经常在社交媒体上就特定话题表达负面情绪，往往表现出认知扭曲，极端情况下甚至出现自杀行为。然而，明显缺乏分析认知路径的方法来帮助心理治疗师在线进行有效的干预。在本研究中，我们从社交媒体收集数据，并建立了提取认知路径的任务，并基于认知理论框架对数据进行注释。我们最初将提取认知路径的任务归类为具有四个主要类别和十九个子类别的分层文本分类。接下来，我们构建了一个文本摘要任务，以帮助心理治疗师快速掌握基本信息。我们的实验评估了深度学习和大型语言模型 (LLM) 在这些任务上的性能。结果表明，我们的深度学习方法在分层文本分类任务中取得了 62.34% 的 micro-F1 分数。同时，在文本摘要任务中，GPT-4 的 Rouge-1 得分为 54.92，Rouge-2 得分为 30.86，超过了实验深度学习模型的表现。然而，它可能会出现幻觉问题。我们已公开所有模型和代码，以支持该领域的进一步研究。</li>
</ul>

<h3>Title: Octopus v3: Technical Report for On-device Sub-billion Multimodal AI  Agent</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Zhiyuan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11459">https://arxiv.org/abs/2404.11459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11459">https://arxiv.org/pdf/2404.11459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11459]] Octopus v3: Technical Report for On-device Sub-billion Multimodal AI  Agent(https://arxiv.org/abs/2404.11459)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, agent</a></li>
<li><strong>Abstract: </strong>A multimodal AI agent is characterized by its ability to process and learn from various types of data, including natural language, visual, and audio inputs, to inform its actions. Despite advancements in large language models that incorporate visual data, such as GPT-4V, effectively translating image-based data into actionable outcomes for AI agents continues to be challenging. In this paper, we introduce a multimodal model that incorporates the concept of functional token specifically designed for AI agent applications. To ensure compatibility with edge devices, our model is optimized to a compact size of less than 1B parameters. Like GPT-4, our model can process both English and Chinese. We demonstrate that this model is capable of operating efficiently on a wide range of edge devices, including as constrained as a Raspberry Pi.</li>
<li><strong>摘要：</strong>多模式人工智能代理的特点是能够处理和学习各种类型的数据，包括自然语言、视觉和音频输入，以通知其行动。尽管合并视觉数据的大型语言模型（例如 GPT-4V）取得了进步，但将基于图像的数据有效地转换为 AI 代理可操作的结果仍然具有挑战性。在本文中，我们介绍了一种多模态模型，该模型结合了专为人工智能代理应用程序设计的功能令牌的概念。为了确保与边缘设备的兼容性，我们的模型被优化为小于 1B 参数的紧凑尺寸。与 GPT-4 一样，我们的模型可以处理英语和中文。我们证明该模型能够在各种边缘设备上高效运行，包括像 Raspberry Pi 这样受限的设备。</li>
</ul>

<h3>Title: Paraphrase and Solve: Exploring and Exploiting the Impact of Surface  Form on Mathematical Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhou, Yada Zhu, Diego Antognini, Yoon Kim, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11500">https://arxiv.org/abs/2404.11500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11500">https://arxiv.org/pdf/2404.11500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11500]] Paraphrase and Solve: Exploring and Exploiting the Impact of Surface  Form on Mathematical Reasoning in Large Language Models(https://arxiv.org/abs/2404.11500)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper studies the relationship between the surface form of a mathematical problem and its solvability by large language models. We find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate, exposing the language model's lack of robustness and sensitivity to the surface form in reasoning through complex problems. To improve mathematical reasoning performance, we propose Self-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths from specific surface forms of the problem. We evaluate our approach on four mathematics reasoning benchmarks over three large language models and show that SCoP improves mathematical reasoning performance over vanilla self-consistency, particularly for problems initially deemed unsolvable. Finally, we provide additional experiments and discussion regarding problem difficulty and surface forms, including cross-model difficulty agreement and paraphrasing transferability, and Variance of Variations (VOV) for language model evaluation.</li>
<li><strong>摘要：</strong>本文研究数学问题的表面形式与其大型语言模型可解性之间的关系。我们发现表面形式的细微改变会显着影响答案分布和解决率，暴露出语言模型在复杂问题推理中缺乏鲁棒性和对表面形式的敏感性。为了提高数学推理性能，我们提出了释义自洽（SCoP），它使推理路径从问题的特定表面形式多样化。我们在三个大型语言模型的四个数学推理基准上评估了我们的方法，并表明 SCoP 比普通的自洽性提高了数学推理性能，特别是对于最初被认为无法解决的问题。最后，我们提供了关于问题难度和表面形式的额外实验和讨论，包括跨模型难度一致性和释义可转移性，以及用于语言模型评估的变体方差（VOV）。</li>
</ul>

<h3>Title: Towards Coarse-to-Fine Evaluation of Inference Efficiency for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yushuo Chen, Tianyi Tang, Erge Xiang, Linjiang Li, Wayne Xin Zhao, Jing Wang, Yunpeng Chai, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11502">https://arxiv.org/abs/2404.11502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11502">https://arxiv.org/pdf/2404.11502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11502]] Towards Coarse-to-Fine Evaluation of Inference Efficiency for Large  Language Models(https://arxiv.org/abs/2404.11502)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In real world, large language models (LLMs) can serve as the assistant to help users accomplish their jobs, and also support the development of advanced applications. For the wide application of LLMs, the inference efficiency is an essential concern, which has been widely studied in existing work, and numerous optimization algorithms and code libraries have been proposed to improve it. Nonetheless, users still find it challenging to compare the effectiveness of all the above methods and understand the underlying mechanisms. In this work, we perform a detailed coarse-to-fine analysis of the inference performance of various code libraries. To evaluate the overall effectiveness, we examine four usage scenarios within two practical applications. We further provide both theoretical and empirical fine-grained analyses of each module in the Transformer architecture. Our experiments yield comprehensive results that are invaluable for researchers to evaluate code libraries and improve inference strategies.</li>
<li><strong>摘要：</strong>在现实世界中，大语言模型（LLM）可以作为帮助用户完成工作的助手，也可以支持高级应用程序的开发。对于LLM的广泛应用，推理效率是一个重要的问题，在现有的工作中已经被广泛研究，并且已经提出了许多优化算法和代码库来改进它。尽管如此，用户仍然发现比较上述所有方法的有效性并了解潜在机制具有挑战性。在这项工作中，我们对各种代码库的推理性能进行了详细的从粗到细的分析。为了评估整体有效性，我们检查了两个实际应用程序中的四种使用场景。我们进一步对 Transformer 架构中的每个模块进行了理论和实证的细粒度分析。我们的实验产生了全面的结果，对于研究人员评估代码库和改进推理策略来说非常宝贵。</li>
</ul>

<h3>Title: Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization</h3>
<ul>
<li><strong>Authors: </strong>Costas Mavromatis, Petros Karypis, George Karypis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11531">https://arxiv.org/abs/2404.11531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11531">https://arxiv.org/pdf/2404.11531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11531]] Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization(https://arxiv.org/abs/2404.11531)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Fusing knowledge from multiple Large Language Models (LLMs) can combine their diverse strengths to achieve improved performance on a given task. However, current fusion approaches either rely on learning-based fusers that do not generalize to new LLMs, or do not take into account how well each LLM understands the input. In this work, we study LLM fusion at test-time, which enables leveraging knowledge from arbitrary user-specified LLMs during inference. We introduce Pack of LLMs (PackLLM), an effective method for test-time fusion that leverages each LLM's expertise, given an input prompt. PackLLM performs model fusion by solving an optimization problem for determining each LLM's importance, so that perplexity over the input prompt is minimized. First, our simple PackLLM-sim variant validates that perplexity is a good indicator for measuring each LLM's expertise. Second, our PackLLM-opt variant approximately solves the perplexity minimization problem via a greedy algorithm. The derived importance weights are used to combine the LLMs during inference. We conduct experiments with over 100 total LLMs on a diverse set of tasks. Experimental results show that (i) perplexity is a reliable measure for LLM fusion, (ii) PackLLM outperforms test-time fusion baselines by 1.89% accuracy points, and (iii) PackLLM can leverage new LLMs to improve performance over learning-based fusion approaches by 3.92-11.94% accuracy points.</li>
<li><strong>摘要：</strong>融合来自多个大型语言模型 (LLM) 的知识可以结合它们的不同优势，以提高给定任务的性能。然而，当前的融合方法要么依赖于基于学习的融合器，而这些融合器不能推广到新的法学硕士，要么没有考虑每个法学硕士对输入的理解程度。在这项工作中，我们在测试时研究 LLM 融合，这使得能够在推理过程中利用来自任意用户指定的 LLM 的知识。我们引入了 LLM 包 (PackLLM)，这是一种在给出输入提示的情况下利用每个 LLM 专业知识的测试时融合的有效方法。 PackLLM 通过解决确定每个 LLM 重要性的优化问题来执行模型融合，从而最大限度地减少输入提示的困惑。首先，我们简单的 PackLLM-sim 变体验证了困惑度是衡量每个 LLM 专业知识的良好指标。其次，我们的 PackLLM-opt 变体通过贪婪算法近似解决了困惑度最小化问题。导出的重要性权重用于在推理过程中组合 LLM。我们与 100 多名法学硕士就不同的任务进行了实验。实验结果表明，(i) 困惑度是 LLM 融合的可靠衡量标准，(ii) PackLLM 比测试时融合基线高出 1.89% 的准确度点，(iii) PackLLM 可以利用新的 LLM 来提高基于学习的融合方法的性能准确度提高 3.92-11.94%。</li>
</ul>

<h3>Title: Select and Reorder: A Novel Approach for Neural Sign Language Production</h3>
<ul>
<li><strong>Authors: </strong>Harry Walsh, Ben Saunders, Richard Bowden</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11532">https://arxiv.org/abs/2404.11532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11532">https://arxiv.org/pdf/2404.11532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11532]] Select and Reorder: A Novel Approach for Neural Sign Language Production(https://arxiv.org/abs/2404.11532)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.</li>
<li><strong>摘要：</strong>手语通常被归类为低资源语言，由于缺乏并行注释数据集，在实现准确翻译方面面临着重大挑战。本文介绍了选择和重新排序 (S&R)，这是一种通过将翻译过程分解为两个不同步骤来解决数据稀缺问题的新颖方法：注释选择 (GS) 和注释重新排序 (GR)。我们的方法利用大型口语模型以及源口语和目标手语之间的大量词汇重叠来建立初始对齐。这两个步骤都利用非自回归 (NAR) 解码来减少计算量并提高推理速度。通过这种任务的解开，我们在 Meine DGS Annotated (mDGS) 数据集上获得了最先进的 BLEU 和 Rouge 分数，证明 BLUE-1 在文本到光泽 (T2G) 翻译方面显着提高了 37.88%。这种创新方法为更有效的手语翻译模型铺平了道路，即使在资源有限的环境中也是如此。</li>
</ul>

<h3>Title: Evaluating Span Extraction in Generative Paradigm: A Reflection on  Aspect-Based Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Soyoung Yang, Won Ik Cho</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11539">https://arxiv.org/abs/2404.11539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11539">https://arxiv.org/pdf/2404.11539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11539]] Evaluating Span Extraction in Generative Paradigm: A Reflection on  Aspect-Based Sentiment Analysis(https://arxiv.org/abs/2404.11539)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the era of rapid evolution of generative language models within the realm of natural language processing, there is an imperative call to revisit and reformulate evaluation methodologies, especially in the domain of aspect-based sentiment analysis (ABSA). This paper addresses the emerging challenges introduced by the generative paradigm, which has moderately blurred traditional boundaries between understanding and generation tasks. Building upon prevailing practices in the field, we analyze the advantages and shortcomings associated with the prevalent ABSA evaluation paradigms. Through an in-depth examination, supplemented by illustrative examples, we highlight the intricacies involved in aligning generative outputs with other evaluative metrics, specifically those derived from other tasks, including question answering. While we steer clear of advocating for a singular and definitive metric, our contribution lies in paving the path for a comprehensive guideline tailored for ABSA evaluations in this generative paradigm. In this position paper, we aim to provide practitioners with profound reflections, offering insights and directions that can aid in navigating this evolving landscape, ensuring evaluations that are both accurate and reflective of generative capabilities.</li>
<li><strong>摘要：</strong>在自然语言处理领域生成语言模型快速发展的时代，迫切需要重新审视和重新制定评估方法，特别是在基于方面的情感分析（ABSA）领域。本文解决了生成范式带来的新挑战，该范式在一定程度上模糊了理解和生成任务之间的传统界限。基于该领域的主流实践，我们分析了与流行的 ABSA 评估范式相关的优点和缺点。通过深入的检查，并辅以说明性示例，我们强调了将生成性输出与其他评估指标（特别是从其他任务（包括回答问题）得出的指标）结合起来所涉及的复杂性。虽然我们避免提倡单一且明确的指标，但我们的贡献在于为这种生成范式中针对 ABSA 评估量身定制的综合指南铺平了道路。在这份立场文件中，我们的目标是为从业者提供深刻的反思，提供有助于驾驭这一不断变化的格局的见解和方向，确保评估既准确又反映生成能力。</li>
</ul>

<h3>Title: Quantifying Multilingual Performance of Large Language Models Across  Languages</h3>
<ul>
<li><strong>Authors: </strong>Zihao Li, Yucheng Shi, Zirui Liu, Fan Yang, Ninghao Liu, Mengnan Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.11553">https://arxiv.org/abs/2404.11553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.11553">https://arxiv.org/pdf/2404.11553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.11553]] Quantifying Multilingual Performance of Large Language Models Across  Languages(https://arxiv.org/abs/2404.11553)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The training process of Large Language Models (LLMs) requires extensive text corpus. However, these data are often unevenly distributed in different languages. As a result, LLMs perform well on common languages, such as English, German, and French, but perform poorly on low-resource languages. However, currently there is no work to quantitatively measure the performance of LLMs in low-resource languages. To fill this gap, we proposed the Language Ranker that aims to benchmark and rank different languages according to the performance of LLMs on those languages. We employ the LLM's performance on the English corpus as a baseline to compare the performances of different languages and English. We have the following three findings: 1. The performance rankings of different LLMs in all languages are roughly the same. 2. LLMs with different sizes have the same partial order of performance. 3. There is a strong correlation between LlaMa2's performance in different languages and the proportion of the pre-training corpus. These findings illustrate that the Language Ranker can be used as an indicator to measure the language performance of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的训练过程需要大量的文本语料库。然而，这些数据在不同语言中的分布往往不均匀。因此，法学硕士在英语、德语和法语等常用语言上表现良好，但在资源匮乏的语言上表现不佳。然而，目前还没有定量衡量低资源语言法学硕士表现的工作。为了填补这一空白，我们提出了语言排名器，旨在根据法学硕士在这些语言上的表现对不同语言进行基准测试和排名。我们以LLM在英语语料库上的表现作为基准来比较不同语言和英语的表现。我们有以下三个发现： 1. 不同语言的LLM的表现排名大致相同。 2. 不同规模的法学硕士具有相同的偏序表现。 3. LlaMa2在不同语言中的表现与预训练语料的比例有很强的相关性。这些发现表明，语言排名可以作为衡量法学硕士语言表现的指标。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
