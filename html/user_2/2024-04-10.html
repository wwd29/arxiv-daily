<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-10</h1>
<h3>Title: SambaLingo: Teaching Large Language Models New Languages</h3>
<ul>
<li><strong>Authors: </strong>Zoltan Csaki, Bo Li, Jonathan Li, Qiantong Xu, Pian Pawakapan, Leon Zhang, Yun Du, Hengyu Zhao, Changran Hu, Urmish Thakker</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05829">https://arxiv.org/abs/2404.05829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05829">https://arxiv.org/pdf/2404.05829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05829]] SambaLingo: Teaching Large Language Models New Languages(https://arxiv.org/abs/2404.05829)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the widespread availability of LLMs, there remains a substantial gap in their capabilities and availability across diverse languages. One approach to address these issues has been to take an existing pre-trained LLM and continue to train it on new languages. While prior works have experimented with language adaptation, many questions around best practices and methodology have not been covered. In this paper, we present a comprehensive investigation into the adaptation of LLMs to new languages. Our study covers the key components in this process, including vocabulary extension, direct preference optimization and the data scarcity problem for human alignment in low-resource languages. We scale these experiments across 9 languages and 2 parameter scales (7B and 70B). We compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing language experts, outperforming all prior published baselines. Additionally, all evaluation code and checkpoints are made public to facilitate future research.</li>
<li><strong>摘要：</strong>尽管法学硕士广泛存在，但其跨不同语言的能力和可用性仍然存在很大差距。解决这些问题的一种方法是采用现有的预训练法学硕士并继续对其进行新语言的培训。虽然之前的工作已经尝试了语言适应，但尚未涵盖有关最佳实践和方法的许多问题。在本文中，我们对法学硕士对新语言的适应进行了全面的调查。我们的研究涵盖了这个过程中的关键组成部分，包括词汇扩展、直接偏好优化以及低资源语言中人类对齐的数据稀缺问题。我们将这些实验扩展到 9 种语言和 2 个参数尺度（7B 和 70B）。我们将我们的模型与 Llama 2、Aya-101、XGLM、BLOOM 和现有语言专家进行比较，优于所有先前发布的基线。此外，所有评估代码和检查点都是公开的，以方便未来的研究。</li>
</ul>

<h3>Title: GeniL: A Multilingual Dataset on Generalizing Language</h3>
<ul>
<li><strong>Authors: </strong>Aida Mostafazadeh Davani, Sagar Gubbi, Sunipa Dev, Shachi Dave, Vinodkumar Prabhakaran</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05866">https://arxiv.org/abs/2404.05866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05866">https://arxiv.org/pdf/2404.05866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05866]] GeniL: A Multilingual Dataset on Generalizing Language(https://arxiv.org/abs/2404.05866)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>LLMs are increasingly transforming our digital ecosystem, but they often inherit societal biases learned from their training data, for instance stereotypes associating certain attributes with specific identity groups. While whether and how these biases are mitigated may depend on the specific use cases, being able to effectively detect instances of stereotype perpetuation is a crucial first step. Current methods to assess presence of stereotypes in generated language rely on simple template or co-occurrence based measures, without accounting for the variety of sentential contexts they manifest in. We argue that understanding the sentential context is crucial for detecting instances of generalization. We distinguish two types of generalizations: (1) language that merely mentions the presence of a generalization ("people think the French are very rude"), and (2) language that reinforces such a generalization ("as French they must be rude"), from non-generalizing context ("My French friends think I am rude"). For meaningful stereotype evaluations, we need to reliably distinguish such instances of generalizations. We introduce the new task of detecting generalization in language, and build GeniL, a multilingual dataset of over 50K sentences from 9 languages (English, Arabic, Bengali, Spanish, French, Hindi, Indonesian, Malay, and Portuguese) annotated for instances of generalizations. We demonstrate that the likelihood of a co-occurrence being an instance of generalization is usually low, and varies across different languages, identity groups, and attributes. We build classifiers to detect generalization in language with an overall PR-AUC of 58.7, with varying degrees of performance across languages. Our research provides data and tools to enable a nuanced understanding of stereotype perpetuation, a crucial step towards more inclusive and responsible language technologies.</li>
<li><strong>摘要：</strong>法学硕士正在日益改变我们的数字生态系统，但他们经常继承从培训数据中学到的社会偏见，例如将某些属性与特定身份群体相关联的刻板印象。虽然是否以及如何减轻这些偏见可能取决于具体的用例，但能够有效地检测刻板印象延续的实例是至关重要的第一步。当前评估生成语言中刻板印象的存在的方法依赖于简单的模板或基于共现的测量，没有考虑它们所体现的各种句子上下文。我们认为理解句子上下文对于检测泛化实例至关重要。我们区分两种类型的概括：（1）仅提及概括存在的语言（“人们认为法国人非常粗鲁”），以及（2）强化这种概括的语言（“作为法国人，他们一定很粗鲁” ），来自非概括性背景（“我的法国朋友认为我很粗鲁”）。为了进行有意义的刻板印象评估，我们需要可靠地区分此类概括实例。我们引入了检测语言泛化的新任务，并构建了 GeniL，这是一个包含来自 9 种语言（英语、阿拉伯语、孟加拉语、西班牙语、法语、印地语、印度尼西亚语、马来语和葡萄牙语）的超过 50K 个句子的多语言数据集，并为泛化实例进行了注释。我们证明，同现作为泛化实例的可能性通常很低，并且在不同的语言、身份组和属性之间存在差异。我们构建分类器来检测语言的泛化，总体 PR-AUC 为 58.7，不同语言的性能程度不同。我们的研究提供了数据和工具，使人们能够对刻板印象的延续有细致入微的理解，这是迈向更具包容性和负责任的语言技术的关键一步。</li>
</ul>

<h3>Title: CodecLM: Aligning Language Models with Tailored Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Zifeng Wang, Chun-Liang Li, Vincent Perot, Long T. Le, Jin Miao, Zizhao Zhang, Chen-Yu Lee, Tomas Pfister</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05875">https://arxiv.org/abs/2404.05875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05875">https://arxiv.org/pdf/2404.05875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05875]] CodecLM: Aligning Language Models with Tailored Synthetic Data(https://arxiv.org/abs/2404.05875)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next-token prediction objective and users' actual goals. To reduce the labor and time cost to collect or annotate data by humans, researchers start to explore the use of LLMs to generate instruction-aligned synthetic data. Recent works focus on generating diverse instructions and applying LLM to increase instruction complexity, often neglecting downstream use cases. It remains unclear how to tailor high-quality data to elicit better instruction-following abilities in different target instruction distributions and LLMs. To this end, we introduce CodecLM, a general framework for adaptively generating high-quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs. Drawing on the Encode-Decode principles, we use LLMs as codecs to guide the data generation process. We first encode seed instructions into metadata, which are concise keywords generated on-the-fly to capture the target instruction distribution, and then decode metadata to create tailored instructions. We also introduce Self-Rubrics and Contrastive Filtering during decoding to tailor data-efficient samples. Extensive experiments on four open-domain instruction following benchmarks validate the effectiveness of CodecLM over the current state-of-the-arts.</li>
<li><strong>摘要：</strong>指令调优已成为将大型语言模型 (LLM) 与特定任务指令对齐的关键，从而减少下一个令牌预测目标与用户实际目标之间的差异。为了减少人类收集或注释数据的劳动力和时间成本，研究人员开始探索使用法学硕士来生成指令对齐的合成数据。最近的工作重点是生成多样化的指令并应用 LLM 来增加指令复杂性，通常忽略下游用例。目前尚不清楚如何定制高质量数据，以在不同的目标指令分布和法学硕士中获得更好的指令跟踪能力。为此，我们引入了 CodecLM，这是一个通用框架，用于自适应生成高质量合成数据，用于与不同下游指令分布和 LLM 进行 LLM 对齐。借鉴编码-解码原则，我们使用 LLM 作为编解码器来指导数据生成过程。我们首先将种子指令编码为元数据，元数据是即时生成的简洁关键字，用于捕获目标指令分布，然后解码元数据以创建定制指令。我们还在解码过程中引入了自量规和对比过滤，以定制数据高效的样本。根据基准对四个开放域指令进行的广泛实验验证了 CodecLM 相对于当前最先进技术的有效性。</li>
</ul>

<h3>Title: Eraser: Jailbreaking Defense in Large Language Models via Unlearning  Harmful Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen, Huiping Zhuang, Cen Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05880">https://arxiv.org/abs/2404.05880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05880">https://arxiv.org/pdf/2404.05880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05880]] Eraser: Jailbreaking Defense in Large Language Models via Unlearning  Harmful Knowledge(https://arxiv.org/abs/2404.05880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the safeguard and generate harmful content. Existing jailbreaking defense methods have failed to address the fundamental issue that harmful knowledge resides within the model, leading to potential jailbreak risks for LLMs. In this paper, we propose a novel defense method called Eraser, which mainly includes three goals: unlearning harmful knowledge, retaining general knowledge, and maintaining safety alignment. The intuition is that if an LLM forgets the specific knowledge required to answer a harmful question, it will no longer have the ability to answer harmful questions. The training of Erase does not actually require the model's own harmful knowledge, and it can benefit from unlearning general answers related to harmful queries, which means it does not need assistance from the red team. The experimental results show that Eraser can significantly reduce the jailbreaking success rate for various attacks without compromising the general capabilities of the model.</li>
<li><strong>摘要：</strong>越狱攻击可以使大型语言模型 (LLM) 绕过防护措施并生成有害内容。现有的越狱防御方法未能解决模型中存在有害知识的根本问题，导致法学硕士存在潜在的越狱风险。在本文中，我们提出了一种名为“橡皮擦”的新型防御方法，该方法主要包括三个目标：忘却有害知识、保留一般知识和保持安全一致性。直觉是，如果法学硕士忘记了回答有害问题所需的具体知识，它将不再有能力回答有害问题。 Erase 的训练实际上并不需要模型自身的有害知识，它可以受益于忘却与有害查询相关的一般答案，这意味着它不需要红队的帮助。实验结果表明，Eraser 可以在不损害模型通用能力的情况下，显着降低各种攻击的越狱成功率。</li>
</ul>

<h3>Title: Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence</h3>
<ul>
<li><strong>Authors: </strong>Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, Kranthi Kiran GV, Jan Kocoń, Bartłomiej Koptyra, Satyapriya Krishna, Ronald McClelland Jr., Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanisław Woźniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05892">https://arxiv.org/abs/2404.05892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05892">https://arxiv.org/pdf/2404.05892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05892]] Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence(https://arxiv.org/abs/2404.05892)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer</li>
<li><strong>摘要：</strong>我们提出了 Eagle (RWKV-5) 和 Finch (RWKV-6)，这是在 RWKV (RWKV-4) 架构上改进的序列模型。我们的架构设计进步包括多头矩阵值状态和动态递归机制，可提高表达能力，同时保持 RNN 的推理效率特征。我们引入了一个包含 1.12 万亿个标记的新多语言语料库和一个基于贪婪匹配的快速标记器，以增强多语言能力。我们训练了四个 Eagle 模型，参数范围为 0.46 到 75 亿个参数，以及两个 Finch 模型，参数范围为 1.6 到 31 亿个参数，发现它们在各种基准测试中都实现了具有竞争力的性能。我们在 Apache 2.0 许可证下在 HuggingFace 上发布了所有模型。模型位于：https://huggingface.co/RWKV 训练代码位于：https://github.com/RWKV/RWKV-LM 推理代码位于：https://github.com/RWKV/ChatRWKV 时间并行训练代码位于：https://github.com/RWKV/RWKV-infctx-trainer</li>
</ul>

<h3>Title: WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Michael Lutz, Arth Bohra, Manvel Saroyan, Artem Harutyunyan, Giovanni Campagna</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05902">https://arxiv.org/abs/2404.05902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05902">https://arxiv.org/pdf/2404.05902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05902]] WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents(https://arxiv.org/abs/2404.05902)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>In the realm of web agent research, achieving both generalization and accuracy remains a challenging problem. Due to high variance in website structure, existing approaches often fail. Moreover, existing fine-tuning and in-context learning techniques fail to generalize across multiple websites. We introduce Wilbur, an approach that uses a differentiable ranking model and a novel instruction synthesis technique to optimally populate a black-box large language model's prompt with task demonstrations from previous runs. To maximize end-to-end success rates, we also propose an intelligent backtracking mechanism that learns and recovers from its mistakes. Finally, we show that our ranking model can be trained on data from a generative auto-curriculum which samples representative goals from an LLM, runs the agent, and automatically evaluates it, with no manual annotation. Wilbur achieves state-of-the-art results on the WebVoyager benchmark, beating text-only models by 8% overall, and up to 36% on certain websites. On the same benchmark, Wilbur is within 5% of a strong multi-modal model despite only receiving textual inputs, and further analysis reveals a substantial number of failures are due to engineering challenges of operating the web.</li>
<li><strong>摘要：</strong>在网络代理研究领域，同时实现泛化和准确性仍然是一个具有挑战性的问题。由于网站结构差异很大，现有方法常常失败。此外，现有的微调和上下文学习技术无法在多个网站上推广。我们介绍 Wilbur，这是一种使用可微分排名模型和新颖的指令合成技术的方法，可以通过之前运行的任务演示来最佳地填充黑盒大型语言模型的提示。为了最大限度地提高端到端的成功率，我们还提出了一种智能回溯机制，可以学习错误并从错误中恢复。最后，我们展示了我们的排名模型可以根据生成自动课程的数据进行训练，该课程从法学硕士中采样代表性目标，运行代理并自动评估它，无需手动注释。 Wilbur 在 WebVoyager 基准测试中取得了最先进的结果，整体性能比纯文本模型高出 8%，在某些网站上高出 36%。在同一基准上，尽管仅接收文本输入，Wilbur 仍与强大的多模式模型相差不到 5%，进一步分析表明，大量故障是由于操作网络的工程挑战造成的。</li>
</ul>

<h3>Title: The Hallucinations Leaderboard -- An Open Effort to Measure  Hallucinations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Giwon Hong, Aryo Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie, Yu Zhao, Laura Perez-Beltrachini, Max Ryabinin, Xuanli He, Pasquale Minervini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05904">https://arxiv.org/abs/2404.05904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05904">https://arxiv.org/pdf/2404.05904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05904]] The Hallucinations Leaderboard -- An Open Effort to Measure  Hallucinations in Large Language Models(https://arxiv.org/abs/2404.05904)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have transformed the Natural Language Processing (NLP) landscape with their remarkable ability to understand and generate human-like text. However, these models are prone to ``hallucinations'' -- outputs that do not align with factual reality or the input context. This paper introduces the Hallucinations Leaderboard, an open initiative to quantitatively measure and compare the tendency of each model to produce hallucinations. The leaderboard uses a comprehensive set of benchmarks focusing on different aspects of hallucinations, such as factuality and faithfulness, across various tasks, including question-answering, summarisation, and reading comprehension. Our analysis provides insights into the performance of different models, guiding researchers and practitioners in choosing the most reliable models for their applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 以其卓越的理解和生成类人文本的能力改变了自然语言处理 (NLP) 的格局。然而，这些模型很容易出现“幻觉”——输出与事实现实或输入上下文不符。本文介绍了幻觉排行榜，这是一项开放倡议，用于定量测量和比较每个模型产生幻觉的趋势。该排行榜使用了一套全面的基准，重点关注幻觉的不同方面，例如事实性和忠实性，以及各种任务，包括回答问题、总结和阅读理解。我们的分析提供了对不同模型性能的见解，指导研究人员和从业者为其应用选择最可靠的模型。</li>
</ul>

<h3>Title: VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page  Understanding and Grounding?</h3>
<ul>
<li><strong>Authors: </strong>Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, Xiang Yue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05955">https://arxiv.org/abs/2404.05955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05955">https://arxiv.org/pdf/2404.05955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05955]] VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page  Understanding and Grounding?(https://arxiv.org/abs/2404.05955)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language models (MLLMs) have shown promise in web-related tasks, but evaluating their performance in the web domain remains a challenge due to the lack of comprehensive benchmarks. Existing benchmarks are either designed for general multimodal tasks, failing to capture the unique characteristics of web pages, or focus on end-to-end web agent tasks, unable to measure fine-grained abilities such as OCR, understanding, and grounding. In this paper, we introduce \bench{}, a multimodal benchmark designed to assess the capabilities of MLLMs across a variety of web tasks. \bench{} consists of seven tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3 series, and GPT-4V(ision) on \bench{}, revealing significant challenges and performance gaps. Further analysis highlights the limitations of current MLLMs, including inadequate grounding in text-rich environments and subpar performance with low-resolution image inputs. We believe \bench{} will serve as a valuable resource for the research community and contribute to the creation of more powerful and versatile MLLMs for web-related applications.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLLM) 在 Web 相关任务中显示出了前景，但由于缺乏全面的基准，评估其在 Web 领域的性能仍然是一个挑战。现有的基准要么是针对一般的多模态任务而设计的，无法捕捉网页的独特特征，要么专注于端到端的Web代理任务，无法衡量OCR、理解和接地等细粒度的能力。在本文中，我们介绍了 \bench{}，这是一个多模式基准测试，旨在评估 MLLM 在各种 Web 任务中的能力。 \bench{} 包含 7 个任务，包含来自 139 个真实网站的 1.5K 个人工管理实例，涵盖 87 个子域。我们在 \bench{} 上评估了 14 个开源 MLLM、Gemini Pro、Claude-3 系列和 GPT-4V(ision)，揭示了重大挑战和性能差距。进一步的分析凸显了当前 MLLM 的局限性，包括在文本丰富的环境中接地不足以及低分辨率图像输入的性能不佳。我们相信 \bench{} 将成为研究社区的宝贵资源，并有助于为网络相关应用创建更强大、更通用的 MLLM。</li>
</ul>

<h3>Title: LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</h3>
<ul>
<li><strong>Authors: </strong>Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, Siva Reddy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05961">https://arxiv.org/abs/2404.05961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05961">https://arxiv.org/pdf/2404.05961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05961]] LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders(https://arxiv.org/abs/2404.05961)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data. Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.</li>
<li><strong>摘要：</strong>大型纯解码器语言模型 (LLM) 是当今大多数 NLP 任务和基准测试中最先进的模型。然而，社区只是慢慢地将这些模型用于文本嵌入任务，这需要丰富的上下文表示。在这项工作中，我们引入了 LLM2Vec，这是一种简单的无监督方法，可以将任何仅解码器的 LLM 转换为强大的文本编码器。 LLM2Vec 包含三个简单步骤：1）启用双向注意力，2）屏蔽下一个标记预测，3）无监督对比学习。我们通过将 LLM2Vec 应用于从 1.3B 到 7B 参数的 3 个流行的 LLM 来证明 LLM2Vec 的有效性，并评估英语单词和序列级任务的转换模型。我们在字级任务上远远优于仅编码器模型，并在大规模文本嵌入基准（MTEB）上达到了新的无监督最先进性能。此外，当将 LLM2Vec 与监督对比学习相结合时，我们在仅使用公开数据进行训练的模型中在 MTEB 上实现了最先进的性能。我们强有力的实证结果和广泛的分析表明，LLM 可以以参数有效的方式有效地转换为通用文本编码器，而不需要昂贵的适应或合成 GPT-4 生成的数据。</li>
</ul>

<h3>Title: THOUGHTSCULPT: Reasoning with Intermediate Revision and Search</h3>
<ul>
<li><strong>Authors: </strong>Yizhou Chi, Kevin Yang, Dan Klein</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05966">https://arxiv.org/abs/2404.05966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05966">https://arxiv.org/pdf/2404.05966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05966]] THOUGHTSCULPT: Reasoning with Intermediate Revision and Search(https://arxiv.org/abs/2404.05966)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We present THOUGHTSCULPT, a general reasoning and search method for tasks with outputs that can be decomposed into components. THOUGHTSCULPT explores a search tree of potential solutions using Monte Carlo Tree Search (MCTS), building solutions one action at a time and evaluating according to any domain-specific heuristic, which in practice is often simply an LLM evaluator. Critically, our action space includes revision actions: THOUGHTSCULPT may choose to revise part of its previous output rather than continuing to build the rest of its output. Empirically, THOUGHTSCULPT outperforms state-of-the-art reasoning methods across three challenging tasks: Story Outline Improvement (up to +30% interestingness), Mini-Crosswords Solving (up to +16% word success rate), and Constrained Generation (up to +10% concept coverage).</li>
<li><strong>摘要：</strong>我们提出了 THOUGHTSCULPT，这是一种通用推理和搜索方法，适用于具有可分解为组件的输出的任务。 THOUGHTSCULPT 使用蒙特卡罗树搜索 (MCTS) 探索潜在解决方案的搜索树，一次构建解决方案并根据任何特定领域的启发式进行评估，这在实践中通常只是一个 LLM 评估器。至关重要的是，我们的行动空间包括修订行动：THOUGHTSCULPT 可能会选择修订其先前输出的部分内容，而不是继续构建其输出的其余部分。根据经验，THOUGHTSCULPT 在三个具有挑战性的任务中优于最先进的推理方法：故事大纲改进（高达 +30% 的趣味性）、迷你填字游戏（高达 +16% 的单词成功率）和约束生成（高达 +16% 的单词成功率）到 +10% 概念覆盖率）。</li>
</ul>

<h3>Title: Optimization Methods for Personalizing Large Language Models through  Retrieval Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Alireza Salemi, Surya Kallumadi, Hamed Zamani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05970">https://arxiv.org/abs/2404.05970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05970">https://arxiv.org/pdf/2404.05970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05970]] Optimization Methods for Personalizing Large Language Models through  Retrieval Augmentation(https://arxiv.org/abs/2404.05970)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper studies retrieval-augmented approaches for personalizing large language models (LLMs), which potentially have a substantial impact on various applications and domains. We propose the first attempt to optimize the retrieval models that deliver a limited number of personal documents to large language models for the purpose of personalized generation. We develop two optimization algorithms that solicit feedback from the downstream personalized generation tasks for retrieval optimization--one based on reinforcement learning whose reward function is defined using any arbitrary metric for personalized generation and another based on knowledge distillation from the downstream LLM to the retrieval model. This paper also introduces a pre- and post-generation retriever selection model that decides what retriever to choose for each LLM input. Extensive experiments on diverse tasks from the language model personalization (LaMP) benchmark reveal statistically significant improvements in six out of seven datasets.</li>
<li><strong>摘要：</strong>本文研究了用于个性化大型语言模型（LLM）的检索增强方法，这可能对各种应用程序和领域产生重大影响。我们提出了首次尝试优化检索模型，将有限数量的个人文档传递给大型语言模型，以实现个性化生成。我们开发了两种优化算法，从下游个性化生成任务中征求反馈以进行检索优化——一种基于强化学习，其奖励函数是使用任意个性化生成指标定义的，另一种基于从下游 LLM 到检索模型的知识蒸馏。本文还介绍了一个生成前和生成后检索器选择模型，该模型决定为每个 LLM 输入选择哪个检索器。对语言模型个性化 (LaMP) 基准的各种任务进行的广泛实验表明，七个数据集中有六个在统计上有显着的改进。</li>
</ul>

<h3>Title: Event-enhanced Retrieval in Real-time Search</h3>
<ul>
<li><strong>Authors: </strong>Yanan Zhang, Xiaoling Bai, Tianhua Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05989">https://arxiv.org/abs/2404.05989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05989">https://arxiv.org/pdf/2404.05989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05989]] Event-enhanced Retrieval in Real-time Search(https://arxiv.org/abs/2404.05989)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>The embedding-based retrieval (EBR) approach is widely used in mainstream search engine retrieval systems and is crucial in recent retrieval-augmented methods for eliminating LLM illusions. However, existing EBR models often face the "semantic drift" problem and insufficient focus on key information, leading to a low adoption rate of retrieval results in subsequent steps. This issue is especially noticeable in real-time search scenarios, where the various expressions of popular events on the Internet make real-time retrieval heavily reliant on crucial event information. To tackle this problem, this paper proposes a novel approach called EER, which enhances real-time retrieval performance by improving the dual-encoder model of traditional EBR. We incorporate contrastive learning to accompany pairwise learning for encoder optimization. Furthermore, to strengthen the focus on critical event information in events, we include a decoder module after the document encoder, introduce a generative event triplet extraction scheme based on prompt-tuning, and correlate the events with query encoder optimization through comparative learning. This decoder module can be removed during inference. Extensive experiments demonstrate that EER can significantly improve the real-time search retrieval performance. We believe that this approach will provide new perspectives in the field of information retrieval. The codes and dataset are available at https://github.com/open-event-hub/Event-enhanced_Retrieval .</li>
<li><strong>摘要：</strong>基于嵌入的检索（EBR）方法广泛应用于主流搜索引擎检索系统中，并且在最近消除法学硕士错觉的检索增强方法中至关重要。然而，现有的EBR模型往往面临“语义漂移”问题以及对关键信息关注不够，导致后续步骤检索结果的采用率较低。这个问题在实时搜索场景中尤为明显，互联网上热门事件的多种表达方式使得实时检索严重依赖于关键事件信息。为了解决这个问题，本文提出了一种名为 EER 的新方法，通过改进传统 EBR 的双编码器模型来增强实时检索性能。我们将对比学习与成对学习结合起来以实现编码器优化。此外，为了加强对事件中关键事件信息的关注，我们在文档编码器之后添加了解码器模块，引入了基于提示调整的生成事件三元组提取方案，并通过比较学习将事件与查询编码器优化相关联。该解码器模块可以在推理过程中删除。大量实验表明，EER 可以显着提高实时搜索检索性能。我们相信这种方法将为信息检索领域提供新的视角。代码和数据集可在 https://github.com/open-event-hub/Event-enhanced_Retrieval 获取。</li>
</ul>

<h3>Title: Privacy Preserving Prompt Engineering: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Kennedy Edemacu, Xintao Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06001">https://arxiv.org/abs/2404.06001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06001">https://arxiv.org/pdf/2404.06001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06001]] Privacy Preserving Prompt Engineering: A Survey(https://arxiv.org/abs/2404.06001)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (PLMs) have demonstrated significant proficiency in solving a wide range of general natural language processing (NLP) tasks. Researchers have observed a direct correlation between the performance of these models and their sizes. As a result, the sizes of these models have notably expanded in recent years, persuading researchers to adopt the term large language models (LLMs) to characterize the larger-sized PLMs. The increased size is accompanied by a distinct capability known as in-context learning (ICL), which represents a specialized form of prompting. This enables the utilization of LLMs for specific downstream tasks by presenting them with demonstration examples while keeping the model parameters frozen. Although interesting, privacy concerns have become a major obstacle in its widespread usage. Multiple studies have examined the privacy risks linked to ICL and prompting in general, and have devised techniques to alleviate these risks. Thus, there is a necessity to organize these mitigation techniques for the benefit of the community. This survey provides a systematic overview of the privacy protection methods employed during ICL and prompting in general. We review, analyze, and compare different methods under this paradigm. Furthermore, we provide a summary of the resources accessible for the development of these frameworks. Finally, we discuss the limitations of these frameworks and offer a detailed examination of the promising areas that necessitate further exploration.</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 在解决各种通用自然语言处理 (NLP) 任务方面表现出出色的能力。研究人员观察到这些模型的性能与其规模之间存在直接相关性。因此，近年来这些模型的规模显着扩大，促使研究人员采用大型语言模型 (LLM) 一词来描述较大规模的 PLM。规模的增加伴随着一种被称为情境学习（ICL）的独特能力，它代表了一种特殊的提示形式。通过向法学硕士提供演示示例，同时保持模型参数冻结，可以将法学硕士用于特定的下游任务。尽管很有趣，但隐私问题已成为其广泛使用的主要障碍。多项研究检查了与 ICL 和一般提示相关的隐私风险，并设计了减轻这些风险的技术。因此，有必要为了社区的利益而组织这些缓解技术。这项调查系统地概述了 ICL 期间采用的隐私保护方法和一般提示。我们在这个范式下回顾、分析和比较不同的方法。此外，我们还提供了可用于开发这些框架的资源的摘要。最后，我们讨论这些框架的局限性，并对需要进一步探索的有前景的领域进行详细检查。</li>
</ul>

<h3>Title: FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation  of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Zhengran Zeng, Wei Ye, Jindong Wang, Yue Zhang, Shikun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06003">https://arxiv.org/abs/2404.06003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06003">https://arxiv.org/pdf/2404.06003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06003]] FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation  of Large Language Models(https://arxiv.org/abs/2404.06003)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid development of large language model (LLM) evaluation methodologies and datasets has led to a profound challenge: integrating state-of-the-art evaluation techniques cost-effectively while ensuring reliability, reproducibility, and efficiency. Currently, there is a notable absence of a unified and adaptable framework that seamlessly integrates various evaluation approaches. Moreover, the reliability of evaluation findings is often questionable due to potential data contamination, with the evaluation efficiency commonly overlooked when facing the substantial costs associated with LLM inference. In response to these challenges, we introduce FreeEval, a modular and scalable framework crafted to enable trustworthy and efficient automatic evaluations of LLMs. Firstly, FreeEval's unified abstractions simplify the integration and improve the transparency of diverse evaluation methodologies, encompassing dynamic evaluation that demand sophisticated LLM interactions. Secondly, the framework integrates meta-evaluation techniques like human evaluation and data contamination detection, which, along with dynamic evaluation modules in the platform, enhance the fairness of the evaluation outcomes. Lastly, FreeEval is designed with a high-performance infrastructure, including distributed computation and caching strategies, enabling extensive evaluations across multi-node, multi-GPU clusters for open-source and proprietary LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）评估方法和数据集的快速发展带来了严峻的挑战：经济高效地集成最先进的评估技术，同时确保可靠性、可重复性和效率。目前，明显缺乏一个统一且适应性强的框架来无缝集成各种评估方法。此外，由于潜在的数据污染，评估结果的可靠性常常受到质疑，而在面临与法学硕士推理相关的巨额成本时，评估效率通常被忽视。为了应对这些挑战，我们推出了 FreeEval，这是一个模块化且可扩展的框架，旨在实现法学硕士的可信且高效的自动评估。首先，FreeEval 的统一抽象简化了集成并提高了不同评估方法的透明度，包括需要复杂的 LLM 交互的动态评估。其次，该框架集成了人工评估、数据污染检测等元评估技术，与平台中的动态评估模块一起增强了评估结果的公平性。最后，FreeEval 采用高性能基础设施设计，包括分布式计算和缓存策略，支持跨多节点、多 GPU 集群对开源和专有法学硕士进行广泛评估。</li>
</ul>

<h3>Title: Identifying Shopping Intent in Product QA for Proactive Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Besnik Fetahu, Nachshon Cohen, Elad Haramaty, Liane Lewin-Eytan, Oleg Rokhlenko, Shervin Malmasi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06017">https://arxiv.org/abs/2404.06017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06017">https://arxiv.org/pdf/2404.06017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06017]] Identifying Shopping Intent in Product QA for Proactive Recommendations(https://arxiv.org/abs/2404.06017)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Voice assistants have become ubiquitous in smart devices allowing users to instantly access information via voice questions. While extensive research has been conducted in question answering for voice search, little attention has been paid on how to enable proactive recommendations from a voice assistant to its users. This is a highly challenging problem that often leads to user friction, mainly due to recommendations provided to the users at the wrong time. We focus on the domain of e-commerce, namely in identifying Shopping Product Questions (SPQs), where the user asking a product-related question may have an underlying shopping need. Identifying a user's shopping need allows voice assistants to enhance shopping experience by determining when to provide recommendations, such as product or deal recommendations, or proactive shopping actions recommendation. Identifying SPQs is a challenging problem and cannot be done from question text alone, and thus requires to infer latent user behavior patterns inferred from user's past shopping history. We propose features that capture the user's latent shopping behavior from their purchase history, and combine them using a novel Mixture-of-Experts (MoE) model. Our evaluation shows that the proposed approach is able to identify SPQs with a high score of F1=0.91. Furthermore, based on an online evaluation with real voice assistant users, we identify SPQs in real-time and recommend shopping actions to users to add the queried product into their shopping list. We demonstrate that we are able to accurately identify SPQs, as indicated by the significantly higher rate of added products to users' shopping lists when being prompted after SPQs vs random PQs.</li>
<li><strong>摘要：</strong>语音助手在智能设备中已经无处不在，允许用户通过语音问题即时访问信息。尽管人们对语音搜索的问答进行了广泛的研究，但很少有人关注如何实现语音助手向用户主动推荐。这是一个极具挑战性的问题，通常会导致用户摩擦，主要是因为在错误的时间向用户提供了推荐。我们专注于电子商务领域，即识别购物产品问题（SPQ），其中询问产品相关问题的用户可能有潜在的购物需求。识别用户的购物需求使语音助手可以确定何时提供推荐（例如产品或优惠推荐或主动购物操作推荐），从而增强购物体验。识别 SPQ 是一个具有挑战性的问题，不能仅通过问题文本来完成，因此需要从用户过去的购物历史推断出潜在的用户行为模式。我们提出了一些功能，可以从用户的购买历史记录中捕获用户的潜在购物行为，并使用新颖的专家混合 (MoE) 模型将它们结合起来。我们的评估表明，所提出的方法能够识别 F1=0.91 高分的 SPQ。此外，基于对真实语音助手用户的在线评估，我们实时识别 SPQ，并向用户推荐购物动作，以将查询的产品添加到他们的购物清单中。我们证明，我们能够准确识别 SPQ，与随机 PQ 相比，在 SPQ 后提示时，将产品添加到用户购物清单的比率明显更高。</li>
</ul>

<h3>Title: All in One: An Empirical Study of GPT for Few-Shot Aspect-Based  Sentiment Anlaysis</h3>
<ul>
<li><strong>Authors: </strong>Baoxing Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06063">https://arxiv.org/abs/2404.06063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06063">https://arxiv.org/pdf/2404.06063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06063]] All in One: An Empirical Study of GPT for Few-Shot Aspect-Based  Sentiment Anlaysis(https://arxiv.org/abs/2404.06063)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Aspect-Based Sentiment Analysis (ABSA) is an indispensable and highly challenging task in natural language processing. Current efforts have focused on specific sub-tasks, making it difficult to comprehensively cover all sub-tasks within the ABSA domain. With the development of Generative Pre-trained Transformers (GPTs), there came inspiration for a one-stop solution to sentiment analysis. In this study, we used GPTs for all sub-tasks of few-shot ABSA while defining a general learning paradigm for this application. We propose the All in One (AiO) model, a simple yet effective two-stage model for all ABSA sub-tasks. In the first stage, a specific backbone network learns the semantic information of the review and generates heuristically enhanced candidates. In the second stage, AiO leverages GPT contextual learning capabilities to generate predictions. The study conducted comprehensive comparative and ablation experiments on five benchmark datasets, and the results show that AiO can effectively handle all ABSA sub-tasks, even with few-shot data.</li>
<li><strong>摘要：</strong>基于方面的情感分析（ABSA）是自然语言处理中不可或缺且极具挑战性的任务。目前的工作主要集中在具体的子任务上，很难全面覆盖ABSA领域内的所有子任务。随着生成式预训练 Transformer (GPT) 的发展，情感分析的一站式解决方案得到了灵感。在这项研究中，我们使用 GPT 来完成少样本 ABSA 的所有子任务，同时为此应用程序定义了通用学习范例。我们提出了 All in One (AiO) 模型，这是一种适用于所有 ABSA 子任务的简单而有效的两阶段模型。在第一阶段，特定的骨干网络学习评论的语义信息并生成启发式增强的候选者。在第二阶段，AiO 利用 GPT 上下文学习功能来生成预测。该研究对五个基准数据集进行了全面的比较和消融实验，结果表明，AiO 可以有效处理所有 ABSA 子任务，即使是少量数据。</li>
</ul>

<h3>Title: SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for  Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Elisei Rykov, Yana Shishkina, Kseniia Petrushina, Kseniia Titova, Sergey Petrakov, Alexander Panchenko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06137">https://arxiv.org/abs/2404.06137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06137">https://arxiv.org/pdf/2404.06137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06137]] SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for  Hallucination Detection(https://arxiv.org/abs/2404.06137)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination</a></li>
<li><strong>Abstract: </strong>In this paper, we present our novel systems developed for the SemEval-2024 hallucination detection task. Our investigation spans a range of strategies to compare model predictions with reference standards, encompassing diverse baselines, the refinement of pre-trained encoders through supervised learning, and an ensemble approaches utilizing several high-performing models. Through these explorations, we introduce three distinct methods that exhibit strong performance metrics. To amplify our training data, we generate additional training samples from unlabelled training subset. Furthermore, we provide a detailed comparative analysis of our approaches. Notably, our premier method achieved a commendable 9th place in the competition's model-agnostic track and 17th place in model-aware track, highlighting its effectiveness and potential.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了为 SemEval-2024 幻觉检测任务开发的新颖系统。我们的研究涵盖了一系列将模型预测与参考标准进行比较的策略，包括不同的基线、通过监督学习对预训练编码器的细化，以及利用多个高性能模型的集成方法。通过这些探索，我们引入了三种不同的方法，它们表现出强大的性能指标。为了放大我们的训练数据，我们从未标记的训练子集中生成额外的训练样本。此外，我们还对我们的方法进行了详细的比较分析。值得注意的是，我们的首要方法在竞赛的模型无关赛道中获得了值得称赞的第 9 名，在模型感知赛道中获得了第 17 名，凸显了其有效性和潜力。</li>
</ul>

<h3>Title: Cendol: Open Instruction-tuned Generative Large Language Models for  Indonesian Languages</h3>
<ul>
<li><strong>Authors: </strong>Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Rifki Afina Putri, Emmanuel Dave, Jhonson Lee, Nuur Shadieq, Wawan Cenggoro, Salsabil Maulana Akbar, Muhammad Ihza Mahendra, Dea Annisayanti Putri, Bryan Wilie, Genta Indra Winata, Alham Fikri Aji, Ayu Purwarianti, Pascale Fung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06138">https://arxiv.org/abs/2404.06138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06138">https://arxiv.org/pdf/2404.06138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06138]] Cendol: Open Instruction-tuned Generative Large Language Models for  Indonesian Languages(https://arxiv.org/abs/2404.06138)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show remarkable human-like capability in various domains and languages. However, a notable quality gap arises in low-resource languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts. To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol's effectiveness across a diverse array of tasks, attaining 20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在各个领域和语言中表现出卓越的类人能力。然而，资源匮乏的语言（例如印度尼西亚土著语言）存在显着的质量差距，导致它们在这种语言环境中无效和低效。为了弥补这一质量差距，我们引入了 Cendol，这是印度尼西亚法学硕士的集合，涵盖各种模型大小的纯解码器和编码器-解码器架构。我们强调了 Cendol 在各种任务中的有效性，实现了 20% 的改进，并展示了其泛化到看不见的任务和印度尼西亚本土语言的能力。此外，尽管珍多模式在捕捉印度尼西亚本土知识和文化价值观方面存在局限性，但它仍显示出人类对人类的好感度有所提高。此外，我们还讨论了用于语言适应的参数高效调优（例如 LoRA）的缺点。或者，我们建议使用词汇适应来提高效率。最后，我们评估了 Cendol 的安全性，并展示了一种语言（例如英语）预训练的安全性可以转移到低资源语言（例如印度尼西亚语），即使没有 RLHF 和安全微调。</li>
</ul>

<h3>Title: Characterizing Multimodal Long-form Summarization: A Case Study on  Financial Reports</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Cao, Natraj Raman, Danial Dervovic, Chenhao Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06162">https://arxiv.org/abs/2404.06162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06162">https://arxiv.org/pdf/2404.06162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06162]] Characterizing Multimodal Long-form Summarization: A Case Study on  Financial Reports(https://arxiv.org/abs/2404.06162)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) expand the power of natural language processing to handle long inputs, rigorous and systematic analyses are necessary to understand their abilities and behavior. A salient application is summarization, due to its ubiquity and controversy (e.g., researchers have declared the death of summarization). In this paper, we use financial report summarization as a case study because financial reports not only are long but also use numbers and tables extensively. We propose a computational framework for characterizing multimodal long-form summarization and investigate the behavior of Claude 2.0/2.1, GPT-4/3.5, and Command. We find that GPT-3.5 and Command fail to perform this summarization task meaningfully. For Claude 2 and GPT-4, we analyze the extractiveness of the summary and identify a position bias in LLMs. This position bias disappears after shuffling the input for Claude, which suggests that Claude has the ability to recognize important information. We also conduct a comprehensive investigation on the use of numeric data in LLM-generated summaries and offer a taxonomy of numeric hallucination. We employ prompt engineering to improve GPT-4's use of numbers with limited success. Overall, our analyses highlight the strong capability of Claude 2 in handling long multimodal inputs compared to GPT-4.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）扩展了自然语言处理的能力来处理长输入，需要进行严格和系统的分析来了解它们的能力和行为。一个突出的应用是摘要，因为它无处不在且存在争议（例如，研究人员已经宣布摘要已消亡）。在本文中，我们以财务报告摘要作为案例研究，因为财务报告不仅篇幅较长，而且大量使用数字和表格。我们提出了一个用于表征多模态长格式摘要的计算框架，并研究了 Claude 2.0/2.1、GPT-4/3.5 和 Command 的行为。我们发现 GPT-3.5 和 Command 无法有意义地执行此摘要任务。对于 Claude 2 和 GPT-4，我们分析了摘要的提取性并确定了法学硕士中的立场偏差。在对 Claude 的输入进行改组后，这种位置偏差消失了，这表明 Claude 具有识别重要信息的能力。我们还对法学硕士生成的摘要中数字数据的使用进行了全面调查，并提供了数字幻觉的分类法。我们采用即时工程来改进 GPT-4 对数字的使用，但成效有限。总体而言，我们的分析强调了与 GPT-4 相比，Claude 2 在处理长多模态输入方面的强大能力。</li>
</ul>

<h3>Title: Clue-Instruct: Text-Based Clue Generation for Educational Crossword  Puzzles</h3>
<ul>
<li><strong>Authors: </strong>Andrea Zugarini, Kamyar Zeinalipour, Surya Sai Kadali, Marco Maggini, Marco Gori, Leonardo Rigutini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06186">https://arxiv.org/abs/2404.06186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06186">https://arxiv.org/pdf/2404.06186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06186]] Clue-Instruct: Text-Based Clue Generation for Educational Crossword  Puzzles(https://arxiv.org/abs/2404.06186)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Crossword puzzles are popular linguistic games often used as tools to engage students in learning. Educational crosswords are characterized by less cryptic and more factual clues that distinguish them from traditional crossword puzzles. Despite there exist several publicly available clue-answer pair databases for traditional crosswords, educational clue-answer pairs datasets are missing. In this article, we propose a methodology to build educational clue generation datasets that can be used to instruct Large Language Models (LLMs). By gathering from Wikipedia pages informative content associated with relevant keywords, we use Large Language Models to automatically generate pedagogical clues related to the given input keyword and its context. With such an approach, we created clue-instruct, a dataset containing 44,075 unique examples with text-keyword pairs associated with three distinct crossword clues. We used clue-instruct to instruct different LLMs to generate educational clues from a given input content and keyword. Both human and automatic evaluations confirmed the quality of the generated clues, thus validating the effectiveness of our approach.</li>
<li><strong>摘要：</strong>填字游戏是流行的语言游戏，通常用作吸引学生学习的工具。教育填字游戏的特点是线索更少、更真实，这与传统的填字游戏不同。尽管存在几个公开可用的传统填字游戏线索答案对数据库，但缺少教育线索答案对数据集。在本文中，我们提出了一种构建教育线索生成数据集的方法，该数据集可用于指导大型语言模型（LLM）。通过从维基百科页面收集与相关关键词相关的信息内容，我们使用大型语言模型自动生成与给定输入关键词及其上下文相关的教学线索。通过这种方法，我们创建了线索指导，这是一个包含 44,075 个独特示例的数据集，其中的文本关键字对与三个不同的填字游戏线索相关联。我们使用线索指导来指导不同的法学硕士根据给定的输入内容和关键字生成教育线索。人工和自动评估都确认了生成线索的质量，从而验证了我们方法的有效性。</li>
</ul>

<h3>Title: VI-OOD: A Unified Representation Learning Framework for Textual  Out-of-distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Li-Ming Zhan, Bo Liu, Xiao-Ming Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06217">https://arxiv.org/abs/2404.06217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06217">https://arxiv.org/pdf/2404.06217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06217]] VI-OOD: A Unified Representation Learning Framework for Textual  Out-of-distribution Detection(https://arxiv.org/abs/2404.06217)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications. While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention. Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data. In this paper, we delve into textual OOD detection with Transformers. We first identify a key problem prevalent in existing OOD detection methods: the biased representation learned through the maximization of the conditional likelihood $p(y\mid x)$ can potentially result in subpar performance. We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution $p(x, y)$ instead of $p(y\mid x)$. VI-OOD is tailored for textual OOD detection by efficiently exploiting the representations of pre-trained Transformers. Through comprehensive experiments on various text classification tasks, VI-OOD demonstrates its effectiveness and wide applicability. Our code has been released at \url{https://github.com/liam0949/LLM-OOD}.</li>
<li><strong>摘要：</strong>分布外（OOD）检测对于确保深度神经网络在各种应用中的安全性和可靠性起着至关重要的作用。虽然视觉数据中的 OOD 检测越来越受到关注，但文本 OOD 检测领域受到的关注较少。只有少数尝试直接将通用的 OOD 检测方法应用于自然语言处理（NLP）任务，而没有充分考虑文本数据的特征。在本文中，我们深入研究了 Transformer 的文本 OOD 检测。我们首先确定现有 OOD 检测方法中普遍存在的一个关键问题：通过最大化条件似然 $p(y\mid x)$ 学习的有偏差表示可能会导致性能不佳。然后，我们提出了一种用于 OOD 检测的新颖变分推理框架 (VI-OOD)，该框架最大化联合分布 $p(x, y)$ 而不是 $p(y\mid x)$ 的可能性。 VI-OOD 通过有效利用预先训练的 Transformer 的表示，专为文本 OOD 检测而定制。通过对各种文本分类任务的综合实验，VI-OOD展示了其有效性和广泛的适用性。我们的代码已在 \url{https://github.com/liam0949/LLM-OOD} 发布。</li>
</ul>

<h3>Title: Low-Cost Generation and Evaluation of Dictionary Example Sentences</h3>
<ul>
<li><strong>Authors: </strong>Bill Cai, Clarence Boon Liang Ng, Daniel Tan, Shelvia Hotama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06224">https://arxiv.org/abs/2404.06224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06224">https://arxiv.org/pdf/2404.06224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06224]] Low-Cost Generation and Evaluation of Dictionary Example Sentences(https://arxiv.org/abs/2404.06224)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Dictionary example sentences play an important role in illustrating word definitions and usage, but manually creating quality sentences is challenging. Prior works have demonstrated that language models can be trained to generate example sentences. However, they relied on costly customized models and word sense datasets for generation and evaluation of their work. Rapid advancements in foundational models present the opportunity to create low-cost, zero-shot methods for the generation and evaluation of dictionary example sentences. We introduce a new automatic evaluation metric called OxfordEval that measures the win-rate of generated sentences against existing Oxford Dictionary sentences. OxfordEval shows high alignment with human judgments, enabling large-scale automated quality evaluation. We experiment with various LLMs and configurations to generate dictionary sentences across word classes. We complement this with a novel approach of using masked language models to identify and select sentences that best exemplify word meaning. The eventual model, FM-MLM, achieves over 85.1% win rate against Oxford baseline sentences according to OxfordEval, compared to 39.8% win rate for prior model-generated sentences.</li>
<li><strong>摘要：</strong>字典例句在说明单词定义和用法方面发挥着重要作用，但手动创建高质量的句子具有挑战性。先前的工作已经证明，可以训练语言模型来生成例句。然而，他们依赖昂贵的定制模型和词义数据集来生成和评估他们的工作。基础模型的快速进步为创建低成本、零样本方法来生成和评估字典例句提供了机会。我们引入了一种名为 OxfordEval 的新自动评估指标，用于衡量生成的句子相对于现有《牛津词典》句子的胜率。 OxfordEval 与人类判断高度一致，可实现大规模自动化质量评估。我们尝试使用各种法学硕士和配置来生成跨词类的字典句子。我们用一种新颖的方法来补充这一点，即使用掩码语言模型来识别和选择最能体现单词含义的句子。根据 OxfordEval，最终模型 FM-MLM 对牛津基准句子的胜率超过 85.1%，而之前模型生成的句子的胜率是 39.8%。</li>
</ul>

<h3>Title: Understanding Cross-Lingual Alignment -- A Survey</h3>
<ul>
<li><strong>Authors: </strong>Katharina Hämmerl, Jindřich Libovický, Alexander Fraser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06228">https://arxiv.org/abs/2404.06228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06228">https://arxiv.org/pdf/2404.06228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06228]] Understanding Cross-Lingual Alignment -- A Survey(https://arxiv.org/abs/2404.06228)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Cross-lingual alignment, the meaningful similarity of representations across languages in multilingual language models, has been an active field of research in recent years. We survey the literature of techniques to improve cross-lingual alignment, providing a taxonomy of methods and summarising insights from throughout the field. We present different understandings of cross-lingual alignment and their limitations. We provide a qualitative summary of results from a large number of surveyed papers. Finally, we discuss how these insights may be applied not only to encoder models, where this topic has been heavily studied, but also to encoder-decoder or even decoder-only models, and argue that an effective trade-off between language-neutral and language-specific information is key.</li>
<li><strong>摘要：</strong>跨语言对齐，即多语言语言模型中跨语言表示的有意义的相似性，近年来一直是一个活跃的研究领域。我们调查了改善跨语言对齐的技术文献，提供了方法分类并总结了整个领域的见解。我们提出了对跨语言对齐及其局限性的不同理解。我们对大量调查论文的结果进行了定性总结。最后，我们讨论如何将这些见解不仅应用于编码器模型（该主题已被深入研究），而且应用于编码器-解码器甚至仅解码器模型，并认为语言中立和语言中立之间的有效权衡特定于语言的信息是关键。</li>
</ul>

<h3>Title: LLMs' Reading Comprehension Is Affected by Parametric Knowledge and  Struggles with Hypothetical Statements</h3>
<ul>
<li><strong>Authors: </strong>Victoria Basmov, Yoav Goldberg, Reut Tsarfaty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06283">https://arxiv.org/abs/2404.06283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06283">https://arxiv.org/pdf/2404.06283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06283]] LLMs' Reading Comprehension Is Affected by Parametric Knowledge and  Struggles with Hypothetical Statements(https://arxiv.org/abs/2404.06283)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The task of reading comprehension (RC), often implemented as context-based question answering (QA), provides a primary means to assess language models' natural language understanding (NLU) capabilities. Yet, when applied to large language models (LLMs) with extensive built-in world knowledge, this method can be deceptive. If the context aligns with the LLMs' internal knowledge, it is hard to discern whether the models' answers stem from context comprehension or from LLMs' internal information. Conversely, using data that conflicts with the models' knowledge creates erroneous trends which distort the results. To address this issue, we suggest to use RC on imaginary data, based on fictitious facts and entities. This task is entirely independent of the models' world knowledge, enabling us to evaluate LLMs' linguistic abilities without the interference of parametric knowledge. Testing ChatGPT, GPT-4, LLaMA 2 and Mixtral on such imaginary data, we uncover a class of linguistic phenomena posing a challenge to current LLMs, involving thinking in terms of alternative, hypothetical scenarios. While all the models handle simple affirmative and negative contexts with high accuracy, they are much more prone to error when dealing with modal and conditional contexts. Crucially, these phenomena also trigger the LLMs' vulnerability to knowledge-conflicts again. In particular, while some models prove virtually unaffected by knowledge conflicts in affirmative and negative contexts, when faced with more semantically involved modal and conditional environments, they often fail to separate the text from their internal knowledge.</li>
<li><strong>摘要：</strong>阅读理解 (RC) 任务通常作为基于上下文的问答 (QA) 实现，提供了评估语言模型自然语言理解 (NLU) 能力的主要手段。然而，当应用于具有广泛内置世界知识的大型语言模型（LLM）时，这种方法可能具有欺骗性。如果上下文与法学硕士的内部知识一致，则很难辨别模型的答案是来自上下文理解还是来自法学硕士的内部信息。相反，使用与模型知识相冲突的数据会产生错误的趋势，从而扭曲结果。为了解决这个问题，我们建议对基于虚构事实和实体的虚构数据使用 RC。这项任务完全独立于模型的世界知识，使我们能够在不受参数知识干扰的情况下评估法学硕士的语言能力。在这些虚构数据上测试 ChatGPT、GPT-4、LLaMA 2 和 Mixtral，我们发现了一类对当前法学硕士构成挑战的语言现象，涉及替代的假设场景的思考。虽然所有模型都能以高精度处理简单的肯定和否定上下文，但在处理模态和条件上下文时，它们更容易出错。至关重要的是，这些现象也再次引发了法学硕士面临知识冲突的脆弱性。特别是，虽然一些模型被证明实际上不受肯定和否定上下文中知识冲突的影响，但当面对更多语义涉及的模态和条件环境时，它们往往无法将文本与其内部知识分开。</li>
</ul>

<h3>Title: RAR-b: Reasoning as Retrieval Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Xiao, G Thomas Hudson, Noura Al Moubayed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06347">https://arxiv.org/abs/2404.06347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06347">https://arxiv.org/pdf/2404.06347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06347]] RAR-b: Reasoning as Retrieval Benchmark(https://arxiv.org/abs/2404.06347)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years. Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models, and take a conscious look at the reasoning abilities stored in them. Addressing this, we pose the question: Can retrievers solve reasoning problems? By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks. Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align. However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding. We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark (RAR-b), a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models. RAR-b is available at https://github.com/gowitheflow-1998/RAR-b.</li>
<li><strong>摘要：</strong>语义文本相似度（STS）和信息检索任务（IR）任务是过去几年记录嵌入模型进展的两个主要途径。在新兴的检索增强生成（RAG）范式下，我们设想需要评估嵌入模型的下一级语言理解能力，并有意识地查看其中存储的推理能力。针对这个问题，我们提出一个问题：检索器可以解决推理问题吗？通过将推理任务转化为检索任务，我们发现，如果没有经过推理级语言理解的专门训练，当前最先进的检索器模型可能还远远不能胜任协助法学硕士的作用，尤其是在推理方面——密集的任务。此外，尽管经过训练以了解指令，但具有指令意识的 IR 模型通常在推理任务的推理时间内没有指令的情况下会更好，这为研究界带来了一个被忽视的检索器-法学硕士行为差距。然而，最近基于解码器的嵌入模型在缩小差距方面显示出了巨大的希望，突出了嵌入模型实现推理级语言理解的途径。我们还表明，尽管当前现成的重排序模型在这些任务上失败了，但通过微调向它们注入推理能力仍然比双编码器更容易，并且我们能够实现状态-通过微调重新排名模型，在所有任务中实现最先进的性能。我们发布了推理检索基准 (RAR-b)，这是一套完整的任务和设置，用于评估检索器模型中存储的推理能力。 RAR-b 可从 https://github.com/gowitheflow-1998/RAR-b 获取。</li>
</ul>

<h3>Title: SurveyAgent: A Conversational System for Personalized and Efficient  Research Survey</h3>
<ul>
<li><strong>Authors: </strong>Xintao Wang, Jiangjie Chen, Nianqi Li, Lida Chen, Xinfeng Yuan, Wei Shi, Xuyang Ge, Rui Xu, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06364">https://arxiv.org/abs/2404.06364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06364">https://arxiv.org/pdf/2404.06364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06364]] SurveyAgent: A Conversational System for Personalized and Efficient  Research Survey(https://arxiv.org/abs/2404.06364)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In the rapidly advancing research fields such as AI, managing and staying abreast of the latest scientific literature has become a significant challenge for researchers. Although previous efforts have leveraged AI to assist with literature searches, paper recommendations, and question-answering, a comprehensive support system that addresses the holistic needs of researchers has been lacking. This paper introduces SurveyAgent, a novel conversational system designed to provide personalized and efficient research survey assistance to researchers. SurveyAgent integrates three key modules: Knowledge Management for organizing papers, Recommendation for discovering relevant literature, and Query Answering for engaging with content on a deeper level. This system stands out by offering a unified platform that supports researchers through various stages of their literature review process, facilitated by a conversational interface that prioritizes user interaction and personalization. Our evaluation demonstrates SurveyAgent's effectiveness in streamlining research activities, showcasing its capability to facilitate how researchers interact with scientific literature.</li>
<li><strong>摘要：</strong>在人工智能等快速发展的研究领域，管理和跟上最新的科学文献已成为研究人员面临的重大挑战。尽管之前的工作已经利用人工智能来协助文献检索、论文推荐和问答，但缺乏满足研究人员整体需求的综合支持系统。本文介绍了 SurveyAgent，这是一种新颖的对话系统，旨在为研究人员提供个性化且高效的研究调查帮助。 SurveyAgent 集成了三个关键模块：用于组织论文的知识管理、用于发现相关文献的推荐以及用于更深层次地参与内容的查询回答。该系统的突出之处在于提供了一个统一的平台，通过优先考虑用户交互和个性化的对话界面，为研究人员完成文献综述过程的各个阶段提供支持。我们的评估证明了 SurveyAgent 在简化研究活动方面的有效性，展示了其促进研究人员与科学文献互动的能力。</li>
</ul>

<h3>Title: ClinLinker: Medical Entity Linking of Clinical Concept Mentions in  Spanish</h3>
<ul>
<li><strong>Authors: </strong>Fernando Gallego, Guillermo López-García, Luis Gasco-Sánchez, Martin Krallinger, Francisco J. Veredas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06367">https://arxiv.org/abs/2404.06367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06367">https://arxiv.org/pdf/2404.06367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06367]] ClinLinker: Medical Entity Linking of Clinical Concept Mentions in  Spanish(https://arxiv.org/abs/2404.06367)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Advances in natural language processing techniques, such as named entity recognition and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinical text analysis. This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedical text mining: initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a contrastive-learning strategy to be tailored to medical concepts in Spanish. This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose. This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data. Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach's performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes. These findings highlight our approach's ability to address language-specific nuances and set a new benchmark in entity linking, offering a potent tool for enhancing the utility of digital medical records. The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest.</li>
<li><strong>摘要：</strong>自然语言处理技术的进步，例如命名实体识别和广泛使用的标准化术语（如 UMLS 或 SNOMED-CT）的规范化，以及电子健康记录的数字化，显着推进了临床文本分析。这项研究提出了 ClinLinker，这是一种采用两阶段管道进行医疗实体链接的新颖方法，该方法利用了域内适应语言模型进行生物医学文本挖掘的潜力：使用基于 SapBERT 的双编码器进行初始候选检索以及随后的重新排名使用交叉编码器，通过遵循对比学习策略进行训练，以适应西班牙语的医学概念。这种方法最初侧重于西班牙语内容，大大优于为同一目的设计的多语言语言模型。即使对于涉及异构医学术语并在原始数据子集上进行训练的复杂场景也是如此。我们的结果使用 25 的 top-k 准确率和其他 top-k 指标进行评估，证明了我们的方法在连接黄金标准语料库 DisTEMIST（疾病）和 MedProcNER（临床程序）的两个不同临床实体上的性能，比之前的基准高出 40 个点DisTEMIST 和 MedProcNER 中的 43 个点，均标准化为 SNOMED-CT 代码。这些发现凸显了我们的方法能够解决特定于语言的细微差别，并在实体链接方面树立新的基准，为增强数字医疗记录的实用性提供了有效的工具。由此产生的系统具有实用价值，既可用于从临床记录中大规模自动生成结构化数据，也可用于详尽提取和协调感兴趣的预定义临床变量。</li>
</ul>

<h3>Title: Latent Distance Guided Alignment Training for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haotian Luo, Wenhao Zheng, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06390">https://arxiv.org/abs/2404.06390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06390">https://arxiv.org/pdf/2404.06390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06390]] Latent Distance Guided Alignment Training for Large Language Models(https://arxiv.org/abs/2404.06390)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Ensuring alignment with human preferences is a crucial characteristic of large language models (LLMs). Presently, the primary alignment methods, RLHF and DPO, require extensive human annotation, which is expensive despite their efficacy. The significant expenses associated with current alignment techniques motivate researchers to investigate the development of annotation-free alignment training methods. In pursuit of improved alignment without relying on external annotation, we introduce Latent Distance Guided Alignment Training (LD-Align). This approach seeks to align the model with a high-quality supervised fine-tune dataset using guidance from a latent space. The latent space is generated through sample reconstruction, akin to auto-encoding. Consequently, we utilize the distance between sample pairs in the latent space to guide DPO-based alignment training. Extensive experimentation and evaluation show the efficacy of our proposed method in achieving notable alignment.</li>
<li><strong>摘要：</strong>确保符合人类偏好是大型语言模型 (LLM) 的一个重要特征。目前，主要的比对方法 RLHF 和 DPO 需要大量的人工注释，尽管它们有效，但成本昂贵。与当前比对技术相关的巨额费用促使研究人员研究无注释比对训练方法的开发。为了在不依赖外部注释的情况下追求改进的对齐，我们引入了潜在距离引导对齐训练（LD-Align）。这种方法旨在使用潜在空间的指导将模型与高质量的监督微调数据集保持一致。潜在空间是通过样本重建生成的，类似于自动编码。因此，我们利用潜在空间中样本对之间的距离来指导基于 DPO 的对齐训练。广泛的实验和评估表明我们提出的方法在实现显着对齐方面的有效性。</li>
</ul>

<h3>Title: Event Extraction in Basque: Typologically motivated Cross-Lingual  Transfer-Learning Analysis</h3>
<ul>
<li><strong>Authors: </strong>Mikel Zubillaga, Oscar Sainz, Ainara Estarrona, Oier Lopez de Lacalle, Eneko Agirre</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06392">https://arxiv.org/abs/2404.06392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06392">https://arxiv.org/pdf/2404.06392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06392]] Event Extraction in Basque: Typologically motivated Cross-Lingual  Transfer-Learning Analysis(https://arxiv.org/abs/2404.06392)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic. We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages. Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality. Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer. In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature. In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting. To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE). The dataset and code are publicly available.</li>
<li><strong>摘要：</strong>跨语言迁移学习广泛应用于低资源语言的事件提取中，涉及用源语言训练并应用于目标语言的多语言语言模型。本文研究源语言和目标语言之间的类型相似性是否会影响跨语言迁移的性能，这是一个尚待探索的话题。我们首先关注巴斯克语作为目标语言，这是一种理想的目标语言，因为它在类型上与周围语言不同。我们对三个事件提取任务的实验表明，源语言和目标语言之间共享的语言特征确实对传输质量有影响。对 72 个语言对的进一步分析表明，对于涉及实体和事件触发识别等标记分类的任务，共同的书写脚本和形态特征可以产生更高质量的跨语言迁移。相反，对于涉及结构预测（例如参数提取）的任务，常见词序是最相关的特征。此外，我们还发现，当增加训练规模时，并非所有语言在跨语言环境中都以相同的方式缩放。为了进行实验，我们引入了 EusIE，这是一个巴斯克语事件提取数据集，它遵循多语言事件提取数据集 (MEE)。数据集和代码是公开的。</li>
</ul>

<h3>Title: MiniCPM: Unveiling the Potential of Small Language Models with Scalable  Training Strategies</h3>
<ul>
<li><strong>Authors: </strong>Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06395">https://arxiv.org/abs/2404.06395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06395">https://arxiv.org/pdf/2404.06395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06395]] MiniCPM: Unveiling the Potential of Small Language Models with Scalable  Training Strategies(https://arxiv.org/abs/2404.06395)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .</li>
<li><strong>摘要：</strong>人们对开发具有多达万亿个参数的大型语言模型 (LLM) 的兴趣日益浓厚，但也引起了对资源效率和实际费用的担忧，特别是考虑到实验成本巨大。这种情况强调了探索小语言模型 (SLM) 作为资源高效替代方案的潜力的重要性。在这种情况下，我们引入了 MiniCPM，特别是 1.2B 和 2.4B 非嵌入参数变体，它们不仅在各自的类别中表现出色，而且还展示了与 7B-13B LLM 相当的功能。在专注于 SLM 的同时，我们的方法在模型和数据维度上都表现出了可扩展性，适合未来的 LLM 研究。关于模型缩放，我们采用广泛的模型风洞实验来实现稳定和最佳的缩放。对于数据扩展，我们引入了预热-稳定-衰减（WSD）学习率调度器（LRS），有利于持续训练和领域适应。我们对 WSD LRS 中发生的有趣的训练动态进行了深入分析。借助 WSD LRS，我们现在能够有效地研究数据模型缩放定律，而无需在模型和数据的两个轴上进行大量的再训练实验，从中我们得出比 Chinchilla Optimal 更高的计算最佳数据模型比率。此外，我们还推出了MiniCPM系列，包括MiniCPM-DPO、MiniCPM-MoE和MiniCPM-128K，其卓越的性能进一步巩固了MiniCPM在各种SLM应用中的基础。 MiniCPM 模型可在 https://github.com/OpenBMB/MiniCPM 上公开获取。</li>
</ul>

<h3>Title: Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Cai, Arjun Arunasalam, Leo Y. Lin, Antonio Bianchi, Z. Berkay Celik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06407">https://arxiv.org/abs/2404.06407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06407">https://arxiv.org/pdf/2404.06407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06407]] Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak(https://arxiv.org/abs/2404.06407)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become increasingly integrated with various applications. To ensure that LLMs do not generate unsafe responses, they are aligned with safeguards that specify what content is restricted. However, such alignment can be bypassed to produce prohibited content using a technique commonly referred to as jailbreak. Different systems have been proposed to perform the jailbreak automatically. These systems rely on evaluation methods to determine whether a jailbreak attempt is successful. However, our analysis reveals that current jailbreak evaluation methods have two limitations. (1) Their objectives lack clarity and do not align with the goal of identifying unsafe responses. (2) They oversimplify the jailbreak result as a binary outcome, successful or not. In this paper, we propose three metrics, safeguard violation, informativeness, and relative truthfulness, to evaluate language model jailbreak. Additionally, we demonstrate how these metrics correlate with the goal of different malicious actors. To compute these metrics, we introduce a multifaceted approach that extends the natural language generation evaluation method after preprocessing the response. We evaluate our metrics on a benchmark dataset produced from three malicious intent datasets and three jailbreak systems. The benchmark dataset is labeled by three annotators. We compare our multifaceted approach with three existing jailbreak evaluation methods. Experiments demonstrate that our multifaceted evaluation outperforms existing methods, with F1 scores improving on average by 17% compared to existing baselines. Our findings motivate the need to move away from the binary view of the jailbreak problem and incorporate a more comprehensive evaluation to ensure the safety of the language model.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已越来越多地与各种应用程序集成。为了确保法学硕士不会产生不安全的响应，它们与指定限制内​​容的保障措施保持一致。然而，可以使用通常称为越狱的技术来绕过这种对齐以产生禁止的内容。已经提出了不同的系统来自动执行越狱。这些系统依靠评估方法来确定越狱尝试是否成功。然而，我们的分析表明，当前的越狱评估方法有两个局限性。 (1) 他们的目标缺乏明确性，并且与识别不安全反应的目标不一致。 (2) 他们将越狱结果过度简化为二元结果，成功与否。在本文中，我们提出了三个指标：安全违规、信息性和相对真实性，来评估语言模型越狱。此外，我们还演示了这些指标如何与不同恶意行为者的目标相关联。为了计算这些指标，我们引入了一种多方面的方法，该方法在预处理响应后扩展了自然语言生成评估方法。我们在由三个恶意意图数据集和三个越狱系统生成的基准数据集上评估我们的指标。基准数据集由三个注释器标记。我们将我们的多方面方法与三种现有的越狱评估方法进行比较。实验表明，我们的多方面评估优于现有方法，与现有基线相比，F1 分数平均提高了 17%。我们的研究结果促使我们需要摆脱越狱问题的二元观点，并纳入更全面的评估，以确保语言模型的安全性。</li>
</ul>

<h3>Title: Text-Based Reasoning About Vector Graphics</h3>
<ul>
<li><strong>Authors: </strong>Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li, Jiajun Wu, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06479">https://arxiv.org/abs/2404.06479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06479">https://arxiv.org/pdf/2404.06479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06479]] Text-Based Reasoning About Vector Graphics(https://arxiv.org/abs/2404.06479)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>While large multimodal models excel in broad vision-language benchmarks, they often struggle with tasks requiring precise perception of low-level visual details, such as comparing line lengths or solving simple mazes. In particular, this failure mode persists in question-answering tasks about vector graphics -- images composed purely of 2D objects and shapes. To address this challenge, we propose the Visually Descriptive Language Model (VDLM), which performs text-based reasoning about vector graphics. VDLM leverages Scalable Vector Graphics (SVG) for a more precise visual description and first uses an off-the-shelf raster-to-SVG algorithm for encoding. Since existing language models cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVG with pretrained language models through a newly introduced intermediate symbolic representation, Primal Visual Description (PVD), comprising primitive attributes (e.g., shape, position, measurement) with their corresponding predicted values. PVD is task-agnostic and represents visual primitives that are universal across all vector graphics. It can be learned with procedurally generated (SVG, PVD) pairs and also enables the direct use of LLMs for generalization to complex reasoning tasks. By casting an image to a text-based representation, we can leverage the power of language models to learn alignment from SVG to visual primitives and generalize to unseen question-answering tasks. Empirical results show that VDLM achieves stronger zero-shot performance compared to state-of-the-art LMMs, such as GPT-4V, in various low-level multimodal perception and reasoning tasks on vector graphics. We additionally present extensive analyses on VDLM's performance, demonstrating that our framework offers better interpretability due to its disentangled perception and reasoning processes. Project page: https://mikewangwzhl.github.io/VDLM/</li>
<li><strong>摘要：</strong>虽然大型多模态模型在广泛的视觉语言基准方面表现出色，但它们经常难以完成需要精确感知低级视觉细节的任务，例如比较线长度或解决简单的迷宫。特别是，这种故障模式在有关矢量图形（纯粹由 2D 对象和形状组成的图像）​​的问答任务中持续存在。为了应对这一挑战，我们提出了视觉描述语言模型（VDLM），它对矢量图形执行基于文本的推理。 VDLM 利用可扩展矢量图形 (SVG) 进行更精确的视觉描述，并首先使用现成的光栅到 SVG 算法进行编码。由于现有语言模型无法在零样本设置下理解原始 SVG，因此 VDLM 通过新引入的中间符号表示、原始视觉描述 (PVD)（包含原始属性（例如形状、位置、测量））将 SVG 与预训练语言模型连接起来及其相应的预测值。 PVD 与任务无关，代表所有矢量图形中通用的视觉基元。它可以通过程序生成的（SVG、PVD）对来学习，并且还可以直接使用 LLM 来泛化到复杂的推理任务。通过将图像转换为基于文本的表示形式，我们可以利用语言模型的强大功能来学习从 SVG 到视觉基元的对齐，并推广到看不见的问答任务。实证结果表明，与 GPT-4V 等最先进的 LMM 相比，VDLM 在矢量图形上的各种低级多模态感知和推理任务中实现了更强的零样本性能。我们还对 VDLM 的性能进行了广泛的分析，证明我们的框架由于其解开的感知和推理过程而提供了更好的可解释性。项目页面：https://mikewangwzhl.github.io/VDLM/</li>
</ul>

<h3>Title: Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06480">https://arxiv.org/abs/2404.06480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06480">https://arxiv.org/pdf/2404.06480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06480]] Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks(https://arxiv.org/abs/2404.06480)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.</li>
<li><strong>摘要：</strong>最近，大型语言模型 (LLM) 社区对增强 LLM 处理超长文档的能力表现出越来越大的兴趣。随着各种长文本技术和模型架构的出现，对模型长文本能力的精确而详细的评估变得越来越重要。现有的长文本评估基准，例如L-Eval和LongBench，都是基于开源数据集构建长文本测试集，主要关注QA和摘要任务。这些数据集包括纠缠在一起的不同长度（从 2k 到 32k+）的测试样本，这使得评估不同长度范围内的模型能力变得具有挑战性。此外，它们不涵盖最新法学硕士声称要实现的超长设置（100k+ 代币）。在本文中，我们介绍了 Ada-LEval，这是一种长度自适应基准，用于评估法学硕士的长上下文理解。 Ada-LEval 包括两个具有挑战性的子集：TSort 和 BestAnswer，它们可以更可靠地评估法学硕士的长上下文能力。这些基准测试支持对测试用例长度的复杂操作，并且可以轻松生成多达 128k 个标记的文本样本。我们使用 Ada-LEval 评估了 4 个最先进的闭源 API 模型和 6 个开源模型。评估结果证明了当前法学硕士的局限性，特别是在超长上下文环境中。我们的代码可在 https://github.com/open-compass/Ada-LEval 获取。</li>
</ul>

<h3>Title: Pitfalls of Conversational LLMs on News Debiasing</h3>
<ul>
<li><strong>Authors: </strong>Ipek Baris Schlicht, Defne Altiok, Maryanne Taouk, Lucie Flek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06488">https://arxiv.org/abs/2404.06488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06488">https://arxiv.org/pdf/2404.06488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06488]] Pitfalls of Conversational LLMs on News Debiasing(https://arxiv.org/abs/2404.06488)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>This paper addresses debiasing in news editing and evaluates the effectiveness of conversational Large Language Models in this task. We designed an evaluation checklist tailored to news editors' perspectives, obtained generated texts from three popular conversational models using a subset of a publicly available dataset in media bias, and evaluated the texts according to the designed checklist. Furthermore, we examined the models as evaluator for checking the quality of debiased model outputs. Our findings indicate that none of the LLMs are perfect in debiasing. Notably, some models, including ChatGPT, introduced unnecessary changes that may impact the author's style and create misinformation. Lastly, we show that the models do not perform as proficiently as domain experts in evaluating the quality of debiased outputs.</li>
<li><strong>摘要：</strong>本文讨论了新闻编辑中的去偏见问题，并评估了会话大型语言模型在此任务中的有效性。我们根据新闻编辑的观点设计了一个评估清单，使用媒体偏见中公开可用数据集的子集从三个流行的对话模型中获取生成的文本，并根据设计的清单评估文本。此外，我们将模型作为评估器进行检查，以检查除偏模型输出的质量。我们的研究结果表明，没有一个法学硕士在消除偏见方面是完美的。值得注意的是，包括 ChatGPT 在内的一些模型引入了不必要的更改，这些更改可能会影响作者的风格并产生错误信息。最后，我们表明，这些模型在评估去偏输出的质量方面不如领域专家那么熟练。</li>
</ul>

<h3>Title: Comparing Two Model Designs for Clinical Note Generation; Is an LLM a  Useful Evaluator of Consistency?</h3>
<ul>
<li><strong>Authors: </strong>Nathan Brake, Thomas Schaaf</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06503">https://arxiv.org/abs/2404.06503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06503">https://arxiv.org/pdf/2404.06503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06503]] Comparing Two Model Designs for Clinical Note Generation; Is an LLM a  Useful Evaluator of Consistency?(https://arxiv.org/abs/2404.06503)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.</li>
<li><strong>摘要：</strong>与患者互动后，医生负责提交临床文档，通常以 SOAP 注释的形式组织。临床记录不仅仅是谈话的总结，还需要使用适当的医学术语。然后可以根据 SOAP 注释的结构提取和组织相关信息。在本文中，我们分析了两种不同的方法来根据对话的音频记录生成 SOAP 注释的不同部分，并根据注释一致性专门检查它们。第一种方法独立生成各个部分，而第二种方法则一起生成它们。在这项工作中，我们使用 PEGASUS-X Transformer 模型，并观察到两种方法都会产生类似的 ROUGE 值（差异小于 1%），并且在事实性指标方面没有差异。我们进行人工评估来衡量一致性的各个方面，并证明像 Llama2 这样的法学硕士可以用来执行相同的任务，并且与人工注释者的协议大致相同。在 Llama2 分析和人类评审员之间，我们观察到年龄、性别和身体部位损伤一致性的 Cohen Kappa 评分者间信度分别为 0.79、1.00 和 0.32。通过这一点，我们展示了利用法学硕士来衡量人类可以识别但目前自动指标无法捕获的质量指标的有用性。这允许将评估扩展到更大的数据集，并且我们发现，通过根据所有先前生成的部分的输出生成每个新部分，可以提高临床记录的一致性。</li>
</ul>

<h3>Title: On the Effect of (Near) Duplicate Subwords in Language Modelling</h3>
<ul>
<li><strong>Authors: </strong>Anton Schäfer, Thomas Hofmann, Imanol Schlag, Tiago Pimentel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06508">https://arxiv.org/abs/2404.06508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06508">https://arxiv.org/pdf/2404.06508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06508]] On the Effect of (Near) Duplicate Subwords in Language Modelling(https://arxiv.org/abs/2404.06508)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Tokenisation is a core part of language models (LMs). It involves splitting a character sequence into subwords which are assigned arbitrary indices before being served to the LM. While typically lossless, however, this process may lead to less sample efficient LM training: as it removes character-level information, it could make it harder for LMs to generalise across similar subwords, such as now and Now. We refer to such subwords as near duplicates. In this paper, we study the impact of near duplicate subwords on LM training efficiency. First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates. We do this by duplicating each subword in our LM's vocabulary, creating perfectly equivalent classes of subwords. Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting. Second, we investigate the impact of naturally occurring near duplicates on LMs. Here, we see that merging them considerably hurts LM performance. Therefore, although subword duplication negatively impacts LM training efficiency, naturally occurring near duplicates may not be as similar as anticipated, limiting the potential for performance improvements.</li>
<li><strong>摘要：</strong>标记化是语言模型（LM）的核心部分。它涉及将字符序列拆分为子词，这些子词在提供给 LM 之前被分配任意索引。然而，虽然通常是无损的，但这个过程可能会导致 LM 训练的样本效率较低：因为它删除了字符级信息，因此可能会使 LM 更难泛化相似的子词，例如 now 和 Now。我们将此类子词称为近似重复项。在本文中，我们研究了近重复子词对 LM 训练效率的影响。首先，我们设计了一个实验，为我们提供了一个上限，即如果我们能够完美地泛化近似重复项，那么我们应该期望模型改进多少。我们通过复制 LM 词汇表中的每个子词来做到这一点，创建完全等效的子词类别。通过实验，我们发现在完全重复的环境中训练时 LM 需要大约 17% 的数据。其次，我们研究自然发生的接近重复对 LM 的影响。在这里，我们看到合并它们会极大地损害 LM 的性能。因此，尽管子字重复会对 LM 训练效率产生负面影响，但自然发生的接近重复可能并不像预期的那样相似，从而限制了性能改进的潜力。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
