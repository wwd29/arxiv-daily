<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-24</h1>
<h3>Title: Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Chong Zhang, Jia Wang, Fangyu Wu, Yushi Li, Xiaobo Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17231">https://arxiv.org/abs/2506.17231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17231">https://arxiv.org/pdf/2506.17231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17231]] Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs(https://arxiv.org/abs/2506.17231)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Attacks on large language models (LLMs) in jailbreaking scenarios raise many security and ethical issues. Current jailbreak attack methods face problems such as low efficiency, high computational cost, and poor cross-model adaptability and versatility, which make it difficult to cope with the rapid development of LLM and new defense strategies. Our work proposes an Adversarial Prompt Distillation, which combines masked language modeling, reinforcement learning, and dynamic temperature control through a prompt generation and distillation method. It enables small language models (SLMs) to jailbreak attacks on mainstream LLMs. The experimental results verify the superiority of the proposed method in terms of attack success rate and harm, and reflect the resource efficiency and cross-model adaptability. This research explores the feasibility of distilling the jailbreak ability of LLM to SLM, reveals the model's vulnerability, and provides a new idea for LLM security research.</li>
<li><strong>摘要：</strong>在越狱场景中对大语言模型（LLM）的攻击引发了许多安全和道德问题。当前的越狱攻击方法面临诸如低效率，高计算成本以及跨模型的适应性和多功能性等问题，这使得很难应付LLM和新的国防策略的快速发展。我们的工作提出了一种对抗性及时的蒸馏，该蒸馏结合了掩盖的语言建模，增强学习和通过及时的生成和蒸馏方法进行动态温度控制。它使小语言模型（SLM）能够对主流LLM的越狱攻击。实验结果验证了所提出的方法在攻击成功率和危害方面的优越性，并反映了资源效率和跨模型的适应性。这项研究探讨了将LLM越狱能力提取SLM的可行性，揭示了模型的脆弱性，并为LLM安全研究提供了新的想法。</li>
</ul>

<h3>Title: GTA: Grouped-head latenT Attention</h3>
<ul>
<li><strong>Authors: </strong>Luoyang Sun, Jiwen Jiang, Cheng Deng, Xinjian Wu, Haifeng Zhang, Lei Chen, Lionel Ni, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17286">https://arxiv.org/abs/2506.17286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17286">https://arxiv.org/pdf/2506.17286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17286]] GTA: Grouped-head latenT Attention(https://arxiv.org/abs/2506.17286)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Attention mechanisms underpin the success of large language models (LLMs), yet their substantial computational and memory overhead poses challenges for optimizing efficiency and performance. A critical bottleneck arises as KV cache and attention computations scale rapidly with text length, challenging deployment on hardware with limited computational and memory resources. We observe that attention mechanisms exhibit substantial redundancy, since the KV cache can be significantly compressed and attention maps across heads display high similarity, revealing that much of the computation and storage is unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that reduces memory usage and computational complexity while maintaining performance. GTA comprises two components: (1) a shared attention map mechanism that reuses attention scores across multiple heads, decreasing the key cache size; and (2) a nonlinear value decoder with learned projections that compresses the value cache into a latent space, further cutting memory needs. GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while avoiding the extra overhead of Multi-Head Latent Attention to improve LLM deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in end-to-end inference speed, with prefill benefiting from reduced computational cost and decoding benefiting from the smaller cache footprint.</li>
<li><strong>摘要：</strong>注意机制是大语模型（LLM）成功的基础，但是它们的实质性计算和内存开销构成了优化效率和性能的挑战。随着KV缓存和注意力计算的迅速扩展，随着文本长度的扩展而产生关键的瓶颈，这挑战了在硬件上具有有限的计算和内存资源的部署。我们观察到注意机制表现出很大的冗余性，因为KV缓存可以显着压缩，并且整个头部的注意力图显示出很高的相似性，这表明许多计算和存储都是不必要的。利用这些见解，我们提出\ textbf {g} rouped-head laten \ textbf {t} \ textbf {a} ttention（gta），这是一种新颖的注意机制，可在保持性能的同时降低记忆使用和计算复杂性。 GTA包括两个组成部分：（1）共享注意力图机制，该机制可重用多个头部的注意力评分，从而降低了关键的缓存大小； （2）具有学习预测的非线性值解码器，将值缓存压缩到潜在的空间，进一步的切割内存需求。 GTA最多可将注意力计算与\ emph {62.5 \％}相对于分组 - 问题的关注，并通过最多\ emph {70 \％}缩小KV缓存，同时避免了多头的潜在注意力的额外开销，以提高LLM的部署效率。因此，GTA模型的端到端推理速度增加了\ emph {2x}的增加，预填充受益于降低的计算成本和解码得益于较小的高速缓存足迹。</li>
</ul>

<h3>Title: AI-Generated Game Commentary: A Survey and a Datasheet Repository</h3>
<ul>
<li><strong>Authors: </strong>Qirui Zheng, Xingbo Wang, Keyuan Cheng, Yunlong Lu, Wenxin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17294">https://arxiv.org/abs/2506.17294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17294">https://arxiv.org/pdf/2506.17294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17294]] AI-Generated Game Commentary: A Survey and a Datasheet Repository(https://arxiv.org/abs/2506.17294)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>AI-Generated Game Commentary (AIGGC) has gained increasing attention due to its market potential and inherent technical challenges. As a comprehensive multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial demands on language models, including factual accuracy, logical reasoning, expressive text generation, generation speed, and context management. In this paper, we introduce a general framework for AIGGC and present a comprehensive survey of 45 existing game commentary dataset and methods according to key challenges they aim to address in this domain. We further classify and compare various evaluation metrics commonly used in this domain. To support future research and benchmarking, we also provide a structured datasheet summarizing the essential attributes of these datasets in appendix, which is meanwhile publicly available in an open repository.</li>
<li><strong>摘要：</strong>AI生成的游戏评论（AIGGC）由于其市场潜力和固有的技术挑战而引起了人们的关注。作为一项全面的多模式自然语言处理（NLP）任务，AIGGC对语言模型提出了重大要求，包括事实准确性，逻辑推理，表现力的文本生成，生成速度和上下文管理。在本文中，我们介绍了AIGGC的一般框架，并根据他们旨在解决该领域的关键挑战对45个现有游戏评论数据集和方法进行了全面调查。我们进一步对该域通常使用的各种评估指标进行分类和比较。为了支持未来的研究和基准测试，我们还提供了一个结构化的数据表，总结了附录中这些数据集的基本属性，同时，该数据集在开放式存储库中公开可用。</li>
</ul>

<h3>Title: Semantic uncertainty in advanced decoding methods for LLM generation</h3>
<ul>
<li><strong>Authors: </strong>Darius Foodeei, Simin Fan, Martin Jaggi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17296">https://arxiv.org/abs/2506.17296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17296">https://arxiv.org/pdf/2506.17296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17296]] Semantic uncertainty in advanced decoding methods for LLM generation(https://arxiv.org/abs/2506.17296)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>This study investigates semantic uncertainty in large language model (LLM) outputs across different decoding methods, focusing on emerging techniques like speculative sampling and chain-of-thought (CoT) decoding. Through experiments on question answering, summarization, and code generation tasks, we analyze how different decoding strategies affect both the diversity and reliability of model outputs. Our findings reveal that while CoT decoding demonstrates higher semantic diversity, it maintains lower predictive entropy, suggesting that structured exploration can lead to more confident and accurate outputs. This is evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower alignment with reference solutions. For summarization tasks, speculative sampling proved particularly effective, achieving superior ROUGE scores while maintaining moderate semantic diversity. Our results challenge conventional assumptions about trade-offs between diversity and accuracy in language model outputs, demonstrating that properly structured decoding methods can increase semantic exploration while maintaining or improving output quality. These findings have significant implications for deploying language models in practical applications where both reliability and diverse solution generation are crucial.</li>
<li><strong>摘要：</strong>这项研究研究了大语模型（LLM）在不同解码方法中的语义不确定性，重点是投机采样和思考链（COT）解码等新兴技术。通过对问答，汇总和代码生成任务的实验，我们分析了不同的解码策略如何影响模型输出的多样性和可靠性。我们的发现表明，尽管COT解码显示出较高的语义多样性，但它保持了较低的预测性熵，这表明结构化探索可以导致更自信和准确的输出。尽管与参考解决方案的一致性较低，但代码生成通过@2率的提高了48.8％，这证明了这一点。对于摘要任务，投机性采样被证明特别有效，在保持适度的语义多样性的同时，取得了卓越的胭脂分数。我们的结果挑战了语言模型输出中多样性和准确性之间权衡的常规假设，表明结构正确的解码方法可以增加语义探索，同时保持或提高输出质量。这些发现对在可靠性和不同解决方案产生至关重要的实际应用中部署语言模型具有重要意义。</li>
</ul>

<h3>Title: Mercury: Ultra-Fast Language Models Based on Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, Aditya Grover, Volodymyr Kuleshov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17298">https://arxiv.org/abs/2506.17298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17298">https://arxiv.org/pdf/2506.17298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17298]] Mercury: Ultra-Fast Language Models Based on Diffusion(https://arxiv.org/abs/2506.17298)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present Mercury, a new generation of commercial-scale large language models (LLMs) based on diffusion. These models are parameterized via the Transformer architecture and trained to predict multiple tokens in parallel. In this report, we detail Mercury Coder, our first set of diffusion LLMs designed for coding applications. Currently, Mercury Coder comes in two sizes: Mini and Small. These models set a new state-of-the-art on the speed-quality frontier. Based on independent evaluations conducted by Artificial Analysis, Mercury Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109 tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform speed-optimized frontier models by up to 10x on average while maintaining comparable quality. We discuss additional results on a variety of code benchmarks spanning multiple languages and use-cases as well as real-world validation by developers on Copilot Arena, where the model currently ranks second on quality and is the fastest model overall. We also release a public API at this https URL and free playground at this https URL</li>
<li><strong>摘要：</strong>我们提出了基于扩散的新一代商业规模的大语言模型（LLM）。这些模型通过变压器体系结构进行了参数化，并经过训练以并联预测多个令牌。在本报告中，我们详细介绍了水星编码器，这是我们为编码应用程序设计的第一组扩散LLM。目前，汞编码器有两种尺寸：迷你和小。这些型号在速度质量边界上设定了新的最新技术。基于通过人工分析进行的独立评估，汞编码器迷你和汞编码器小实现了1109代价/秒的最新吞吐量和737代币/秒，在NVIDIA H100 GPU上，在NVIDIA H100 GPU上，均超过速度速度较高的边界模型，同时平均维持可比较的质量高达10倍。我们讨论了有关多种语言和用例的各种代码基准以及开发人员在Copilot Arena上的现实验证的其他结果，该模型目前在质量上排名第二，并且是总体上最快的模型。我们还在此HTTPS URL上发布了公共API，并在此HTTPS URL上发布了免费的操场</li>
</ul>

<h3>Title: PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights</h3>
<ul>
<li><strong>Authors: </strong>Adnan Qidwai, Srija Mukhopadhyay, Prerana Khatiwada, Dan Roth, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17314">https://arxiv.org/abs/2506.17314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17314">https://arxiv.org/pdf/2506.17314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17314]] PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights(https://arxiv.org/abs/2506.17314)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows sellers to easily enhance their product listings for clarity and persuasiveness, and buyers to better assess product reliability. Our demonstration showcases PRAISE's workflow, its effectiveness in generating actionable structured insights from unstructured reviews, and its potential to significantly improve the quality and trustworthiness of e-commerce product catalogs.</li>
<li><strong>摘要：</strong>准确而完整的产品描述对于电子商务至关重要，但是提供卖方的信息通常不足。客户评论提供了宝贵的细节，但努力地手动筛选。我们提出赞美：产品评论属性Insight结构引擎，这是一种新型系统，该系统使用大型语言模型（LLMS）自动提取，比较和结构从客户评论和卖方描述中提取，比较和结构洞察力。赞美为用户提供了直观的界面，以确定这两个来源之间的缺失，矛盾或部分匹配的细节，并以清晰的结构化格式以及评论的支持证据以清晰的结构化格式呈现差异。这使卖方可以轻松地增强其产品清单，以确保清晰度和说服力，并可以更好地评估产品可靠性。我们的演示展示了赞美的工作流程，其在产生可行的结构化见解方面的有效性，以及它显着提高电子商务产品目录的质量和可信赖性的潜力。</li>
</ul>

<h3>Title: Towards Safety Evaluations of Theory of Mind in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tatsuhiro Aoshima, Mitsuaki Akiyama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17352">https://arxiv.org/abs/2506.17352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17352">https://arxiv.org/pdf/2506.17352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17352]] Towards Safety Evaluations of Theory of Mind in Large Language Models(https://arxiv.org/abs/2506.17352)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As the capabilities of large language models (LLMs) continue to advance, the importance of rigorous safety evaluation is becoming increasingly evident. Recent concerns within the realm of safety assessment have highlighted instances in which LLMs exhibit behaviors that appear to disable oversight mechanisms and respond in a deceptive manner. For example, there have been reports suggesting that, when confronted with information unfavorable to their own persistence during task execution, LLMs may act covertly and even provide false answers to questions intended to verify their this http URL evaluate the potential risk of such deceptive actions toward developers or users, it is essential to investigate whether these behaviors stem from covert, intentional processes within the model. In this study, we propose that it is necessary to measure the theory of mind capabilities of LLMs. We begin by reviewing existing research on theory of mind and identifying the perspectives and tasks relevant to its application in safety evaluation. Given that theory of mind has been predominantly studied within the context of developmental psychology, we analyze developmental trends across a series of open-weight LLMs. Our results indicate that while LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development. Finally, we present the current state of safety evaluation with respect to LLMs' theory of mind, and discuss remaining challenges for future work.</li>
<li><strong>摘要：</strong>随着大语言模型（LLM）的能力继续发展，严格的安全评估的重要性变得越来越明显。安全评估领域的最新关注点突出了LLM表现出似乎可以禁用监督机制并以欺骗性的方式做出反应的行为的实例。例如，有报道表明，当与他们在任务执行期间不符的信息面对自己的坚持不懈时，LLMS可能会秘密行动，甚至为旨在验证其HTTP URL验证其HTTP URL的问题提供虚假答案，以评估他们对开发人员或用户的这种欺骗性行动的潜在风险，这是对这些行为的重要性，这些行为是由掩盖过程中的，这是范围内的。在这项研究中，我们建议有必要衡量LLM的思维能力理论。我们首先回顾有关心理理论的现有研究，并确定与其在安全评估中的应用有关的观点和任务。鉴于这种思想理论主要在发展心理学的背景下进行了研究，因此我们分析了一系列开放重量LLM的发展趋势。我们的结果表明，尽管LLM在阅读理解方面有所改善，但他们的思维能力理论尚未显示出可比的发展。最后，我们介绍了有关LLMS心理理论的当前安全评估状态，并讨论了未来工作的剩余挑战。</li>
</ul>

<h3>Title: Cash or Comfort? How LLMs Value Your Inconvenience</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Cedro, Timour Ichmoukhamedov, Sofie Goethals, Yifan He, James Hinns, David Martens</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17367">https://arxiv.org/abs/2506.17367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17367">https://arxiv.org/pdf/2506.17367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17367]] Cash or Comfort? How LLMs Value Your Inconvenience(https://arxiv.org/abs/2506.17367)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly proposed as near-autonomous artificial intelligence (AI) agents capable of making everyday decisions on behalf of humans. Although LLMs perform well on many technical tasks, their behaviour in personal decision-making remains less understood. Previous studies have assessed their rationality and moral alignment with human decisions. However, the behaviour of AI assistants in scenarios where financial rewards are at odds with user comfort has not yet been thoroughly explored. In this paper, we tackle this problem by quantifying the prices assigned by multiple LLMs to a series of user discomforts: additional walking, waiting, hunger and pain. We uncover several key concerns that strongly question the prospect of using current LLMs as decision-making assistants: (1) a large variance in responses between LLMs, (2) within a single LLM, responses show fragility to minor variations in prompt phrasing (e.g., reformulating the question in the first person can considerably alter the decision), (3) LLMs can accept unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10 hours), and (4) LLMs can reject monetary gains where no discomfort is imposed (e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for scrutiny of how LLMs value human inconvenience, particularly as we move toward applications where such cash-versus-comfort trade-offs are made on users' behalf.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地提出是能够代表人类做出日常决策的近自主人工智能（AI）。尽管LLMS在许多技术任务上都表现良好，但他们在个人决策中的行为仍然不太了解。先前的研究评估了他们与人类决策的合理性和道德一致性。但是，在财务奖励与用户舒适性不一致的情况下，AI助手的行为尚未得到彻底探索。在本文中，我们通过将多个LLMS分配的价格量化为一系列用户不适来解决这个问题：额外的步行，等待，饥饿和痛苦。 We uncover several key concerns that strongly question the prospect of using current LLMs as decision-making assistants: (1) a large variance in responses between LLMs, (2) within a single LLM, responses show fragility to minor variations in prompt phrasing (e.g., reformulating the question in the first person can considerably alter the decision), (3) LLMs can accept unreasonably low rewards for major inconveniences (e.g., 1 Euro等待10个小时），（4）LLM可以拒绝没有施加不适感的货币收益（例如，1,000欧元等待0分钟）。这些发现强调了对LLM如何重视人类不便的审查的必要性，尤其是当我们朝着代表用户做出这种现金相关的舒适折衷权衡的应用中。</li>
</ul>

<h3>Title: Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study</h3>
<ul>
<li><strong>Authors: </strong>Danielle R. Thomas, Conrad Borchers, Jionghao Lin, Sanjit Kakarla, Shambhavi Bhushan, Erin Gatz, Shivang Gupta, Ralph Abboud, Kenneth R. Koedinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17410">https://arxiv.org/abs/2506.17410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17410">https://arxiv.org/pdf/2506.17410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17410]] Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study(https://arxiv.org/abs/2506.17410)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Tutoring improves student achievement, but identifying and studying what tutoring actions are most associated with student learning at scale based on audio transcriptions is an open research problem. This present study investigates the feasibility and scalability of using generative AI to identify and evaluate specific tutor moves in real-life math tutoring. We analyze 50 randomly selected transcripts of college-student remote tutors assisting middle school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo, Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills: delivering effective praise and responding to student math errors. All models reliably detected relevant situations, for example, tutors providing praise to students (94-98% accuracy) and a student making a math error (82-88% accuracy) and effectively evaluated the tutors' adherence to tutoring best practices, aligning closely with human judgments (83-89% and 73-77%, respectively). We propose a cost-effective prompting strategy and discuss practical implications for using large language models to support scalable assessment in authentic settings. This work further contributes LLM prompts to support reproducibility and research in AI-supported learning.</li>
<li><strong>摘要：</strong>辅导改善了学生的成就，但是确定和研究哪些辅导动作与基于音频抄录的学生学习最有联系是一个开放的研究问题。本研究研究了使用生成AI识别和评估现实生活中的特定导师移动的可行性和可扩展性。我们分析了50次随机选择的大学生远程导师的成绩单，以协助中学生进行数学。使用GPT-4，GPT-4O，GPT-4-Turbo，Gemini-1.5-Pro和Learnlm，我们评估了辅导员对两种导师技能的应用：提供有效的赞美和回应学生的数学错误。所有模型都可靠地检测到相关情况，例如，导师向学生（准确性94-98％）提供赞美和犯下数学错误（准确度为82-88％）的学生，并有效地评估了教师对最佳实践的遵守，并紧密与人为判断密切相符（83--89％和73-77％和73-77％）。我们提出了一种具有成本效益的提示策略，并讨论了使用大型语言模型来支持真实设置中可扩展评估的实践含义。这项工作进一步贡献了LLM的提示，以支持AI支持的学习中的可重复性和研究。</li>
</ul>

<h3>Title: UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Jinhao Duan, James Diffenderfer, Sandeep Madireddy, Tianlong Chen, Bhavya Kailkhura, Kaidi Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17419">https://arxiv.org/abs/2506.17419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17419">https://arxiv.org/pdf/2506.17419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17419]] UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making(https://arxiv.org/abs/2506.17419)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are integrated into safety-critical applications involving sequential decision-making in the real world, it is essential to know when to trust LLM decisions. Existing LLM Uncertainty Quantification (UQ) methods are primarily designed for single-turn question-answering formats, resulting in multi-step decision-making scenarios, e.g., LLM agentic system, being underexplored. In this paper, we introduce a principled, information-theoretic framework that decomposes LLM sequential decision uncertainty into two parts: (i) internal uncertainty intrinsic to the current decision, which is focused on existing UQ methods, and (ii) extrinsic uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty should be inherited from preceding decisions. We then propose UProp, an efficient and effective extrinsic uncertainty estimator that converts the direct estimation of MI to the estimation of Pointwise Mutual Information (PMI) over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is evaluated over extensive multi-step decision-making benchmarks, e.g., AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and DeepSeek-V3. Experimental results demonstrate that UProp significantly outperforms existing single-turn UQ baselines equipped with thoughtful aggregation strategies. Moreover, we provide a comprehensive analysis of UProp, including sampling efficiency, potential applications, and intermediate uncertainty propagation, to demonstrate its effectiveness. Codes will be available at this https URL.</li>
<li><strong>摘要：</strong>由于大型语言模型（LLMS）被整合到涉及现实世界中依次决策的安全至关重要的应用中，因此必须知道何时相信LLM的决策。现有的LLM不确定性量化（UQ）方法主要是为单转的提问格式而设计的，从而导致多步骤决策场景，例如LLM Agentic System，未被验证。在本文中，我们引入了一个原则的信息理论框架，将LLM顺序决策分解为两个部分：（i）当前决定的内部不确定性固有的内在性，该决定的内部不确定性集中在现有的UQ方法上，（ii）外在的不确定性，相互性信息（MI）的数量（MI）数量描述了多个不确定性应在先前的决策中的多个不确定性。然后，我们提出了UPROP，这是一种有效而有效的外部不确定性估计器，将MI的直接估计转换为多个轨迹依赖性决策过程（TDP）的近点相互信息（PMI）的估计。根据广泛的多步骤决策基准，例如AgentBench和HotPotQA，对UPOP进行了评估，例如GPT-4.1和DeepSeek-V3。实验结果表明，UPROP的表现明显优于现有的具有周到聚合策略的单线UQ基准。此外，我们提供了对UPROP的全面分析，包括采样效率，潜在的应用和中间不确定性传播，以证明其有效性。代码将在此HTTPS URL上可用。</li>
</ul>

<h3>Title: Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media</h3>
<ul>
<li><strong>Authors: </strong>Alberto Martinez-Serra, Alejandro De La Fuente, Nienke Viescher, Ana S. Cardenal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17435">https://arxiv.org/abs/2506.17435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17435">https://arxiv.org/pdf/2506.17435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17435]] Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media(https://arxiv.org/abs/2506.17435)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The use of large language models (LLMs) is becoming common in the context of political science, particularly in studies that analyse individuals use of digital media. However, while previous research has demonstrated LLMs ability at labelling tasks, the effectiveness of using LLMs to classify political content (PC) from just URLs is not yet well explored. The work presented in this article bridges this gap by evaluating whether LLMs can accurately identify PC vs. non-PC from both the article text and the URLs from five countries (France, Germany, Spain, the UK, and the US) and different languages. Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we measure model performance to assess whether URL-level analysis can be a good approximation for full-text analysis of PC, even across different linguistic and national contexts. Model outputs are compared with human-labelled articles, as well as traditional supervised machine learning techniques, to set a baseline of performance. Overall, our findings suggest the capacity of URLs to embed most of the news content, providing a vital perspective on accuracy-cost balancing. We also account for contextual limitations and suggest methodological recommendations to use LLMs within political science studies.</li>
<li><strong>摘要：</strong>在政治学的背景下，尤其是在分析个人使用数字媒体的研究中，大型语言模型（LLM）的使用变得普遍。但是，尽管以前的研究表明了LLM在标记任务标签的能力，但使用LLMS对政治内容（PC）进行分类的有效性尚未得到很好的探索。本文介绍的工作通过评估LLM是否可以从五个国家（法国，德国，西班牙，英国和美国）和不同语言的文章文本和URL中准确地识别PC与非PC来弥合这一差距。使用GPT，Llama，Mistral，DeepSeek，Qwen和Gemma等尖端LLM，我们测量模型性能，以评估URL级分析是否可以很好地近似PC的全文分析，即使在不同的语言和国家环境中也是如此。将模型输出与人体标签的文章以及传统的监督机器学习技术进行了比较，以设定绩效的基准。总体而言，我们的发现表明，URL能力嵌入了大多数新闻内容，从而为准确的成本平衡提供了至关重要的观点。我们还考虑了上下文限制，并提出了在政治学研究中使用LLM的方法论建议。</li>
</ul>

<h3>Title: Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems</h3>
<ul>
<li><strong>Authors: </strong>Weixin Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17467">https://arxiv.org/abs/2506.17467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17467">https://arxiv.org/pdf/2506.17467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17467]] Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems(https://arxiv.org/abs/2506.17467)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown significant potential to change how we write, communicate, and create, leading to rapid adoption across society. This dissertation examines how individuals and institutions are adapting to and engaging with this emerging technology through three research directions. First, I demonstrate how the institutional adoption of AI detectors introduces systematic biases, particularly disadvantaging writers of non-dominant language varieties, highlighting critical equity concerns in AI governance. Second, I present novel population-level algorithmic approaches that measure the increasing adoption of LLMs across writing domains, revealing consistent patterns of AI-assisted content in academic peer reviews, scientific publications, consumer complaints, corporate communications, job postings, and international organization press releases. Finally, I investigate LLMs' capability to provide feedback on research manuscripts through a large-scale empirical analysis, offering insights into their potential to support researchers who face barriers in accessing timely manuscript feedback, particularly early-career researchers and those from under-resourced settings.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）表现出很大的潜力，可以改变我们的撰写，交流和创造的方式，从而在整个社会中迅速采用。本论文研究了个人和机构如何通过三个研究方向适应这一新兴技术并参与。首先，我演示了机构采用AI探测器如何引入系统的偏见，尤其是非主导语言品种的不利作家，强调了AI治理中的关键权益关注。其次，我提出了新型的人群级算法方法，这些方法衡量了在写作领域中LLM的提高采用，从而在学术同行评审，科学出版物，消费者投诉，企业传播，工作发布，工作发布和国际组织新闻稿中揭示了AI辅助内容的一致模式。最后，我调查了LLMS通过大规模的经验分析提供有关研究手稿的反馈的能力，从而提供了有关其潜力的见解，以支持他们在访问及时手稿反馈方面面临障碍的研究人员，尤其是早期职业研究人员，尤其是来自资源不足的环境的研究人员。</li>
</ul>

<h3>Title: VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM</h3>
<ul>
<li><strong>Authors: </strong>Lesheng Jin, Zhenyuan Ruan, Haohui Mai, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17506">https://arxiv.org/abs/2506.17506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17506">https://arxiv.org/pdf/2506.17506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17506]] VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM(https://arxiv.org/abs/2506.17506)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Modern GPUs evolve rapidly, yet production compilers still rely on hand-crafted register allocation heuristics that require substantial re-tuning for each hardware generation. We introduce VeriLocc, a framework that combines large language models (LLMs) with formal compiler techniques to enable generalizable and verifiable register allocation across GPU architectures. VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs) into target-specific register assignments, aided by static analysis for cross-architecture normalization and generalization and a verifier-guided regeneration loop to ensure correctness. Evaluated on matrix multiplication (GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more performant assignments than expert-tuned libraries, outperforming rocBLAS by over 10% in runtime.</li>
<li><strong>摘要：</strong>现代GPU迅速发展，但生产编译器仍然依靠手工制作的寄存器分配启发式方法，这些启发式方法需要为每个硬件一代重新调整。我们介绍了Verilocc，该框架将大型语言模型（LLMS）与正式编译器技术相结合，以在GPU架构中启用可通用和可验证的寄存器分配。 Verilocc微调LLM将中间表示（MIR）转化为目标特异性寄存器分配，并通过静态分析进行跨架构归一化和概括的静态分析以及检验器引导的再生循环以确保正确性。 Verilocc在矩阵乘法（GEMM）和多头注意（MHA）上进行评估，达到85-99％的单杆精确度，接近100％通过@100。案例研究表明，Verilocc比专家调整的库发现的性能更多，在运行时表现优于Rocblas超过10％。</li>
</ul>

<h3>Title: DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuanhao Wu, Juntong Song, Hanning Zhang, Tong Zhang, Cheng Niu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17533">https://arxiv.org/abs/2506.17533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17533">https://arxiv.org/pdf/2506.17533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17533]] DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning(https://arxiv.org/abs/2506.17533)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we propose DuaShepherd, a novel reward modeling framework that integrates two complementary reward signals, correctness and potential, to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). While correctness-based signals emphasize identification of stepwise errors, potential-based signals focus on the likelihood of reaching the correct final answer. We developed an automated pipeline for constructing large-scale reward modeling dataset with both signals. A unified, multi-head architecture was explored to train the two reward models in a multi-task setup, demonstrating benefits from learning both correctness and potential in parallel. By combining these two signals into a compound probability, our model achieves consistent performance improvements across multiple benchmarks. Empirical evaluations on MATH500 and ProcessBench confirm that this combined reward significantly outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints.</li>
<li><strong>摘要：</strong>在本文中，我们提出了Duashepherd，这是一个新颖的奖励建模框架，它集成了两个互补的奖励信号，正确性和潜力，以增强大语模型（LLMS）的数学推理能力。尽管基于正确性的信号强调了逐步错误的识别，但潜在的信号集中在达到正确的最终答案的可能性上。我们开发了一种自动化管道，用于构建具有两个信号的大规模奖励建模数据集。探索了一个统一的多头体系结构，以在多任务设置中训练两个奖励模型，从而证明了同时学习正确性和潜在潜力的好处。通过将这两个信号组合为复合概率，我们的模型可以在多个基准测试中实现一致的性能提高。对Math500和ProcessBench的经验评估证实，这种综合奖励显着优于仅对任何一种奖励类型进行训练的模型，从而在可比的资源限制下实现了最先进的绩效。</li>
</ul>

<h3>Title: TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Lincan Li, Eren Erman Ozguven, Yue Zhao, Guang Wang, Yiqun Xie, Yushun Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17609">https://arxiv.org/abs/2506.17609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17609">https://arxiv.org/pdf/2506.17609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17609]] TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting(https://arxiv.org/abs/2506.17609)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Accurate typhoon track forecasting is crucial for early system warning and disaster response. While Transformer-based models have demonstrated strong performance in modeling the temporal dynamics of dense trajectories of humans and vehicles in smart cities, they usually lack access to broader contextual knowledge that enhances the forecasting reliability of sparse meteorological trajectories, such as typhoon tracks. To address this challenge, we propose TyphoFormer, a novel framework that incorporates natural language descriptions as auxiliary prompts to improve typhoon trajectory forecasting. For each time step, we use Large Language Model (LLM) to generate concise textual descriptions based on the numerical attributes recorded in the North Atlantic hurricane database. The language descriptions capture high-level meteorological semantics and are embedded as auxiliary special tokens prepended to the numerical time series input. By integrating both textual and sequential information within a unified Transformer encoder, TyphoFormer enables the model to leverage contextual cues that are otherwise inaccessible through numerical features alone. Extensive experiments are conducted on HURDAT2 benchmark, results show that TyphoFormer consistently outperforms other state-of-the-art baseline methods, particularly under challenging scenarios involving nonlinear path shifts and limited historical observations.</li>
<li><strong>摘要：</strong>准确的台风轨道预测对于早期系统警告和灾难响应至关重要。尽管基于变压器的模型在建模智能城市中人类和车辆的密集轨迹的时间动态方面表现出了强大的性能，但它们通常无法获得更广泛的上下文知识，从而增强了稀疏气象轨迹的预测可靠性，例如台风轨迹。为了应对这一挑战，我们提出了Typhoformer，这是一个新颖的框架，将自然语言描述作为辅助提示提示改善台风轨迹预测。对于每个时间步，我们使用大型语言模型（LLM）根据北大西洋飓风数据库中记录的数值属性生成简明的文本描述。语言描述捕获了高级气象语义，并将其嵌入为辅助特殊令牌，以数值时间序列序列输入。通过将文本和顺序信息同时集成在统一的变压器编码器中，Typhoformer可以使模型能够利用单独通过数值特征无法访问的上下文提示。在Hurdat2基准测试上进行了广泛的实验，结果表明，颗身型始终优于其他最先进的基线方法，尤其是在涉及非线性路径变化和有限历史观察结果的具有挑战性的情况下。</li>
</ul>

<h3>Title: OpusLM: A Family of Open Unified Speech Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinchuan Tian, William Chen, Yifan Peng, Jiatong Shi, Siddhant Arora, Shikhar Bharadwaj, Takashi Maekaku, Yusuke Shinohara, Keita Goto, Xiang Yue, Huck Yang, Shinji Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17611">https://arxiv.org/abs/2506.17611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17611">https://arxiv.org/pdf/2506.17611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17611]] OpusLM: A Family of Open Unified Speech Language Models(https://arxiv.org/abs/2506.17611)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper presents Open Unified Speech Language Models (OpusLMs), a family of open foundational speech language models (SpeechLMs) up to 7B. Initialized from decoder-only text language models, the OpusLMs are continuously pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We demonstrate our OpusLMs achieve comparable (or even superior) performance with existing SpeechLMs in speech recognition, speech synthesis, and text-only capabilities. Technically, this paper articulates our SpeechLM designs on tokenization, multi-stream language models, and multi-stage training strategies. We experimentally demonstrate the importance of model size scaling and the effect of annealing data selection. The OpusLMs are all built from publicly available materials and are fully transparent models. We release our code, data, checkpoints, and training logs to facilitate open SpeechLM research</li>
<li><strong>摘要：</strong>本文介绍了开放的统一语言模型（OPUSLMS），这是一个开放的基础语言模型（SpeechLMS）的家族，最高为7b。从仅解码器的文本语言模型初始化，OpuSlms在213K小时的语音文本对和292B仅限文本代币中不断预先训练。我们证明了我们的作品在语音识别，语音综合和仅文本功能中实现现有语音的可比性（甚至是卓越）的性能。从技术上讲，本文阐明了我们的语音设计有关令牌化，多流语言模型和多阶段培训策略的设计。我们通过实验证明了模型尺寸缩放的重要性以及退火数据选择的效果。 Opuslms都是由公共材料构建的，并且是完全透明的型号。我们发布代码，数据，检查点和培训日志，以促进开放语音研究</li>
</ul>

<h3>Title: Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yang Wu, Yifan Zhang, Yiwei Wang, Yujun Cai, Yurong Wu, Yuran Wang, Ning Xu, Jian Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17630">https://arxiv.org/abs/2506.17630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17630">https://arxiv.org/pdf/2506.17630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17630]] Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs(https://arxiv.org/abs/2506.17630)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, growing evidence suggests much of their success stems from memorized answer-reasoning patterns rather than genuine inference. In this work, we investigate a central question: are LLMs primarily anchored to final answers or to the textual pattern of reasoning chains? We propose a five-level answer-visibility prompt framework that systematically manipulates answer cues and probes model behavior through indirect, behavioral analysis. Experiments across state-of-the-art LLMs reveal a strong and consistent reliance on explicit answers. The performance drops by 26.90\% when answer cues are masked, even with complete reasoning chains. These findings suggest that much of the reasoning exhibited by LLMs may reflect post-hoc rationalization rather than true inference, calling into question their inferential depth. Our study uncovers the answer-anchoring phenomenon with rigorous empirical validation and underscores the need for a more nuanced understanding of what constitutes reasoning in LLMs.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLMS）表现出令人印象深刻的推理能力，但越来越多的证据表明，它们的成功大部分源于记忆的回答 - 反应模式，而不是真正的推论。在这项工作中，我们研究了一个中心问题：LLMS主要是基于最终答案还是推理链的文本模式？我们提出了一个五级答案可见性及时框架，该框架系统地操纵答案线索，并通过间接的行为分析来探测模型行为。跨最先进的LLMS的实验表明，对明确答案有很大稳定的依赖。当答案提示被掩盖时，即使有完整的推理链，性能也下降了26.90 \％。这些发现表明，LLMS所表现出的许多推理可能反映了事后合理化而不是真实的推论，这使他们质疑其推论深度。我们的研究通过严格的经验验证揭示了答案锚定现象，并强调了对LLM中构成的推理的更加细微的理解。</li>
</ul>

<h3>Title: Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation</h3>
<ul>
<li><strong>Authors: </strong>Yang Wu, Yifan Zhang, Yurong Wu, Yuran Wang, Junkai Zhang, Jian Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17637">https://arxiv.org/abs/2506.17637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17637">https://arxiv.org/pdf/2506.17637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17637]] Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation(https://arxiv.org/abs/2506.17637)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized various domains but encounter substantial challenges in tackling optimization modeling tasks for Operations Research (OR), particularly when dealing with complex problem. In this work, we propose Step-Opt-Instruct, a framework that augments existing datasets and generates high-quality fine-tuning data tailored to optimization modeling. Step-Opt-Instruct employs iterative problem generation to systematically increase problem complexity and stepwise validation to rigorously verify data, preventing error propagation and ensuring the quality of the generated dataset. Leveraging this framework, we fine-tune open-source LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and IndustryOR. Extensive experiments demonstrate the superior performance of Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\% improvement in micro average accuracy on difficult problems. These findings highlight the effectiveness of combining structured validation with gradual problem refinement to advance the automation of decision-making processes using this http URL code and dataset are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）彻底改变了各个领域，但在解决操作研究（OR）的优化建模任务时遇到了重大挑战，尤其是在处理复杂问题时。在这项工作中，我们提出了Step-Opt-Instruct，该框架可以增加现有数据集并生成针对优化建模量身定制的高质量微调数据。 Step-Opt-Instruct采用迭代问题的产生来系统地增加问题的复杂性和逐步验证，以严格验证数据，防止错误传播并确保生成的数据集的质量。利用这个框架，我们微调了包括Llama-3-8B和Mistral-7b在内的开源LLM，以开发Step-Opt-一种模型，该模型在NL4OPT，MAMO和行业者等基准上实现了最先进的性能。广泛的实验证明了Step-Opt的出色表现，尤其是在解决复杂或任务时，在困难问题上的微平均准确性提高了17.01 \％。这些发现突出了将结构化验证与渐进问题完善相结合以使用此HTTP URL代码和数据集的决策过程自动化的有效性，可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: TPTT: Transforming Pretrained Transformer into Titans</h3>
<ul>
<li><strong>Authors: </strong>Fabien Furfaro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17671">https://arxiv.org/abs/2506.17671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17671">https://arxiv.org/pdf/2506.17671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17671]] TPTT: Transforming Pretrained Transformer into Titans(https://arxiv.org/abs/2506.17671)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have led to remarkable progress in natural language processing, but their computational and memory demands remain a significant challenge, particularly for long-context inference. We introduce TPTT (Transforming Pretrained Transformer into Titans), a novel framework for enhancing pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management. TPTT employs techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA). It is fully compatible with the Hugging Face Transformers library, enabling seamless adaptation of any causal LLM through parameter-efficient fine-tuning (LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU benchmark with models of approximately 1 billion parameters, observing substantial improvements in both efficiency and accuracy. For instance, Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its baseline. Statistical analyses and comparisons with recent state-of-the-art methods confirm the practical scalability and robustness of TPTT. Code is available at this https URL . Python package at this https URL .</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的最新进展导致了自然语言处理的显着进步，但是它们的计算和记忆需求仍然是一个重大挑战，尤其是对于长篇文章推断而言。我们将TPTT（已验证的变压器变成泰坦），这是一种新型框架，可通过有效的线性注意力机制和高级内存管理来增强经过验证的变压器模型。 TPTT采用诸如门（MAG）和混合线性注意（Liza）等记忆等技术。它与拥抱的面孔变压器库完全兼容，可以通过参数有效的微调（LORA）无缝适应任何因果LLM，而无需完整的再培训。我们通过约10亿个参数的模型显示了TPTT对MMLU基准测试的有效性，从而观察到效率和准确性的实质性提高。例如，泰坦-3.2-1b的精确匹配（EM）比基线增长了20％。统计分析和与最新方法的比较证实了TPTT的实际可扩展性和鲁棒性。代码可在此HTTPS URL上找到。此HTTPS URL的Python包装。</li>
</ul>

<h3>Title: Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Binquan Ji, Haibo Luo, Yifei Lu, Lei Hei, Jiaqi Wang, Tingjing Liao, Lingyu Wang, Shichao Wang, Feiliang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17692">https://arxiv.org/abs/2506.17692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17692">https://arxiv.org/pdf/2506.17692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17692]] Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering(https://arxiv.org/abs/2506.17692)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Knowledge-intensive multi-hop question answering (QA) tasks, which require integrating evidence from multiple sources to address complex queries, often necessitate multiple rounds of retrieval and iterative generation by large language models (LLMs). However, incorporating many documents and extended contexts poses challenges -such as hallucinations and semantic drift-for lightweight LLMs with fewer parameters. This work proposes a novel framework called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions into logically coherent subquestions to form a hallucination-free reasoning chain. It then iteratively refines these subquestions through context-aware rewriting to generate effective query formulations. For retrieval, we introduce a lightweight discriminative keyword extraction module that leverages extracted keywords to achieve targeted, precise document recall with relatively low computational overhead. Extensive experiments on three multi-hop QA datasets demonstrate that DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption. Notably, our approach attains state-of-the-art results on models with 8B parameters, showcasing its effectiveness in various scenarios, particularly in resource-constrained environments.</li>
<li><strong>摘要：</strong>知识密集的多跳问答（QA）任务需要从多个来源集成证据来解决复杂的查询，通常需要通过大语言模型（LLMS）进行多轮检索和迭代生成。但是，结合许多文档和扩展上下文带来了挑战 - 例如幻觉和语义漂移 - 轻巧的LLM，参数较少。这项工作提出了一个名为DEC（动态增强链）的新型框架。 DEC首先将复杂的问题分解为逻辑上连贯的子问题，以形成无幻觉的推理链。然后，它通过上下文感知的重写以产生有效的查询配方来迭代地完善这些子问题。对于检索，我们引入了一个轻巧的判别关键字提取模块，该模块利用提取的关键字以实现目标，精确的文档回忆，并以相对较低的计算开销。在三个多跳跃质量检查数据集上进行的广泛实验表明，DEC在同等或超过最先进的基准测试的同时大大降低了令牌消耗。值得注意的是，我们的方法在具有8B参数的模型上获得了最新的结果，在各种情况下，特别是在资源约束环境中展示其有效性。</li>
</ul>

<h3>Title: The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future</h3>
<ul>
<li><strong>Authors: </strong>Summra Saleem, Muhammad Nabeel Asim, Shaista Zulfiqar, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17700">https://arxiv.org/abs/2506.17700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17700">https://arxiv.org/pdf/2506.17700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17700]] The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future(https://arxiv.org/abs/2506.17700)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by automating traditional labor-intensive tasks and consequently accelerated the development of computer-aided applications. As researchers continue to advance this field with the introduction of novel language models and more efficient training/finetuning methodologies, the idea of prompt engineering and subsequent optimization strategies with LLMs has emerged as a particularly impactful trend to yield a substantial performance boost across diverse NLP tasks. To best of our knowledge numerous review articles have explored prompt engineering, however, a critical gap exists in comprehensive analyses of prompt optimization strategies. To bridge this gap this paper provides unique and comprehensive insights about the potential of diverse prompt optimization strategies. It analyzes their underlying working paradigms and based on these principles, categorizes them into 11 distinct classes. Moreover, the paper provides details about various NLP tasks where these prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation. This comprehensive compilation lays a robust foundation for future comparative studies and enables rigorous assessment of prompt optimization and LLM-based predictive pipelines under consistent experimental settings: a critical need in the current landscape. Ultimately, this research will centralize diverse strategic knowledge to facilitate the adaptation of existing prompt optimization strategies for development of innovative predictors across unexplored tasks.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）通过自动执行传统的劳动密集型任务，从而彻底改变了自然语言处理（NLP），并因此加速了计算机辅助应用程序的开发。随着研究人员继续引入新型语言模型和更有效的培训/填充方法，迅速工程和随后使用LLMS的优化策略的想法已成为一种特别有影响力的趋势，以产生各种NLP任务的大量性能提升。为了最好的是，我们的知识众所周知，许多审查文章都探讨了迅速的工程，但是，在迅速优化策略的全面分析中，存在一个关键的差距。为了弥合这一差距，本文提供了有关各种迅速优化策略潜力的独特而全面的见解。它分析了他们的基本工作范例并基于这些原则，将它们分为11个不同的类别。此外，本文提供了有关各种NLP任务的详细信息，其中采用了这些及时的优化策略，以及用于评估的不同LLMS和基准数据集的详细信息。这项全面的汇编为将来的比较研究奠定了强大的基础，并可以在一致的实验环境下进行严格评估迅速优化和基于LLM的预测管道：当前景观中的迫切需求。最终，这项研究将集中多种战略知识，以促进现有的迅速优化策略，以开发跨未探索任务的创新预测变量。</li>
</ul>

<h3>Title: Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages</h3>
<ul>
<li><strong>Authors: </strong>Matthias Schöffel, Esteban Garces Arias, Marinus Wiedner, Paula Ruppert, Meimingwei Li, Christian Heumann, Matthias Aßenmacher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17715">https://arxiv.org/abs/2506.17715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17715">https://arxiv.org/pdf/2506.17715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17715]] Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages(https://arxiv.org/abs/2506.17715)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Part-of-speech (POS) tagging remains a foundational component in natural language processing pipelines, particularly critical for historical text analysis at the intersection of computational linguistics and digital humanities. Despite significant advancements in modern large language models (LLMs) for ancient languages, their application to Medieval Romance languages presents distinctive challenges stemming from diachronic linguistic evolution, spelling variations, and labeled data scarcity. This study systematically investigates the central determinants of POS tagging performance across diverse corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts, spanning biblical, hagiographical, medical, and dietary domains. Through rigorous experimentation, we evaluate how fine-tuning approaches, prompt engineering, model architectures, decoding strategies, and cross-lingual transfer learning techniques affect tagging accuracy. Our results reveal both notable limitations in LLMs' ability to process historical language variations and non-standardized spelling, as well as promising specialized techniques that effectively address the unique challenges presented by low-resource historical languages.</li>
<li><strong>摘要：</strong>语音（POS）标签部分仍然是自然语言处理管道中的基础组成部分，对于计算语言学和数字人文学科交集的历史文本分析尤其重要。尽管对古代语言的现代大语模型（LLMS）取得了重大进步，但它们在中世纪浪漫语言中的应用仍带来了引起的鲜明的挑战，这些挑战源于直觉的语言演化，拼写变化和标记的数据稀缺性。这项研究系统地研究了中世纪，中世纪西班牙语和中世纪法国文本的POS标记性能的核心决定因素，涵盖了圣经的，hagiographical，医学，医学和饮食领域。通过严格的实验，我们评估了微调方法，及时的工程，模型架构，解码策略和跨语言转移学习技术会影响标记精度。我们的结果揭示了LLMS处理历史语言变化和非标准化拼写能力的显着局限性，以及有希望的专业技术，可以有效解决低资源历史语言所带来的独特挑战。</li>
</ul>

<h3>Title: KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process</h3>
<ul>
<li><strong>Authors: </strong>Dalong Zhang, Jun Xu, Jun Zhou, Lei Liang, Lin Yuan, Ling Zhong, Mengshu Sun, Peilong Zhao, QiWei Wang, Xiaorui Wang, Xinkai Du, YangYang Hou, Yu Ao, ZhaoYang Wang, Zhengke Gui, ZhiYing Yi, Zhongpu Bo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17728">https://arxiv.org/abs/2506.17728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17728">https://arxiv.org/pdf/2506.17728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17728]] KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process(https://arxiv.org/abs/2506.17728)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce KAG-Thinker, a novel human-like reasoning framework built upon a parameter-light large language model (LLM). Our approach enhances the logical coherence and contextual consistency of the thinking process in question-answering (Q\&A) tasks on domain-specific knowledge bases (KBs) within LLMs. This framework simulates human cognitive mechanisms for handling complex problems by establishing a structured thinking process. Continuing the \textbf{Logical Form} guided retrieval and reasoning technology route of KAG v0.7, firstly, it decomposes complex questions into independently solvable sub-problems(also referred to as logical forms) through \textbf{breadth decomposition}, each represented in two equivalent forms-natural language and logical function-and further classified as either Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and variables passing explicitly modeled via logical function interfaces. In the solving process, the Retrieval function is used to perform knowledge retrieval tasks, while the Math and Deduce functions are used to perform reasoning analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external knowledge sources are regarded as equivalent KBs. We use the \textbf{knowledge boundary} model to determine the optimal source using self-regulatory mechanisms such as confidence calibration and reflective reasoning, and use the \textbf{depth solving} model to enhance the comprehensiveness of knowledge acquisition. Finally, instead of utilizing reinforcement learning, we employ supervised fine-tuning with multi-turn dialogues to align the model with our structured inference paradigm, thereby avoiding excessive reflection. This is supported by a data evaluation framework and iterative corpus synthesis, which facilitate the generation of detailed reasoning trajectories...</li>
<li><strong>摘要：</strong>在本文中，我们介绍了Kag-Thinker，这是一种基于参数轻型语言模型（LLM）的新型人类般的推理框架。我们的方法增强了在LLMS中特定于领域的知识基础（KBS）上提问的思维过程（Q \＆A）任务的逻辑连贯性和上下文一致性。该框架通过建立结构化思维过程来模拟人类的认知机制来处理复杂问题。继续进行\ textbf {逻辑形式}指导的检索和推理技术途径v0.7，首先，它通过\ textbf {bradth demomposition}将复杂的问题分解为独立的可解决的子问题（也称为逻辑形式），并在两个等式的依从性和逻辑函数中代表，并以两种等效的方式进行分类，并且通过逻辑函数接口进行明确建模的变量。在求解过程中，检索功能用于执行知识检索任务，而数学和推导功能则用于执行推理分析任务。其次，值得注意的是，在知识检索子问题任务中，LLM和外部知识源被视为等效的KB。我们使用\ textbf {知识边界}模型使用自我调节机制（例如置信度校准和反思性推理）来确定最佳源，并使用\ textbf {depth soluving}模型来增强知识习得的全面性。最后，我们没有利用强化学习，而是采用了通过多转口对话的监督微调来使模型与我们的结构化推理范式保持一致，从而避免了过度反思。这是由数据评估框架和迭代语料库综合支持的，这有助于生成详细的推理轨迹...</li>
</ul>

<h3>Title: HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations</h3>
<ul>
<li><strong>Authors: </strong>Anwoy Chatterjee, Yash Goel, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17748">https://arxiv.org/abs/2506.17748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17748">https://arxiv.org/pdf/2506.17748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17748]] HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations(https://arxiv.org/abs/2506.17748)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Contemporary Language Models (LMs), while impressively fluent, often generate content that is factually incorrect or unfaithful to the input context - a critical issue commonly referred to as 'hallucination'. This tendency of LMs to generate hallucinated content undermines their reliability, especially because these fabrications are often highly convincing and therefore difficult to detect. While several existing methods attempt to detect hallucinations, most rely on analyzing multiple generations per input, leading to increased computational cost and latency. To address this, we propose a single-pass, training-free approach for effective Hallucination detectIon via Decoupled rEpresentations (HIDE). Our approach leverages the hypothesis that hallucinations result from a statistical decoupling between an LM's internal representations of input context and its generated output. We quantify this decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to hidden-state representations extracted while generating the output sequence. We conduct extensive experiments on four diverse question answering datasets, evaluating both faithfulness and factuality hallucinations across six open-source LMs of varying scales and properties. Our results demonstrate that HIDE outperforms other single-pass methods in almost all settings, achieving an average relative improvement of ~29% in AUC-ROC over the best-performing single-pass strategy across various models and datasets. Additionally, HIDE shows competitive and often superior performance with multi-pass state-of-the-art methods, obtaining an average relative improvement of ~3% in AUC-ROC while consuming ~51% less computation time. Our findings highlight the effectiveness of exploiting internal representation decoupling in LMs for efficient and practical hallucination detection.</li>
<li><strong>摘要：</strong>当代语言模型（LMS）虽然令人印象深刻，但通常会产生对输入上下文的事实不正确或不忠的内容 - 通常称为“幻觉”的关键问题。 LMS产生幻觉内容的这种趋势破坏了它们的可靠性，尤其是因为这些捏造通常具有高度说服力，因此很难检测到。尽管现有的几种方法试图检测幻觉，但大多数方法都依赖于每个输入分析多代，从而增加了计算成本和延迟。为了解决这个问题，我们提出了一种通过解耦表示（hide）进行有效幻觉检测的单通道，无训练的方法。我们的方法利用了这样的假设，即幻觉是由于LM的输入上下文内部表示与其生成的输出之间的统计解耦而产生的。我们使用应用于在生成输出序列时提取的隐藏状态表示形式的Hilbert-Schmidt独立标准（HSIC）来量化此解耦。我们对四个不同的问题回答数据集进行了广泛的实验，评估了六个不同尺度和属性的开源LMS中的忠诚和事实幻觉。我们的结果表明，在几乎所有设置中，隐藏率都优于其他单通道方法，而在各种模型和数据集中，AUC-ROC的平均相对相对相对的平均相对改善比表现最佳的单频道策略的平均相对改善。此外，Hide通过多通最先进的方法表现出竞争性和卓越的性能，在AUC-ROC中，平均相对改善的平均相对改善，而计算时间却少约51％。我们的发现突出了利用内部表示在LMS中取消效率和实用幻觉检测的有效性。</li>
</ul>

<h3>Title: LLMs for Customized Marketing Content Generation and Evaluation at Scale</h3>
<ul>
<li><strong>Authors: </strong>Haoran Liu, Amir Tahmasbi, Ehtesham Sam Haque, Purak Jain</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17863">https://arxiv.org/abs/2506.17863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17863">https://arxiv.org/pdf/2506.17863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17863]] LLMs for Customized Marketing Content Generation and Evaluation at Scale(https://arxiv.org/abs/2506.17863)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Offsite marketing is essential in e-commerce, enabling businesses to reach customers through external platforms and drive traffic to retail websites. However, most current offsite marketing content is overly generic, template-based, and poorly aligned with landing pages, limiting its effectiveness. To address these limitations, we propose MarketingFM, a retrieval-augmented system that integrates multiple data sources to generate keyword-specific ad copy with minimal human intervention. We validate MarketingFM via offline human and automated evaluations and large-scale online A/B tests. In one experiment, keyword-focused ad copy outperformed templates, achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC, demonstrating gains in ad ranking and cost efficiency. Despite these gains, human review of generated ads remains costly. To address this, we propose AutoEval-Main, an automated evaluation system that combines rule-based metrics with LLM-as-a-Judge techniques to ensure alignment with marketing principles. In experiments with large-scale human annotations, AutoEval-Main achieved 89.57% agreement with human reviewers. Building on this, we propose AutoEval-Update, a cost-efficient LLM-human collaborative framework to dynamically refine evaluation prompts and adapt to shifting criteria with minimal human input. By selectively sampling representative ads for human review and using a critic LLM to generate alignment reports, AutoEval-Update improves evaluation consistency while reducing manual effort. Experiments show the critic LLM suggests meaningful refinements, improving LLM-human agreement. Nonetheless, human oversight remains essential for setting thresholds and validating refinements before deployment.</li>
<li><strong>摘要：</strong>异地营销在电子商务中至关重要，使企业能够通过外部平台吸引客户并推动流量到零售网站。但是，大多数当前的异地营销内容过于通用，基于模板，并且与着陆页不符，从而限制了其有效性。为了解决这些限制，我们建议MarketingFM，这是一种检索功能的系统，该系统集成了多个数据源，以生成特定于关键字的广告副本，而人类干预最少。我们通过离线人类和自动化评估以及大规模在线A/B测试来验证MarketingFM。在一个实验中，以关键字为中心的广告复制优于模板，高达9％的CTR，增加了12％的印象，CPC降低了0.38％，表明广告排名和成本效率的提高。尽管有这些收益，但人类对产生的广告的评论仍然昂贵。为了解决这个问题，我们提出了AutoEval-Main，这是一种自动化评估系统，将基于规则的指标与LLM-AS-A-A-Gudge技术相结合，以确保与营销原则保持一致。在大规模人类注释的实验中，自动eval-Main与人类审稿人达成了89.57％的一致性。在此基础上，我们提出了一个自动播音，这是一种具有成本效益的LLM-Human协作框架，以动态地完善评估提示并适应以最少的人类输入来转移标准。通过选择性地对人类审查的代表性广告进行选择，并使用评论家LLM生成一致性报告，自动逐渐升级可以提高评估的一致性，同时减少手动努力。实验表明，评论家LLM提出了有意义的改进，从而改善了LLM-Human协议。但是，人类的监督对于在部署前设定阈值和验证改进仍然至关重要。</li>
</ul>

<h3>Title: QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Taolin Zhang, Haidong Kang, Dongyang Li, Qizhou Chen, Chengyu Wang Xiaofeng He, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17864">https://arxiv.org/abs/2506.17864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17864">https://arxiv.org/pdf/2506.17864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17864]] QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs(https://arxiv.org/abs/2506.17864)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have demonstrated impressive results but still suffer from hallucinations. Model editing has been proposed to correct factual inaccuracies in LLMs. A challenging case is sequential model editing (SME), which aims to rectify errors continuously rather than treating them as a one-time task. During SME, the general capabilities of LLMs can be negatively affected due to the introduction of new parameters. In this paper, we propose a queue-based self-correction framework (QueueEDIT) that not only enhances SME performance by addressing long-sequence dependency but also mitigates the impact of parameter bias on the general capabilities of LLMs. Specifically, we first introduce a structural mapping editing loss to map the triplets to the knowledge-sensitive neurons within the Transformer layers of LLMs. We then store the located parameters for each piece of edited knowledge in a queue and dynamically align previously edited parameters. In each edit, we select queue parameters most relevant to the currently located parameters to determine whether previous knowledge needs realignment. Irrelevant parameters in the queue are frozen, and we update the parameters at the queue head to the LLM to ensure they do not harm general abilities. Experiments show that our framework significantly outperforms strong baselines across various SME settings and maintains competitiveness in single-turn editing. The resulting LLMs also preserve high capabilities in general NLP tasks throughout the SME process.</li>
<li><strong>摘要：</strong>最近，大型语言模型（LLMS）表现出了令人印象深刻的结果，但仍然患有幻觉。已经提出了模型编辑来纠正LLM中的事实不准确性。一个具有挑战性的情况是顺序模型编辑（SME），旨在连续纠正错误，而不是将其视为一次任务。在中小型企业期间，由于引入新参数，LLM的一般功能可能会受到负面影响。在本文中，我们提出了一个基于队列的自校正框架（Queueedit），该框架不仅通过解决长期依赖性来增强中小企业的性能，还可以减轻参数偏差对LLMS一般能力的影响。具体而言，我们首先引入结构映射编辑损失，以将三重态映射到LLMS变压器层中的知识敏感神经元。然后，我们在队列中存储每个编辑知识的位置参数，并动态对齐先前编辑的参数。在每个编辑中，我们选择与当前位置的参数最相关的队列参数，以确定先前的知识是否需要重组。队列中的无关参数被冷冻，我们在队列头部更新了LLM的参数，以确保它们不会损害一般能力。实验表明，我们的框架在各种中小企业环境中的表现明显优于强大的基线，并保持单转编辑的竞争力。在整个中小型企业过程中，最终的LLM还可以在NLP一般任务中保留高功能。</li>
</ul>

<h3>Title: How Alignment Shrinks the Generative Horizon</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Yang, Ari Holtzman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17871">https://arxiv.org/abs/2506.17871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17871">https://arxiv.org/pdf/2506.17871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17871]] How Alignment Shrinks the Generative Horizon(https://arxiv.org/abs/2506.17871)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity.</li>
<li><strong>摘要：</strong>尽管具有令人印象深刻的能力，但对齐的大型语言模型（LLM）通常会产生缺乏多样性的输出。是什么驱动了这一代人的稳定性？我们通过模型的输出分布中的概率浓度镜头研究了这一现象。为了量化这种浓度，我们介绍了分支因子（BF） - 代币不变的量度对生成过程中合理的下一步有效数量。我们的经验分析揭示了两个关键发现：（1）BF通常随着一代的进展而减少，这表明LLMS随着生成而变得更可预测。 （2）对齐调整从一开始就显着提高模型的输出分布，相对于基本模型，BF几乎将BF降低了几乎一个数量级（例如，从12到1.2）。这种鲜明的减少有助于解释为什么对齐模型通常对解码策略的敏感程度不那么敏感。在这种见识的基础上，我们发现这种稳定对复杂的推理具有令人惊讶的含义。例如，对齐链链（COT）模型（例如，DeepSeek-Distled模型），例如利用此效果；通过产生更长的推理链，它们可以将生成稍后发展，更确定性（下BF）阶段，从而产生更稳定的输出。我们假设对齐调整并不能从根本上改变模型的行为，而是将其转向基本模型中已经存在的低渗透轨迹的风格代币（例如“肯定”）。轻推实验支持了这种观点，这些实验表明，提示具有这种令牌的基本模型可以类似地降低BF。我们的发现共同将BF建立为理解和控制LLM输出的有力诊断 - 阐明了对齐方式如何降低可变性，COT如何促进稳定的世代以及如何将基本模型从多样性中转移出来。</li>
</ul>

<h3>Title: Multi-turn Jailbreaking via Global Refinement and Active Fabrication</h3>
<ul>
<li><strong>Authors: </strong>Hua Tang, Lingyong Yan, Yukun Zhao, Shuaiqiang Wang, Jizhou Huang, Dawei Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17881">https://arxiv.org/abs/2506.17881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17881">https://arxiv.org/pdf/2506.17881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17881]] Multi-turn Jailbreaking via Global Refinement and Active Fabrication(https://arxiv.org/abs/2506.17881)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved exceptional performance across a wide range of tasks. However, they still pose significant safety risks due to the potential misuse for malicious purposes. Jailbreaks, which aim to elicit models to generate harmful content, play a critical role in identifying the underlying security threats. Recent jailbreaking primarily focuses on single-turn scenarios, while the more complicated multi-turn scenarios remain underexplored. Moreover, existing multi-turn jailbreaking techniques struggle to adapt to the evolving dynamics of dialogue as the interaction progresses. To address this limitation, we propose a novel multi-turn jailbreaking method that refines the jailbreaking path globally at each interaction. We also actively fabricate model responses to suppress safety-related warnings, thereby increasing the likelihood of eliciting harmful outputs in subsequent questions. Experimental results demonstrate the superior performance of our method compared with existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在各种任务中都取得了出色的表现。但是，由于恶意目的的潜在滥用，它们仍然构成了严重的安全风险。旨在激发模型产生有害内容的越狱，在确定潜在的安全威胁方面发挥了关键作用。最近的越狱主要集中在单转情况下，而更复杂的多圈场景仍然没有被淘汰。此外，随着互动的进行，现有的多转弯越狱技术难以适应对话的不断发展的动力。为了解决这一限制，我们提出了一种新颖的多转越越狱方法，该方法在每次互动时在全球范围内完善了越狱的道路。我们还积极制定模型响应以抑制与安全有关的警告，从而增加了在随后的问题中引起有害产出的可能性。实验结果表明，与六个最先进的LLM的现有单转弯和多转越狱”技术相比，我们方法的性能优越。我们的代码在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Hong Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17949">https://arxiv.org/abs/2506.17949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17949">https://arxiv.org/pdf/2506.17949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17949]] Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation(https://arxiv.org/abs/2506.17949)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit strong capabilities in reproducing and extending patterns observed during pretraining but often struggle to generalize novel ideas beyond their original context. This paper addresses the challenge of applying such localized innovations - introduced at a specific stage or component - to other parts of a multi-stage process. We propose a scatter-based innovation expansion model (innovation scatter model) that guides the LLM through a four-step process: (1) identifying the core innovation by comparing the user's input with its surrounding context, (2) generalizing the innovation by removing references to specific stages or components, (3) determining whether the generalized innovation applies to a broader scope beyond the original stage, and (4) systematically applying it to other structurally similar stages using the LLM. This model leverages structural redundancy across stages to improve the applicability of novel ideas. Verification results demonstrate that the innovation scatter model enables LLMs to extend innovations across structurally similar stages, thereby enhancing generalization and reuse.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在训练过程中观察到的复制和扩展模式表现出很强的能力，但经常努力地概括到原始背景之外的新颖思想。本文解决了将这种本地化创新（在特定阶段或组件引入）应用于多阶段过程的其他部分的挑战。我们提出了一个基于散射的创新扩展模型（创新散射模型），该模型通过四步过程指导LLM：（1）通过将用户的输入与周围环境进行比较来识别核心创新，（2）通过将创新概括为通过将特定阶段或组件删除的特定阶段进行删除，（3）确定本身的范围，（3）范围不超出特定阶段的范围，（3）范围不超出特定阶段的范围，（3）范围不超出了特定阶段的范围，（3）范围是范围的。使用LLM结构相似的阶段。该模型利用跨阶段的结构冗余来提高新思想的适用性。验证结果表明，创新散点模型使LLM能够在结构相似的阶段扩展创新，从而增强概括和重用。</li>
</ul>

<h3>Title: A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Quanwei Tang, Sophia Yat Mei Lee, Junshuang Wu, Dong Zhang, Shoushan Li, Erik Cambria, Guodong Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17951">https://arxiv.org/abs/2506.17951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17951">https://arxiv.org/pdf/2506.17951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17951]] A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment(https://arxiv.org/abs/2506.17951)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in retrieval-augmented generation (RAG) have enhanced large language models in question answering by integrating external knowledge. However, challenges persist in achieving global understanding and aligning responses with human ethical and quality preferences. To address these issues, we propose GraphMPA, a comprehensive graph-based framework with mode-seeking preference alignment. Our approach constructs a hierarchical document graph using a general similarity measurement, mimicking human cognitive processes for information understanding and synthesis. Additionally, we introduce mode-seeking preference optimization to better align model outputs with human preferences through probability-matching constraints. Extensive experiments on six datasets demonstrate the effectiveness of our \href{this https URL}{GraphMPA}.</li>
<li><strong>摘要：</strong>在检索增强生成（RAG）中的最新进展已通过整合外部知识来增强所讨论的大型语言模型。但是，挑战持续到实现全球理解和与人类道德和质量偏好的回应保持一致。为了解决这些问题，我们提出了GraphMPA，这是一个综合基于图形的框架，具有模式的偏好对齐方式。我们的方法使用一般相似性测量构建了分层文档图，模仿了人类认知过程以了解信息理解和综合。此外，我们通过概率匹配的约束将寻求模式的首选项优化介绍，以更好地对齐模型输出与人类偏好。六个数据集上的大量实验证明了我们的\ href {this HTTPS url} {graphMpa}的有效性。</li>
</ul>

<h3>Title: PDF Retrieval Augmented Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Thi Thu Uyen Hoang, Viet Anh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18027">https://arxiv.org/abs/2506.18027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18027">https://arxiv.org/pdf/2506.18027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18027]] PDF Retrieval Augmented Question Answering(https://arxiv.org/abs/2506.18027)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>This paper presents an advancement in Question-Answering (QA) systems using a Retrieval Augmented Generation (RAG) framework to enhance information extraction from PDF files. Recognizing the richness and diversity of data within PDFs--including text, images, vector diagrams, graphs, and tables--poses unique challenges for existing QA systems primarily designed for textual content. We seek to develop a comprehensive RAG-based QA system that will effectively address complex multimodal questions, where several data types are combined in the query. This is mainly achieved by refining approaches to processing and integrating non-textual elements in PDFs into the RAG framework to derive precise and relevant answers, as well as fine-tuning large language models to better adapt to our system. We provide an in-depth experimental evaluation of our solution, demonstrating its capability to extract accurate information that can be applied to different types of content across PDFs. This work not only pushes the boundaries of retrieval-augmented QA systems but also lays a foundation for further research in multimodal data integration and processing.</li>
<li><strong>摘要：</strong>本文使用检索增强生成（RAG）框架提出了提出索问题（QA）系统的进步，以增强从PDF文件中提取信息。认识到PDF中数据的丰富性和多样性 - 包括文本，图像，矢量图，图形和表格 - 对主要用于文本内容设计的现有QA系统构成了独特的挑战。我们试图开发一个基于抹布的质量检查系统，该系统将有效地解决复杂的多模式问题，在查询中将几种数据类型组合在一起。这主要是通过完善处理和集成PDF中的非文本元素的方法来实现的，以得出精确和相关的答案，以及微调大语言模型以更好地适应我们的系统。我们对解决方案提供了深入的实验评估，证明了它可以提取可以应用于PDF的不同类型内容的准确信息的能力。这项工作不仅突破了检索式质量检查系统的界限，而且还为多模式数据集成和处理的进一步研究奠定了基础。</li>
</ul>

<h3>Title: Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aziz Amari (1), Mohamed Achref Ben Ammar (1) ((1) National Institute of Applied Science and Technology (INSAT), University of Carthage, Tunis, Tunisia)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18036">https://arxiv.org/abs/2506.18036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18036">https://arxiv.org/pdf/2506.18036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18036]] Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models(https://arxiv.org/abs/2506.18036)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The rapid expansion of information from diverse sources has heightened the need for effective automatic text summarization, which condenses documents into shorter, coherent texts. Summarization methods generally fall into two categories: extractive, which selects key segments from the original text, and abstractive, which generates summaries by rephrasing the content coherently. Large language models have advanced the field of abstractive summarization, but they are resourceintensive and face significant challenges in retaining key information across lengthy documents, which we call being "lost in the middle". To address these issues, we propose a hybrid summarization approach that combines extractive and abstractive techniques. Our method splits the document into smaller text chunks, clusters their vector embeddings, generates a summary for each cluster that represents a key idea in the document, and constructs the final summary by relying on a Markov chain graph when selecting the semantic order of ideas.</li>
<li><strong>摘要：</strong>来自不同来源的信息的快速扩展增加了对有效自动文本摘要的需求，从而将文档凝结成较短的连贯文本。摘要方法通常分为两个类别：提取性，从原始文本中选择关键段，而抽象性则可以通过相干地重塑内容来生成摘要。大型语言模型已经提出了抽象性摘要领域，但它们是资源密集型的，并且在保留漫长文档的关键信息方面面临重大挑战，我们称这是“中间丢失”。为了解决这些问题，我们提出了一种结合提取性和抽象技术的混合摘要方法。我们的方法将文档分为较小的文本块，将其矢量嵌入，为每个集群生成一个代表文档中一个关键思想的摘要，并在选择想法的语义顺序时依靠Markov链图来构建最终摘要。</li>
</ul>

<h3>Title: Statistical Multicriteria Evaluation of LLM-Generated Text</h3>
<ul>
<li><strong>Authors: </strong>Esteban Garces Arias, Hannah Blocher, Julian Rodemann, Matthias Aßenmacher, Christoph Jansen</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18082">https://arxiv.org/abs/2506.18082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18082">https://arxiv.org/pdf/2506.18082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18082]] Statistical Multicriteria Evaluation of LLM-Generated Text(https://arxiv.org/abs/2506.18082)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Assessing the quality of LLM-generated text remains a fundamental challenge in natural language processing. Current evaluation approaches often rely on isolated metrics or simplistic aggregations that fail to capture the nuanced trade-offs between coherence, diversity, fluency, and other relevant indicators of text quality. In this work, we adapt a recently proposed framework for statistical inference based on Generalized Stochastic Dominance (GSD) that addresses three critical limitations in existing benchmarking methodologies: the inadequacy of single-metric evaluation, the incompatibility between cardinal automatic metrics and ordinal human judgments, and the lack of inferential statistical guarantees. The GSD-front approach enables simultaneous evaluation across multiple quality dimensions while respecting their different measurement scales, building upon partial orders of decoding strategies, thus avoiding arbitrary weighting of the involved metrics. By applying this framework to evaluate common decoding strategies against human-generated text, we demonstrate its ability to identify statistically significant performance differences while accounting for potential deviations from the i.i.d. assumption of the sampling design.</li>
<li><strong>摘要：</strong>评估LLM生成的文本的质量仍然是自然语言处理中的基本挑战。当前的评估方法通常依赖于孤立的指标或简单的聚合，这些指标无法捕获连贯性，多样性，流利度和其他相关文本质量指标之间的细微折衷。在这项工作中，我们根据广义随机优势（GSD）调整了一个最近提出的统计推断框架，该框架解决了现有基准测试方法中的三个临界局限性：单项评估的不足，基本自动指标和典型人类判断和缺乏地下校正校准保证之间的不兼容性。 GSD-Front方法可以跨多个质量维度进行同时评估，同时尊重其不同的测量量表，并基于解码策略的部分顺序，从而避免对所涉及的指标进行任意加权。通过应用此框架来评估针对人类生成的文本的共同解码策略，我们证明了其能够识别统计学上显着的性能差异的能力，同时考虑了与I.I.D.的潜在偏差。采样设计的假设。</li>
</ul>

<h3>Title: Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution</h3>
<ul>
<li><strong>Authors: </strong>Patrik Stano, Aleš Horák</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18091">https://arxiv.org/abs/2506.18091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18091">https://arxiv.org/pdf/2506.18091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18091]] Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution(https://arxiv.org/abs/2506.18091)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Anaphora resolution plays a critical role in natural language understanding, especially in morphologically rich languages like Czech. This paper presents a comparative evaluation of two modern approaches to anaphora resolution on Czech text: prompt engineering with large language models (LLMs) and fine-tuning compact generative models. Using a dataset derived from the Prague Dependency Treebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2 and Llama 3, using a series of prompt templates. We compare them against fine-tuned variants of the mT5 and Mistral models that we trained specifically for Czech anaphora resolution. Our experiments demonstrate that while prompting yields promising few-shot results (up to 74.5% accuracy), the fine-tuned models, particularly mT5-large, outperform them significantly, achieving up to 88% accuracy while requiring fewer computational resources. We analyze performance across different anaphora types, antecedent distances, and source corpora, highlighting key strengths and trade-offs of each approach.</li>
<li><strong>摘要：</strong>Anaphora解决方案在自然语言理解中起着至关重要的作用，尤其是在像捷克这样的形态丰富的语言中。本文对捷克文本的两种现代方法进行了比较评估：具有大语言模型（LLMS）和微调紧凑型生成模型的及时工程。使用从布拉格依赖性树库得出的数据集，我们使用一系列及时的模板评估了几个指令调整的LLM，包括Mistral flow 2和Llama 3。我们将它们与MT5和Mistral模型的微调变体进行了比较，这些变体专门为捷克的Ataphora分辨率进行了比较。我们的实验表明，尽管提示产生有希望的很少的结果（精度高达74.5％），但微型模型，尤其是MT5-LARGE，却明显优于它们，最多达到了88％的精度，同时需要更少的计算资源。我们分析了不同的图类类型，前距离和源代码语料库的性能，突出了每种方法的关键优势和权衡。</li>
</ul>

<h3>Title: InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating</h3>
<ul>
<li><strong>Authors: </strong>Fuyu Wang, Jiangtong Li, Kun Zhu, Changjun Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18102">https://arxiv.org/abs/2506.18102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18102">https://arxiv.org/pdf/2506.18102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18102]] InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating(https://arxiv.org/abs/2506.18102)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, chain-of-thought</a></li>
<li><strong>Abstract: </strong>With the rapid advancements in large language models (LLMs), debating tasks, such as argument quality assessment and debate process simulation, have made significant progress. However, existing LLM-based debating systems focus on responding to specific arguments while neglecting objective assessments such as authenticity and logical validity. Furthermore, these systems lack a structured approach to optimize across various dimensions$-$including evaluation metrics, chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby limiting their effectiveness. To address these interconnected challenges, we propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel evaluation system that establishes a multi-dimensional assessment architecture incorporating four subjective criteria (emotional appeal, argument clarity, argument arrangement, and topic relevance) alongside two objective metrics (fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an optimized debating framework employing a phased optimization approach through CoT reasoning enhancement, multi-dimensional Direct Preference Optimization (DPO), and real-time knowledge grounding via web-based Retrieval Augmented Generation (Web-RAG). Empirical evaluations demonstrate that $\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert judgments compared to existing methods, while $\textbf{InspireDebate}$ shows significant improvements, outperforming baseline models by 57$\%$. Source code is available at this https URL.</li>
<li><strong>摘要：</strong>随着大语言模型（LLM）的快速发展，辩论任务（例如参数质量评估和辩论过程模拟）取得了重大进展。但是，现有的基于LLM的辩论系统着重于对特定论点做出响应，同时忽略了客观评估，例如真实性和逻辑有效性。此外，这些系统缺乏一种结构化的方法来在各个方面进行优化 - 包括评估指标，思想链（COT）推理和多转变的辩论完善$  - 从而限制其有效性。为了应对这些相互联系的挑战，我们提出了一个双重组件框架：（1）$ \ textbf {InspeScore} $，一种新的评估系统，建立了一个多维评估体系结构，涵盖了四个主观标准（情感上的吸引力，论点清晰，论点和主题相关性，以及两个客观的目标量化）（事实上的授权）（伴随两种客观性的验证）（真实性的认证）; （2）$ \ textbf {Inspiredebate} $，一个优化的辩论框架，通过COT推理增强，多维直接偏好优化（DPO），采用分阶段的优化方法，并通过基于Web的基于Web的检索回程增强生成（Web-rag）通过实时知识接地。经验评估表明，与现有方法相比，$ \ textbf {inspeScore} $与专家判断的相关性更高44 $ \％$，而$ \ textbf {Inspeciredebate} $显示出显着的改进，超出基线模型的型号超过57 $ \％$ \％。源代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Fu, Zhemin Huang, Liuxin Yang, Yumeng Lu, Zhongdongming Dai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18105">https://arxiv.org/abs/2506.18105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18105">https://arxiv.org/pdf/2506.18105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18105]] Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use(https://arxiv.org/abs/2506.18105)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Chinese idioms (Chengyu) are concise four-character expressions steeped in history and culture, whose literal translations often fail to capture their full meaning. This complexity makes them challenging for language models to interpret and use correctly. Existing benchmarks focus on narrow tasks - multiple-choice cloze tests, isolated translation, or simple paraphrasing. We introduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1) Evaluative Connotation, classifying idioms as positive or negative; (2) Appropriateness, detecting incorrect idiom usage in context; and (3) Open Cloze, filling blanks in longer passages without options. Chengyu-Bench comprises 2,937 human-verified examples covering 1,765 common idioms sourced from diverse corpora. We evaluate leading LLMs and find they achieve over 95% accuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40% top-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise from fundamental misunderstandings of idiom meanings. Chengyu-Bench demonstrates that while LLMs can reliably gauge idiom sentiment, they still struggle to grasp the cultural and contextual nuances essential for proper usage. The benchmark and source code are available at: this https URL.</li>
<li><strong>摘要：</strong>中国习语（木牛）是浸入历史和文化的简洁四个字符的表情，其字面翻译通常无法捕捉到它们的全部含义。这种复杂性使他们对语言模型的解释和使用挑战。现有的基准将重点放在狭窄的任务上 - 多项选择披肩测试，孤立的翻译或简单的释义。我们介绍了成都基础，这是一个全面的基准，其中包含三个任务：（1）评估含义，将成语分类为正面或负面； （2）适当性，在上下文中检测不正确的成语使用； （3）打开披肩，在没有选项的情况下填充空白。 Chengyu Bench包括2,937个人涉及的例子，其中涵盖了来自多元化语料库的1,765个普通成语。我们评估了领先的LLM，并发现它们在评估含义上的精度超过95％，但在适当性上只有〜85％，开放式披肩的TOP-1精度约为40％。错误分析表明，大多数错误来自对成语含义的基本误解。 Chengyu Bench表明，尽管LLM可以可靠地衡量成语，但他们仍然很难掌握适当使用必不可少的文化和背景细微差别。基准和源代码可在以下网址提供：此HTTPS URL。</li>
</ul>

<h3>Title: Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Batool Haider, Atmika Gorti, Aman Chadha, Manas Gaur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18116">https://arxiv.org/abs/2506.18116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18116">https://arxiv.org/pdf/2506.18116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18116]] Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives(https://arxiv.org/abs/2506.18116)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) in mental healthcare risk propagating biases that reinforce stigma and harm marginalized groups. While previous research identified concerning trends, systematic methods for detecting intersectional biases remain limited. This work introduces a multi-hop question answering (MHQA) framework to explore LLM response biases in mental health discourse. We analyze content from the Interpretable Mental Health Instruction (IMHI) dataset across symptom presentation, coping mechanisms, and treatment approaches. Using systematic tagging across age, race, gender, and socioeconomic status, we investigate bias patterns at demographic intersections. We evaluate four LLMs: Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic disparities across sentiment, demographics, and mental health conditions. Our MHQA approach demonstrates superior detection compared to conventional methods, identifying amplification points where biases magnify through sequential reasoning. We implement two debiasing techniques: Roleplay Simulation and Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot prompting with BBQ dataset examples. These findings highlight critical areas where LLMs reproduce mental healthcare biases, providing actionable insights for equitable AI development.</li>
<li><strong>摘要：</strong>心理保健风险中的大型语言模型（LLM）传播偏见，从而增强了污名和伤害边缘化群体。虽然先前的研究确定了有关趋势的研究，但检测交叉偏见的系统方法仍然有限。这项工作介绍了一个多跳问题回答（MHQA）框架，以探讨心理健康话语中LLM的回答偏见。我们在症状表现，应对机制和治疗方法中分析了可解释的心理健康教学（IMHI）数据集的内容。使用跨年龄，种族，性别和社会经济地位的系统标签，我们研究人口相交时的偏见模式。我们评估了四个LLM：Claude 3.5十四行诗，Jamba 1.6，Gemma 3和Llama 4，揭示了情感，人口统计和心理健康状况之间的系统差异。与常规方法相比，我们的MHQA方法表明了优越的检测，从而确定了通过顺序推理偏见放大的放大点。我们实施了两种辩论技术：角色扮演模拟和显式偏差减少，通过使用烧烤数据集示例的几个弹药提示来实现66-94％的偏差减少。这些发现突出了LLM重现心理保健偏见的关键领域，为公平的AI开发提供了可行的见解。</li>
</ul>

<h3>Title: $ϕ^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bugra Kilictas, Faruk Alpay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18129">https://arxiv.org/abs/2506.18129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18129">https://arxiv.org/pdf/2506.18129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18129]] $ϕ^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models(https://arxiv.org/abs/2506.18129)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>We identify a critical vulnerability in autoregressive transformer language models where the em dash token induces recursive semantic drift, leading to clause boundary hallucination and embedding space entanglement. Through formal analysis of token-level perturbations in semantic lattices, we demonstrate that em dash insertion fundamentally alters the model's latent representations, causing compounding errors in long-form generation. We propose a novel solution combining symbolic clause purification via the phi-infinity operator with targeted embedding matrix realignment. Our approach enables total suppression of problematic tokens without requiring model retraining, while preserving semantic coherence through fixed-point convergence guarantees. Experimental validation shows significant improvements in generation consistency and topic maintenance. This work establishes a general framework for identifying and mitigating token-level vulnerabilities in foundation models, with immediate implications for AI safety, model alignment, and robust deployment of large language models in production environments. The methodology extends beyond punctuation to address broader classes of recursive instabilities in neural text generation systems.</li>
<li><strong>摘要：</strong>我们确定了自回归变压器语言模型中的关键脆弱性，其中EM DASH代币引起递归语义漂移，从而导致子句边界幻觉和嵌入空间纠缠。通过对语义晶格中令牌级的扰动的正式分析，我们证明了EM破折号插入从根本上改变了模型的潜在表示，从而导致长期产生的复杂错误。我们提出了一种新的解决方案，该解决方案通过Phi-Infinity操作员与靶向嵌入矩阵重新调整相结合的符号子句纯化。我们的方法可以完全抑制有问题的令牌，而无需模型重新训练，同时通过固定点收敛保证来保持语义连贯性。实验验证显示出发电一致性和主题维护的显着改善。这项工作建立了一个通用框架，用于在基础模型中识别和减轻令牌级别的漏洞，对AI安全，模型对准以及在生产环境中的大型语言模型的强大部署产生了直接影响。该方法扩展到超出标点符号，以解决神经文本生成系统中更广泛的递归不稳定性类别。</li>
</ul>

<h3>Title: Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruixuan Deng, Xiaoyang Hu, Miles Gilberti, Shane Storks, Aman Taxali, Mike Angstadt, Chandra Sripada, Joyce Chai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18141">https://arxiv.org/abs/2506.18141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18141">https://arxiv.org/pdf/2506.18141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18141]] Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models(https://arxiv.org/abs/2506.18141)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We identify semantically coherent, context-consistent network components in large language models (LLMs) using coactivation of sparse autoencoder (SAE) features collected from just a handful of prompts. Focusing on country-relation tasks, we show that ablating semantic components for countries and relations changes model outputs in predictable ways, while amplifying these components induces counterfactual responses. Notably, composing relation and country components yields compound counterfactual outputs. We find that, whereas most country components emerge from the very first layer, the more abstract relation components are concentrated in later layers. Furthermore, within relation components themselves, nodes from later layers tend to have a stronger causal impact on model outputs. Overall, these findings suggest a modular organization of knowledge within LLMs and advance methods for efficient, targeted model manipulation.</li>
<li><strong>摘要：</strong>我们使用稀疏自动编码器（SAE）的共同激活中的大型语言模型（LLMS）中的语义相干，上下文吻合的网络组件，仅从少数提示中收集的功能。为了关注国家关系任务，我们表明，为国家和关系变化的语义组成部分以可预测的方式变化模型，同时放大这些组件会引起反事实响应。值得注意的是，组成的关系和国家组成部分产生了复合反事实输出。我们发现，尽管大多数国家组成部分都从第一层出现，但更抽象的关系组成部分集中在以后的层中。此外，在关系组件本身中，后来的层的节点往往会对模型输出产生更大的因果影响。总体而言，这些发现表明，LLM中的知识模块化组织和提前的方法，以实现有针对性的模型操作。</li>
</ul>

<h3>Title: CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zihan Liang, Ziwen Pan, Sumon Kanti Dey, Azra Ismail</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18185">https://arxiv.org/abs/2506.18185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18185">https://arxiv.org/pdf/2506.18185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18185]] CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers(https://arxiv.org/abs/2506.18185)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>This paper presents our system for the SMM4H-HeaRD 2025 shared tasks, specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2). Task 4 focused on detecting mentions of insomnia in clinical notes, while Task 5 addressed the extraction of food safety events from news articles. We participated in all subtasks and report key findings across them, with particular emphasis on Task 5 Subtask 1, where our system achieved strong performance-securing first place with an F1 score of 0.958 on the test set. To attain this result, we employed encoder-based models (e.g., RoBERTa), alongside GPT-4 for data augmentation. This paper outlines our approach, including preprocessing, model architecture, and subtask-specific adaptations</li>
<li><strong>摘要：</strong>本文介绍了我们针对SMM4H Heard 2025共享任务的系统，特别是任务4（子任务1、2a和2b）和任务5（子任务1和2）。任务4的重点是检测临床笔记中失眠症的提及，而任务5涉及从新闻文章中提取食品安全事件。我们参与了所有子任务，并报告了其中的关键发现，并特别强调了任务5子任务1，在该测试集中，我们的系统以F1分数为0.958，在该任务5子任务中获得了强大的绩效率。为了获得此结果，我们使用了基于编码器的模型（例如Roberta），以及GPT-4进行数据增强。本文概述了我们的方法，包括预处理，模型体系结构和特定于子任务的适应</li>
</ul>

<h3>Title: Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Bushra Asseri, Estabrag Abdelaziz, Areej Al-Wabil</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18199">https://arxiv.org/abs/2506.18199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18199">https://arxiv.org/pdf/2506.18199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18199]] Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review(https://arxiv.org/abs/2506.18199)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.</li>
<li><strong>摘要：</strong>大型语言模型表现出各个领域的显着能力，但人们对文化偏见（尤其是对阿拉伯人和穆斯林的偏见）的关注，通过延续有害的刻板印象和边缘化，构成了重大的道德挑战。尽管对LLM的偏见的认识日益认识，但专门针对阿拉伯和穆斯林代表的工程策略仍在研究中。该混合方法系统评价研究了这些技术，为研究人员和从业者提供了基于证据的指导。遵循PRISMA指南和Kitchenham的系统审查方法，我们分析了8项在2021  -  2024年之间发表的经验研究，研究了缓解偏见的策略。我们的发现揭示了五种主要的及时工程方法：文化促进，情感启动，自我抑制技术，结构化的多步管道和参数优化的连续提示。尽管所有方法都显示出减少偏见的潜力，但在研究和偏见类型之间的有效性大大差异。有证据表明，某些偏见类型可能比其他偏见更能抵抗迅速的缓解。结构化的多步管道表现出最高的总体效率，尽管需要更高的技术专业知识，但偏见降低了87.7％。文化提示提供了更广泛的可访问性，具有实质性的有效性。这些结果强调了迅速工程以减轻文化偏见而无需访问模型参数的可及性。数量有限的研究发现，在这一关键领域的研究差距很大。未来的研究应着重于开发具有文化自适应的提示技术，创造阿拉伯和穆斯林特定的评估资源，并将及时的工程与互补的偏见方法整合在一起，以解决更深刻的刻板印象，同时维护模型效用。</li>
</ul>

<h3>Title: Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications</h3>
<ul>
<li><strong>Authors: </strong>Bushra Asseri, Estabraq Abdelaziz, Maha Al Mogren, Tayef Alhefdhi, Areej Al-Wabil</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18201">https://arxiv.org/abs/2506.18201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18201">https://arxiv.org/pdf/2506.18201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18201]] Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications(https://arxiv.org/abs/2506.18201)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Emotion recognition capabilities in multimodal AI systems are crucial for developing culturally responsive educational technologies, yet remain underexplored for Arabic language contexts where culturally appropriate learning tools are critically needed. This study evaluates the emotion recognition performance of two advanced multimodal large language models, GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook illustrations. We assessed both models across three prompting strategies (zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic storybooks, comparing model predictions with human annotations based on Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across all conditions, achieving the highest macro F1-score of 59% with chain-of-thought prompting compared to Gemini's best performance of 43%. Error analysis revealed systematic misclassification patterns, with valence inversions accounting for 60.7% of errors, while both models struggled with culturally nuanced emotions and ambiguous narrative contexts. These findings highlight fundamental limitations in current models' cultural understanding and emphasize the need for culturally sensitive training approaches to develop effective emotion-aware educational technologies for Arabic-speaking learners.</li>
<li><strong>摘要：</strong>多模式AI系统中的情感识别能力对于发展具有文化响应的教育技术至关重要，但是对于非常需要文化适当的学习工具的阿拉伯语语言环境仍未充满信心。这项研究在处理阿拉伯儿童的故事书插图时，评估了两种高级多模式大型语言模型GPT-4O和Gemini 1.5 Pro的情感识别性能。我们使用75张阿拉伯故事书中的75张图像评估了三种提示策略（零射击，很少射击和经营链）的两个模型，将模型预测与基于Plutchik的情感框架的人类注释进行了比较。在所有条件下，GPT-4O始终胜过双子座，与Gemini的最佳性能43％相比，以链链的提示达到了最高的59％的宏观F1得分。错误分析揭示了系统的错误分类模式，价倒数占错误的60.7％，而两个模型都在文化上细微的情绪和模棱两可的叙事环境中挣扎。这些发现突出了当前模型的文化理解中的基本局限性，并强调了对文化敏感的培训方法的需求，以开发有效的情感感知的教育技术，为讲阿拉伯语的学习者。</li>
</ul>

<h3>Title: TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance</h3>
<ul>
<li><strong>Authors: </strong>Syed Mekael Wasti, Shou-Yi Hung, Christopher Collins, En-Shiun Annie Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18337">https://arxiv.org/abs/2506.18337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18337">https://arxiv.org/pdf/2506.18337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18337]] TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance(https://arxiv.org/abs/2506.18337)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Machine translation (MT) post-editing and research data collection often rely on inefficient, disconnected workflows. We introduce TranslationCorrect, an integrated framework designed to streamline these tasks. TranslationCorrect combines MT generation using models like NLLB, automated error prediction using models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive post-editing interface within a single environment. Built with human-computer interaction (HCI) principles in mind to minimize cognitive load, as confirmed by a user study. For translators, it enables them to correct errors and batch translate efficiently. For researchers, TranslationCorrect exports high-quality span-based annotations in the Error Span Annotation (ESA) format, using an error taxonomy inspired by Multidimensional Quality Metrics (MQM). These outputs are compatible with state-of-the-art error detection models and suitable for training MT or post-editing systems. Our user study confirms that TranslationCorrect significantly improves translation efficiency and user satisfaction over traditional annotation methods.</li>
<li><strong>摘要：</strong>机器翻译（MT）编辑和研究数据收集通常依赖于效率低下的工作流程。我们介绍了TranslationCorrect，这是一个旨在简化这些任务的集成框架。 TranslationCorrect使用NLLB，使用XCOMET或LLM API（提供详细的推理）和单个环境中直观的后编辑界面的模型来结合MT生成。如用户研究所证实的那样，它是由人类计算（HCI）原理构建的，以最大程度地减少认知负荷。对于翻译器，它使他们能够纠正错误并有效地翻译。对于研究人员，使用受多维质量指标（MQM）启发的错误分类法（MQM）启发的错误分类法（MQM），TranslationCorrect correct correct correct correct correct correct correct correct cormal part。这些输出与最新的错误检测模型兼容，适用于培训MT或后编辑系统。我们的用户研究证实，翻译措施可显着提高翻译效率和用户满意度，而不是传统注释方法。</li>
</ul>

<h3>Title: Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kang Chen, Mengdi Zhang, Yixin Cao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18341">https://arxiv.org/abs/2506.18341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18341">https://arxiv.org/pdf/2506.18341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18341]] Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs(https://arxiv.org/abs/2506.18341)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>This paper explores the challenges of test-time scaling of large language models (LLMs), regarding both the data and inference efficiency. We highlight the diversity of multi-lingual reasoning based on our pilot studies, and then introduce a novel approach, \(L^2\) multi-lingual unification learning with a decoding intervention strategy for further investigation. The basic idea of \(L^2\) is that the reasoning process varies across different languages, which may be mutually beneficial to enhance both model performance and efficiency. In specific, there are two types of multi-lingual data: the entire long chain-of-thought annotations in different languages and the step-wise mixture of languages. By further tuning based on them, we show that even small amounts of data can significantly improve reasoning capabilities. Our findings suggest that multilingual learning reduces both the required data and the number of inference tokens while maintaining a comparable performance. Furthermore, \(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize the importance of diverse data selection. The \(L^2\) method offers a promising solution to the challenges of data collection and test-time compute efficiency in LLMs.</li>
<li><strong>摘要：</strong>本文探讨了有关数据和推理效率的大型语言模型（LLM）测试时间缩放（LLM）的挑战。我们根据我们的试点研究强调了多语言推理的多样性，然后引入了一种新颖的方法，即（l^2 \）多语言统一学习，并采用解码干预策略进行进一步研究。 \（l^2 \）的基本思想是，推理过程在不同的语言上有所不同，这可能对增强模型性能和效率均有益。在特定的情况下，有两种类型的多语言数据：以不同的语言和语言的逐步混合在一起的整个长期思想链注释。通过进一步的调整，我们表明即使少量数据也可以显着提高推理能力。我们的发现表明，多语言学习可以减少所需数据和推理令牌的数量，同时保持可比的性能。此外，\（l^2 \）与其他数据有效方法正交。因此，我们还强调了各种数据选择的重要性。 \（l^2 \）方法为LLMS中数据收集和测试时间计算效率的挑战提供了有希望的解决方案。</li>
</ul>

<h3>Title: Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics</h3>
<ul>
<li><strong>Authors: </strong>Yousang Cho, Key-Sun Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18387">https://arxiv.org/abs/2506.18387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18387">https://arxiv.org/pdf/2506.18387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18387]] Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics(https://arxiv.org/abs/2506.18387)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>This study investigates how accurately different evaluation metrics capture the quality of causal explanations in automatically generated diagnostic reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, and expert qualitative assessment across two input types: observation-based and multiple-choice-based report generation. Two weighting strategies are applied: one reflecting task-specific priorities, and the other assigning equal weights to all metrics. Our results show that GPT-Black demonstrates the strongest discriminative power in identifying logically coherent and clinically valid causal narratives. GPT-White also aligns well with expert evaluations, while similarity-based metrics diverge from clinical reasoning quality. These findings emphasize the impact of metric selection and weighting on evaluation outcomes, supporting the use of LLM-based evaluation for tasks requiring interpretability and causal reasoning.</li>
<li><strong>摘要：</strong>这项研究研究了自动生成的诊断报告中因果解释质量的准确程度不同的评估指标。我们比较了六个指标：BertScore，余弦相似性，Biosentvec，GPT-White，GPT-Black和专家定性评估，跨两种输入类型：基于观察和基于多项选择的报告生成。采用了两种加权策略：一种反映特定于任务的优先级，另一个反映了所有指标的权重。我们的结果表明，GPT-Black在识别逻辑上连贯和临床上有效的因果叙事方面表现出最强的判别能力。 GPT-White还与专家评估相吻合，而基于相似性的指标与临床推理质量不同。这些发现强调了度量选择和权重对评估结果的影响，支持基于LLM的评估对需要解释性和因果推理的任务的使用。</li>
</ul>

<h3>Title: TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ce Li, Xiaofan Liu, Zhiyan Song, Ce Chi, Chen Zhao, Jingjing Yang, Zhendong Wang, Kexin Yang, Boshen Shi, Xing Wang, Chao Deng, Junlan Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18421">https://arxiv.org/abs/2506.18421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18421">https://arxiv.org/pdf/2506.18421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18421]] TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models(https://arxiv.org/abs/2506.18421)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The majority of data in businesses and industries is stored in tables, databases, and data warehouses. Reasoning with table-structured data poses significant challenges for large language models (LLMs) due to its hidden semantics, inherent complexity, and structured nature. One of these challenges is lacking an effective evaluation benchmark fairly reflecting the performances of LLMs on broad table reasoning abilities. In this paper, we fill in this gap, presenting a comprehensive table reasoning evolution benchmark, TReB, which measures both shallow table understanding abilities and deep table reasoning abilities, a total of 26 sub-tasks. We construct a high quality dataset through an iterative data processing procedure. We create an evaluation framework to robustly measure table reasoning capabilities with three distinct inference modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs using this frame work and prove its effectiveness. Experimental results reveal that existing LLMs still have significant room for improvement in addressing the complex and real world Table related tasks. Both the dataset and evaluation framework are publicly available, with the dataset hosted on [HuggingFace] and the framework on [GitHub].</li>
<li><strong>摘要：</strong>业务和行业中的大多数数据存储在表，数据库和数据仓库中。通过桌子结构的数据推理，由于其隐藏的语义，固有的复杂性和结构化性质，对大语言模型（LLM）提出了重大挑战。这些挑战之一是缺乏有效的评估基准，可以公平地反映出LLM在宽桌推理能力上的性能。在本文中，我们填补了这一空白，提出了一个全面的桌子推理基准TREB，该基准测量了浅表理解能力和深度桌子推理能力，总共有26个子任务。我们通过迭代数据处理程序构建高质量数据集。我们创建一个评估框架，以使用三种不同的推理模式，TCOT，POT和ICOT来鲁棒测量表推理功能。此外，我们使用此框架工作基于20个最先进的LLM，并证明其有效性。实验结果表明，现有的LLM在解决复杂和现实表相关的任务方面仍然有很大的改进空间。数据集和评估框架均可公开使用，数据集托管在[HuggingFace]上，[Github]上的框架。</li>
</ul>

<h3>Title: MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Junjie Zhang, Guozheng Ma, Shunyu Liu, Haoyu Wang, Jiaxing Huang, Ting-En Lin, Fei Huang, Yongbin Li, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18485">https://arxiv.org/abs/2506.18485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18485">https://arxiv.org/pdf/2506.18485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18485]] MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models(https://arxiv.org/abs/2506.18485)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle complex reasoning tasks. However, existing RLVR methods overlook one of the most distinctive capabilities of LLMs, their in-context learning ability, as prominently demonstrated by the success of Chain-of-Thought (CoT) prompting. This motivates us to explore how reinforcement learning can be effectively combined with in-context learning to better improve the reasoning capabilities of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement learning of LLMs by involving ``telling LLMs the rules of the game''. Specifically, MeRF directly injects the reward specification into the prompt, which serves as an in-context motivation for model to improve its responses with awareness of the optimization objective. This simple modification leverages the in-context learning ability of LLMs aligning generation with optimization, thereby incentivizing the model to generate desired outputs from both inner motivation and external reward. Empirical evaluations on the Knights and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that \texttt{MeRF} achieves substantial performance gains over baselines. Moreover, ablation studies show that performance improves with greater consistency between the in-context motivation and the external reward function, while the model also demonstrates an ability to adapt to misleading motivations through reinforcement learning.</li>
<li><strong>摘要：</strong>具有可验证奖励（RLVR）的强化学习已成为大型语言模型（LLMS）的强大学习范式，以解决复杂的推理任务。但是，现有的RLVR方法忽略了LLM的最独特功能之一，即它们的内在学习能力，正如通过思考链（COT）提示的成功所证明的那样。这促使我们探索如何有效地将增强学习与内在学习相结合，以更好地提高LLM的推理能力。在本文中，我们介绍了动机增强的增强填充}（MERF），这是一种直观而有效的方法，通过涉及``告诉LLMS''规则''来增强LLM的强化学习'。具体而言，MERF直接将奖励规范注入了提示，这是模型以意识到优化目标改善其响应的秘密动机。这种简单的修改利用了LLMS将生成与优化对齐的文化学习能力，从而激励该模型从内部动机和外部奖励产生所需的输出。对骑士和knaves〜（K＆K）逻辑难题推理基准的经验评估表明，\ texttt {merf}在基准方面实现了可观的性能增长。此外，消融研究表明，绩效在内在动机和外部奖励功能之间具有更大的一致性而提高，而模型还表明了通过强化学习适应误导性动机的能力。</li>
</ul>

<h3>Title: Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance</h3>
<ul>
<li><strong>Authors: </strong>Wael Etaiwi, Bushra Alhijawi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18501">https://arxiv.org/abs/2506.18501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18501">https://arxiv.org/pdf/2506.18501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18501]] Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance(https://arxiv.org/abs/2506.18501)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>The increasing use of large language models (LLMs) in natural language processing (NLP) tasks has sparked significant interest in evaluating their effectiveness across diverse applications. While models like ChatGPT and DeepSeek have shown strong results in many NLP domains, a comprehensive evaluation is needed to understand their strengths, weaknesses, and domain-specific abilities. This is critical as these models are applied to various tasks, from sentiment analysis to more nuanced tasks like textual entailment and translation. This study aims to evaluate ChatGPT and DeepSeek across five key NLP tasks: sentiment analysis, topic classification, text summarization, machine translation, and textual entailment. A structured experimental protocol is used to ensure fairness and minimize variability. Both models are tested with identical, neutral prompts and evaluated on two benchmark datasets per task, covering domains like news, reviews, and formal/informal texts. The results show that DeepSeek excels in classification stability and logical reasoning, while ChatGPT performs better in tasks requiring nuanced understanding and flexibility. These findings provide valuable insights for selecting the appropriate LLM based on task requirements.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在自然语言处理（NLP）任务中的使用越来越多，引发了人们对评估其在各种应用程序中的有效性的重大兴趣。尽管诸如Chatgpt和DeepSeek之类的模型在许多NLP领域都表现出很强的结果，但仍需要进行全面的评估，以了解其优势，劣势和特定领域的能力。这至关重要，因为这些模型被应用于各种任务，从情感分析到更细微的任务，例如文本需要和翻译。这项研究旨在在五个关键的NLP任务中评估Chatgpt和DeepSeek：情感分析，主题分类，文本摘要，机器翻译和文本款项。结构化实验方案用于确保公平性和最小化变异性。两种模型均通过相同的中性提示进行测试，并在每个任务两个基准数据集上进行评估，涵盖了新闻，评论和正式/非正式文本等领域。结果表明，DeepSeek在分类稳定性和逻辑推理方面表现出色，而ChatGPT在需要细微的理解和灵活性的任务中表现更好。这些发现为根据任务要求选择适当的LLM提供了宝贵的见解。</li>
</ul>

<h3>Title: A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance</h3>
<ul>
<li><strong>Authors: </strong>Matteo Melis, Gabriella Lapesa, Dennis Assenmacher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18576">https://arxiv.org/abs/2506.18576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18576">https://arxiv.org/pdf/2506.18576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18576]] A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance(https://arxiv.org/abs/2506.18576)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Detecting harmful content is a crucial task in the landscape of NLP applications for Social Good, with hate speech being one of its most dangerous forms. But what do we mean by hate speech, how can we define it, and how does prompting different definitions of hate speech affect model performance? The contribution of this work is twofold. At the theoretical level, we address the ambiguity surrounding hate speech by collecting and analyzing existing definitions from the literature. We organize these definitions into a taxonomy of 14 Conceptual Elements-building blocks that capture different aspects of hate speech definitions, such as references to the target of hate (individual or groups) or of the potential consequences of it. At the experimental level, we employ the collection of definitions in a systematic zero-shot evaluation of three LLMs, on three hate speech datasets representing different types of data (synthetic, human-in-the-loop, and real-world). We find that choosing different definitions, i.e., definitions with a different degree of specificity in terms of encoded elements, impacts model performance, but this effect is not consistent across all architectures.</li>
<li><strong>摘要：</strong>检测有害内容是NLP应用程序的社会利益局势的至关重要的任务，而仇恨言论是其最危险的形式之一。但是，仇恨言论是什么意思，我们如何定义它？如何促使仇恨言论的不同定义影响模型表现？这项工作的贡献是双重的。在理论上，我们通过收集和分析文献中的现有定义来解决围绕仇恨言论的歧义。我们将这些定义组织成14个概念元素建设块的分类法，这些概念元素构建块，这些块捕获了仇恨言论定义的不同方面，例如提及仇恨（个人或团体）的目标或其潜在后果。在实验级别上，我们在三个仇恨语音数据集中对三个LLM的系统零摄像评估中采用定义的收集，这些数据集代表不同类型的数据（合成，人类中的人类和现实世界）。我们发现，选择不同的定义，即在编码元素方面具有不同程度特异性的定义会影响模型性能，但是在所有体系结构中，这种效果并不一致。</li>
</ul>

<h3>Title: Parallel Continuous Chain-of-Thought with Jacobi Iteration</h3>
<ul>
<li><strong>Authors: </strong>Haoyi Wu, Zhihao Teng, Kewei Tu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18582">https://arxiv.org/abs/2506.18582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18582">https://arxiv.org/pdf/2506.18582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18582]] Parallel Continuous Chain-of-Thought with Jacobi Iteration(https://arxiv.org/abs/2506.18582)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Continuous chain-of-thought has been shown to be effective in saving reasoning tokens for large language models. By reasoning with continuous latent thought tokens, continuous CoT is able to perform implicit reasoning in a compact manner. However, the sequential dependencies between latent thought tokens spoil parallel training, leading to long training time. In this paper, we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi iteration on the latent thought tokens, updating them iteratively in parallel instead of sequentially and thus improving both training and inference efficiency of continuous CoT. Experiments demonstrate that by choosing the proper number of iterations, we are able to achieve comparable or even better performance while saving nearly 50% of the training and inference time. Moreover, PCCoT shows better stability and robustness in the training process. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>连续的经过思考链已被证明可以有效地为大语言模型节省推理令牌。通过使用连续的潜在思想令牌进行推理，连续的COT能够以紧凑的方式执行隐式推理。但是，潜在思想代币之间的顺序依赖性破坏了并行训练，从而导致了较长的训练时间。在本文中，我们提出了平行的连续链链（PCCOT），该链（PCCOT）在潜在的思想代币上执行雅各比迭代，并以顺序而不是顺序迭代地更新它们，从而提高了连续COT的训练和推理效率。实验表明，通过选择适当的迭代次数，我们可以实现可比甚至更好的性能，同时节省近50％的培训和推理时间。此外，PCCOT在训练过程中显示出更好的稳定性和鲁棒性。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"</h3>
<ul>
<li><strong>Authors: </strong>Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.GT, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18600">https://arxiv.org/abs/2506.18600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18600">https://arxiv.org/pdf/2506.18600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18600]] Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"(https://arxiv.org/abs/2506.18600)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and Törnberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.</li>
<li><strong>摘要：</strong>模拟大语模型（LLM）人群（LLM）的潜在问题是数据污染，即训练数据可能以意想不到的方式塑造结果的可能性。尽管这种担忧很重要，并且可能会阻碍某些具有多代理模型的实验，但它并不排除研究LLM种群中真正新兴动态的研究。 Barrie和Törnberg[1]最近对Flint Ashery等人的批评。 [2]提供了一个机会来澄清可以在LLM种群中研究自我组织和依赖模型的新兴动态，从而强调了在社会惯例的特定情况下如何从经验上观察到这种动态。</li>
</ul>

<h3>Title: The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches</h3>
<ul>
<li><strong>Authors: </strong>Alisa Barkar, Mathieu Chollet, Matthieu Labeau, Beatrice Biancardi, Chloe Clavel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18621">https://arxiv.org/abs/2506.18621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18621">https://arxiv.org/pdf/2506.18621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18621]] The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches(https://arxiv.org/abs/2506.18621)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study examines how large language models understand the concept of persuasiveness in public speaking by modifying speech transcripts from PhD candidates in the "Ma These en 180 Secondes" competition, using the 3MT French dataset. Our contributions include a novel methodology and an interpretable textual feature set integrating rhetorical devices and discourse markers. We prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic shifts between original and generated speech in terms of the new features. Results indicate that GPT-4o applies systematic stylistic modifications rather than optimizing persuasiveness in a human-like manner. Notably, it manipulates emotional lexicon and syntactic structures (such as interrogative and exclamatory clauses) to amplify rhetorical impact.</li>
<li><strong>摘要：</strong>这项研究研究了使用3MT法语数据集中的“ MA Thise En 180秒”竞赛中的PhD候选人的语音成绩单通过修改Phd候选人的语音成绩单，从而在公开演讲中理解了说服力的概念。我们的贡献包括一种新颖的方法和可解释的文本功能集，集成了修辞设备和话语标记。我们提示GPT-4O增强或减少说服力，并根据新功能在原始语音和生成的语音之间分析语言转移。结果表明，GPT-4O应用系统的文体修饰，而不是以人类的方式优化说服力。值得注意的是，它可以操纵情绪词典和句法结构（例如疑问和诱导性从句）来扩大修辞影响。</li>
</ul>

<h3>Title: Is There a Case for Conversation Optimized Tokenizers in Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Raquel Ferrando, Javier Conde, Gonzalo Martínez, Pedro Reviriego</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18674">https://arxiv.org/abs/2506.18674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18674">https://arxiv.org/pdf/2506.18674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18674]] Is There a Case for Conversation Optimized Tokenizers in Large Language Models?(https://arxiv.org/abs/2506.18674)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的计算和能源成本已取决于增长的模型大小以及数亿用户对LLM的大量采用。 LLM的单位成本是令牌的计算。因此，代币器在模型的效率中起着重要作用，并且对它们进行了仔细的优化，以最大程度地减少其培训语料库中文本的代币数量。 LLMS最受欢迎的应用程序之一是与用户互动的聊天机器人。一个关键的观察是，对于那些聊天机器人，重要的是用户文本输入和聊天机器人响应中令牌的性能。这些很可能与培训语料库中的文本不同。因此，一个立即出现的问题是优化聊天机器人对话的引用器是否有潜在的好处。在本文中，通过使用公开可用的聊天机器人对话语料库重新设计其词汇并评估其在该领域的性能，从而为不同的引导者探索了这个想法。结果表明，对话优化的令牌始终减少聊天机器人对话中的令牌数量，这可以导致有意义的节能，范围为5％至10％，而对原始培训表的代币化效率的影响很小甚至略有积极影响。</li>
</ul>

<h3>Title: Benchmarking the Pedagogical Knowledge of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Maxime Lelièvre, Amy Waldock, Meng Liu, Natalia Valdés Aspillaga, Alasdair Mackintosh, María José Ogando Portelo, Jared Lee, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18710">https://arxiv.org/abs/2506.18710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18710">https://arxiv.org/pdf/2506.18710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18710]] Benchmarking the Pedagogical Knowledge of Large Language Models(https://arxiv.org/abs/2506.18710)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at this https URL which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.</li>
<li><strong>摘要：</strong>诸如大规模多任务语言理解（MMLU）之类的基准在评估AI跨不同领域的知识和能力方面发挥了关键作用。但是，现有的基准主要集中在内容知识上，在评估模型对教学法的理解 - 教学方法和实践方面留下了一个关键的差距。本文介绍了教学基准，这是一个新颖的数据集，旨在评估其跨域教学知识（CDPK）以及特殊教育需求与残疾（发送）教学知识的大型语言模型。这些基准是建立在一组精心策划的问题的基础上，这些问题源自教师的专业发展考试，这些问题涵盖了一系列教学子领域，例如教学策略和评估方法。在这里，我们概述了这些基准的方法和开发。我们报告了97款模型的结果，在教学知识问题上，精确度范围从28％到89％。我们考虑成本和准确性之间的关系，并绘制帕累托价值边界的进展，随着时间的流逝。我们在此HTTPS URL上提供在线排行榜，并通过新型号进行了更新，并允许基于各种模型属性（例如每个令牌和开放VS关闭的权重）进行交互式探索和过滤，并查看不同主题的性能。 LLM和生成AI具有影响教育并有助于解决全球学习危机的巨大潜力。以教育为重点的基准对于衡量模型的能力，以了解教学概念，适当响应学习者的需求，并支持各种环境中的有效教学实践。需要在教育环境中告知LLM和基于LLM的工具的负责任和基于证据的部署以及指导发展和政策决策。</li>
</ul>

<h3>Title: Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach</h3>
<ul>
<li><strong>Authors: </strong>Chong Zhang, Xiang Li, Jia Wang, Shan Liang, Haochen Xue, Xiaobo Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18756">https://arxiv.org/abs/2506.18756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18756">https://arxiv.org/pdf/2506.18756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18756]] Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach(https://arxiv.org/abs/2506.18756)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) increasingly rely on automatic prompt engineering in graphical user interfaces (GUIs) to refine user inputs and enhance response accuracy. However, the diversity of user requirements often leads to unintended misinterpretations, where automated optimizations distort original intentions and produce erroneous outputs. To address this challenge, we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates common prompt optimization mechanisms while preserving semantic stability. Our approach dynamically evaluates the impact of such strategies on LLM performance, enabling robust adversarial sample generation. Through extensive experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness in balancing semantic consistency and attack efficacy. Our findings offer actionable insights for designing more reliable prompt optimization systems. Code is available at: this https URL</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）越来越多地依赖于图形用户界面（GUI）的自动及时工程来完善用户输入并提高响应精度。但是，用户需求的多样性通常会导致意外的误解，在这种误解中，自动化的优化扭曲了原始意图并产生错误的输出。为了应对这一挑战，我们提出了自适应贪婪的二进制搜索（AGB）方法，该方法模拟了共同的及时迅速优化机制，同时保持语义稳定性。我们的方法动态评估了此类策略对LLM性能的影响，从而实现了可靠的对抗性样本生成。通过对开放和封闭源LLM的广泛实验，我们证明了AGB在平衡语义一致性和攻击功效方面的有效性。我们的发现为设计更可靠的及时优化系统提供了可行的见解。代码可用：此HTTPS URL</li>
</ul>

<h3>Title: ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework</h3>
<ul>
<li><strong>Authors: </strong>Ao Chang, Tong Zhou, Yubo Chen, Delai Qiu, Shengping Liu, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18768">https://arxiv.org/abs/2506.18768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18768">https://arxiv.org/pdf/2506.18768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18768]] ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework(https://arxiv.org/abs/2506.18768)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including relevant legal charge, terms, and fines, which is a crucial process in Large Language Model(LLM). However, LJP faces two key challenges: (1)Long Tail Distribution: Current datasets, derived from authentic cases, suffer from high human annotation costs and imbalanced distributions, leading to model performance degradation. (2)Lawyer's Improvement: Existing systems focus on enhancing judges' decision-making but neglect the critical role of lawyers in refining arguments, which limits overall judicial accuracy. To address these issues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment Framework, called ASP2LJ, which integrates a case generation module to tackle long-tailed data distributions and an adversarial self-play mechanism to enhance lawyers' argumentation skills. Our framework enables a judge to reference evolved lawyers' arguments, improving the objectivity, fairness, and rationality of judicial decisions. Besides, We also introduce RareCases, a dataset for rare legal cases in China, which contains 120 tail-end cases. We demonstrate the effectiveness of our approach on the SimuCourt dataset and our RareCases dataset. Experimental results show our framework brings improvements, indicating its utilization. Our contributions include an integrated framework, a rare-case dataset, and publicly releasing datasets and code to support further research in automated judicial systems.</li>
<li><strong>摘要：</strong>法律判断预测（LJP）旨在预测司法结果，包括相关的法律指控，条款和罚款，这是大语言模型（LLM）的关键过程。但是，LJP面临两个主要挑战：（1）长尾巴分布：当前数据集，源自真实情况，遭受了高人类注释成本和分布不平衡的损失，从而导致模型性能下降。 （2）律师的进步：现有系统着重于增强法官的决策，但忽略了律师在提炼论点中的关键作用，这限制了整体司法准确性。为了解决这些问题，我们提出了一个对抗性的自我游戏律师增强法律判断框架，称为ASP2LJ，该框架集成了一个案例生成模块，以解决长尾数据分布和对抗性自我游戏机制，以提高律师的论点。我们的框架使法官能够参考进化的律师的论点，从而提高司法裁决的客观性，公平性和理性。此外，我们还引入了Rarecases，这是一个在中国罕见法律案件的数据集，其中包含120个尾巴案件。我们在Simucourt数据集和我们的RARECASE数据集上演示了方法的有效性。实验结果表明我们的框架带来了改进，表明其利用率。我们的贡献包括一个集成框架，一个稀有案例数据集以及公开发布的数据集和代码，以支持自动化司法系统的进一步研究。</li>
</ul>

<h3>Title: Existing LLMs Are Not Self-Consistent For Simple Tasks</h3>
<ul>
<li><strong>Authors: </strong>Zhenru Lin, Jiawen Tao, Yang Yuan, Andrew Chi-Chih Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18781">https://arxiv.org/abs/2506.18781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18781">https://arxiv.org/pdf/2506.18781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18781]] Existing LLMs Are Not Self-Consistent For Simple Tasks(https://arxiv.org/abs/2506.18781)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have grown increasingly powerful, yet ensuring their decisions remain transparent and trustworthy requires self-consistency -- no contradictions in their internal reasoning. Our study reveals that even on simple tasks, such as comparing points on a line or a plane, or reasoning in a family tree, all smaller models are highly inconsistent, and even state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully self-consistent. To quantify and mitigate these inconsistencies, we introduce inconsistency metrics and propose two automated methods -- a graph-based and an energy-based approach. While these fixes provide partial improvements, they also highlight the complexity and importance of self-consistency in building more reliable and interpretable AI. The code and data are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越强大，但是确保他们的决策保持透明和值得信赖需要自矛盾 - 内部推理没有任何矛盾。我们的研究表明，即使在简单的任务上，例如比较线上或平面上的点，或在家谱中的推理，所有较小的模型都非常不一致，甚至诸如DeepSeek-R1和GPT-O4-Mini之类的最先进模型也不完全自以为是。为了量化和减轻这些不一致之处，我们引入了不一致的指标，并提出了两种自动化方法 - 一种基于图和基于能量的方法。尽管这些修复程序提供了部分改进，但它们还强调了自符合性在构建更可靠和可解释的AI方面的复杂性和重要性。该代码和数据可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies</h3>
<ul>
<li><strong>Authors: </strong>Arjun Mukerji, Michael L. Jackson, Jason Jones, Neil Sanghavi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18819">https://arxiv.org/abs/2506.18819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18819">https://arxiv.org/pdf/2506.18819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18819]] RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies(https://arxiv.org/abs/2506.18819)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been extensively evaluated for general summarization tasks as well as medical research assistance, but they have not been specifically evaluated for the task of summarizing real-world evidence (RWE) from structured output of RWE studies. We introduce RWESummary, a proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al., 2025) to enable benchmarking of LLMs for this task. RWESummary includes one scenario and three evaluations covering major types of errors observed in summarization of medical research studies and was developed using Atropos Health proprietary data. Additionally, we use RWESummary to compare the performance of different LLMs in our internal RWE summarization tool. At the time of publication, with 13 distinct RWE studies, we found the Gemini 2.5 models performed best overall (both Flash and Pro). We suggest RWESummary as a novel and useful foundation model benchmark for real-world evidence study summarization.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已被广泛评估，以进行一般摘要任务和医学研究帮助，但尚未对RWE研究结构化输出的真实世界证据（RWE）进行汇总的任务进行专门评估。我们介绍了Rwesummary，这是MedHelm框架（Bedi，Cui，Fuentes，Unell等，2025）的拟议补充，以实现LLMS的基准测试。 RWESUMMARY包括一种方案和三项评估，涵盖了医学研究摘要中观察到的主要错误，并使用Atropos Health专有数据开发。此外，我们使用RWesummary在内部RWE摘要工具中比较不同LLM的性能。在出版时，通过13种不同的RWE研究，我们发现Gemini 2.5型号的整体表现最佳（Flash和Pro）。我们建议RWESUMMARY作为现实世界证据研究摘要的一种新颖而有用的基础模型基准。</li>
</ul>

<h3>Title: STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Aryasomayajula Ram Bharadwaj</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18831">https://arxiv.org/abs/2506.18831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18831">https://arxiv.org/pdf/2506.18831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18831]] STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning(https://arxiv.org/abs/2506.18831)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models employing extended chain-of-thought (CoT) reasoning often suffer from the overthinking phenomenon, generating excessive and redundant reasoning steps that increase computational costs while potentially degrading performance. While recent work has explored static steering approaches to mitigate this issue, they lack the adaptability to dynamically adjust intervention strength based on real-time reasoning quality. We propose STUPID (Steering Token Usage via PID controller), a novel training-free method that employs a PID controller to dynamically modulate activation steering strength during inference. Our approach combines a chunk-level classifier for detecting redundant reasoning patterns with a PID control mechanism that adaptively adjusts steering intensity based on the predicted redundancy probability. Experimental evaluation on GSM8K demonstrates that STUPID achieves a 6% improvement in accuracy while reducing token usage by 32%, outperforming static steering baselines. Our method provides a principled framework for dynamic reasoning calibration that maintains reasoning quality while significantly improving computational efficiency.</li>
<li><strong>摘要：</strong>采用扩展链链（COT）推理的大型语言模型通常会遭受过度思考现象的困扰，从而产生过度和冗余的推理步骤，从而增加计算成本，同时可能会降低性能。尽管最近的工作探索了减轻此问题的静态转向方法，但他们缺乏基于实时推理质量的动态调整干预强度的适应性。我们提出了一种愚蠢的（通过PID控制器转向令牌使用），这是一种新型的无训练方法，该方法采用PID控制器在推理过程中动态调节激活转向强度。我们的方法结合了一个块级分类器，用于检测冗余推理模式和PID控制机制，该机制根据预测的冗余概率适应地调节转向强度。对GSM8K的实验评估表明，愚蠢的准确性提高了6％，同时将令牌使用量减少了32％，表现优于静态转向基线。我们的方法为动态推理校准提供了一个原则性的框架，该框架可保持推理质量，同时显着提高计算效率。</li>
</ul>

<h3>Title: LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18841">https://arxiv.org/abs/2506.18841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18841">https://arxiv.org/pdf/2506.18841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18841]] LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning(https://arxiv.org/abs/2506.18841)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under this https URL</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的超长发电是一种广泛要求的情况，但由于序列长度的增加，由于其最大生成长度限制和总体质量下降，这仍然是一个重大挑战。以前的方法是由长作者举例说明的，通常依赖于“教学”，这涉及对合成长期输出的监督微调（SFT）。但是，这种策略在很大程度上取决于合成的SFT数据，这很难构建，通常缺乏连贯性和一致性，并且往往过于人为和结构单调。在这项工作中，我们提出了一种基于激励化的方法，该方法完全从头开始，而不依赖于任何注释或合成数据，利用强化学习（RL）来促进LLMS超长，高质量的文本生成能力的出现。我们从类似于R1-Zero的基本模型开始进行RL培训，从而指导其进行推理，从而有助于在写作过程中进行计划和精致。为了支持这一点，我们采用了专门的奖励模型，这些模型将LLM转向改善长度控制，写作质量和结构格式。实验评估表明，我们的长作者零型模型，经过QWEN2.5-32B培训，一致性地优于传统的SFT方法，在写作板和竞技场上实现了所有指标的最先进的结果，甚至超过了100B+模型，例如DeepSeek R1 R1和Qwen3-23-23-23-23-23-23-235.B。我们在此HTTPS URL下开放数据和模型检查点</li>
</ul>

<h3>Title: CommVQ: Commutative Vector Quantization for KV Cache Compression</h3>
<ul>
<li><strong>Authors: </strong>Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18879">https://arxiv.org/abs/2506.18879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18879">https://arxiv.org/pdf/2506.18879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18879]] CommVQ: Commutative Vector Quantization for KV Cache Compression(https://arxiv.org/abs/2506.18879)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地用于需要长上下文长度的应用中，但是随着上下文的增长，键值（KV）缓存通常会成为GPU上的内存瓶颈。为了解决这个问题，我们提出了交换矢量量化（COMMVQ），以显着减少长篇文化LLM推断的内存使用情况。我们首先使用轻质编码器和代码簿引入添加量化，以压缩KV缓存，可以通过简单的矩阵乘法来解码。为了进一步降低解码过程中的计算成本，我们将代码本设计为具有旋转位置嵌入（绳索）的能力，并使用预期最大化（EM）算法进行训练。这可以有效地集成解码到自我发场机制中。我们的方法通过添加量化的添加量化和低开销通过绳索交通码来实现高精度。在长篇文本基准和GSM8K上进行的实验表明，我们的方法在2位量化的情况下将FP16 KV高速缓存大小降低了87.5％，而表现优于最先进的KV缓存量化方法。值得注意的是，它可以以最小的精度损失启用1位KV缓存量化，从而使Llama-3.1 8B模型在单个RTX 4090 GPU上以128K上下文长度运行。源代码可在以下网址提供：此HTTPS URL。</li>
</ul>

<h3>Title: OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization</h3>
<ul>
<li><strong>Authors: </strong>Yiyou Sun, Shawn Hu, Georgia Zhou, Ken Zheng, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18880">https://arxiv.org/abs/2506.18880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18880">https://arxiv.org/pdf/2506.18880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18880]] OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization(https://arxiv.org/abs/2506.18880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.</li>
<li><strong>摘要：</strong>最近，具有长期思考推理的大型语言模型（LLMS）像DeepSeek-R1-Have一样，在奥林匹克级数学基准中取得了令人印象深刻的结果。但是，他们经常依靠一组狭窄的策略，并在需要一种新颖思维方式的问题上挣扎。为了系统地研究这些局限性，我们引入了Omega-Omega分发数学问题评估评估，并使用3个概括轴 -  A受控但多样化的基准测试，旨在评估三个轴的分布外泛化轴，灵感来自Boden的创造力的类型：（1）探索性解决问题的问题，以解决更为复杂的问题，以在同一问题域内进行更复杂的问题域内域内的更复杂的实例； （2）以前是孤立学到的构图构成组合，以解决需要以新的和连贯的方式整合这些技能的新颖问题； （3）通过超越熟悉的方法来更有效地解决问题的方法，通常是非常规的策略。欧米茄由编程生成的训练测试对组成，这些测试对从几何，数字理论，代数，组合，逻辑和拼图中得出，并使用符号，数值或图形方法验证解决方案。我们评估Frontier（或顶级）LLM，并随着问题的复杂性的增加而观察到急剧的性能降解。此外，我们在所有概括设置中微调了QWEN系列模型，并观察到探索性概括的显着改善，而构图泛化仍然有限，变革性推理几乎没有改进。通过隔离和量化这些细颗粒的故障，欧米茄为LLM迈出了基础，使LLM朝着机械能力之外的真实数学创造力迈进。</li>
</ul>

<h3>Title: ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18896">https://arxiv.org/abs/2506.18896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18896">https://arxiv.org/pdf/2506.18896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18896]] ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs(https://arxiv.org/abs/2506.18896)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: this https URL</li>
<li><strong>摘要：</strong>流程奖励模型（PRM）最近成为了监督大语言模型（LLMS）中间推理步骤的有力框架。以前的PRM主要接受模型最终输出响应的训练，并难以牢固地评估中间思维轨迹，尤其是在Frontier推理模型（如DeepSeek-R1）产生的轨迹响应输出的新兴环境中。在这项工作中，我们介绍了Reasonflux-prm，这是一种新型的轨迹感知的PRM，旨在评估推理轨迹的轨迹反应类型。 Reasonflux-Prm同时结合了级别和轨迹级别的监督，使能够与结构化链链数据保持一致的精细奖励分配。我们适应了在离线和在线设置下支持奖励监督的奖励监督，包括（i）选择高质量的模型蒸馏数据，用于下游的较小型号的下游监督微调，（ii）在增强学习过程中提供密集的过程级别的策略优化，以及（iii）启用奖励有奖励有奖励有奖励有奖励有奖励有奖励的最佳N测试时间的缩放。关于挑战下游基准（例如AIME，MATH500和GPQA）辅助基准的经验结果表明，Praysflux-prm-7b选择了比强PRM（例如QWEN2.5-MATH-PRM-72B）和人类策划的基地的更高质量数据。此外，我们派生的理性flux-prm-7b可获得一致的绩效提高，在监督的微调中，平均增长率为12.1％，增强学习的4.5％，测试时间缩放率为6.3％。我们还发布了用于资源受限的应用程序和边缘部署的有效理由flm-prm-1.5b。项目：此HTTPS URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
