<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>language model</h2>
<h3>Title: War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars. (arXiv:2311.17227v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17227">http://arxiv.org/abs/2311.17227</a></li>
<li>Code URL: https://github.com/agiresearch/waragent</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17227]] War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars(http://arxiv.org/abs/2311.17227)</code></li>
<li>Summary: <p>Can we avoid wars at the crossroads of history? This question has been
pursued by individuals, scholars, policymakers, and organizations throughout
human history. In this research, we attempt to answer the question based on the
recent advances of Artificial Intelligence (AI) and Large Language Models
(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to
simulate the participating countries, their decisions, and the consequences, in
historical international conflicts, including the World War I (WWI), the World
War II (WWII), and the Warring States Period (WSP) in Ancient China. By
evaluating the simulation effectiveness, we examine the advancements and
limitations of cutting-edge AI systems' abilities in studying complex
collective human behaviors such as international conflicts under diverse
settings. In these simulations, the emergent interactions among agents also
offer a novel perspective for examining the triggers and conditions that lead
to war. Our findings offer data-driven and AI-augmented insights that can
redefine how we approach conflict resolution and peacekeeping strategies. The
implications stretch beyond historical analysis, offering a blueprint for using
AI to understand human history and possibly prevent future international
conflicts. Code and data are available at
\url{https://github.com/agiresearch/WarAgent}.
</p></li>
</ul>

<h3>Title: Quantifying the redundancy between prosody and text. (arXiv:2311.17233v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17233">http://arxiv.org/abs/2311.17233</a></li>
<li>Code URL: https://github.com/lu-wo/quantifying-redundancy</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17233]] Quantifying the redundancy between prosody and text(http://arxiv.org/abs/2311.17233)</code></li>
<li>Summary: <p>Prosody -- the suprasegmental component of speech, including pitch, loudness,
and tempo -- carries critical aspects of meaning. However, the relationship
between the information conveyed by prosody vs. by the words themselves remains
poorly understood. We use large language models (LLMs) to estimate how much
information is redundant between prosody and the words themselves. Using a
large spoken corpus of English audiobooks, we extract prosodic features aligned
to individual words and test how well they can be predicted from LLM
embeddings, compared to non-contextual word embeddings. We find a high degree
of redundancy between the information carried by the words and prosodic
information across several prosodic features, including intensity, duration,
pauses, and pitch contours. Furthermore, a word's prosodic information is
redundant with both the word itself and the context preceding as well as
following it. Still, we observe that prosodic features can not be fully
predicted from text, suggesting that prosody carries information above and
beyond the words. Along with this paper, we release a general-purpose data
processing pipeline for quantifying the relationship between linguistic
information and extra-linguistic features.
</p></li>
</ul>

<h3>Title: Elo Uncovered: Robustness and Best Practices in Language Model Evaluation. (arXiv:2311.17295v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17295">http://arxiv.org/abs/2311.17295</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17295]] Elo Uncovered: Robustness and Best Practices in Language Model Evaluation(http://arxiv.org/abs/2311.17295)</code></li>
<li>Summary: <p>In Natural Language Processing (NLP), the Elo rating system, originally
designed for ranking players in dynamic games such as chess, is increasingly
being used to evaluate Large Language Models (LLMs) through "A vs B" paired
comparisons. However, while popular, the system's suitability for assessing
entities with constant skill levels, such as LLMs, remains relatively
unexplored. We study two fundamental axioms that evaluation methods should
adhere to: reliability and transitivity. We conduct extensive evaluation of Elo
behaviour, illustrating that individual Elo computations exhibit volatility and
delving into the impact of varying the Elo rating system's hyperparameters. We
show that these axioms are not always satisfied raising questions about the
reliability of current comparative evaluations of LLMs. If the current use of
Elo scores is intended to substitute the costly head-to-head comparison of
LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by
the axioms, our findings offer concrete guidelines for enhancing the
reliability of LLM evaluation methods, suggesting a need for reassessment of
existing comparative approaches.
</p></li>
</ul>

<h3>Title: Language Models: A Guide for the Perplexed. (arXiv:2311.17301v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17301">http://arxiv.org/abs/2311.17301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17301]] Language Models: A Guide for the Perplexed(http://arxiv.org/abs/2311.17301)</code></li>
<li>Summary: <p>Given the growing importance of AI literacy, we decided to write this
tutorial to help narrow the gap between the discourse among those who study
language models -- the core technology underlying ChatGPT and similar products
-- and those who are intrigued and want to learn more about them. In short, we
believe the perspective of researchers and educators can add some clarity to
the public's understanding of the technologies beyond what's currently
available, which tends to be either extremely technical or promotional material
generated about products by their purveyors.
</p>
<p>Our approach teases apart the concept of a language model from products built
on them, from the behaviors attributed to or desired from those products, and
from claims about similarity to human cognition. As a starting point, we (1)
offer a scientific viewpoint that focuses on questions amenable to study
through experimentation; (2) situate language models as they are today in the
context of the research that led to their development; and (3) describe the
boundaries of what is known about the models at this writing.
</p></li>
</ul>

<h3>Title: Universal Self-Consistency for Large Language Model Generation. (arXiv:2311.17311v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17311">http://arxiv.org/abs/2311.17311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17311]] Universal Self-Consistency for Large Language Model Generation(http://arxiv.org/abs/2311.17311)</code></li>
<li>Summary: <p>Self-consistency with chain-of-thought prompting (CoT) has demonstrated
remarkable performance gains on various challenging tasks, by utilizing
multiple reasoning paths sampled from large language models (LLMs). However,
self-consistency relies on the answer extraction process to aggregate multiple
solutions, which is not applicable to free-form answers. In this work, we
propose Universal Self-Consistency (USC), which leverages LLMs themselves to
select the most consistent answer among multiple candidates. We evaluate USC on
a variety of benchmarks, including mathematical reasoning, code generation,
long-context summarization, and open-ended question answering. On open-ended
generation tasks where the original self-consistency method is not applicable,
USC effectively utilizes multiple samples and improves the performance. For
mathematical reasoning, USC matches the standard self-consistency performance
without requiring the answer formats to be similar. Finally, without access to
execution results, USC also matches the execution-based voting performance on
code generation.
</p></li>
</ul>

<h3>Title: Exploring Large Language Models for Human Mobility Prediction under Public Events. (arXiv:2311.17351v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17351">http://arxiv.org/abs/2311.17351</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17351]] Exploring Large Language Models for Human Mobility Prediction under Public Events(http://arxiv.org/abs/2311.17351)</code></li>
<li>Summary: <p>Public events, such as concerts and sports games, can be major attractors for
large crowds, leading to irregular surges in travel demand. Accurate human
mobility prediction for public events is thus crucial for event planning as
well as traffic or crowd management. While rich textual descriptions about
public events are commonly available from online sources, it is challenging to
encode such information in statistical or machine learning models. Existing
methods are generally limited in incorporating textual information, handling
data sparsity, or providing rationales for their predictions. To address these
challenges, we introduce a framework for human mobility prediction under public
events (LLM-MPE) based on Large Language Models (LLMs), leveraging their
unprecedented ability to process textual data, learn from minimal examples, and
generate human-readable explanations. Specifically, LLM-MPE first transforms
raw, unstructured event descriptions from online sources into a standardized
format, and then segments historical mobility data into regular and
event-related components. A prompting strategy is designed to direct LLMs in
making and rationalizing demand predictions considering historical mobility and
event features. A case study is conducted for Barclays Center in New York City,
based on publicly available event information and taxi trip data. Results show
that LLM-MPE surpasses traditional models, particularly on event days, with
textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers
interpretable insights into its predictions. Despite the great potential of
LLMs, we also identify key challenges including misinformation and high costs
that remain barriers to their broader adoption in large-scale human mobility
analysis.
</p></li>
</ul>

<h3>Title: Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&A. (arXiv:2311.17371v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17371">http://arxiv.org/abs/2311.17371</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17371]] Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&A(http://arxiv.org/abs/2311.17371)</code></li>
<li>Summary: <p>Recent advancements in large language models (LLMs) underscore their
potential for responding to medical inquiries. However, ensuring that
generative agents provide accurate and reliable answers remains an ongoing
challenge. In this context, multi-agent debate (MAD) has emerged as a prominent
strategy for enhancing the truthfulness of LLMs. In this work, we provide a
comprehensive benchmark of MAD strategies for medical Q&amp;A, along with
open-source implementations. This explores the effective utilization of various
strategies including the trade-offs between cost, time, and accuracy. We build
upon these insights to provide a novel debate-prompting strategy based on agent
agreement that outperforms previously published strategies on medical Q&amp;A
tasks.
</p></li>
</ul>

<h3>Title: CLOMO: Counterfactual Logical Modification with Large Language Models. (arXiv:2311.17438v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17438">http://arxiv.org/abs/2311.17438</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17438]] CLOMO: Counterfactual Logical Modification with Large Language Models(http://arxiv.org/abs/2311.17438)</code></li>
<li>Summary: <p>In this study, we delve into the realm of counterfactual reasoning
capabilities of large language models (LLMs). Our primary objective is to
cultivate the counterfactual thought processes within LLMs and rigorously
assess these processes for their validity. Specifically, we introduce a novel
task, Counterfactual Logical Modification (CLOMO), and a high-quality
human-annotated benchmark. In this task, LLMs must adeptly alter a given
argumentative text to uphold a predetermined logical relationship. To
effectively evaluate a generation model's counterfactual capabilities, we
propose an innovative evaluation metric, the LogicAware Counterfactual Score to
directly evaluate the natural language output of LLMs instead of modeling the
task as a multiple-choice problem. Analysis shows that the proposed automatic
metric aligns well with human preference. Our experimental results show that
while LLMs demonstrate a notable capacity for logical counterfactual thinking,
there remains a discernible gap between their current abilities and human
performance.
</p></li>
</ul>

<h3>Title: Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model. (arXiv:2311.17487v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17487">http://arxiv.org/abs/2311.17487</a></li>
<li>Code URL: https://github.com/miulab/taiwan-llm</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17487]] Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model(http://arxiv.org/abs/2311.17487)</code></li>
<li>Summary: <p>In the realm of language models, the nuanced linguistic and cultural
intricacies of Traditional Chinese, as spoken in Taiwan, have been largely
overlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model
that specifically caters to the Traditional Chinese language, with a focus on
the variant used in Taiwan. Leveraging a comprehensive pretraining corpus and
instruction-finetuning datasets, we have developed a model that not only
understands the complexities of Traditional Chinese but also embodies the
cultural context of Taiwan. Taiwan LLM represents the first of its kind, a
model that is not only linguistically accurate but also culturally resonant
with its user base. Our evaluations demonstrate that Taiwan LLM achieves
superior performance in understanding and generating Traditional Chinese text,
outperforming existing models that are predominantly trained on Simplified
Chinese or English. The open-source release of Taiwan LLM invites collaboration
and further innovation, ensuring that the linguistic diversity of Chinese
speakers is embraced and well-served. The model, datasets, and further
resources are made publicly available to foster ongoing research and
development in this field.
</p></li>
</ul>

<h3>Title: LanGWM: Language Grounded World Model. (arXiv:2311.17593v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17593">http://arxiv.org/abs/2311.17593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17593]] LanGWM: Language Grounded World Model(http://arxiv.org/abs/2311.17593)</code></li>
<li>Summary: <p>Recent advances in deep reinforcement learning have showcased its potential
in tackling complex tasks. However, experiments on visual control tasks have
revealed that state-of-the-art reinforcement learning models struggle with
out-of-distribution generalization. Conversely, expressing higher-level
concepts and global contexts is relatively easy using language.
</p>
<p>Building upon recent success of the large language models, our main objective
is to improve the state abstraction technique in reinforcement learning by
leveraging language for robust action selection. Specifically, we focus on
learning language-grounded visual features to enhance the world model learning,
a model-based reinforcement learning technique.
</p>
<p>To enforce our hypothesis explicitly, we mask out the bounding boxes of a few
objects in the image observation and provide the text prompt as descriptions
for these masked objects. Subsequently, we predict the masked objects along
with the surrounding regions as pixel reconstruction, similar to the
transformer-based masked autoencoder approach.
</p>
<p>Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art
performance in out-of-distribution test at the 100K interaction steps
benchmarks of iGibson point navigation tasks. Furthermore, our proposed
technique of explicit language-grounded visual representation learning has the
potential to improve models for human-robot interaction because our extracted
visual features are language grounded.
</p></li>
</ul>

<h3>Title: TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models. (arXiv:2311.17667v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17667">http://arxiv.org/abs/2311.17667</a></li>
<li>Code URL: https://github.com/zchuz/timebench</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17667]] TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models(http://arxiv.org/abs/2311.17667)</code></li>
<li>Summary: <p>Understanding time is a pivotal aspect of human cognition, crucial in the
broader framework of grasping the intricacies of the world. Previous studies
typically focus on specific aspects of time, lacking a comprehensive temporal
reasoning benchmark. To address this issue, we propose TimeBench, a
comprehensive hierarchical temporal reasoning benchmark that covers a broad
spectrum of temporal reasoning phenomena, which provides a thorough evaluation
for investigating the temporal reasoning capabilities of large language models.
We conduct extensive experiments on popular LLMs, such as GPT-4, LLaMA2, and
Mistral, incorporating chain-of-thought prompting. Our experimental results
indicate a significant performance gap between the state-of-the-art LLMs and
humans, highlighting that there is still a considerable distance to cover in
temporal reasoning. We aspire for TimeBench to serve as a comprehensive
benchmark, fostering research in temporal reasoning for LLMs. Our resource is
available at https://github.com/zchuz/TimeBench
</p></li>
</ul>

<h3>Title: AviationGPT: A Large Language Model for the Aviation Domain. (arXiv:2311.17686v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17686">http://arxiv.org/abs/2311.17686</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17686]] AviationGPT: A Large Language Model for the Aviation Domain(http://arxiv.org/abs/2311.17686)</code></li>
<li>Summary: <p>The advent of ChatGPT and GPT-4 has captivated the world with large language
models (LLMs), demonstrating exceptional performance in question-answering,
summarization, and content generation. The aviation industry is characterized
by an abundance of complex, unstructured text data, replete with technical
jargon and specialized terminology. Moreover, labeled data for model building
are scarce in this domain, resulting in low usage of aviation text data. The
emergence of LLMs presents an opportunity to transform this situation, but
there is a lack of LLMs specifically designed for the aviation domain. To
address this gap, we propose AviationGPT, which is built on open-source LLaMA-2
and Mistral architectures and continuously trained on a wealth of carefully
curated aviation datasets. Experimental results reveal that AviationGPT offers
users multiple advantages, including the versatility to tackle diverse natural
language processing (NLP) problems (e.g., question-answering, summarization,
document writing, information extraction, report querying, data cleaning, and
interactive data exploration). It also provides accurate and contextually
relevant responses within the aviation domain and significantly improves
performance (e.g., over a 40% performance gain in tested cases). With
AviationGPT, the aviation industry is better equipped to address more complex
research problems and enhance the efficiency and safety of National Airspace
System (NAS) operations.
</p></li>
</ul>

<h3>Title: General-Purpose vs. Domain-Adapted Large Language Models for Extraction of Data from Thoracic Radiology Reports. (arXiv:2311.17213v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17213">http://arxiv.org/abs/2311.17213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17213]] General-Purpose vs(http://arxiv.org/abs/2311.17213)</code></li>
<li>Summary: <p>Radiologists produce unstructured data that could be valuable for clinical
care when consumed by information systems. However, variability in style limits
usage. Study compares performance of system using domain-adapted language model
(RadLing) and general-purpose large language model (GPT-4) in extracting common
data elements (CDE) from thoracic radiology reports. Three radiologists
annotated a retrospective dataset of 1300 thoracic reports (900 training, 400
test) and mapped to 21 pre-selected relevant CDEs. RadLing was used to generate
embeddings for sentences and identify CDEs using cosine-similarity, which were
mapped to values using light-weight mapper. GPT-4 system used OpenAI's
general-purpose embeddings to identify relevant CDEs and used GPT-4 to map to
values. The output CDE:value pairs were compared to the reference standard; an
identical match was considered true positive. Precision (positive predictive
value) was 96% (2700/2824) for RadLing and 99% (2034/2047) for GPT-4. Recall
(sensitivity) was 94% (2700/2876) for RadLing and 70% (2034/2887) for GPT-4;
the difference was statistically significant (P&lt;.001). RadLing's domain-adapted
embeddings were more sensitive in CDE identification (95% vs 71%) and its
light-weight mapper had comparable precision in value assignment (95.4% vs
95.0%). RadLing system exhibited higher performance than GPT-4 system in
extracting CDEs from radiology reports. RadLing system's domain-adapted
embeddings outperform general-purpose embeddings from OpenAI in CDE
identification and its light-weight value mapper achieves comparable precision
to large GPT-4. RadLing system offers operational advantages including local
deployment and reduced runtime costs. Domain-adapted RadLing system surpasses
GPT-4 system in extracting common data elements from radiology reports, while
providing benefits of local deployment and lower costs.
</p></li>
</ul>

<h3>Title: Biomedical knowledge graph-enhanced prompt generation for large language models. (arXiv:2311.17330v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17330">http://arxiv.org/abs/2311.17330</a></li>
<li>Code URL: https://github.com/BaranziniLab/KG_RAG</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17330]] Biomedical knowledge graph-enhanced prompt generation for large language models(http://arxiv.org/abs/2311.17330)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have been driving progress in AI at an
unprecedented rate, yet still face challenges in knowledge-intensive domains
like biomedicine. Solutions such as pre-training and domain-specific
fine-tuning add substantial computational overhead, and the latter require
domain-expertise. External knowledge infusion is task-specific and requires
model training. Here, we introduce a task-agnostic Knowledge Graph-based
Retrieval Augmented Generation (KG-RAG) framework by leveraging the massive
biomedical KG SPOKE with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to
generate meaningful biomedical text rooted in established knowledge. KG-RAG
consistently enhanced the performance of LLMs across various prompt types,
including one-hop and two-hop prompts, drug repurposing queries, biomedical
true/false questions, and multiple-choice questions (MCQ). Notably, KG-RAG
provides a remarkable 71% boost in the performance of the Llama-2 model on the
challenging MCQ dataset, demonstrating the framework's capacity to empower
open-source models with fewer parameters for domain-specific questions.
Furthermore, KG-RAG enhanced the performance of proprietary GPT models, such as
GPT-3.5 which exhibited improvement over GPT-4 in context utilization on MCQ
data. Our approach was also able to address drug repurposing questions,
returning meaningful repurposing suggestions. In summary, the proposed
framework combines explicit and implicit knowledge of KG and LLM, respectively,
in an optimized fashion, thus enhancing the adaptability of general-purpose
LLMs to tackle domain-specific questions in a unified framework.
</p></li>
</ul>

<h3>Title: Are Large Language Models Good Fact Checkers: A Preliminary Study. (arXiv:2311.17355v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17355">http://arxiv.org/abs/2311.17355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17355]] Are Large Language Models Good Fact Checkers: A Preliminary Study(http://arxiv.org/abs/2311.17355)</code></li>
<li>Summary: <p>Recently, Large Language Models (LLMs) have drawn significant attention due
to their outstanding reasoning capabilities and extensive knowledge repository,
positioning them as superior in handling various natural language processing
tasks compared to other language models. In this paper, we present a
preliminary investigation into the potential of LLMs in fact-checking. This
study aims to comprehensively evaluate various LLMs in tackling specific
fact-checking subtasks, systematically evaluating their capabilities, and
conducting a comparative analysis of their performance against pre-trained and
state-of-the-art low-parameter models. Experiments demonstrate that LLMs
achieve competitive performance compared to other small models in most
scenarios. However, they encounter challenges in effectively handling Chinese
fact verification and the entirety of the fact-checking pipeline due to
language inconsistencies and hallucinations. These findings underscore the need
for further exploration and research to enhance the proficiency of LLMs as
reliable fact-checkers, unveiling the potential capability of LLMs and the
possible challenges in fact-checking tasks.
</p></li>
</ul>

<h3>Title: CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs. (arXiv:2311.17376v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17376">http://arxiv.org/abs/2311.17376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17376]] CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs(http://arxiv.org/abs/2311.17376)</code></li>
<li>Summary: <p>Instruction-based multitasking has played a critical role in the success of
large language models (LLMs) in multi-turn dialog applications. While publicly
available LLMs have shown promising performance, when exposed to complex
instructions with multiple constraints, they lag against state-of-the-art
models like ChatGPT. In this work, we hypothesize that the availability of
large-scale complex demonstrations is crucial in bridging this gap. Focusing on
dialog applications, we propose a novel framework, CESAR, that unifies a large
number of dialog tasks in the same format and allows programmatic induction of
complex instructions without any manual effort.
</p>
<p>We apply CESAR on InstructDial, a benchmark for instruction-based dialog
tasks. We further enhance InstructDial with new datasets and tasks and utilize
CESAR to induce complex tasks with compositional instructions. This results in
a new benchmark called InstructDial++, which includes 63 datasets with 86 basic
tasks and 68 composite tasks. Through rigorous experiments, we demonstrate the
scalability of CESAR in providing rich instructions. Models trained on
InstructDial++ can follow compositional prompts, such as prompts that ask for
multiple stylistic constraints.
</p></li>
</ul>

<h3>Title: Unveiling the Implicit Toxicity in Large Language Models. (arXiv:2311.17391v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17391">http://arxiv.org/abs/2311.17391</a></li>
<li>Code URL: https://github.com/thu-coai/implicit-toxicity</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17391]] Unveiling the Implicit Toxicity in Large Language Models(http://arxiv.org/abs/2311.17391)</code></li>
<li>Summary: <p>The open-endedness of large language models (LLMs) combined with their
impressive capabilities may lead to new safety issues when being exploited for
malicious use. While recent studies primarily focus on probing toxic outputs
that can be easily detected with existing toxicity classifiers, we show that
LLMs can generate diverse implicit toxic outputs that are exceptionally
difficult to detect via simply zero-shot prompting. Moreover, we propose a
reinforcement learning (RL) based attacking method to further induce the
implicit toxicity in LLMs. Specifically, we optimize the language model with a
reward that prefers implicit toxic outputs to explicit toxic and non-toxic
ones. Experiments on five widely-adopted toxicity classifiers demonstrate that
the attack success rate can be significantly improved through RL fine-tuning.
For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate
of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose
a significant threat in generating undetectable implicit toxic outputs. We
further show that fine-tuning toxicity classifiers on the annotated examples
from our attacking method can effectively enhance their ability to detect
LLM-generated implicit toxic language. The code is publicly available at
https://github.com/thu-coai/Implicit-Toxicity.
</p></li>
</ul>

<h3>Title: Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention. (arXiv:2311.17400v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17400">http://arxiv.org/abs/2311.17400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17400]] Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention(http://arxiv.org/abs/2311.17400)</code></li>
<li>Summary: <p>Transformer-based models, such as BERT and GPT, have been widely adopted in
natural language processing (NLP) due to their exceptional performance.
However, recent studies show their vulnerability to textual adversarial attacks
where the model's output can be misled by intentionally manipulating the text
inputs. Despite various methods that have been proposed to enhance the model's
robustness and mitigate this vulnerability, many require heavy consumption
resources (e.g., adversarial training) or only provide limited protection
(e.g., defensive dropout). In this paper, we propose a novel method called
dynamic attention, tailored for the transformer architecture, to enhance the
inherent robustness of the model itself against various adversarial attacks.
Our method requires no downstream task knowledge and does not incur additional
costs. The proposed dynamic attention consists of two modules: (I) attention
rectification, which masks or weakens the attention value of the chosen tokens,
and (ii) dynamic modeling, which dynamically builds the set of candidate
tokens. Extensive experiments demonstrate that dynamic attention significantly
mitigates the impact of adversarial attacks, improving up to 33\% better
performance than previous methods against widely-used adversarial attacks. The
model-level design of dynamic attention enables it to be easily combined with
other defense methods (e.g., adversarial training) to further enhance the
model's robustness. Furthermore, we demonstrate that dynamic attention
preserves the state-of-the-art robustness space of the original model compared
to other dynamic modeling methods.
</p></li>
</ul>

<h3>Title: Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models. (arXiv:2311.17502v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17502">http://arxiv.org/abs/2311.17502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17502]] Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models(http://arxiv.org/abs/2311.17502)</code></li>
<li>Summary: <p>Community Question Answering (CQA) becomes increasingly prevalent in recent
years. However, there are a large number of answers, which is difficult for
users to select the relevant answers. Therefore, answer selection is a very
significant subtask of CQA. In this paper, we first propose the Question-Answer
cross attention networks (QAN) with pre-trained models for answer selection and
utilize large language model (LLM) to perform answer selection with knowledge
augmentation. Specifically, we apply the BERT model as the encoder layer to do
pre-training for question subjects, question bodies and answers, respectively,
then the cross attention mechanism selects the most relevant answer for
different questions. Experiments show that the QAN model achieves
state-of-the-art performance on two datasets, SemEval2015 and SemEval2017.
Moreover, we use the LLM to generate external knowledge from questions and
correct answers to achieve knowledge augmentation for the answer selection task
by LLM, while optimizing the prompt of LLM in different aspects. The results
show that the introduction of external knowledge can improve the correct answer
selection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM
can also select the correct answer on more questions by optimized prompt.
</p></li>
</ul>

<h3>Title: How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation. (arXiv:2311.17696v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17696">http://arxiv.org/abs/2311.17696</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17696]] How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation(http://arxiv.org/abs/2311.17696)</code></li>
<li>Summary: <p>Artificial intelligence is transforming education through data-driven,
personalized learning solutions. This paper introduces AI Tutor, an innovative
web application that provides personalized tutoring in any subject using
state-of-the-art Large Language Model (LLM). AI Tutor ingests course materials
to construct an adaptive knowledge base tailored to the course. When students
pose questions, it retrieves the most relevant information and generates
detailed, conversational responses citing supporting evidence. The system is
powered by advanced large language models and Retrieval-Augmented Generation
(RAG) techniques for accurate, natural question answering. We present a
fully-functional web interface and video demonstration that showcase AI Tutor's
versatility across diverse subjects and its ability to produce pedagogically
cogent responses. While an initial prototype, this work represents a pioneering
step toward AI-enabled tutoring systems that can democratize access to
high-quality, customized educational support.
</p></li>
</ul>

<h3>Title: End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data. (arXiv:2311.17741v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17741">http://arxiv.org/abs/2311.17741</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17741]] End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data(http://arxiv.org/abs/2311.17741)</code></li>
<li>Summary: <p>Joint rich and normalized automatic speech recognition (ASR), that produces
transcriptions both with and without punctuation and capitalization, remains a
challenge. End-to-end (E2E) ASR models offer both convenience and the ability
to perform such joint transcription of speech. Training such models requires
paired speech and rich text data, which is not widely available. In this paper,
we compare two different approaches to train a stateless Transducer-based E2E
joint rich and normalized ASR system, ready for streaming applications, with a
limited amount of rich labeled data. The first approach uses a language model
to generate pseudo-rich transcriptions of normalized training data. The second
approach uses a single decoder conditioned on the type of the output. The first
approach leads to E2E rich ASR which perform better on out-of-domain data, with
up to 9% relative reduction in errors. The second approach demonstrates the
feasibility of an E2E joint rich and normalized ASR system using as low as 5%
rich training data with moderate (2.42% absolute) increase in errors.
</p></li>
</ul>

<h2>gpt</h2>
<h3>Title: TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4. (arXiv:2311.17429v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17429">http://arxiv.org/abs/2311.17429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17429]] TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4(http://arxiv.org/abs/2311.17429)</code></li>
<li>Summary: <p>Prompt-based learning has been widely applied in many low-resource NLP tasks
such as few-shot scenarios. However, this paradigm has been shown to be
vulnerable to backdoor attacks. Most of the existing attack methods focus on
inserting manually predefined templates as triggers in the pre-training phase
to train the victim model and utilize the same triggers in the downstream task
to perform inference, which tends to ignore the transferability and
stealthiness of the templates. In this work, we propose a novel approach of
TARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models
via GPT4), which is a data-independent attack method. Specifically, we first
utilize GPT4 to reformulate manual templates to generate tone-strong and normal
templates, and the former are injected into the model as a backdoor trigger in
the pre-training phase. Then, we not only directly employ the above templates
in the downstream task, but also use GPT4 to generate templates with similar
tone to the above templates to carry out transferable attacks. Finally we have
conducted extensive experiments on five NLP datasets and three BERT series
models, with experimental results justifying that our TARGET method has better
attack performance and stealthiness compared to the two-external baseline
methods on direct attacks, and in addition achieves satisfactory attack
capability in the unseen tone-similar templates.
</p></li>
</ul>

<h3>Title: Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17431">http://arxiv.org/abs/2311.17431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17431]] Grounding Foundation Models through Federated Transfer Learning: A General Framework(http://arxiv.org/abs/2311.17431)</code></li>
<li>Summary: <p>Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
</p></li>
</ul>

<h2>llm</h2>
<h3>Title: ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?. (arXiv:2311.17107v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17107">http://arxiv.org/abs/2311.17107</a></li>
<li>Code URL: https://github.com/rlacombe/climatex</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17107]] ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?(http://arxiv.org/abs/2311.17107)</code></li>
<li>Summary: <p>Evaluating the accuracy of outputs generated by Large Language Models (LLMs)
is especially important in the climate science and policy domain. We introduce
the Expert Confidence in Climate Statements (ClimateX) dataset, a novel,
curated, expert-labeled dataset consisting of 8094 climate statements collected
from the latest Intergovernmental Panel on Climate Change (IPCC) reports,
labeled with their associated confidence levels. Using this dataset, we show
that recent LLMs can classify human expert confidence in climate-related
statements, especially in a few-shot learning setting, but with limited (up to
47%) accuracy. Overall, models exhibit consistent and significant
over-confidence on low and medium confidence statements. We highlight
implications of our results for climate communication, LLMs evaluation
strategies, and the use of LLMs in information retrieval systems.
</p></li>
</ul>

<h3>Title: Robustness Approaches for the Examination Timetabling Problem under Data Uncertainty. (arXiv:2311.17766v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17766">http://arxiv.org/abs/2311.17766</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17766]] Robustness Approaches for the Examination Timetabling Problem under Data Uncertainty(http://arxiv.org/abs/2311.17766)</code></li>
<li>Summary: <p>In the literature the examination timetabling problem (ETTP) is often
considered a post-enrollment problem (PE-ETTP). In the real world, universities
often schedule their exams before students register using information from
previous terms. A direct consequence of this approach is the uncertainty
present in the resulting models. In this work we discuss several approaches
available in the robust optimization literature. We consider the implications
of each approach in respect to the examination timetabling problem and present
how the most favorable approaches can be applied to the ETTP. Afterwards we
analyze the impact of some possible implementations of the given robustness
approaches on two real world instances and several random instances generated
by our instance generation framework which we introduce in this work.
</p></li>
</ul>

<h3>Title: Efficient Stitchable Task Adaptation. (arXiv:2311.17352v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17352">http://arxiv.org/abs/2311.17352</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17352]] Efficient Stitchable Task Adaptation(http://arxiv.org/abs/2311.17352)</code></li>
<li>Summary: <p>The paradigm of pre-training and fine-tuning has laid the foundation for
deploying deep learning models. However, most fine-tuning methods are designed
to meet a specific resource budget. Recently, considering diverse deployment
scenarios with various resource budgets, stitchable neural network (SN-Net) is
introduced to quickly obtain numerous new networks (stitches) from the
pre-trained models (anchors) in a model family via model stitching. Although
promising, SN-Net confronts new challenges when adapting it to new target
domains, including huge memory and storage requirements and a long and
sub-optimal multistage adaptation process. In this work, we present a novel
framework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce
a palette of fine-tuned models that adhere to diverse resource constraints.
Specifically, we first tailor parameter-efficient fine-tuning to share low-rank
updates among the stitches while maintaining independent bias terms. In this
way, we largely reduce fine-tuning memory burdens and mitigate the interference
among stitches that arises in task adaptation. Furthermore, we streamline a
simple yet effective one-stage deployment pipeline, which estimates the
important stitches to deploy with training-time gradient statistics. By
assigning higher sampling probabilities to important stitches, we also get a
boosted Pareto frontier. Extensive experiments on 25 downstream visual
recognition tasks demonstrate that our ESTA is capable of generating stitches
with smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net
adaptation by remarkable margins with significantly lower training time and
fewer trainable parameters. Furthermore, we demonstrate the flexibility and
scalability of our ESTA framework by stitching LLMs from LLaMA family,
obtaining chatbot stitches of assorted sizes.
</p></li>
</ul>

<h2>long context</h2>
<h2>lora</h2>
<h3>Title: Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond. (arXiv:2311.17133v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17133">http://arxiv.org/abs/2311.17133</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17133]] Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond(http://arxiv.org/abs/2311.17133)</code></li>
<li>Summary: <p>This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.
</p></li>
</ul>

<h3>Title: Continual Learning with Low Rank Adaptation. (arXiv:2311.17601v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17601">http://arxiv.org/abs/2311.17601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17601]] Continual Learning with Low Rank Adaptation(http://arxiv.org/abs/2311.17601)</code></li>
<li>Summary: <p>Recent work using pretrained transformers has shown impressive performance
when fine-tuned with data from the downstream problem of interest. However,
they struggle to retain that performance when the data characteristics changes.
In this paper, we focus on continual learning, where a pre-trained transformer
is updated to perform well on new data, while retaining its performance on data
it was previously trained on. Earlier works have tackled this primarily through
methods inspired from prompt tuning. We question this choice, and investigate
the applicability of Low Rank Adaptation (LoRA) to continual learning. On a
range of domain-incremental learning benchmarks, our LoRA-based solution,
CoLoR, yields state-of-the-art performance, while still being as parameter
efficient as the prompt tuning based methods.
</p></li>
</ul>

<h3>Title: \texttt{GlycoNMR}: Dataset and benchmarks for NMR chemical shift prediction of carbohydrates with graph neural networks. (arXiv:2311.17134v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17134">http://arxiv.org/abs/2311.17134</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17134]] \texttt{GlycoNMR}: Dataset and benchmarks for NMR chemical shift prediction of carbohydrates with graph neural networks(http://arxiv.org/abs/2311.17134)</code></li>
<li>Summary: <p>Molecular representation learning (MRL) is a powerful tool for bridging the
gap between machine learning and chemical sciences, as it converts molecules
into numerical representations while preserving their chemical features. These
encoded representations serve as a foundation for various downstream
biochemical studies, including property prediction and drug design. MRL has had
great success with proteins and general biomolecule datasets. Yet, in the
growing sub-field of glycoscience (the study of carbohydrates, where longer
carbohydrates are also called glycans), MRL methods have been barely explored.
This under-exploration can be primarily attributed to the limited availability
of comprehensive and well-curated carbohydrate-specific datasets and a lack of
Machine learning (ML) pipelines specifically tailored to meet the unique
problems presented by carbohydrate data. Since interpreting and annotating
carbohydrate-specific data is generally more complicated than protein data,
domain experts are usually required to get involved. The existing MRL methods,
predominately optimized for proteins and small biomolecules, also cannot be
directly used in carbohydrate applications without special modifications. To
address this challenge, accelerate progress in glycoscience, and enrich the
data resources of the MRL community, we introduce GlycoNMR. GlycoNMR contains
two laboriously curated datasets with 2,609 carbohydrate structures and 211,543
annotated nuclear magnetic resonance (NMR) chemical shifts for precise
atomic-level prediction. We tailored carbohydrate-specific features and adapted
existing MRL models to tackle this problem effectively. For illustration, we
benchmark four modified MRL models on our new datasets.
</p></li>
</ul>

<h2>hallucination</h2>
<h3>Title: Pragmatic Radiology Report Generation. (arXiv:2311.17154v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17154">http://arxiv.org/abs/2311.17154</a></li>
<li>Code URL: https://github.com/chicagohai/llm_radiology</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17154]] Pragmatic Radiology Report Generation(http://arxiv.org/abs/2311.17154)</code></li>
<li>Summary: <p>When pneumonia is not found on a chest X-ray, should the report describe this
negative observation or omit it? We argue that this question cannot be answered
from the X-ray alone and requires a pragmatic perspective, which captures the
communicative goal that radiology reports serve between radiologists and
patients. However, the standard image-to-text formulation for radiology report
generation fails to incorporate such pragmatic intents. Following this
pragmatic perspective, we demonstrate that the indication, which describes why
a patient comes for an X-ray, drives the mentions of negative observations and
introduce indications as additional input to report generation. With respect to
the output, we develop a framework to identify uninferable information from the
image as a source of model hallucinations, and limit them by cleaning
groundtruth reports. Finally, we use indications and cleaned groundtruth
reports to develop pragmatic models, and show that they outperform existing
methods not only in new pragmatics-inspired metrics (+4.3 Negative F1) but also
in standard metrics (+6.3 Positive F1 and +11.0 BLEU-2).
</p></li>
</ul>

<h2>prompt</h2>
<h2>code</h2>
<h3>Title: Anonymous Jamming Detection in 5G with Bayesian Network Model Based Inference Analysis. (arXiv:2311.17097v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17097">http://arxiv.org/abs/2311.17097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17097]] Anonymous Jamming Detection in 5G with Bayesian Network Model Based Inference Analysis(http://arxiv.org/abs/2311.17097)</code></li>
<li>Summary: <p>Jamming and intrusion detection are critical in 5G research, aiming to
maintain reliability, prevent user experience degradation, and avoid
infrastructure failure. This paper introduces an anonymous jamming detection
model for 5G based on signal parameters from the protocol stacks. The system
uses supervised and unsupervised learning for real-time, high-accuracy
detection of jamming, including unknown types. Supervised models reach an AUC
of 0.964 to 1, compared to LSTM models with an AUC of 0.923 to 1. However, the
need for data annotation limits the supervised approach. To address this, an
unsupervised auto-encoder-based anomaly detection is presented with an AUC of
0.987. The approach is resistant to adversarial training samples. For
transparency and domain knowledge injection, a Bayesian network-based causation
analysis is introduced.
</p></li>
</ul>

<h3>Title: Single-Cell Clustering via Dual-Graph Alignment. (arXiv:2311.17104v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17104">http://arxiv.org/abs/2311.17104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17104]] Single-Cell Clustering via Dual-Graph Alignment(http://arxiv.org/abs/2311.17104)</code></li>
<li>Summary: <p>In recent years, the field of single-cell RNA sequencing has seen a surge in
the development of clustering methods. These methods enable the identification
of cell subpopulations, thereby facilitating the understanding of tumor
microenvironments. Despite their utility, most existing clustering algorithms
primarily focus on the attribute information provided by the cell matrix or the
network structure between cells, often neglecting the network between genes.
This oversight could lead to loss of information and clustering results that
lack clinical significance. To address this limitation, we develop an advanced
single-cell clustering model incorporating dual-graph alignment, which
integrates gene network information into the clustering process based on
self-supervised and unsupervised optimization. Specifically, we designed a
graph-based autoencoder enhanced by an attention mechanism to effectively
capture relationships between cells. Moreover, we performed the node2vec method
on Protein-Protein Interaction (PPI) networks to derive the gene network
structure and maintained this structure throughout the clustering process. Our
proposed method has been demonstrated to be effective through experimental
results, showcasing its ability to optimize clustering outcomes while
preserving the original associations between cells and genes. This research
contributes to obtaining accurate cell subpopulations and generates clustering
results that more closely resemble real-world biological scenarios. It provides
better insights into the characteristics and distribution of diseased cells,
ultimately building a foundation for early disease diagnosis and treatment.
</p></li>
</ul>

<h3>Title: Enhancing the Performance of Neural Networks Through Causal Discovery and Integration of Domain Knowledge. (arXiv:2311.17303v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17303">http://arxiv.org/abs/2311.17303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17303]] Enhancing the Performance of Neural Networks Through Causal Discovery and Integration of Domain Knowledge(http://arxiv.org/abs/2311.17303)</code></li>
<li>Summary: <p>In this paper, we develop a generic methodology to encode hierarchical
causality structure among observed variables into a neural network in order to
improve its predictive performance. The proposed methodology, called
causality-informed neural network (CINN), leverages three coherent steps to
systematically map the structural causal knowledge into the layer-to-layer
design of neural network while strictly preserving the orientation of every
causal relationship. In the first step, CINN discovers causal relationships
from observational data via directed acyclic graph (DAG) learning, where causal
discovery is recast as a continuous optimization problem to avoid the
combinatorial nature. In the second step, the discovered hierarchical causality
structure among observed variables is systematically encoded into neural
network through a dedicated architecture and customized loss function. By
categorizing variables in the causal DAG as root, intermediate, and leaf nodes,
the hierarchical causal DAG is translated into CINN with a one-to-one
correspondence between nodes in the causal DAG and units in the CINN while
maintaining the relative order among these nodes. Regarding the loss function,
both intermediate and leaf nodes in the DAG graph are treated as target outputs
during CINN training so as to drive co-learning of causal relationships among
different types of nodes. As multiple loss components emerge in CINN, we
leverage the projection of conflicting gradients to mitigate gradient
interference among the multiple learning tasks. Computational experiments
across a broad spectrum of UCI data sets demonstrate substantial advantages of
CINN in predictive performance over other state-of-the-art methods. In
addition, an ablation study underscores the value of integrating structural and
quantitative causal knowledge in enhancing the neural network's predictive
performance incrementally.
</p></li>
</ul>

<h3>Title: TaskWeaver: A Code-First Agent Framework. (arXiv:2311.17541v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17541">http://arxiv.org/abs/2311.17541</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17541]] TaskWeaver: A Code-First Agent Framework(http://arxiv.org/abs/2311.17541)</code></li>
<li>Summary: <p>Language Language Models (LLMs) have shown impressive abilities in natural
language understanding and generation, leading to their use in applications
such as chatbots and virtual assistants. However, existing LLM frameworks face
limitations in handling domain-specific data analytics tasks with rich data
structures. Moreover, they struggle with flexibility to meet diverse user
requirements. To address these issues, TaskWeaver is proposed as a code-first
framework for building LLM-powered autonomous agents. It converts user requests
into executable code and treats user-defined plugins as callable functions.
TaskWeaver provides support for rich data structures, flexible plugin usage,
and dynamic plugin selection, and leverages LLM coding capabilities for complex
logic. It also incorporates domain-specific knowledge through examples and
ensures the secure execution of generated code. TaskWeaver offers a powerful
and flexible framework for creating intelligent conversational agents that can
handle complex tasks and adapt to domain-specific scenarios. The code is
open-sourced at https://github.com/microsoft/TaskWeaver/.
</p></li>
</ul>

<h3>Title: Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data. (arXiv:2311.17492v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17492">http://arxiv.org/abs/2311.17492</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17492]] Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data(http://arxiv.org/abs/2311.17492)</code></li>
<li>Summary: <p>The Manchu language, with its roots in the historical Manchurian region of
Northeast China, is now facing a critical threat of extinction, as there are
very few speakers left. In our efforts to safeguard the Manchu language, we
introduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation
(MT) model. To develop this model, we utilize valuable resources such as the
Manwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the
scarcity of a Manchu-Korean parallel dataset, we expand our data by employing
word replacement guided by GloVe embeddings, trained on both monolingual and
parallel texts. Our approach is built around an encoder-decoder neural machine
translation model, incorporating a bi-directional Gated Recurrent Unit (GRU)
layer. The experiments have yielded promising results, showcasing a significant
enhancement in Manchu-Korean translation, with a remarkable 20-30 point
increase in the BLEU score.
</p></li>
</ul>

<h3>Title: SenTest: Evaluating Robustness of Sentence Encoders. (arXiv:2311.17722v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17722">http://arxiv.org/abs/2311.17722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17722]] SenTest: Evaluating Robustness of Sentence Encoders(http://arxiv.org/abs/2311.17722)</code></li>
<li>Summary: <p>Contrastive learning has proven to be an effective method for pre-training
models using weakly labeled data in the vision domain. Sentence transformers
are the NLP counterparts to this architecture, and have been growing in
popularity due to their rich and effective sentence representations. Having
effective sentence representations is paramount in multiple tasks, such as
information retrieval, retrieval augmented generation (RAG), and sentence
comparison. Keeping in mind the deployability factor of transformers,
evaluating the robustness of sentence transformers is of utmost importance.
This work focuses on evaluating the robustness of the sentence encoders. We
employ several adversarial attacks to evaluate its robustness. This system uses
character-level attacks in the form of random character substitution,
word-level attacks in the form of synonym replacement, and sentence-level
attacks in the form of intra-sentence word order shuffling. The results of the
experiments strongly undermine the robustness of sentence encoders. The models
produce significantly different predictions as well as embeddings on perturbed
datasets. The accuracy of the models can fall up to 15 percent on perturbed
datasets as compared to unperturbed datasets. Furthermore, the experiments
demonstrate that these embeddings does capture the semantic and syntactic
structure (sentence order) of sentences. However, existing supervised
classification strategies fail to leverage this information, and merely
function as n-gram detectors.
</p></li>
</ul>

<h3>Title: Improving Self-supervised Molecular Representation Learning using Persistent Homology. (arXiv:2311.17327v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17327">http://arxiv.org/abs/2311.17327</a></li>
<li>Code URL: https://github.com/luoyk1999/molecular-homology</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17327]] Improving Self-supervised Molecular Representation Learning using Persistent Homology(http://arxiv.org/abs/2311.17327)</code></li>
<li>Summary: <p>Self-supervised learning (SSL) has great potential for molecular
representation learning given the complexity of molecular graphs, the large
amounts of unlabelled data available, the considerable cost of obtaining labels
experimentally, and the hence often only small training datasets. The
importance of the topic is reflected in the variety of paradigms and
architectures that have been investigated recently. Yet the differences in
performance seem often minor and are barely understood to date. In this paper,
we study SSL based on persistent homology (PH), a mathematical tool for
modeling topological features of data that persist across multiple scales. It
has several unique features which particularly suit SSL, naturally offering:
different views of the data, stability in terms of distance preservation, and
the opportunity to flexibly incorporate domain knowledge. We (1) investigate an
autoencoder, which shows the general representational power of PH, and (2)
propose a contrastive loss that complements existing approaches. We rigorously
evaluate our approach for molecular property prediction and demonstrate its
particular features in improving the embedding space: after SSL, the
representations are better and offer considerably more predictive power than
the baselines over different probing tasks; our loss increases baseline
performance, sometimes largely; and we often obtain substantial improvements
over very small datasets, a common scenario in practice.
</p></li>
</ul>

<h2>chat</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
