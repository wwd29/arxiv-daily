<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-02</h1>
<h3>Title: Optimized Task Assignment and Predictive Maintenance for Industrial  Machines using Markov Decision Process</h3>
<ul>
<li><strong>Authors: </strong>Ali Nasir, Samir Mekid, Zaid Sawlan, Omar Alsawafy</a></li>
<li><strong>Subjects: </strong>cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00042">https://arxiv.org/abs/2402.00042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00042">https://arxiv.org/pdf/2402.00042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00042]] Optimized Task Assignment and Predictive Maintenance for Industrial  Machines using Markov Decision Process(https://arxiv.org/abs/2402.00042)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>This paper considers a distributed decision-making approach for manufacturing task assignment and condition-based machine health maintenance. Our approach considers information sharing between the task assignment and health management decision-making agents. We propose the design of the decision-making agents based on Markov decision processes. The key advantage of using a Markov decision process-based approach is the incorporation of uncertainty involved in the decision-making process. The paper provides detailed mathematical models along with the associated practical execution strategy. In order to demonstrate the effectiveness and practical applicability of our proposed approach, we have included a detailed numerical case study that is based on open source milling machine tool degradation data. Our case study indicates that the proposed approach offers flexibility in terms of the selection of cost parameters and it allows for offline computation and analysis of the decision-making policy. These features create and opportunity for the future work on learning of the cost parameters associated with our proposed model using artificial intelligence.</li>
<li><strong>摘要：</strong>本文考虑了一种用于制造任务分配和基于状态的机器健康维护的分布式决策方法。我们的方法考虑任务分配和健康管理决策代理之间的信息共享。我们提出基于马尔可夫决策过程的决策代理设计。使用基于马尔可夫决策过程的方法的主要优点是在决策过程中纳入了不确定性。本文提供了详细的数学模型以及相关的实际执行策略。为了证明我们提出的方法的有效性和实际适用性，我们提供了基于开源铣床刀具退化数据的详细数值案例研究。我们的案例研究表明，所提出的方法在成本参数的选择方面提供了灵活性，并且允许对决策政策进行离线计算和分析。这些功能为未来使用人工智能学习与我们提出的模型相关的成本参数的工作创造了机会。</li>
</ul>

<h3>Title: Introducing PetriRL: An Innovative Framework for JSSP Resolution  Integrating Petri nets and Event-based Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Sofiene Lassoued, Andreas Schwung</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00046">https://arxiv.org/abs/2402.00046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00046">https://arxiv.org/pdf/2402.00046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00046]] Introducing PetriRL: An Innovative Framework for JSSP Resolution  Integrating Petri nets and Event-based Reinforcement Learning(https://arxiv.org/abs/2402.00046)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag, agent</a></li>
<li><strong>Abstract: </strong>Quality scheduling in industrial job shops is crucial. Although neural networks excel in solving these problems, their limited explainability hinders their widespread industrial adoption. In this research, we introduce an innovative framework for solving job shop scheduling problems (JSSP). Our methodology leverages Petri nets to model the job shop, not only improving explainability but also enabling direct incorporation of raw data without the need to preprocess JSSP instances into disjunctive graphs. The Petri net, with its controlling capacities, also governs the automated components of the process, allowing the agent to focus on critical decision-making, particularly resource allocation. The integration of event-based control and action masking in our approach yields competitive performance on public test benchmarks. Comparative analyses across a wide spectrum of optimization solutions, including heuristics, metaheuristics, and learning-based algorithms, highlight the competitiveness of our approach in large instances and its superiority over all competitors in small to medium-sized scenarios. Ultimately, our approach not only demonstrates a robust ability to generalize across various instance sizes but also leverages the Petri net's graph nature to dynamically add job operations during the inference phase without the need for agent retraining, thereby enhancing flexibility.</li>
<li><strong>摘要：</strong>工业车间的质量调度至关重要。尽管神经网络擅长解决这些问题，但其有限的可解释性阻碍了其广泛的工业应用。在这项研究中，我们引入了一种解决作业车间调度问题（JSSP）的创新框架。我们的方法利用 Petri 网对作业车间进行建模，不仅提高了可解释性，而且还可以直接合并原始数据，而无需将 JSSP 实例预处理为析取图。 Petri 网具有控制能力，还可以管理流程的自动化组件，使代理能够专注于关键决策，特别是资源分配。我们的方法中基于事件的控制和动作屏蔽的集成在公共测试基准上产生了具有竞争力的性能。对各种优化解决方案（包括启发式、元启发式和基于学习的算法）的比较分析，突出了我们的方法在大型实例中的竞争力以及在中小型场景中相对于所有竞争对手的优越性。最终，我们的方法不仅展示了跨各种实例大小进行泛化的强大能力，而且还利用 Petri 网的图性质在推理阶段动态添加作业操作，而无需代理重新训练，从而增强了灵活性。</li>
</ul>

<h3>Title: IICONGRAPH: improved Iconographic and Iconological Statements in  Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Bruno Sartini</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00048">https://arxiv.org/abs/2402.00048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00048">https://arxiv.org/pdf/2402.00048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00048]] IICONGRAPH: improved Iconographic and Iconological Statements in  Knowledge Graphs(https://arxiv.org/abs/2402.00048)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Iconography and iconology are fundamental domains when it comes to understanding artifacts of cultural heritage. Iconography deals with the study and interpretation of visual elements depicted in artifacts and their symbolism, while iconology delves deeper, exploring the underlying cultural and historical meanings. Despite the advances in representing cultural heritage with Linked Open Data (LOD), recent studies show persistent gaps in the representation of iconographic and iconological statements in current knowledge graphs (KGs). To address them, this paper presents IICONGRAPH, a KG that was created by refining and extending the iconographic and iconological statements of ArCo (the Italian KG of cultural heritage) and Wikidata. The development of IICONGRAPH was also driven by a series of requirements emerging from research case studies that were unattainable in the non-reengineered versions of the KGs. The evaluation results demonstrate that IICONGRAPH not only outperforms ArCo and Wikidata through domain-specific assessments from the literature but also serves as a robust platform for addressing the formulated research questions. IICONGRAPH is released and documented in accordance with the FAIR principles to guarantee the resource's reusability. The algorithms used to create it and assess the research questions have also been made available to ensure transparency and reproducibility. While future work focuses on ingesting more data into the KG, and on implementing it as a backbone of LLM-based question answering systems, the current version of IICONGRAPH still emerges as a valuable asset, contributing to the evolving landscape of cultural heritage representation within Knowledge Graphs, the Semantic Web, and beyond.</li>
<li><strong>摘要：</strong>图像学和图像学是理解文化遗产文物的基本领域。图像学涉及对文物中描绘的视觉元素及其象征意义的研究和解释，而图像学则进行更深入的研究，探索潜在的文化和历史意义。尽管在利用链接开放数据（LOD）表示文化遗产方面取得了进展，但最近的研究表明，当前知识图谱（KG）中的图像学和图像学陈述的表示仍然存在差距。为了解决这些问题，本文提出了 IICONGRAPH，这是一个通过完善和扩展 ArCo（意大利文化遗产 KG）和维基数据的图像学和图像学陈述而创建的 KG。 IICONGRAPH 的开发还受到研究案例研究中出现的一系列要求的推动，这些要求在 KG 的非重新设计版本中是无法实现的。评估结果表明，IICONGRAPH 不仅通过文献中特定领域的评估优于 ArCo 和 Wikidata，而且还可以作为解决已制定的研究问题的强大平台。 IICONGRAPH按照FAIR原则发布和记录，保证资源的可重用性。用于创建它和评估研究问题的算法也已提供，以确保透明度和可重复性。虽然未来的工作重点是将更多数据引入知识图谱，并将其作为基于法学硕士的问答系统的支柱实施，但当前版本的 IICONGRAPH 仍然是一项宝贵的资产，有助于知识领域内文化遗产表示的不断发展的格局图、语义网等等。</li>
</ul>

<h3>Title: Zero-shot Sequential Neuro-symbolic Reasoning for Automatically  Generating Architecture Schematic Designs</h3>
<ul>
<li><strong>Authors: </strong>Milin Kodnongbua, Lawrence H. Curtis, Adriana Schulz</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00052">https://arxiv.org/abs/2402.00052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00052">https://arxiv.org/pdf/2402.00052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00052]] Zero-shot Sequential Neuro-symbolic Reasoning for Automatically  Generating Architecture Schematic Designs(https://arxiv.org/abs/2402.00052)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, rag</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel automated system for generating architecture schematic designs aimed at streamlining complex decision-making at the multifamily real estate development project's outset. Leveraging the combined strengths of generative AI (neuro reasoning) and mathematical program solvers (symbolic reasoning), the method addresses both the reliance on expert insights and technical challenges in architectural schematic design. To address the large-scale and interconnected nature of design decisions needed for designing a whole building, we proposed a novel sequential neuro-symbolic reasoning approach, emulating traditional architecture design processes from initial concept to detailed layout. To remove the need to hand-craft a cost function to approximate the desired objectives, we propose a solution that uses neuro reasoning to generate constraints and cost functions that the symbolic solvers can use to solve. We also incorporate feedback loops for each design stage to ensure a tight integration between neuro and symbolic reasoning. Developed using GPT-4 without further training, our method's effectiveness is validated through comparative studies with real-world buildings. Our method can generate various building designs in accordance with the understanding of the neighborhood, showcasing its potential to transform the realm of architectural schematic design.</li>
<li><strong>摘要：</strong>本文介绍了一种用于生成建筑原理图设计的新型自动化系统，旨在简化多户型房地产开发项目一开始的复杂决策。该方法利用生成式人工智能（神经推理）和数学程序求解器（符号推理）的综合优势，解决了建筑方案设计中对专家见解的依赖和技术挑战。为了解决设计整个建筑所需的设计决策的大规模和相互关联的性质，我们提出了一种新颖的顺序神经符号推理方法，模拟从最初概念到详细布局的传统建筑设计过程。为了消除手工制作成本函数来近似期望目标的需要，我们提出了一种解决方案，该解决方案使用神经推理来生成符号求解器可以用来求解的约束和成本函数。我们还为每个设计阶段纳入反馈循环，以确保神经推理和符号推理之间的紧密集成。我们的方法是使用 GPT-4 开发的，无需进一步培训，通过与现实世界建筑的比较研究验证了我们方法的有效性。我们的方法可以根据对社区的理解生成各种建筑设计，展示了其改变建筑方案设计领域的潜力。</li>
</ul>

<h3>Title: Merging plans with incomplete knowledge about actions and goals through  an agent-based reputation system</h3>
<ul>
<li><strong>Authors: </strong>Javier Carbo, Jose M Molina, Miguel A Patricio</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00064">https://arxiv.org/abs/2402.00064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00064">https://arxiv.org/pdf/2402.00064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00064]] Merging plans with incomplete knowledge about actions and goals through  an agent-based reputation system(https://arxiv.org/abs/2402.00064)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Managing transition plans is one of the major problems of people with cognitive disabilities. Therefore, finding an automated way to generate such plans would be a helpful tool for this community. In this paper we have specifically proposed and compared different alternative ways to merge plans formed by sequences of actions of unknown similarities between goals and actions executed by several operator agents which cooperate between them applying such actions over some passive elements (node agents) that require additional executions of another plan after some time of use. Such ignorance of the similarities between plan actions and goals would justify the use of a distributed recommendation system that would provide an useful plan to be applied for a certain goal to a given operator agent, generated from the known results of previous executions of different plans by other operator agents. Here we provide the general framework of execution (agent system), and the different merging algorithms applied to this problem. The proposed agent system would act as an useful cognitive assistant for people with intelectual disabilities such as autism.</li>
<li><strong>摘要：</strong>管理过渡计划是认知障碍人士面临的主要问题之一。因此，找到一种自动生成此类计划的方法对于该社区来说将是一个有用的工具。在本文中，我们特别提出并比较了不同的替代方法来合并由目标和由多个操作员代理执行的操作之间未知相似性的操作序列形成的计划，这些操作员代理在它们之间合作，将这些操作应用于一些需要额外的被动元素（节点代理）。使用一段时间后执行另一个计划。这种对计划行动和目标之间相似性的无知将证明使用分布式推荐系统是合理的，该系统将向给定的操作代理提供适用于特定目标的有用计划，该计划是根据不同计划的先前执行的已知结果生成的其他运营商代理。在这里，我们提供了执行的一般框架（代理系统），以及应用于该问题的不同合并算法。拟议的代理系统将为患有自闭症等智力障碍的人充当有用的认知助手。</li>
</ul>

<h3>Title: TrackGPT -- A generative pre-trained transformer for cross-domain entity  trajectory forecasting</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Stroh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00066">https://arxiv.org/abs/2402.00066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00066">https://arxiv.org/pdf/2402.00066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00066]] TrackGPT -- A generative pre-trained transformer for cross-domain entity  trajectory forecasting(https://arxiv.org/abs/2402.00066)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The forecasting of entity trajectories at future points in time is a critical capability gap in applications across both Commercial and Defense sectors. Transformers, and specifically Generative Pre-trained Transformer (GPT) networks have recently revolutionized several fields of Artificial Intelligence, most notably Natural Language Processing (NLP) with the advent of Large Language Models (LLM) like OpenAI's ChatGPT. In this research paper, we introduce TrackGPT, a GPT-based model for entity trajectory forecasting that has shown utility across both maritime and air domains, and we expect to perform well in others. TrackGPT stands as a pioneering GPT model capable of producing accurate predictions across diverse entity time series datasets, demonstrating proficiency in generating both long-term forecasts with sustained accuracy and short-term forecasts with high precision. We present benchmarks against state-of-the-art deep learning techniques, showing that TrackGPT's forecasting capability excels in terms of accuracy, reliability, and modularity. Importantly, TrackGPT achieves these results while remaining domain-agnostic and requiring minimal data features (only location and time) compared to models achieving similar performance. In conclusion, our findings underscore the immense potential of applying GPT architectures to the task of entity trajectory forecasting, exemplified by the innovative TrackGPT model.</li>
<li><strong>摘要：</strong>未来时间点的实体轨迹预测是商业和国防部门应用中的关键能力差距。 Transformers，特别是生成式预训练 Transformer (GPT) 网络最近彻底改变了人工智能的多个领域，尤其是随着 OpenAI 的 ChatGPT 等大型语言模型 (LLM) 的出现，自然语言处理 (NLP) 发生了革命性的变化。在这篇研究论文中，我们介绍了 TrackGPT，这是一种基于 GPT 的实体轨迹预测模型，该模型已在海上和空中领域显示出实用性，并且我们预计在其他领域也能表现良好。 TrackGPT 是一种开创性的 GPT 模型，能够在不同的实体时间序列数据集上生成准确的预测，证明了其能够熟练地生成持续准确的长期预测和高精度的短期预测。我们提供了针对最先进深度学习技术的基准，表明 TrackGPT 的预测能力在准确性、可靠性和模块化方面表现出色。重要的是，与实现类似性能的模型相比，TrackGPT 实现了这些结果，同时保持与领域无关，并且需要最少的数据特征（仅位置和时间）。总之，我们的研究结果强调了将 GPT 架构应用于实体轨迹预测任务的巨大潜力，创新的 TrackGPT 模型就是例证。</li>
</ul>

<h3>Title: GPT4Battery: An LLM-driven Framework for Adaptive State of Health  Estimation of Raw Li-ion Batteries</h3>
<ul>
<li><strong>Authors: </strong>Yuyuan Feng, Guosheng Hu, Zhihong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00068">https://arxiv.org/abs/2402.00068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00068">https://arxiv.org/pdf/2402.00068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00068]] GPT4Battery: An LLM-driven Framework for Adaptive State of Health  Estimation of Raw Li-ion Batteries(https://arxiv.org/abs/2402.00068)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>State of health (SOH) is a crucial indicator for assessing the degradation level of batteries that cannot be measured directly but requires estimation. Accurate SOH estimation enhances detection, control, and feedback for Li-ion batteries, allowing for safe and efficient energy management and guiding the development of new-generation batteries. Despite the significant progress in data-driven SOH estimation, the time and resource-consuming degradation experiments for generating lifelong training data pose a challenge in establishing one large model capable of handling diverse types of Li-ion batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity. Hence, this paper utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries. To match the real scenario where unlabeled data sequentially arrives in use with distribution shifts, the proposed model is modified by a test-time training technique to ensure estimation accuracy even at the battery's end of life. The validation results demonstrate that the proposed framework achieves state-of-the-art accuracy on four widely recognized datasets collected from 62 batteries. Furthermore, we analyze the theoretical challenges of cross-battery estimation and provide a quantitative explanation of the effectiveness of our method.</li>
<li><strong>摘要：</strong>健康状态（SOH）是评估电池退化程度的重要指标，无法直接测量，但需要估计。准确的SOH估算增强了对锂离子电池的检测、控制和反馈，从而实现安全高效的能源管理并指导新一代电池的开发。尽管数据驱动的 SOH 估计取得了重大进展，但生成终生训练数据的耗时和资源消耗的退化实验对建立能够处理不同类型锂离子电池（例如跨化学、交叉电池）的大型模型提出了挑战。 -制造商和跨能力。因此，本文利用大语言模型（LLM）强大的泛化能力提出了一种新的框架，用于跨不同电池的适应性 SOH 估计。为了匹配未标记数据按分布变化顺序到达使用的真实场景，通过测试时训练技术对所提出的模型进行了修改，以确保即使在电池寿命结束时也能确保估计准确性。验证结果表明，所提出的框架在从 62 个电池收集的四个广泛认可的数据集上实现了最先进的准确性。此外，我们分析了跨电池估计的理论挑战，并对我们的方法的有效性提供了定量解释。</li>
</ul>

<h3>Title: Unraveling the Impact of Initial Choices and In-Loop Interventions on  Learning Dynamics in Autonomous Scanning Probe Microscopy</h3>
<ul>
<li><strong>Authors: </strong>Boris N. Slautin, Yongtao Liu, Hiroshi Funakubo, Sergei V. Kalinin</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00071">https://arxiv.org/abs/2402.00071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00071">https://arxiv.org/pdf/2402.00071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00071]] Unraveling the Impact of Initial Choices and In-Loop Interventions on  Learning Dynamics in Autonomous Scanning Probe Microscopy(https://arxiv.org/abs/2402.00071)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, code</a></li>
<li><strong>Abstract: </strong>The current focus in Autonomous Experimentation (AE) is on developing robust workflows to conduct the AE effectively. This entails the need for well-defined approaches to guide the AE process, including strategies for hyperparameter tuning and high-level human interventions within the workflow loop. This paper presents a comprehensive analysis of the influence of initial experimental conditions and in-loop interventions on the learning dynamics of Deep Kernel Learning (DKL) within the realm of AE in Scanning Probe Microscopy. We explore the concept of 'seed effect', where the initial experiment setup has a substantial impact on the subsequent learning trajectory. Additionally, we introduce an approach of the seed point interventions in AE allowing the operator to influence the exploration process. Using a dataset from Piezoresponse Force Microscopy (PFM) on PbTiO3 thin films, we illustrate the impact of the 'seed effect' and in-loop seed interventions on the effectiveness of DKL in predicting material properties. The study highlights the importance of initial choices and adaptive interventions in optimizing learning rates and enhancing the efficiency of automated material characterization. This work offers valuable insights into designing more robust and effective AE workflows in microscopy with potential applications across various characterization techniques. The analysis code that supports the funding is publicly available at https://github.com/Slautin/2024_Seed_effect_DKL_BO.</li>
<li><strong>摘要：</strong>自主实验 (AE) 当前的重点是开发强大的工作流程以有效地进行 AE。这需要明确的方法来指导 AE 过程，包括超参数调整策略和工作流程循环中的高级人工干预。本文全面分析了扫描探针显微镜 AE 领域中初始实验条件和循环干预对深度核学习 (DKL) 学习动态的影响。我们探索“种子效应”的概念，即初始实验设置对后续的学习轨迹产生重大影响。此外，我们引入了一种在 AE 中进行种子点干预的方法，允许操作员影响探索过程。使用 PbTiO3 薄膜上的压电响应力显微镜 (PFM) 的数据集，我们说明了“种子效应”和循环种子干预对 DKL 预测材料特性的有效性的影响。该研究强调了初始选择和适应性干预在优化学习率和提高自动化材料表征效率方面的重要性。这项工作为在显微镜中设计更强大、更有效的 AE 工作流程提供了宝贵的见解，并在各种表征技术中具有潜在的应用。支持资助的分析代码可在 https://github.com/Slautin/2024_Seed_effect_DKL_BO 上公开获取。</li>
</ul>

<h3>Title: Exploitation Strategies in Conditional Markov Chain Search: A case study  on the three-index assignment problem</h3>
<ul>
<li><strong>Authors: </strong>Sahil Patel, Daniel Karapetyan</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00076">https://arxiv.org/abs/2402.00076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00076">https://arxiv.org/pdf/2402.00076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00076]] Exploitation Strategies in Conditional Markov Chain Search: A case study  on the three-index assignment problem(https://arxiv.org/abs/2402.00076)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>The Conditional Markov Chain Search (CMCS) is a framework for automated design of metaheuristics for discrete combinatorial optimisation problems. Given a set of algorithmic components such as hill climbers and mutations, CMCS decides in which order to apply those components. The decisions are dictated by the CMCS configuration that can be learnt offline. CMCS does not have an acceptance criterion; any moves are accepted by the framework. As a result, it is particularly good in exploration but is not as good at exploitation. In this study, we explore several extensions of the framework to improve its exploitation abilities. To perform a computational study, we applied the framework to the three-index assignment problem. The results of our experiments showed that a two-stage CMCS is indeed superior to a single-stage CMCS.</li>
<li><strong>摘要：</strong>条件马尔可夫链搜索 (CMCS) 是用于离散组合优化问题的元启发式自动设计框架。给定一组算法组件（例如登山器和突变），CMCS 决定应用这些组件的顺序。这些决策由可离线学习的 CMCS 配置决定。 CMCS 没有验收标准；框架接受任何移动。因此，它特别擅长探索，但不擅长开发。在本研究中，我们探索了该框架的几种扩展，以提高其开发能力。为了进行计算研究，我们将该框架应用于三索引分配问题。我们的实验结果表明，两级 CMCS 确实优于单级 CMCS。</li>
</ul>

<h3>Title: Modeling Access Differences to Reduce Disparity in Resource Allocation</h3>
<ul>
<li><strong>Authors: </strong>Kenya Andrews, Mesrob Ohannessian, Tanya Berger-Wolf</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00083">https://arxiv.org/abs/2402.00083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00083">https://arxiv.org/pdf/2402.00083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00083]] Modeling Access Differences to Reduce Disparity in Resource Allocation(https://arxiv.org/abs/2402.00083)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Motivated by COVID-19 vaccine allocation, where vulnerable subpopulations are simultaneously more impacted in terms of health and more disadvantaged in terms of access to the vaccine, we formalize and study the problem of resource allocation when there are inherent access differences that correlate with advantage and disadvantage. We identify reducing resource disparity as a key goal in this context and show its role as a proxy to more nuanced downstream impacts. We develop a concrete access model that helps quantify how a given allocation translates to resource flow for the advantaged vs. the disadvantaged, based on the access gap between them. We then provide a methodology for access-aware allocation. Intuitively, the resulting allocation leverages more vaccines in locations with higher vulnerable populations to mitigate the access gap and reduce overall disparity. Surprisingly, knowledge of the access gap is often not needed to perform access-aware allocation. To support this formalism, we provide empirical evidence for our access model and show that access-aware allocation can significantly reduce resource disparity and thus improve downstream outcomes. We demonstrate this at various scales, including at county, state, national, and global levels.</li>
<li><strong>摘要：</strong>在 COVID-19 疫苗分配的推动下，弱势群体在健康方面同时受到更大的影响，并且在获得疫苗方面处于更不利的地位，我们正式确定并研究了当存在与优势和相关的固有获取差异时的资源分配问题。坏处。我们将减少资源差距确定为在此背景下的一个关键目标，并展示其作为更微妙的下游影响的代表的作用。我们开发了一个具体的访问模型，有助于根据优势群体与弱势群体之间的访问差距，量化给定的分配如何转化为资源流。然后，我们提供了一种访问感知分配的方法。直观地说，由此产​​生的分配会在弱势群体较多的地区利用更多疫苗，以缩小获取差距并缩小总体差距。令人惊讶的是，执行访问感知分配通常不需要了解访问间隙。为了支持这种形式主义，我们为我们的访问模型提供了经验证据，并表明访问感知分配可以显着减少资源差距，从而改善下游结果。我们在不同的层面上展示了这一点，包括县、州、国家和全球层面。</li>
</ul>

<h3>Title: Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy  Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuecheng Niu, Akinori Ito, Takashi Nose</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00085">https://arxiv.org/abs/2402.00085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00085">https://arxiv.org/pdf/2402.00085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00085]] Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy  Learning(https://arxiv.org/abs/2402.00085)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, agent</a></li>
<li><strong>Abstract: </strong>Training task-oriented dialog agents based on reinforcement learning is time-consuming and requires a large number of interactions with real users. How to grasp dialog policy within limited dialog experiences remains an obstacle that makes the agent training process less efficient. In addition, most previous frameworks start training by randomly choosing training samples, which differs from the human learning method and hurts the efficiency and stability of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a curiosity-driven curriculum learning framework based on a state-of-the-art model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ). Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively, following two opposite training strategies: classic curriculum learning and its reverse version. Our results show that by introducing scheduled learning and curiosity, the new framework leads to a significant improvement over the DDQ and Deep Q-learning(DQN). Surprisingly, we found that traditional curriculum learning was not always effective. Specifically, according to the experimental results, the easy-first and difficult-first strategies are more suitable for SC-DDQ and DDQ. To analyze our results, we adopted the entropy of sampled actions to depict action exploration and found that training strategies with high entropy in the first stage and low entropy in the last stage lead to better performance.</li>
<li><strong>摘要：</strong>基于强化学习训练面向任务的对话代理非常耗时，并且需要与真实用户进行大量交互。如何在有限的对话经验中掌握对话策略仍然是导致代理训练过程效率较低的一个障碍。此外，以往的框架大多通过随机选择训练样本来开始训练，这与人类的学习方法不同，损害了训练的效率和稳定性。因此，我们提出了 Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)，这是一种好奇心驱动的课程学习框架，基于最先进的基于模型的强化学习对话模型 Deep Dyna-Q (DDQ)。此外，我们分别遵循两种相反的训练策略：经典课程学习及其反向版本，分别设计了 SC-DDQ 和 DDQ 的学习计划。我们的结果表明，通过引入计划学习和好奇心，新框架比 DDQ 和深度 Q 学习（DQN）有了显着改进。令人惊讶的是，我们发现传统课程学习并不总是有效。具体来说，根据实验结果，先易后难的策略更适合SC-DDQ和DDQ。为了分析我们的结果，我们采用采样动作的熵来描述动作探索，发现第一阶段采用高熵、最后阶段采用低熵的训练策略会带来更好的性能。</li>
</ul>

<h3>Title: Comparing Template-based and Template-free Language Model Probing</h3>
<ul>
<li><strong>Authors: </strong>Sagi Shaier, Kevin Bennett, Lawrence E Hunter, Katharina von der Wense</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00123">https://arxiv.org/abs/2402.00123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00123">https://arxiv.org/pdf/2402.00123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00123]] Comparing Template-based and Template-free Language Model Probing(https://arxiv.org/abs/2402.00123)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets -- 4 template-based and 6 template-free -- in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches? (RQ2) Do models' absolute scores differ between the two approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and domain-specific models? Our findings are: 1) Template-free and template-based approaches often rank models differently, except for the top domain-specific models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts. 3) Perplexity is negatively correlated with accuracy in the template-free approach, but, counter-intuitively, they are positively correlated for template-based probing. 4) Models tend to predict the same answers frequently across prompts for template-based probing, which is less common when employing template-free techniques.</li>
<li><strong>摘要：</strong>完形填空语言模型 (LM) 探测与 1) 专家制作的模板和 2) 自然出现的文本之间的差异经常被忽视。在这里，我们在一般和生物医学领域的 10 个探索性英语数据集（4 个基于模板和 6 个无模板）上评估 16 个不同的 LM，以回答以下研究问题：（RQ1）两种方法之间的模型排名是否不同？ (RQ2) 两种方法之间模型的绝对分数是否不同？ （RQ3）通用模型和特定领域模型之间 RQ1 和 RQ2 的答案是否不同？我们的发现是：1）无模板和基于模板的方法通常对模型进行不同的排名，除了顶级的特定领域模型。 2) 比较并行的无模板和基于模板的提示时，分数下降高达 42% Acc@1。 3）在无模板方法中，困惑度与准确性呈负相关，但与直觉相反，它们对于基于模板的探测呈正相关。 4) 模型倾向于在基于模板的探测提示中频繁预测相同的答案，这在采用无模板技术时不太常见。</li>
</ul>

<h3>Title: Multimodal Neurodegenerative Disease Subtyping Explained by ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Diego Machado Reyes, Hanqing Chao, Juergen Hahn, Li Shen, Pingkun Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00137">https://arxiv.org/abs/2402.00137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00137">https://arxiv.org/pdf/2402.00137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00137]] Multimodal Neurodegenerative Disease Subtyping Explained by ChatGPT(https://arxiv.org/abs/2402.00137)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is the most prevalent neurodegenerative disease; yet its currently available treatments are limited to stopping disease progression. Moreover, effectiveness of these treatments is not guaranteed due to the heterogenetiy of the disease. Therefore, it is essential to be able to identify the disease subtypes at a very early stage. Current data driven approaches are able to classify the subtypes at later stages of AD or related disorders, but struggle when predicting at the asymptomatic or prodromal stage. Moreover, most existing models either lack explainability behind the classification or only use a single modality for the assessment, limiting scope of its analysis. Thus, we propose a multimodal framework that uses early-stage indicators such as imaging, genetics and clinical assessments to classify AD patients into subtypes at early stages. Similarly, we build prompts and use large language models, such as ChatGPT, to interpret the findings of our model. In our framework, we propose a tri-modal co-attention mechanism (Tri-COAT) to explicitly learn the cross-modal feature associations. Our proposed model outperforms baseline models and provides insight into key cross-modal feature associations supported by known biological mechanisms.</li>
<li><strong>摘要：</strong>阿尔茨海默病 (AD) 是最常见的神经退行性疾病；但目前可用的治疗方法仅限于阻止疾病进展。此外，由于疾病的异质性，这些治疗的有效性无法得到保证。因此，必须能够在很早期阶段识别疾病亚型。目前的数据驱动方法能够对 AD 或相关疾病后期的亚型进行分类，但在无症状或前驱阶段进行预测时却很困难。此外，大多数现有模型要么缺乏分类背后的可解释性，要么仅使用单一模式进行评估，从而限制了其分析范围。因此，我们提出了一个多模式框架，利用影像学、遗传学和临床评估等早期指标将 AD 患者早期分类为亚型。同样，我们构建提示并使用大型语言模型（例如 ChatGPT）来解释模型的结果。在我们的框架中，我们提出了一种三模态共同注意机制（Tri-COAT）来明确学习跨模态特征关联。我们提出的模型优于基线模型，并提供了对已知生物机制支持的关键跨模式特征关联的洞察。</li>
</ul>

<h3>Title: Making a Long Story Short in Conversation Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yufei Tao, Tiernan Mines, Ameeta Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00143">https://arxiv.org/abs/2402.00143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00143">https://arxiv.org/pdf/2402.00143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00143]] Making a Long Story Short in Conversation Modeling(https://arxiv.org/abs/2402.00143)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, lora</a></li>
<li><strong>Abstract: </strong>Conversation systems accommodate diverse users with unique personalities and distinct writing styles. Within the domain of multi-turn dialogue modeling, this work studies the impact of varied utterance lengths on the quality of subsequent responses generated by conversation models. Using GPT-3 as the base model, multiple dialogue datasets, and several metrics, we conduct a thorough exploration of this aspect of conversational models. Our analysis sheds light on the complex relationship between utterance lengths and the quality of follow-up responses generated by dialogue systems. Empirical findings suggests that, for certain types of conversations, utterance lengths can be reduced by up to 72% without any noticeable difference in the quality of follow-up responses.</li>
<li><strong>摘要：</strong>对话系统适应具有独特个性和独特写作风格的不同用户。在多轮对话建模领域，这项工作研究了不同的话语长度对对话模型生成的后续响应质量的影响。使用 GPT-3 作为基础模型、多个对话数据集和多个指标，我们对对话模型的这方面进行了彻底的探索。我们的分析揭示了话语长度与对话系统生成的后续响应质量之间的复杂关系。实证结果表明，对于某些类型的对话，话语长度最多可以减少 72%，而后续响应的质量没有任何明显差异。</li>
</ul>

<h3>Title: Fully Data-Driven Model for Increasing Sampling Rate Frequency of  Seismic Data using Super-Resolution Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Navid Gholizadeh, Javad Katebi</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00153">https://arxiv.org/abs/2402.00153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00153">https://arxiv.org/pdf/2402.00153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00153]] Fully Data-Driven Model for Increasing Sampling Rate Frequency of  Seismic Data using Super-Resolution Generative Adversarial Networks(https://arxiv.org/abs/2402.00153)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>High-quality data is one of the key requirements for any engineering application. In earthquake engineering practice, accurate data is pivotal in predicting the response of structure or damage detection process in an Structural Health Monitoring (SHM) application with less uncertainty. However, obtaining high-resolution data is fraught with challenges, such as significant costs, extensive data channels, and substantial storage requirements. To address these challenges, this study employs super-resolution generative adversarial networks (SRGANs) to improve the resolution of time-history data such as the data obtained by a sensor network in an SHM application, marking the first application of SRGANs in earthquake engineering domain. The time-series data are transformed into RGB values, converting raw data into images. SRGANs are then utilized to upscale these low-resolution images, thereby enhancing the overall sensor resolution. This methodology not only offers potential reductions in data storage requirements but also simplifies the sensor network, which could result in lower installation and maintenance costs. The proposed SRGAN method is rigorously evaluated using real seismic data, and its performance is compared with traditional enhancement techniques. The findings of this study pave the way for cost-effective and efficient improvements in the resolution of sensors used in SHM systems, with promising implications for the safety and sustainability of infrastructures worldwide.</li>
<li><strong>摘要：</strong>高质量数据是任何工程应用的关键要求之一。在地震工程实践中，准确的数据对于预测结构响应或结构健康监测 (SHM) 应用中的损伤检测过程至关重要，且不确定性较小。然而，获取高分辨率数据充满挑战，例如高昂的成本、广泛的数据通道和大量的存储需求。为了应对这些挑战，本研究采用超分辨率生成对抗网络（SRGAN）来提高时程数据的分辨率，例如SHM应用中传感器网络获得的数据，这标志着SRGAN在地震工程领域的首次应用。时间序列数据被转换为 RGB 值，将原始数据转换为图像。然后利用 SRGAN 来升级这些低分辨率图像，从而提高传感器的整体分辨率。这种方法不仅可以减少数据存储需求，还可以简化传感器网络，从而降低安装和维护成本。使用真实地震数据对所提出的 SRGAN 方法进行了严格评估，并将其性能与传统增强技术进行了比较。这项研究的结果为 SHM 系统中使用的传感器的分辨率的成本效益和高效改进铺平了道路，对全球基础设施的安全性和可持续性具有良好的影响。</li>
</ul>

<h3>Title: Large Language Models for Mathematical Reasoning: Progresses and  Challenges</h3>
<ul>
<li><strong>Authors: </strong>Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00157">https://arxiv.org/abs/2402.00157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00157">https://arxiv.org/pdf/2402.00157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00157]] Large Language Models for Mathematical Reasoning: Progresses and  Challenges(https://arxiv.org/abs/2402.00157)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges within this domain. To the best of our knowledge, this survey stands as one of the first extensive examinations of the landscape of LLMs in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.</li>
<li><strong>摘要：</strong>数学推理是评估人类智力基本认知能力的基石。近年来，旨在自动解决数学问题的大型语言模型（LLM）的发展显着激增。然而，数学问题类型的范围广阔且多种多样，面向法学硕士的技术正在不同的数据集和设置中进行评估。这种多样性使得辨别这个新兴领域的真正进步和障碍变得具有挑战性。这项调查致力于解决四个关键维度：i）对已调查的各种数学问题及其相应数据集的全面探索； ii) 对为解决数学问题而提出的法学硕士导向技术的范围进行检查； iii) 影响法学硕士解决数学问题的因素和关注点概述； iv) 阐明该领域持续存在的挑战。据我们所知，这项调查是对数学领域法学硕士前景的首次广泛调查之一，为这个快速发展的领域的当前状态、成就和未来挑战提供了整体视角。</li>
</ul>

<h3>Title: Dolma: an Open Corpus of Three Trillion Tokens for Language Model  Pretraining Research</h3>
<ul>
<li><strong>Authors: </strong>Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00159">https://arxiv.org/abs/2402.00159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00159">https://arxiv.org/pdf/2402.00159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00159]] Dolma: an Open Corpus of Three Trillion Tokens for Language Model  Pretraining Research(https://arxiv.org/abs/2402.00159)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reproduction of our work. In this report, we document Dolma, including its design principles, details about its construction, and a summary of its contents. We interleave this report with analyses and experimental results from training language models on intermediate states of Dolma to share what we have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modeling.</li>
<li><strong>摘要：</strong>语言模型已成为解决各种自然语言处理任务的关键技术，但有关如何开发性能最佳的语言模型的许多细节尚未报道。特别是，有关其预训练语料库的信息很少被讨论：商业语言模型很少提供有关其数据的任何信息；即使是开放模型也很少发布它们所训练的数据集，或者重现它们的确切方法。因此，进行某些语言建模研究具有挑战性，例如了解训练数据如何影响模型功能并形成其局限性。为了促进语言模型预训练的开放研究，我们发布了 Dolma，这是一个包含 3 万亿个代币的英语语料库，由网络内容、科学论文、代码、公共领域书籍、社交媒体和百科全书材料的多种组合构建而成。此外，我们还开源数据管理工具包，以便进一步实验和复制我们的工作。在这份报告中，我们记录了卓玛，包括其设计原理、构造细节以及内容摘要。我们将这份报告与在卓玛中间状态上训练语言模型的分析和实验结果相结合，以分享我们对重要数据管理实践的了解，包括内容或质量过滤器、重复数据删除和多源混合的作用。 Dolma 已用于训练 OLMo，这是一种最先进的开放语言模型和框架，旨在构建和研究语言建模科学。</li>
</ul>

<h3>Title: Multimodal Clinical Pseudo-notes for Emergency Department Prediction  Tasks using Multiple Embedding Model for EHR (MEME)</h3>
<ul>
<li><strong>Authors: </strong>Simon A. Lee, Sujay Jain, Alex Chen, Arabdha Biswas, Jennifer Fang, Akos Rudas, Jeffrey N. Chiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00160">https://arxiv.org/abs/2402.00160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00160">https://arxiv.org/pdf/2402.00160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00160]] Multimodal Clinical Pseudo-notes for Emergency Department Prediction  Tasks using Multiple Embedding Model for EHR (MEME)(https://arxiv.org/abs/2402.00160)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this work, we introduce Multiple Embedding Model for EHR (MEME), an approach that views Electronic Health Records (EHR) as multimodal data. This approach incorporates "pseudo-notes", textual representations of tabular EHR concepts such as diagnoses and medications, and allows us to effectively employ Large Language Models (LLMs) for EHR representation. This framework also adopts a multimodal approach, embedding each EHR modality separately. We demonstrate the effectiveness of MEME by applying it to several tasks within the Emergency Department across multiple hospital systems. Our findings show that MEME surpasses the performance of both single modality embedding methods and traditional machine learning approaches. However, we also observe notable limitations in generalizability across hospital institutions for all tested models.</li>
<li><strong>摘要：</strong>在这项工作中，我们引入了 EHR 的多重嵌入模型 (MEME)，这是一种将电子健康记录 (EHR) 视为多模态数据的方法。这种方法结合了“伪注释”，即表格 EHR 概念（例如诊断和药物）的文本表示，并使我们能够有效地采用大型语言模型 (LLM) 进行 EHR 表示。该框架还采用多模式方法，分别嵌入每个 EHR 模式。我们通过将 MEME 应用于多个医院系统急诊科的多项任务来证明 MEME 的有效性。我们的研究结果表明，MEME 超越了单一模态嵌入方法和传统机器学习方法的性能。然而，我们也观察到所有测试模型在医院机构中的普遍性存在显着局限性。</li>
</ul>

<h3>Title: Behind the Myth of Exploration in Policy Gradients</h3>
<ul>
<li><strong>Authors: </strong>Adrien Bolland, Gaspard Lambrechts, Damien Ernst</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00162">https://arxiv.org/abs/2402.00162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00162">https://arxiv.org/pdf/2402.00162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00162]] Behind the Myth of Exploration in Policy Gradients(https://arxiv.org/abs/2402.00162)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. To compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. First, they make it possible to smooth the learning objective and to eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. In light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.</li>
<li><strong>摘要：</strong>策略梯度算法是解决具有连续状态和动作空间的控制问题的有效强化学习方法。为了计算接近最优的策略，在实践中必须将探索术语包含在学习目标中。尽管这些术语的有效性通常是由探索环境的内在需要来证明的，但我们提出了一种新颖的分析并区分了这些技术的两种不同含义。首先，它们可以平滑学习目标并消除局部最优，同时保留全局最大值。其次，他们修改梯度估计，增加随机参数更新最终提供最优策略的概率。鉴于这些影响，我们讨论并说明了基于熵加成的实证探索策略，强调了它们的局限性，并为未来设计和分析此类策略的工作开辟了途径。</li>
</ul>

<h3>Title: De-identification is not always enough</h3>
<ul>
<li><strong>Authors: </strong>Atiquer Rahman Sarkar, Yao-Shun Chuang, Noman Mohammed, Xiaoqian Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00179">https://arxiv.org/abs/2402.00179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00179">https://arxiv.org/pdf/2402.00179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00179]] De-identification is not always enough(https://arxiv.org/abs/2402.00179)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>For sharing privacy-sensitive data, de-identification is commonly regarded as adequate for safeguarding privacy. Synthetic data is also being considered as a privacy-preserving alternative. Recent successes with numerical and tabular data generative models and the breakthroughs in large generative language models raise the question of whether synthetically generated clinical notes could be a viable alternative to real notes for research purposes. In this work, we demonstrated that (i) de-identification of real clinical notes does not protect records against a membership inference attack, (ii) proposed a novel approach to generate synthetic clinical notes using the current state-of-the-art large language models, (iii) evaluated the performance of the synthetically generated notes in a clinical domain task, and (iv) proposed a way to mount a membership inference attack where the target model is trained with synthetic data. We observed that when synthetically generated notes closely match the performance of real data, they also exhibit similar privacy concerns to the real data. Whether other approaches to synthetically generated clinical notes could offer better trade-offs and become a better alternative to sensitive real notes warrants further investigation.</li>
<li><strong>摘要：</strong>对于共享隐私敏感数据，去标识化通常被认为足以保护隐私。合成数据也被视为一种保护隐私的替代方案。最近数字和表格数据生成模型的成功以及大型生成语言模型的突破提出了一个问题：综合生成的临床笔记是否可以成为用于研究目的的真实笔记的可行替代方案。在这项工作中，我们证明了（i）真实临床记录的去识别并不能保护记录免受成员推理攻击，（ii）提出了一种使用当前最先进的大型记录生成合成临床记录的新方法。语言模型，（iii）评估了临床领域任务中综合生成的注释的性能，以及（iv）提出了一种发起成员推理攻击的方法，其中目标模型使用合成数据进行训练。我们观察到，当综合生成的注释与真实数据的性能非常匹配时，它们也表现出与真实数据类似的隐私问题。综合生成临床记录的其他方法是否可以提供更好的权衡，并成为敏感真实记录的更好替代品，值得进一步研究。</li>
</ul>

<h3>Title: Decentralised, Collaborative, and Privacy-preserving Machine Learning  for Multi-Hospital Data</h3>
<ul>
<li><strong>Authors: </strong>Congyu Fang, Adam Dziedzic, Lin Zhang, Laura Oliva, Amol Verma, Fahad Razak, Nicolas Papernot, Bo Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00205">https://arxiv.org/abs/2402.00205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00205">https://arxiv.org/pdf/2402.00205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00205]] Decentralised, Collaborative, and Privacy-preserving Machine Learning  for Multi-Hospital Data(https://arxiv.org/abs/2402.00205)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Machine Learning (ML) has demonstrated its great potential on medical data analysis. Large datasets collected from diverse sources and settings are essential for ML models in healthcare to achieve better accuracy and generalizability. Sharing data across different healthcare institutions is challenging because of complex and varying privacy and regulatory requirements. Hence, it is hard but crucial to allow multiple parties to collaboratively train an ML model leveraging the private datasets available at each party without the need for direct sharing of those datasets or compromising the privacy of the datasets through collaboration. In this paper, we address this challenge by proposing Decentralized, Collaborative, and Privacy-preserving ML for Multi-Hospital Data (DeCaPH). It offers the following key benefits: (1) it allows different parties to collaboratively train an ML model without transferring their private datasets; (2) it safeguards patient privacy by limiting the potential privacy leakage arising from any contents shared across the parties during the training process; and (3) it facilitates the ML model training without relying on a centralized server. We demonstrate the generalizability and power of DeCaPH on three distinct tasks using real-world distributed medical datasets: patient mortality prediction using electronic health records, cell-type classification using single-cell human genomes, and pathology identification using chest radiology images. We demonstrate that the ML models trained with DeCaPH framework have an improved utility-privacy trade-off, showing it enables the models to have good performance while preserving the privacy of the training data points. In addition, the ML models trained with DeCaPH framework in general outperform those trained solely with the private datasets from individual parties, showing that DeCaPH enhances the model generalizability.</li>
<li><strong>摘要：</strong>机器学习（ML）已经展示了其在医疗数据分析方面的巨大潜力。从不同来源和设置收集的大型数据集对于医疗保健中的 ML 模型实现更好的准确性和通用性至关重要。由于复杂且不同的隐私和监管要求，在不同医疗机构之间共享数据具有挑战性。因此，允许多方利用各方可用的私有数据集协作训练机器学习模型，而不需要直接共享这些数据集或通过协作损害数据集的隐私，这是很困难但至关重要的。在本文中，我们通过提出多医院数据的去中心化、协作和隐私保护机器学习 (DeCaPH) 来应对这一挑战。它具有以下主要优点：（1）它允许不同各方协作训练机器学习模型，而无需传输其私有数据集； (2) 保护患者隐私，限制培训过程中各方共享的任何内容可能导致的隐私泄露； (3)它有利于机器学习模型的训练，而不依赖于集中式服务器。我们使用现实世界的分布式医疗数据集证明了 DeCaPH 在三个不同任务上的通用性和强大功能：使用电子健康记录进行患者死亡率预测、使用单细胞人类基因组进行细胞类型分类以及使用胸部放射学图像进行病理学识别。我们证明，使用 DeCaPH 框架训练的 ML 模型具有改进的实用性与隐私权衡，表明它使模型能够具有良好的性能，同时保留训练数据点的隐私。此外，使用 DeCaPH 框架训练的 ML 模型总体上优于仅使用各方私有数据集训练的机器学习模型，这表明 DeCaPH 增强了模型的通用性。</li>
</ul>

<h3>Title: Learning Label Hierarchy with Supervised Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Ruixue Lian, William A. Sethares, Junjie Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00232">https://arxiv.org/abs/2402.00232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00232">https://arxiv.org/pdf/2402.00232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00232]] Learning Label Hierarchy with Supervised Contrastive Learning(https://arxiv.org/abs/2402.00232)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Supervised contrastive learning (SCL) frameworks treat each class as independent and thus consider all classes to be equally important. This neglects the common scenario in which label hierarchy exists, where fine-grained classes under the same category show more similarity than very different ones. This paper introduces a family of Label-Aware SCL methods (LASCL) that incorporates hierarchical information to SCL by leveraging similarities between classes, resulting in creating a more well-structured and discriminative feature space. This is achieved by first adjusting the distance between instances based on measures of the proximity of their classes with the scaled instance-instance-wise contrastive. An additional instance-center-wise contrastive is introduced to move within-class examples closer to their centers, which are represented by a set of learnable label parameters. The learned label parameters can be directly used as a nearest neighbor classifier without further finetuning. In this way, a better feature representation is generated with improvements of intra-cluster compactness and inter-cluster separation. Experiments on three datasets show that the proposed LASCL works well on text classification of distinguishing a single label among multi-labels, outperforming the baseline supervised approaches. Our code is publicly available.</li>
<li><strong>摘要：</strong>监督对比学习（SCL）框架将每个类视为独立的，因此认为所有类都同等重要。这忽略了存在标签层次结构的常见场景，即同一类别下的细粒度类比非常不同的类表现出更多的相似性。本文介绍了一系列标签感知 SCL 方法 (LASCL)，该方法通过利用类之间的相似性将分层信息合并到 SCL 中，从而创建结构更良好、更具区分性的特征空间。这是通过首先根据类的接近程度与缩放的实例-实例对比来调整实例之间的距离来实现的。引入了额外的实例中心对比，以使类内示例更接近其中心，这些中心由一组可学习的标签参数表示。学习到的标签参数可以直接用作最近邻分类器，无需进一步微调。通过这种方式，通过改进簇内紧凑性和簇间分离度来生成更好的特征表示。对三个数据集的实验表明，所提出的 LASCL 在区分多标签中的单个标签的文本分类方面效果良好，优于基线监督方法。我们的代码是公开的。</li>
</ul>

<h3>Title: Exploring the limits of decoder-only models trained on public speech  recognition corpora</h3>
<ul>
<li><strong>Authors: </strong>Ankit Gupta, George Saon, Brian Kingsbury</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00235">https://arxiv.org/abs/2402.00235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00235">https://arxiv.org/pdf/2402.00235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00235]] Exploring the limits of decoder-only models trained on public speech  recognition corpora(https://arxiv.org/abs/2402.00235)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>The emergence of industrial-scale speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines. Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance. In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using public English ASR corpora alone. Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We release our codebase and model checkpoints under permissive license.</li>
<li><strong>摘要：</strong>Whisper 和 USM 等工业规模语音识别 (ASR) 模型的出现，分别在 100 万小时的弱标记和 1200 万小时的纯音频专有数据上进行训练，导致对大规模公共 ASR 语料库和竞争性开放的需求更加强烈。源管道。与上述模型不同，大型语言模型通常基于 Transformer 解码器，目前尚不清楚仅在公共数据上训练的仅解码器模型是否可以提供有竞争力的性能。在这项工作中，我们研究了仅使用公共英语 ASR 语料库获得最佳性能所需的训练数据集和建模组件的选择等因素。我们的仅解码器 Transformer for ASR (DOTA) 模型在几乎所有英语 ASR 基准测试中全面优于 Whisper 的编码器-解码器开源复制 (OWSM)，并且在 15 个测试集中的 7 个测试集上优于 Whisper large-v3。我们在许可下发布代码库和模型检查点。</li>
</ul>

<h3>Title: Positional Encoding Helps Recurrent Neural Networks Handle a Large  Vocabulary</h3>
<ul>
<li><strong>Authors: </strong>Takashi Morita</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00236">https://arxiv.org/abs/2402.00236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00236">https://arxiv.org/pdf/2402.00236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00236]] Positional Encoding Helps Recurrent Neural Networks Handle a Large  Vocabulary(https://arxiv.org/abs/2402.00236)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding "time-stamps" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly "redundant". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural oscillations in biological brains.</li>
<li><strong>摘要：</strong>本研究利用综合基准讨论了位置编码对循环神经网络 (RNN) 的影响。位置编码为时间序列中的数据点添加“时间戳”，并补充了 Transformer 神经网络的功能，该网络缺乏表示数据顺序的固有机制。相比之下，RNN 可以自行对数据点的时间信息进行编码，从而使位置编码的使用看起来“多余”。尽管如此，实证研究揭示了位置编码的有效性，即使与 RNN 结合使用，特别是在处理产生不同观察结果的大词汇量时。这些发现为 RNN 的新研究方向铺平了道路，涉及输入驱动和自主时间表示的结合。此外，根据位置编码的正弦实现与生物大脑中的神经振荡之间的亲和力，讨论了计算/模拟结果的生物学含义。</li>
</ul>

<h3>Title: CNN-FL for Biotechnology Industry Empowered by Internet-of-BioNano  Things and Digital Twins</h3>
<ul>
<li><strong>Authors: </strong>Mohammad (Behdad)Jamshidi, Dinh Thai Hoang, Diep N. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00238">https://arxiv.org/abs/2402.00238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00238">https://arxiv.org/pdf/2402.00238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00238]] CNN-FL for Biotechnology Industry Empowered by Internet-of-BioNano  Things and Digital Twins(https://arxiv.org/abs/2402.00238)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Digital twins (DTs) are revolutionizing the biotechnology industry by enabling sophisticated digital representations of biological assets, microorganisms, drug development processes, and digital health applications. However, digital twinning at micro and nano scales, particularly in modeling complex entities like bacteria, presents significant challenges in terms of requiring advanced Internet of Things (IoT) infrastructure and computing approaches to achieve enhanced accuracy and scalability. In this work, we propose a novel framework that integrates the Internet of Bio-Nano Things (IoBNT) with advanced machine learning techniques, specifically convolutional neural networks (CNN) and federated learning (FL), to effectively tackle the identified challenges. Within our framework, IoBNT devices are deployed to gather image-based biological data across various physical environments, leveraging the strong capabilities of CNNs for robust machine vision and pattern recognition. Subsequently, FL is utilized to aggregate insights from these disparate data sources, creating a refined global model that continually enhances accuracy and predictive reliability, which is crucial for the effective deployment of DTs in biotechnology. The primary contribution is the development of a novel framework that synergistically combines CNN and FL, augmented by the capabilities of the IoBNT. This novel approach is specifically tailored to enhancing DTs in the biotechnology industry. The results showcase enhancements in the reliability and safety of microorganism DTs, while preserving their accuracy. Furthermore, the proposed framework excels in energy efficiency and security, offering a user-friendly and adaptable solution. This broadens its applicability across diverse sectors, including biotechnology and pharmaceutical industries, as well as clinical and hospital settings.</li>
<li><strong>摘要：</strong>数字孪生 (DT) 通过实现生物资产、微生物、药物开发流程和数字健康应用的复杂数字表示，正在彻底改变生物技术行业。然而，微米和纳米尺度的数字孪生，特别是在对细菌等复杂实体进行建模时，在需要先进的物联网 (IoT) 基础设施和计算方法来实现更高的准确性和可扩展性方面提出了重大挑战。在这项工作中，我们提出了一种新颖的框架，将生物纳米物联网（IoBNT）与先进的机器学习技术（特别是卷积神经网络（CNN）和联邦学习（FL））相结合，以有效应对已确定的挑战。在我们的框架内，部署 IoBNT 设备来收集各种物理环境中基于图像的生物数据，利用 CNN 的强大功能来实现强大的机器视觉和模式识别。随后，FL 用于聚合来自这些不同数据源的见解，创建一个完善的全局模型，不断提高准确性和预测可靠性，这对于在生物技术中有效部署 DT 至关重要。主要贡献是开发了一种新颖的框架，该框架协同结合了 CNN 和 FL，并通过 IoBNT 的功能进行了增强。这种新颖的方法是专门为增强生物技术行业的数字技术而定制的。结果表明，微生物 DT 的可靠性和安全性得到了提高，同时保持了准确性。此外，所提出的框架在能源效率和安全性方面表现出色，提供了用户友好且适应性强的解决方案。这扩大了其在不同领域的适用性，包括生物技术和制药行业以及临床和医院环境。</li>
</ul>

<h3>Title: Spectral Norm of Convolutional Layers with Circular and Zero Paddings</h3>
<ul>
<li><strong>Authors: </strong>Blaise Delattre, Quentin Barthélemy, Alexandre Allauzen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00240">https://arxiv.org/abs/2402.00240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00240">https://arxiv.org/pdf/2402.00240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00240]] Spectral Norm of Convolutional Layers with Circular and Zero Paddings(https://arxiv.org/abs/2402.00240)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>This paper leverages the use of \emph{Gram iteration} an efficient, deterministic, and differentiable method for computing spectral norm with an upper bound guarantee. Designed for circular convolutional layers, we generalize the use of the Gram iteration to zero padding convolutional layers and prove its quadratic convergence. We also provide theorems for bridging the gap between circular and zero padding convolution's spectral norm. We design a \emph{spectral rescaling} that can be used as a competitive $1$-Lipschitz layer that enhances network robustness. Demonstrated through experiments, our method outperforms state-of-the-art techniques in precision, computational cost, and scalability. The code of experiments is available at https://github.com/blaisedelattre/lip4conv.</li>
<li><strong>摘要：</strong>本文利用 \emph{Gram iteration} 来计算具有上限保证的谱范数，这是一种高效、确定性且可微分的方法。专为循环卷积层而设计，我们将 Gram 迭代的使用推广到零填充卷积层并证明其二次收敛。我们还提供了弥合圆形卷积和零填充卷积的谱范数之间差距的定理。我们设计了一个\emph{光谱重新缩放}，可以用作具有竞争力的$1$-Lipschitz 层，增强网络的鲁棒性。通过实验证明，我们的方法在精度、计算成本和可扩展性方面优于最先进的技术。实验代码可在 https://github.com/blaisedelattre/lip4conv 获取。</li>
</ul>

<h3>Title: Efficient Non-Parametric Uncertainty Quantification for Black-Box Large  Language Models and Decision Planning</h3>
<ul>
<li><strong>Authors: </strong>Yao-Hung Hubert Tsai, Walter Talbott, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00251">https://arxiv.org/abs/2402.00251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00251">https://arxiv.org/pdf/2402.00251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00251]] Efficient Non-Parametric Uncertainty Quantification for Black-Box Large  Language Models and Decision Planning(https://arxiv.org/abs/2402.00251)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, agent</a></li>
<li><strong>Abstract: </strong>Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.</li>
<li><strong>摘要：</strong>使用大型语言模型 (LLM) 的分步决策规划在人工智能代理开发中越来越受到关注。本文重点研究具有不确定性估计的决策规划，以解决语言模型中的幻觉问题。现有的方法要么是白盒的，要么是计算要求较高的，限制了黑盒专有法学硕士在预算内的使用。该论文的第一个贡献是 LLM 的非参数不确定性量化方法，通过单个推理有效地估计动态输入决策之间的逐点依赖性，而无需访问令牌逻辑。该估计量告知决策可信度的统计解释。第二个贡献概述了决策代理的系统设计，根据“洗澡”等用户提示生成“打开浴室灯”等动作。当多个操作具有较高的估计逐点依赖性时，将要求用户提供首选项。总之，我们的不确定性估计和决策代理设计为人工智能代理开发提供了一种经济高效的方法。</li>
</ul>

<h3>Title: Computational Experiments Meet Large Language Model Based Agents: A  Survey and Perspective</h3>
<ul>
<li><strong>Authors: </strong>Qun Ma, Xiao Xue, Deyu Zhou, Xiangning Yu, Donghua Liu, Xuwen Zhang, Zihan Zhao, Yifan Shen, Peilin Ji, Juanjuan Li, Gang Wang, Wanpeng Ma</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00262">https://arxiv.org/abs/2402.00262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00262">https://arxiv.org/pdf/2402.00262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00262]] Computational Experiments Meet Large Language Model Based Agents: A  Survey and Perspective(https://arxiv.org/abs/2402.00262)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, agent</a></li>
<li><strong>Abstract: </strong>Computational experiments have emerged as a valuable method for studying complex systems, involving the algorithmization of counterfactuals. However, accurately representing real social systems in Agent-based Modeling (ABM) is challenging due to the diverse and intricate characteristics of humans, including bounded rationality and heterogeneity. To address this limitation, the integration of Large Language Models (LLMs) has been proposed, enabling agents to possess anthropomorphic abilities such as complex reasoning and autonomous learning. These agents, known as LLM-based Agent, offer the potential to enhance the anthropomorphism lacking in ABM. Nonetheless, the absence of explicit explainability in LLMs significantly hinders their application in the social sciences. Conversely, computational experiments excel in providing causal analysis of individual behaviors and complex phenomena. Thus, combining computational experiments with LLM-based Agent holds substantial research potential. This paper aims to present a comprehensive exploration of this fusion. Primarily, it outlines the historical development of agent structures and their evolution into artificial societies, emphasizing their importance in computational experiments. Then it elucidates the advantages that computational experiments and LLM-based Agents offer each other, considering the perspectives of LLM-based Agent for computational experiments and vice versa. Finally, this paper addresses the challenges and future trends in this research domain, offering guidance for subsequent related studies.</li>
<li><strong>摘要：</strong>计算实验已成为研究复杂系统的一种有价值的方法，涉及反事实的算法化。然而，由于人类具有多样且复杂的特征，包括有限理性和异质性，在基于主体的建模（ABM）中准确地表示真实的社会系统具有挑战性。为了解决这一限制，人们提出了大型语言模型（LLM）的集成，使代理能够拥有复杂推理和自主学习等拟人化能力。这些代理被称为基于 LLM 的代理，提供了增强 ABM 所缺乏的拟人化的潜力。尽管如此，法学硕士缺乏明确的可解释性极大地阻碍了其在社会科学中的应用。相反，计算实验擅长提供个体行为和复杂现象的因果分析。因此，将计算实验与基于 LLM 的 Agent 相结合具有巨大的研究潜力。本文旨在对这种融合进行全面的探索。它首先概述了主体结构的历史发展及其向人工社会的演变，强调了它们在计算实验中的重要性。然后，考虑基于LLM的Agent对计算实验的看法，反之亦然，阐明了计算实验和基于LLM的Agent相互提供的优势。最后，本文探讨了该研究领域面临的挑战和未来趋势，为后续相关研究提供指导。</li>
</ul>

<h3>Title: Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective  Perturbation on Model-Based Contrastive Learning Detector would be Better</h3>
<ul>
<li><strong>Authors: </strong>Shengchao Liu, Xiaoming Liu, Yichen Wang, Zehua Cheng, Chengzhengxu Li, Zhaohan Zhang, Yu Lan, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00263">https://arxiv.org/abs/2402.00263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00263">https://arxiv.org/pdf/2402.00263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00263]] Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective  Perturbation on Model-Based Contrastive Learning Detector would be Better(https://arxiv.org/abs/2402.00263)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag</a></li>
<li><strong>Abstract: </strong>The burgeoning capabilities of large language models (LLMs) have raised growing concerns about abuse. DetectGPT, a zero-shot metric-based unsupervised machine-generated text detector, first introduces perturbation and shows great performance improvement. However, DetectGPT's random perturbation strategy might introduce noise, limiting the distinguishability and further performance improvements. Moreover, its logit regression module relies on setting the threshold, which harms the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel detector, \modelname{}, which uses selective strategy perturbation to relieve the important information loss caused by random masking, and multi-pair contrastive learning to capture the implicit pattern information during perturbation, facilitating few-shot performance. The experiments show that \modelname{} outperforms the SOTA method by 1.20\% in accuracy on average on four public datasets. We further analyze the effectiveness, robustness, and generalization of our perturbation method.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）不断发展的能力引起了人们对滥用的日益关注。 DetectGPT 是一种基于零样本度量的无监督机器生成文本检测器，首先引入了扰动，并显示出巨大的性能改进。然而，DetectGPT 的随机扰动策略可能会引入噪声，限制可区分性和进一步的性能改进。而且，其logit回归模块依赖于阈值的设置，这损害了单个或小批量输入的通用性和适用性。因此，我们提出了一种新颖的检测器 \modelname{}，它使用选择性策略扰动来减轻随机掩蔽造成的重要信息丢失，并使用多对对比学习来捕获扰动期间的隐式模式信息，从而促进小样本性能。实验表明，\modelname{} 在四个公共数据集上的准确率平均优于 SOTA 方法 1.20\%。我们进一步分析了我们的扰动方法的有效性、鲁棒性和泛化性。</li>
</ul>

<h3>Title: An Accurate and Low-Parameter Machine Learning Architecture for Next  Location Prediction</h3>
<ul>
<li><strong>Authors: </strong>Calvin Jary, Nafiseh Kahani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00306">https://arxiv.org/abs/2402.00306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00306">https://arxiv.org/pdf/2402.00306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00306]] An Accurate and Low-Parameter Machine Learning Architecture for Next  Location Prediction(https://arxiv.org/abs/2402.00306)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Next location prediction is a discipline that involves predicting a users next location. Its applications include resource allocation, quality of service, energy efficiency, and traffic management. This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices. To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters. We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million. This reduced the total size of the model parameters from 791 MB down to 8 MB. Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of twenty, and the overall accuracy was increased from 80.16% to 82.54%. This improvement allows for modest base stations and edge devices which do not have a large amount of memory or storage, to deploy and utilize the proposed ML architecture for next location prediction.</li>
<li><strong>摘要：</strong>下一个位置预测是一门涉及预测用户下一个位置的学科。其应用包括资源分配、服务质量、能源效率和流量管理。本文提出了一种节能、小型、低参数的机器学习 (ML) 架构，用于准确预测下一个位置，可部署在适度的基站和边缘设备上。为了实现这一目标，我们对整个城市的完整人类移动模式进行了一百次超参数实验，以确定一个精确的机器学习架构，该架构可以使用最少的模型参数达到准确度的稳定水平。我们成功地将已发布的 ML 架构中的模型参数数量从 2.02 亿减少到 200 万。这将模型参数的总大小从 791 MB 减少到 8 MB。此外，这将训练时间减少了四倍，训练所需的图形处理单元 (GPU) 内存量减少了二十倍，总体准确率从 80.16% 提高到 82.54%。这一改进允许没有大量内存或存储的适度基站和边缘设备部署和利用所提出的 ML 架构来预测下一个位置。</li>
</ul>

<h3>Title: PirateNets: Physics-informed Deep Learning with Residual Adaptive  Networks</h3>
<ul>
<li><strong>Authors: </strong>Sifan Wang, Bowen Li, Yuhan Chen, Paris Perdikaris</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00326">https://arxiv.org/abs/2402.00326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00326">https://arxiv.org/pdf/2402.00326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00326]] PirateNets: Physics-informed Deep Learning with Residual Adaptive  Networks(https://arxiv.org/abs/2402.00326)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initialization scheme allows us to encode appropriate inductive biases corresponding to a given PDE system into the network architecture. We provide comprehensive empirical evidence showing that PirateNets are easier to optimize and can gain accuracy from considerably increased depth, ultimately achieving state-of-the-art results across various benchmarks. All code and data accompanying this manuscript will be made publicly available at \url{https://github.com/PredictiveIntelligenceLab/jaxpi}.</li>
<li><strong>摘要：</strong>虽然物理信息神经网络 (PINN) 已成为一种流行的深度学习框架，用于解决由偏微分方程 (PDE) 控制的正向和逆向问题，但众所周知，当采用更大、更深的神经网络架构时，它们的性能会下降。我们的研究发现，这种反直觉行为的根源在于使用了具有不合适初始化方案的多层感知器（MLP）架构，这导致网络导数的可训练性较差，并最终导致不稳定的最小化PDE 剩余损失。为了解决这个问题，我们引入了物理信息残差自适应网络（PirateNets），这是一种新颖的架构，旨在促进深度 PINN 模型的稳定和高效训练。 PirateNets 利用一种新颖的自适应残差连接，允许网络初始化为浅层网络，并在训练过程中逐渐加深。我们还表明，所提出的初始化方案允许我们将与给定 PDE 系统相对应的适当归纳偏差编码到网络架构中。我们提供全面的经验证据，表明 PirateNet 更容易优化，并且可以通过显着增加的深度获得准确性，最终在各种基准测试中实现最先进的结果。本手稿随附的所有代码和数据将在 \url{https://github.com/PredictiveIntelligenceLab/jaxpi} 上公开提供。</li>
</ul>

<h3>Title: IndiVec: An Exploration of Leveraging Large Language Models for Media  Bias Detection with Fine-Grained Bias Indicators</h3>
<ul>
<li><strong>Authors: </strong>Luyang Lin, Lingzhi Wang, Xiaoyan Zhao, Jing Li, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00345">https://arxiv.org/abs/2402.00345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00345">https://arxiv.org/pdf/2402.00345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00345]] IndiVec: An Exploration of Leveraging Large Language Models for Media  Bias Detection with Fine-Grained Bias Indicators(https://arxiv.org/abs/2402.00345)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora, rag</a></li>
<li><strong>Abstract: </strong>This study focuses on media bias detection, crucial in today's era of influential social media platforms shaping individual attitudes and opinions. In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models. IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques. When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input's bias label. IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing explicit top-k indicators to interpret bias predictions). Experimental results on four political bias datasets highlight IndiVec's significant superiority over baselines. Furthermore, additional experiments and analysis provide profound insights into the framework's effectiveness.</li>
<li><strong>摘要：</strong>这项研究的重点是媒体偏见检测，这在当今有影响力的社交媒体平台塑造个人态度和观点的时代至关重要。与之前的工作主要依赖于训练针对特定数据集的特定模型，从而导致域外数据的适应性有限和性能不佳相比，我们引入了一种基于大型语言模型的通用偏差检测框架 IndiVec。 IndiVec 首先构建细粒度的媒体偏差数据库，利用大型语言模型和向量数据库技术强大的指令跟踪功能。当遇到用于偏差检测的新输入时，我们的框架会自动从向量数据库中选择最相关的指标，并采用多数投票来确定输入的偏差标签。与以前的方法相比，IndiVec 因其适应性（在不同来源的不同数据集上表现出一致的性能）和可解释性（提供明确的 top-k 指标来解释偏差预测）而表现出色。四个政治偏见数据集的实验结果凸显了 IndiVec 相对于基线的显着优势。此外，额外的实验和分析为该框架的有效性提供了深刻的见解。</li>
</ul>

<h3>Title: Machine Unlearning for Image-to-Image Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Guihong Li, Hsiang Hsu, Chun-Fu (Richard)Chen, Radu Marculescu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00351">https://arxiv.org/abs/2402.00351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00351">https://arxiv.org/pdf/2402.00351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00351]] Machine Unlearning for Image-to-Image Generative Models(https://arxiv.org/abs/2402.00351)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, code</a></li>
<li><strong>Abstract: </strong>Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models. Our code is available at https://github.com/jpmorganchase/l2l-generator-unlearning.</li>
<li><strong>摘要：</strong>机器遗忘已经成为一种新的范式，它故意忘记给定模型中的数据样本，以遵守严格的法规。然而，现有的机器忘记学习方法主要集中在分类模型上，而生成模型的忘记学习领域相对尚未被探索。本文充当了一座桥梁，通过为图像到图像生成模型提供机器取消学习的统一框架来解决这一差距。在此框架内，我们提出了一种计算高效的算法，以严格的理论分析为基础，该算法证明保留样本的性能下降可以忽略不计，同时有效地从遗忘样本中删除信息。对两个大型数据集ImageNet-1K和Places-365的实证研究进一步表明，我们的算法不依赖于保留样本的可用性，这进一步符合数据保留策略。据我们所知，这项工作是第一个代表针对图像到图像生成模型专门定制的机器取消学习的系统性、理论性和实证性探索的工作。我们的代码可在 https://github.com/jpmorganchase/l2l-generator-unlearning 获取。</li>
</ul>

<h3>Title: Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM  Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, Yulia Tsvetkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00367">https://arxiv.org/abs/2402.00367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00367">https://arxiv.org/pdf/2402.00367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00367]] Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM  Collaboration(https://arxiv.org/abs/2402.00367)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.</li>
<li><strong>摘要：</strong>尽管努力扩展大型语言模型 (LLM) 的知识，但鉴于知识不断发展的性质，知识差距（LLM 中缺失或过时的信息）可能始终持续存在。在这项工作中，我们研究了识别法学硕士知识差距的方法，并在存在知识差距时避免回答问题。我们首先通过微调/提示来调整现有方法来进行模型校准或适应，并分析它们避免生成低置信度输出的能力。出于自我反思失败和过度依赖保留集的动机，我们提出了两种基于模型协作的新方法，即法学硕士以合作或竞争的方式探索其他法学硕士的知识差距。三位法学硕士在具有不同知识领域的四项 QA 任务上进行的广泛实验表明，与最强基线相比，揭示法学硕士知识差距的合作和竞争方法在弃权准确性方面实现了高达 19.3% 的提高。进一步的分析表明，我们提出的机制可以帮助识别检索增强中的失败案例，并查明多跳推理中的知识差距。</li>
</ul>

<h3>Title: What Does the Bot Say? Opportunities and Risks of Large Language Models  in Social Media Bot Detection</h3>
<ul>
<li><strong>Authors: </strong>Shangbin Feng, Herun Wan, Ningnan Wang, Zhaoxuan Tan, Minnan Luo, Yulia Tsvetkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00371">https://arxiv.org/abs/2402.00371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00371">https://arxiv.org/pdf/2402.00371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00371]] What Does the Bot Say? Opportunities and Risks of Large Language Models  in Social Media Bot Detection(https://arxiv.org/abs/2402.00371)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems.</li>
<li><strong>摘要：</strong>社交媒体机器人检测一直是机器学习机器人检测器的进步与逃避检测的对抗性机器人策略之间的军备竞赛。在这项工作中，我们通过研究最先进的大型语言模型 (LLM) 在社交机器人检测中的机遇和风险，将军备竞赛提升到了一个新的水平。为了研究这些机会，我们通过提出一种异构专家混合框架来划分和征服不同的用户信息模式，设计了新颖的基于 LLM 的机器人检测器。为了阐明风险，我们探讨了法学硕士指导下操纵用户文本和结构化信息以逃避检测的可能性。在两个数据集上使用三个 LLM 进行的广泛实验表明，仅对 1,000 个带注释的示例进行指令调整就可以产生专门的 LLM，其在两个数据集上的性能比最先进的基线高出 9.1%，而 LLM 引导的操作策略可以显着降低现有机器人检测器的性能下降高达 29.6%，并损害机器人检测系统的校准和可靠性。</li>
</ul>

<h3>Title: Computational Morphology and Lexicography Modeling of Modern Standard  Arabic Nominals</h3>
<ul>
<li><strong>Authors: </strong>Christian Khairallah, Reham Marzouk, Salam Khalifa, Mayar Nassar, Nizar Habash</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00385">https://arxiv.org/abs/2402.00385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00385">https://arxiv.org/pdf/2402.00385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00385]] Computational Morphology and Lexicography Modeling of Modern Standard  Arabic Nominals(https://arxiv.org/abs/2402.00385)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Modern Standard Arabic (MSA) nominals present many morphological and lexical modeling challenges that have not been consistently addressed previously. This paper attempts to define the space of such challenges, and leverage a recently proposed morphological framework to build a comprehensive and extensible model for MSA nominals. Our model design addresses the nominals' intricate morphotactics, as well as their paradigmatic irregularities. Our implementation showcases enhanced accuracy and consistency compared to a commonly used MSA morphological analyzer and generator. We make our models publicly available.</li>
<li><strong>摘要：</strong>现代标准阿拉伯语 (MSA) 名词存在许多形态和词汇建模挑战，这些挑战以前尚未得到一致解决。本文试图定义此类挑战的空间，并利用最近提出的形态框架为 MSA 名义词构建全面且可扩展的模型。我们的模型设计解决了名词复杂的形态学及其范例的不规则性。与常用的 MSA 形态分析仪和生成器相比，我们的实现展示了更高的准确性和一致性。我们公开我们的模型。</li>
</ul>

<h3>Title: Efficient Exploration for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, Benjamin Van Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00396">https://arxiv.org/abs/2402.00396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00396">https://arxiv.org/pdf/2402.00396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00396]] Efficient Exploration for LLMs(https://arxiv.org/abs/2402.00396)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, agent</a></li>
<li><strong>Abstract: </strong>We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.</li>
<li><strong>摘要：</strong>我们提供的证据表明，有效探索收集人类反馈以改进大型语言模型会带来巨大好处。在我们的实验中，代理依次生成查询，同时根据收到的反馈拟合奖励模型。我们表现​​最好的代理使用双汤普森采样生成查询，并由认知神经网络表示不确定性。我们的结果表明，高效的探索可以通过更少的查询实现高水平的性能。此外，不确定性估计和勘探方案的选择都起着至关重要的作用。</li>
</ul>

<h3>Title: Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic  Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zhanyu Liu, Guanjie Zheng, Yanwei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00397">https://arxiv.org/abs/2402.00397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00397">https://arxiv.org/pdf/2402.00397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00397]] Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic  Forecasting(https://arxiv.org/abs/2402.00397)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, the traffic data of the data-scarce target city could query the traffic pattern bank, facilitating the aggregation of meta-knowledge. This meta-knowledge, in turn, assumes a pivotal role as a robust guide in subsequent processes involving graph reconstruction and forecasting. Empirical assessments conducted on real-world traffic datasets affirm the superior performance of MTPB, surpassing existing methods across various categories and exhibiting numerous attributes conducive to the advancement of cross-city few-shot forecasting methodologies. The code is available in https://github.com/zhyliu00/MTPB.</li>
<li><strong>摘要：</strong>交通预测对于智能交通系统 (ITS) 至关重要，有助于有效的资源分配和有效的交通控制。然而，其有效性往往严重依赖于丰富的交通数据，而许多城市由于设备支持有限而缺乏足够的数据，这给交通预测带来了重大挑战。认识到这一挑战，我们做出了一个值得注意的观察：不同城市的交通模式表现出相似性。基于这一关键见解，我们提出了一种跨城市少量交通预测问题的解决方案，称为多尺度交通模式库（MTPB）。 MTPB首先利用数据丰富的源城市启动学习过程，通过时空感知的预训练过程有效获取全面的交通知识。随后，该框架采用先进的聚类技术，根据所学知识系统地生成多尺度流量模式库。接下来，数据稀缺的目标城市的交通数据可以查询交通模式库，便于元知识的聚合。反过来，这种元知识在涉及图重建和预测的后续过程中发挥着关键作用，作为强有力的指导。对现实世界交通数据集进行的实证评估证实了 MTPB 的卓越性能，超越了各个类别的现有方法，并展示了有利于跨城市少样本预测方法进步的众多属性。代码可在 https://github.com/zhyliu00/MTPB 中获取。</li>
</ul>

<h3>Title: Investigating Bias Representations in Llama 2 Chat via Activation  Steering</h3>
<ul>
<li><strong>Authors: </strong>Dawn Lu, Nina Rimsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00402">https://arxiv.org/abs/2402.00402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00402">https://arxiv.org/pdf/2402.00402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00402]] Investigating Bias Representations in Llama 2 Chat via Activation  Steering(https://arxiv.org/abs/2402.00402)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal biases, which raises questions about the model's nuanced understanding of different forms of bias. This work also provides valuable insights into effective red-teaming strategies for LLMs using activation steering, particularly emphasizing the importance of integrating a refusal vector.</li>
<li><strong>摘要：</strong>我们解决大型语言模型 (LLM) 中社会偏见的挑战，重点关注 Llama 2 7B 聊天模型。随着法学硕士越来越多地融入具有重大社会影响的决策过程，必须确保这些模型不会强化现有的偏见。我们的方法采用激活引导来探索和减轻与性别、种族和宗教相关的偏见。该方法利用从 StereoSet 数据集导出的转向向量和自定义 GPT4 生成的性别偏见提示，操纵模型激活来引导响应朝向或远离有偏差的输出。我们的研究结果揭示了 Llama 2 7B Chat 中固有的性别偏见，即使在人类反馈强化学习 (RLHF) 之后仍然存在。我们还观察到偏差与模型拒绝响应的倾向之间存在可预测的负相关性。值得注意的是，我们的研究发现，RLHF 往往会增加模型对不同形式的社会偏见的表示的相似性，这引发了关于模型对不同形式的偏见的细致理解的问题。这项工作还为使用激活引导的法学硕士的有效红队策略提供了宝贵的见解，特别强调了整合拒绝向量的重要性。</li>
</ul>

<h3>Title: Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated  Student Essay Detection</h3>
<ul>
<li><strong>Authors: </strong>Xinlin Peng, Ying Zhou, Ben He, Le Sun, Yingfei Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00412">https://arxiv.org/abs/2402.00412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00412">https://arxiv.org/pdf/2402.00412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00412]] Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated  Student Essay Detection(https://arxiv.org/abs/2402.00412)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection. Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset. The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks. Specifically, we explore word substitution and sentence substitution perturbation methods that effectively evade detection while maintaining the quality of the generated essays. This highlights the urgent need for more accurate and robust methods to detect AI-generated student essays in the education domain.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在文本生成任务中表现出了卓越的能力。然而，这些模型的使用存在固有的风险，包括但不限于抄袭、假新闻传播以及教育练习中的问题。尽管已经提出了几种检测器来解决这些问题，但它们对抗对抗性扰动的有效性，特别是在学生论文写作的背景下，在很大程度上仍未得到探索。本文旨在通过构建 AIG-ASAP（一个人工智能生成的学生论文数据集）来弥补这一差距，采用一系列文本扰动方法，有望在逃避检测的同时生成高质量的论文。通过实证实验，我们评估了当前 AIGC 探测器在 AIG-ASAP 数据集上的性能。结果表明，可以使用简单的自动对抗性攻击轻松绕过现有检测器。具体来说，我们探索了单词替换和句子替换扰动方法，这些方法可以有效地逃避检测，同时保持生成的论文的质量。这凸显了教育领域迫切需要更准确、更强大的方法来检测人工智能生成的学生论文。</li>
</ul>

<h3>Title: Prompt-Time Symbolic Knowledge Capture with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tolga Çöplü, Arto Bendiken, Andrii Skomorokhov, Eduard Bateiko, Stephen Cobb, Joshua J. Bouw (Haltia, Inc.)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00414">https://arxiv.org/abs/2402.00414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00414">https://arxiv.org/pdf/2402.00414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00414]] Prompt-Time Symbolic Knowledge Capture with Large Language Models(https://arxiv.org/abs/2402.00414)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>Augmenting large language models (LLMs) with user-specific knowledge is crucial for real-world applications, such as personal AI assistants. However, LLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper investigates utilizing the existing LLM capabilities to enable prompt-driven knowledge capture, with a particular emphasis on knowledge graphs. We address this challenge by focusing on prompt-to-triple (P2T) generation. We explore three methods: zero-shot prompting, few-shot prompting, and fine-tuning, and then assess their performance via a specialized synthetic dataset. Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.</li>
<li><strong>摘要：</strong>使用特定于用户的知识增强大型语言模型 (LLM) 对于个人 AI 助理等现实应用至关重要。然而，法学硕士本质上缺乏即时驱动的知识获取机制。本文研究了利用现有的法学硕士功能来实现提示驱动的知识捕获，特别强调知识图。我们通过专注于即时三重 (P2T) 发电来应对这一挑战。我们探索了三种方法：零样本提示、少样本提示和微调，然后通过专门的合成数据集评估它们的性能。我们的代码和数据集可在 https://github.com/HaltiaAI/paper-PTSKC 上公开获取。</li>
</ul>

<h3>Title: From PARIS to LE-PARIS: Toward Patent Response Automation with  Recommender Systems and Collaborative Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jung-Mei Chu, Hao-Cheng Lo, Jieh Hsiang, Chun-Chieh Cho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00421">https://arxiv.org/abs/2402.00421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00421">https://arxiv.org/pdf/2402.00421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00421]] From PARIS to LE-PARIS: Toward Patent Response Automation with  Recommender Systems and Collaborative Large Language Models(https://arxiv.org/abs/2402.00421)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of our proposed hybrid recommender system tailored for OA (both LLM-based and non-LLM-based) (study 3), the quality of response generation (study 4), and the practical value of the systems in real-world scenarios via user studies (study 5). Results demonstrate that both PARIS and LE-PARIS significantly meet key metrics and positively impact attorney performance.</li>
<li><strong>摘要：</strong>在专利申请中，及时有效地回应审查意见通知书（OA）对于获得专利至关重要，但过去的自动化和人工智能研究几乎没有解决这一问题。为了解决这一差距，我们的研究引入了专利局行动响应情报系统（PARIS）及其高级版本，大型语言模型增强型 PARIS（LE-PARIS）。这些系统旨在提高专利律师协作处理 OA 响应的效率。该系统的主要功能包括OA主题数据库的构建、响应模板的开发以及推荐系统和基于LLM的响应生成的实施。我们的验证涉及使用美国专利商标局专利局审查意见数据库以及六年来律师与我们系统互动的纵向数据进行多范式分析。通过五项研究，我们使用主题建模和提出的德尔菲流程检查了 OA 主题（研究 1 和 2）的建设性，我们提出的针对 OA 量身定制的混合推荐系统（基于法学硕士和非法学硕士）的有效性（研究 3）、响应生成的质量（研究 4）以及通过用户研究系统在现实场景中的实用价值（研究 5）。结果表明，PARIS 和 LE-PARIS 均显着满足关键指标，并对律师绩效产生积极影响。</li>
</ul>

<h3>Title: Merging Multi-Task Models via Weight-Ensembling Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Anke Tang, Li Shen, Yong Luo, Nan Yin, Lefei Zhang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00433">https://arxiv.org/abs/2402.00433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00433">https://arxiv.org/pdf/2402.00433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00433]] Merging Multi-Task Models via Weight-Ensembling Mixture of Experts(https://arxiv.org/abs/2402.00433)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent. We conduct the conventional multi-task model merging experiments and evaluate the generalization and robustness of our method. The results demonstrate the effectiveness of our method and provide a comprehensive understanding of our method. The code is available at https://anonymous.4open.science/r/weight-ensembling_MoE-67C9/</li>
<li><strong>摘要：</strong>将在不同任务上训练的各种特定于任务的基于 Transformer 的模型合并到单个统一模型中可以同时执行所有任务。以前的方法（以任务算术为例）已被证明既有效又可扩展。现有方法主要集中于在原始模型参数空间内寻找静态最优解。一个显着的挑战是减轻不同模型参数之间的干扰，这会大大降低性能。在本文中，我们建议合并大部分参数，同时将 Transformer 层的 MLP 升级为权重集成混合专家 (MoE) 模块，该模块可以根据输入动态集成共享知识和特定于任务的知识，从而提供更灵活的解决方案，可以适应每个实例的特定需求。我们的主要见解是，通过识别和分离共享知识和特定于任务的知识，然后动态集成它们，我们可以在很大程度上缓解参数干扰问题。我们进行了传统的多任务模型合并实验，并评估了我们方法的泛化性和鲁棒性。结果证明了我们方法的有效性并提供了对我们方法的全面理解。代码可在 https://anonymous.4open.science/r/weight-ensembling_MoE-67C9/ 获取</li>
</ul>

<h3>Title: Improving Dialog Safety using Socially Aware Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Souvik Das, Rohini K. Srihari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00446">https://arxiv.org/abs/2402.00446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00446">https://arxiv.org/pdf/2402.00446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00446]] Improving Dialog Safety using Socially Aware Contrastive Learning(https://arxiv.org/abs/2402.00446)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag, agent</a></li>
<li><strong>Abstract: </strong>State-of-the-art conversational AI systems raise concerns due to their potential risks of generating unsafe, toxic, unethical, or dangerous content. Previous works have developed datasets to teach conversational agents the appropriate social paradigms to respond effectively to specifically designed hazardous content. However, models trained on these adversarial datasets still struggle to recognize subtle unsafe situations that appear naturally in conversations or introduce an inappropriate response in a casual context. To understand the extent of this problem, we study prosociality in both adversarial and casual dialog contexts and audit the response quality of general-purpose language models in terms of propensity to produce unsafe content. We propose a dual-step fine-tuning process to address these issues using a socially aware n-pair contrastive loss. Subsequently, we train a base model that integrates prosocial behavior by leveraging datasets like Moral Integrity Corpus (MIC) and ProsocialDialog. Experimental results on several dialog datasets demonstrate the effectiveness of our approach in generating socially appropriate responses.</li>
<li><strong>摘要：</strong>最先进的对话式人工智能系统因其产生不安全、有毒、不道德或危险内容的潜在风险而引起关注。之前的工作已经开发了数据集来教导会话代理适当的社交范式，以有效地响应专门设计的危险内容。然而，在这些对抗性数据集上训练的模型仍然难以识别对话中自然出现的微妙的不安全情况，或者在随意的环境中引入不恰当的反应。为了了解这个问题的严重程度，我们研究了对抗性和随意对话环境中的亲社会性，并根据产生不安全内容的倾向来审核通用语言模型的响应质量。我们提出了一个双步微调过程，使用社会意识的 n 对对比损失来解决这些问题。随后，我们利用道德诚信语料库 (MIC) 和 ProsocialDialog 等数据集来训练集成亲社会行为的基本模型。几个对话数据集的实验结果证明了我们的方法在生成适合社交的响应方面的有效性。</li>
</ul>

<h3>Title: A Survey of Data-Efficient Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Wei Ju, Siyu Yi, Yifan Wang, Qingqing Long, Junyu Luo, Zhiping Xiao, Ming Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00447">https://arxiv.org/abs/2402.00447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00447">https://arxiv.org/pdf/2402.00447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00447]] A Survey of Data-Efficient Graph Learning(https://arxiv.org/abs/2402.00447)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including self-supervised graph learning, semi-supervised graph learning, and few-shot graph learning. Also, we state promising directions for future research, contributing to the evolution of graph machine learning.</li>
<li><strong>摘要：</strong>图结构数据普遍存在于从社交网络到生化分析等领域，是各种现实世界系统的基础。虽然图神经网络在对此类数据进行建模方面表现出熟练程度，但它们的成功通常依赖于大量标记数据，这在注释资源有限的实际场景中提出了挑战。为了解决这个问题，人们付出了巨大的努力，通过探索各种最小监督方法来提高低资源环境下的图机器学习性能。在本文中，我们引入了数据高效图学习（DEGL）的新概念作为研究前沿，并提出了第一份总结 DEGL 当前进展的调查。我们首先强调使用大量标记数据训练模型所固有的挑战，为我们探索 DEGL 铺平道路。接下来，我们从几个关键方面系统地回顾了该主题的最新进展，包括自监督图学习、半监督图学习和小样本图学习。此外，我们还阐述了未来研究的有希望的方向，为图机器学习的发展做出贡献。</li>
</ul>

<h3>Title: RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient  Minimum Radiation Exposure Pathway</h3>
<ul>
<li><strong>Authors: </strong>Biswajit Sadhu, Trijit Sadhu, S. Anand</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00468">https://arxiv.org/abs/2402.00468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00468">https://arxiv.org/pdf/2402.00468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00468]] RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient  Minimum Radiation Exposure Pathway(https://arxiv.org/abs/2402.00468)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>Recent advancements in deep reinforcement learning (DRL) techniques have sparked its multifaceted applications in the automation sector. Managing complex decision-making problems with DRL encourages its use in the nuclear industry for tasks such as optimizing radiation exposure to the personnel during normal operating conditions and potential accidental scenarios. However, the lack of efficient reward function and effective exploration strategy thwarted its implementation in the development of radiation-aware autonomous unmanned aerial vehicle (UAV) for achieving maximum radiation protection. Here, in this article, we address these intriguing issues and introduce a deep Q-learning based architecture (RadDQN) that operates on a radiation-aware reward function to provide time-efficient minimum radiation-exposure pathway in a radiation zone. We propose a set of unique exploration strategies that fine-tune the extent of exploration and exploitation based on the state-wise variation in radiation exposure during training. Further, we benchmark the predicted path with grid-based deterministic method. We demonstrate that the formulated reward function in conjugation with adequate exploration strategy is effective in handling several scenarios with drastically different radiation field distributions. When compared to vanilla DQN, our model achieves a superior convergence rate and higher training stability.</li>
<li><strong>摘要：</strong>深度强化学习（DRL）技术的最新进展激发了其在自动化领域的多方面应用。利用 DRL 管理复杂的决策问题，鼓励其在核工业中使用，以完成诸如优化正常操作条件和潜在意外情况下人员的辐射暴露等任务。然而，缺乏有效的奖励函数和有效的探索策略阻碍了其在辐射感知自主无人机（UAV）开发中的实施，以实现最大程度的辐射防护。在本文中，我们解决了这些有趣的问题，并介绍了一种基于深度 Q 学习的架构 (RadDQN)，该架构在辐射感知奖励函数上运行，以在辐射区域提供省时的最小辐射暴露路径。我们提出了一套独特的探索策略，根据训练期间辐射暴露的状态变化来微调探索和利用的程度。此外，我们使用基于网格的确定性方法对预测路径进行基准测试。我们证明，与适当的探索策略相结合的制定的奖励函数可以有效地处理具有截然不同的辐射场分布的几种场景。与普通 DQN 相比，我们的模型实现了卓越的收敛速度和更高的训练稳定性。</li>
</ul>

<h3>Title: SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection  Framework for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianhan Xu, Zhe Hu, Ling Chen, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00474">https://arxiv.org/abs/2402.00474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00474">https://arxiv.org/pdf/2402.00474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00474]] SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection  Framework for Large Language Models(https://arxiv.org/abs/2402.00474)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the original LLMs. Notably, this improvement is particularly pronounced for unseen medical tasks, showing an improvement of up to 30%.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展已在各种自然语言处理 (NLP) 任务中展现出卓越的性能。然而，由于缺乏医学领域知识，它们在医学领域的有效应用受到阻碍。在本研究中，我们提出了 SA-MDKIF，这是一个可扩展且适应性强的框架，旨在通过指令调整将医学知识注入通用 LLM，从而实现各种下游任务的适应性。 SA-MDKIF由技能培训和技能适应两个阶段组成。在第一阶段，我们定义了 12 种基本医疗技能，并使用 AdaLoRA 基于我们构建的统一格式的教学数据集来训练这些技能。在下一阶段，我们使用特定于任务的下游数据训练技能路由器，并使用该路由器在推理过程中将获得的技能与 LLM 集成。在9个不同医疗任务上的实验结果表明，SA-MDKIF与原来的LLM相比性能提高了10-20%。值得注意的是，这种改进对于看不见的医疗任务尤其明显，显示出高达 30% 的改进。</li>
</ul>

<h3>Title: EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuchen Pan, Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00518">https://arxiv.org/abs/2402.00518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00518">https://arxiv.org/pdf/2402.00518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00518]] EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit  Large Language Models(https://arxiv.org/abs/2402.00518)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.</li>
<li><strong>摘要：</strong>这项工作介绍了 EE-Tuning，这是一种轻量级且经济的解决方案，用于训练/调整早期退出大语言模型 (LLM)。与全参数预训练的常见方法相比，EE-Tuning 通过以参数高效方式调整的附加早期退出层来增强任何预训练（并且可能经过微调）的标准 LLM，这需要大量更少的计算资源和训练数据。我们的 EE-Tuning 实施通过广泛的性能优化实现了出色的训练效率，并且由于其与 3D 并行性的完全兼容而实现了可扩展性。系统实验的结果验证了 EE-Tuning 的有效性，证实可以在有限的培训预算下实现有效的提前退出 LLM 推理。为了让社区能够早期退出 LLM，我们在 https://github.com/pan-x-c/EE-LLM 发布了 EE-Tuning 实现的源代码。</li>
</ul>

<h3>Title: Superfiltering: Weak-to-Strong Data Filtering for Fast  Instruction-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00530">https://arxiv.org/abs/2402.00530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00530">https://arxiv.org/pdf/2402.00530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00530]] Superfiltering: Weak-to-Strong Data Filtering for Fast  Instruction-Tuning(https://arxiv.org/abs/2402.00530)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of our approach.</li>
<li><strong>摘要：</strong>指令调整对于提高法学硕士至关重要，但通常会受到低质量和冗余数据的影响。事实证明，用于指令调整的数据过滤对于提高调整过程的效率和性能非常重要。但由于法学硕士参与这一过程，这也导致了额外的成本和计算。为了降低过滤成本，我们研究了超级过滤：我们可以使用更小、更弱的模型来选择数据来微调更大、更强的模型吗？尽管弱语言模型和强语言模型之间存在性能差距，但我们发现它们感知指令难度和数据选择结果的能力高度一致。这使我们能够使用更小、更高效的模型来过滤用于训练更大语言模型的指令数据。它不仅大大加快了数据过滤速度，而且经过过滤数据微调的 LLM 在标准基准测试中实现了更好的性能。大量的实验验证了我们方法的有效性和效率。</li>
</ul>

<h3>Title: Preconditioning for Physics-Informed Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Songming Liu, Chang Su, Jiachen Yao, Zhongkai Hao, Hang Su, Youjia Wu, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00531">https://arxiv.org/abs/2402.00531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00531">https://arxiv.org/pdf/2402.00531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00531]] Preconditioning for Physics-Informed Neural Networks(https://arxiv.org/abs/2402.00531)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Physics-informed neural networks (PINNs) have shown promise in solving various partial differential equations (PDEs). However, training pathologies have negatively affected the convergence and prediction accuracy of PINNs, which further limits their practical applications. In this paper, we propose to use condition number as a metric to diagnose and mitigate the pathologies in PINNs. Inspired by classical numerical analysis, where the condition number measures sensitivity and stability, we highlight its pivotal role in the training dynamics of PINNs. We prove theorems to reveal how condition number is related to both the error control and convergence of PINNs. Subsequently, we present an algorithm that leverages preconditioning to improve the condition number. Evaluations of 18 PDE problems showcase the superior performance of our method. Significantly, in 7 of these problems, our method reduces errors by an order of magnitude. These empirical findings verify the critical role of the condition number in PINNs' training.</li>
<li><strong>摘要：</strong>物理信息神经网络 (PINN) 在求解各种偏微分方程 (PDE) 方面表现出了良好的前景。然而，训练病理学对 PINN 的收敛和预测精度产生了负面影响，这进一步限制了它们的实际应用。在本文中，我们建议使用条件数作为诊断和减轻 PINN 中病理状况的指标。受经典数值分析的启发，条件数衡量灵敏度和稳定性，我们强调了它在 PINN 训练动态中的关键作用。我们证明定理来揭示条件数如何与 PINN 的误差控制和收敛相关。随后，我们提出了一种利用预处理来改进条件数的算法。对 18 个 PDE 问题的评估展示了我们方法的优越性能。值得注意的是，在其中 7 个问题中，我们的方法将错误减少了一个数量级。这些实证结果验证了条件数在 PINN 训练中的关键作用。</li>
</ul>

<h3>Title: A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for  Verifiers of Reasoning Chains</h3>
<ul>
<li><strong>Authors: </strong>Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, Mor Geva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00559">https://arxiv.org/abs/2402.00559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00559">https://arxiv.org/pdf/2402.00559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00559]] A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for  Verifiers of Reasoning Chains(https://arxiv.org/abs/2402.00559)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Prompting language models to provide step-by-step answers (e.g., "Chain-of-Thought") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. Reveal includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models.</li>
<li><strong>摘要：</strong>提示语言模型提供逐步答案（例如“思维链”）是复杂推理任务的主要方法，更准确的推理链通常可以提高下游任务的性能。最近的文献讨论了验证推理步骤的自动方法，以评估和提高其正确性。然而，没有细粒度的步骤级数据集可以对此类验证方法进行彻底评估，从而阻碍了这一方向的进展。我们引入了 Reveal：推理验证评估，这是一个新的数据集，用于对开放域问答设置中复杂思想链推理的自动验证器进行基准测试。 Reveal 包括跨各种数据集和最先进语言模型的语言模型答案中每个推理步骤的相关性、证据段落归属和逻辑正确性的综合标签。</li>
</ul>

<h3>Title: Continuous Unsupervised Domain Adaptation Using Stabilized  Representations and Experience Replay</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Rostami</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00580">https://arxiv.org/abs/2402.00580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00580">https://arxiv.org/pdf/2402.00580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00580]] Continuous Unsupervised Domain Adaptation Using Stabilized  Representations and Experience Replay(https://arxiv.org/abs/2402.00580)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We introduce an algorithm for tackling the problem of unsupervised domain adaptation (UDA) in continual learning (CL) scenarios. The primary objective is to maintain model generalization under domain shift when new domains arrive continually through updating a base model when only unlabeled data is accessible in subsequent tasks. While there are many existing UDA algorithms, they typically require access to both the source and target domain datasets simultaneously. Conversely, existing CL approaches can handle tasks that all have labeled data. Our solution is based on stabilizing the learned internal distribution to enhances the model generalization on new domains. The internal distribution is modeled by network responses in hidden layer. We model this internal distribution using a Gaussian mixture model (GMM ) and update the model by matching the internally learned distribution of new domains to the estimated GMM. Additionally, we leverage experience replay to overcome the problem of catastrophic forgetting, where the model loses previously acquired knowledge when learning new tasks. We offer theoretical analysis to explain why our algorithm would work. We also offer extensive comparative and analytic experiments to demonstrate that our method is effective. We perform experiments on four benchmark datasets to demonstrate that our approach is effective.</li>
<li><strong>摘要：</strong>我们引入了一种算法来解决持续学习（CL）场景中的无监督域适应（UDA）问题。主要目标是当新域不断到达时，在后续任务中只能访问未标记的数据时，通过更新基本模型来维持域转移下的模型泛化。虽然有许多现有的 UDA 算法，但它们通常需要同时访问源域数据集和目标域数据集。相反，现有的 CL 方法可以处理所有具有标记数据的任务。我们的解决方案基于稳定学习的内部分布，以增强新领域的模型泛化。内部分布由隐藏层中的网络响应建模。我们使用高斯混合模型 (GMM) 对该内部分布进行建模，并通过将新域的内部学习分布与估计的 GMM 进行匹配来更新模型。此外，我们利用经验回放来克服灾难性遗忘的问题，即模型在学习新任务时会丢失以前获得的知识。我们提供理论分析来解释我们的算法为何有效。我们还提供广泛的比较和分析实验来证明我们的方法是有效的。我们对四个基准数据集进行实验，以证明我们的方法是有效的。</li>
</ul>

<h3>Title: Uncertainty-Aware Partial-Label Learning</h3>
<ul>
<li><strong>Authors: </strong>Tobias Fuchs, Florian Kalinke, Klemens Böhm</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00592">https://arxiv.org/abs/2402.00592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00592">https://arxiv.org/pdf/2402.00592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00592]] Uncertainty-Aware Partial-Label Learning(https://arxiv.org/abs/2402.00592)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.</li>
<li><strong>摘要：</strong>在现实应用程序中，人们经常会遇到标签不明确的数据，其中不同的注释者分配冲突的类标签。部分标签学习允许在这种弱监督环境中训练分类器。虽然最先进的方法已经具有良好的预测性能，但它们经常遭受校准错误的不确定性估计的影响。然而，经过良好校准的不确定性估计很重要，尤其是在医学和自动驾驶等安全关键领域。在本文中，我们提出了一种利用 Dempster-Shafer 理论的新颖的基于最近邻的部分标签学习算法。对人工和真实数据集的大量实验表明，所提出的方法提供了经过良好校准的不确定性估计，并实现了有竞争力的预测性能。此外，我们证明我们的算法是风险一致的。</li>
</ul>

<h3>Title: Deep Clustering Using the Soft Silhouette Score: Towards Compact and  Well-Separated Clusters</h3>
<ul>
<li><strong>Authors: </strong>Georgios Vardakas, Ioannis Papakostas, Aristidis Likas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00608">https://arxiv.org/abs/2402.00608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00608">https://arxiv.org/pdf/2402.00608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00608]] Deep Clustering Using the Soft Silhouette Score: Towards Compact and  Well-Separated Clusters(https://arxiv.org/abs/2402.00608)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets. Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance. The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset. In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient. Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient. When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters. In addition, we introduce an autoencoder-based deep learning architecture that is suitable for optimizing the soft silhouette objective function. The proposed deep clustering method has been tested and compared with several well-studied deep clustering methods on various benchmark datasets, yielding very satisfactory clustering results.</li>
<li><strong>摘要：</strong>无监督学习在大数据时代越来越受到重视，它提供了一种从未标记的数据集中提取有价值的见解的方法。深度聚类已成为一个重要的无监督类别，旨在利用神经网络的非线性映射能力来增强聚类性能。大多数深度聚类文献侧重于最小化某些嵌入空间中的簇内变异性，同时保持学习的表示与原始高维数据集一致。在这项工作中，我们提出了软轮廓，轮廓系数的概率公式。软轮廓奖励紧凑且明显分离的聚类解决方案，如传统的轮廓系数。当在深度聚类框架内进行优化时，软轮廓会引导学习到的表示形成紧凑且分离良好的聚类。此外，我们引入了一种基于自动编码器的深度学习架构，适用于优化软轮廓目标函数。所提出的深度聚类方法已经在各种基准数据集上进行了测试，并与几种经过充分研究的深度聚类方法进行了比较，产生了非常令人满意的聚类结果。</li>
</ul>

<h3>Title: Actor Identification in Discourse: A Challenge for LLMs?</h3>
<ul>
<li><strong>Authors: </strong>Ana Barić, Sean Papay, Sebastian Padó</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00620">https://arxiv.org/abs/2402.00620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00620">https://arxiv.org/pdf/2402.00620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00620]] Actor Identification in Discourse: A Challenge for LLMs?(https://arxiv.org/abs/2402.00620)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The identification of political actors who put forward claims in public debate is a crucial step in the construction of discourse networks, which are helpful to analyze societal debates. Actor identification is, however, rather challenging: Often, the locally mentioned speaker of a claim is only a pronoun ("He proposed that [claim]"), so recovering the canonical actor name requires discourse understanding. We compare a traditional pipeline of dedicated NLP components (similar to those applied to the related task of coreference) with a LLM, which appears a good match for this generation task. Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse. Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form. This points to an underlying issue in LLMs with controlling generated output. Indeed, a hybrid model combining the LLM with a classifier to normalize its output substantially outperforms both initial models.</li>
<li><strong>摘要：</strong>识别在公共辩论中提出主张的政治行动者是构建话语网络的关键一步，有助于分析社会辩论。然而，演员识别相当具有挑战性：通常，本地提到的某个主张的说话者只是一个代词（“他提议[主张]”），因此恢复规范的演员姓名需要话语理解。我们将专用 NLP 组件的传统流程（类似于应用于共指相关任务的组件）与 LLM 进行比较，这似乎非常适合此生成任务。对报纸报道中的德国演员语料库进行评估后，我们惊讶地发现法学硕士的表现更差。进一步的分析表明，法学硕士非常擅长识别正确的参考文献，但很难生成正确的规范形式。这指出了法学硕士中控制生成输出的一个根本问题。事实上，将 LLM 与分类器相结合以标准化其输出的混合模型大大优于两个初始模型。</li>
</ul>

<h3>Title: Prosody in Cascade and Direct Speech-to-Text Translation: a case study  on Korean Wh-Phrases</h3>
<ul>
<li><strong>Authors: </strong>Giulio Zhou, Tsz Kin Lam, Alexandra Birch, Barry Haddow</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00632">https://arxiv.org/abs/2402.00632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00632">https://arxiv.org/pdf/2402.00632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00632]] Prosody in Cascade and Direct Speech-to-Text Translation: a case study  on Korean Wh-Phrases(https://arxiv.org/abs/2402.00632)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Speech-to-Text Translation (S2TT) has typically been addressed with cascade systems, where speech recognition systems generate a transcription that is subsequently passed to a translation model. While there has been a growing interest in developing direct speech translation systems to avoid propagating errors and losing non-verbal content, prior work in direct S2TT has struggled to conclusively establish the advantages of integrating the acoustic signal directly into the translation process. This work proposes using contrastive evaluation to quantitatively measure the ability of direct S2TT systems to disambiguate utterances where prosody plays a crucial role. Specifically, we evaluated Korean-English translation systems on a test set containing wh-phrases, for which prosodic features are necessary to produce translations with the correct intent, whether it's a statement, a yes/no question, a wh-question, and more. Our results clearly demonstrate the value of direct translation systems over cascade translation models, with a notable 12.9% improvement in overall accuracy in ambiguous cases, along with up to a 15.6% increase in F1 scores for one of the major intent categories. To the best of our knowledge, this work stands as the first to provide quantitative evidence that direct S2TT models can effectively leverage prosody. The code for our evaluation is openly accessible and freely available for review and utilisation.</li>
<li><strong>摘要：</strong>语音到文本翻译 (S2TT) 通常通过级联系统来解决，其中语音识别系统生成转录，随后将其传递到翻译模型。尽管人们对开发直接语音翻译系统以避免传播错误和丢失非语言内容的兴趣日益浓厚，但直接 S2TT 的先前工作一直在努力确定将声学信号直接集成到翻译过程中的优势。这项工作建议使用对比评估来定量测量直接 S2TT 系统消除韵律发挥关键作用的话语歧义的能力。具体来说，我们在包含 wh 短语的测试集上评估了韩语-英语翻译系统，对于这些短语，韵律特征对于生成具有正确意图的翻译是必要的，无论是陈述、是/否问题、wh 问题等等。我们的结果清楚地证明了直接翻译系统相对于级联翻译模型的价值，在模棱两可的情况下，整体准确率显着提高了 12.9%，其中一个主要意图类别的 F1 分数提高了 15.6%。据我们所知，这项工作首次提供了直接 S2TT 模型可以有效利用韵律的定量证据。我们的评估代码可公开访问并免费供审查和使用。</li>
</ul>

<h3>Title: Learning Planning-based Reasoning by Trajectories Collection and Process  Reward Synthesizing</h3>
<ul>
<li><strong>Authors: </strong>Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy F. Chen, Shafiq Joty</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00658">https://arxiv.org/abs/2402.00658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00658">https://arxiv.org/pdf/2402.00658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00658]] Learning Planning-based Reasoning by Trajectories Collection and Process  Reward Synthesizing(https://arxiv.org/abs/2402.00658)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, lora, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在通过逐步的基本原理生成来处理复杂推理任务方面表现出了巨大的潜力。然而，最近的研究引起了人们对其推理过程中的幻觉和缺陷的担忧。我们正在做出巨大努力来提高所生成理由的可靠性和忠实度。一些方法将推理建模为规划，而另一些方法则侧重于过程监督的注释。然而，由于频繁评估中间推理状态和广泛的探索空间，基于规划的搜索过程通常会导致高延迟。此外，通过人工注释监督推理过程成本高昂，而且难以扩展法学硕士培训。为了解决这些问题，在本文中，我们提出了一个框架，通过对收集的轨迹进行直接偏好优化（DPO）来学习基于计划的推理，这些轨迹根据综合过程奖励进行排名。我们在具有挑战性的逻辑推理基准上的结果证明了我们的学习框架的有效性，表明我们的 7B 模型可以超越 GPT-3.5-Turbo 等强大的同行。</li>
</ul>

<h3>Title: Improving Weak-to-Strong Generalization with Scalable Oversight and  Ensemble Learning</h3>
<ul>
<li><strong>Authors: </strong>Jitao Sang, Yuhang Wang, Jing Zhang, Yanxu Zhu, Chao Kong, Junhong Ye, Shuyu Wei, Jinlin Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00667">https://arxiv.org/abs/2402.00667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00667">https://arxiv.org/pdf/2402.00667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00667]] Improving Weak-to-Strong Generalization with Scalable Oversight and  Ensemble Learning(https://arxiv.org/abs/2402.00667)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks. The W2SG framework has opened new possibilities for empirical research in this evolving field. Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence. In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students. In the second phase, an automatic alignment evaluator is employed as the weak supervisor. By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong supervision over stronger student models.We also provide an initial validation of the proposed approach for the first phase. Using the SciQ task as example, we explore ensemble learning for weak teacher models through bagging and boosting. Scalable oversight is explored through two auxiliary settings: human-AI interaction and AI-AI debate. Additionally, the paper discusses the impact of improved weak supervision on enhancing weak-to-strong generalization based on in-context learning. Experiment code and dataset will be released at https://github.com/ADaM-BJTU/W2SG.</li>
<li><strong>摘要：</strong>本文介绍了 OpenAI 最近关于弱到强泛化（W2SG）的超对齐工作的后续研究。 Superalignment 专注于确保高级人工智能系统在处理复杂、高风险的任务时与人类价值观和意图保持一致。 W2SG 框架为这一不断发展的领域的实证研究开辟了新的可能性。我们的研究模拟了 W2SG 框架下超排列的两个阶段：一般超人类模型的发展和超级智能的进展。第一阶段，基于人类监督，通过可扩展监督和集成学习的结合来提高弱监督的质量，缩小弱教师和强学生之间的能力差距。在第二阶段，采用自动对齐评估器作为弱监督器。通过递归更新这个自动对齐器，弱教师模型的能力得到同步增强，实现对更强学生模型的从弱到强的监督。我们还对第一阶段所提出的方法进行了初步验证。以 SciQ 任务为例，我们通过 bagging 和 boosting 探索弱教师模型的集成学习。通过两个辅助设置探索可扩展的监督：人与人工智能的交互和人工智能与人工智能的辩论。此外，本文还讨论了改进的弱监督对基于上下文学习增强弱到强泛化的影响。实验代码和数据集将在https://github.com/ADaM-BJTU/W2SG发布。</li>
</ul>

<h3>Title: Non-Exchangeable Conformal Language Generation with Nearest Neighbors</h3>
<ul>
<li><strong>Authors: </strong>Dennis Ulmer, Chrysoula Zerva, André F.T. Martins</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00707">https://arxiv.org/abs/2402.00707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00707">https://arxiv.org/pdf/2402.00707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00707]] Non-Exchangeable Conformal Language Generation with Nearest Neighbors(https://arxiv.org/abs/2402.00707)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination, rag</a></li>
<li><strong>Abstract: </strong>Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees.</li>
<li><strong>摘要：</strong>量化自动生成文本中的不确定性对于让人类检查潜在的幻觉并使系统更加可靠非常重要。共形预测是一个有吸引力的框架，可以提供充满统计保证的预测，但是，它在文本生成中的应用具有挑战性，因为任何 i.i.d.假设是不现实的。在本文中，我们通过利用不可交换的共形预测的最新结果来弥补这一差距，这仍然确保了覆盖范围。结果，不可交换的共形核采样，是共形预测框架到基于最近邻的生成的新颖扩展。我们的方法可以事后用于任意模型，无需额外训练，并提供具有统计保证的标记级校准预测集。机器翻译和语言建模的实验在生成质量方面取得了令人鼓舞的成果。通过还生成具有良好覆盖范围的更严格的预测集，我们因此提供了一种更具理论原理的方法来执行保形保证的采样。</li>
</ul>

<h3>Title: Explaining Text Classifiers with Counterfactual Representations</h3>
<ul>
<li><strong>Authors: </strong>Pirmin Lemberger, Antoine Saillenfest</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00711">https://arxiv.org/abs/2402.00711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00711">https://arxiv.org/pdf/2402.00711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00711]] Explaining Text Classifiers with Counterfactual Representations(https://arxiv.org/abs/2402.00711)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the representation space. Second, we study a real world scenario where our counterfactuals can be leveraged both for explaining a classifier and for bias mitigation.</li>
<li><strong>摘要：</strong>一种对分类器有充分动机的解释方法利用反事实，这些假设事件是除了一个分类特征之外在所有方面都与真实观察结果相同的假设事件。然而，构建这种反事实对文本提出了特定的挑战，因为某些属性值可能不一定与真实世界的事件相符。在本文中，我们提出了一种通过干预文本表示空间来生成反事实的简单方法，从而绕过了这一限制。我们认为，我们的干预措施具有最小的破坏性，并且它们在理论上是合理的，因为它们与 Pearl 因果推理框架中定义的反事实相一致。为了验证我们的方法，我们首先在反事实合成数据集上进行实验，从而可以直接比较基于真实反事实（通过显式文本干预获得）的分类器预测和通过表示空间干预得出的反事实。其次，我们研究了一个现实世界的场景，其中我们的反事实既可以用来解释分类器，也可以用来缓解偏差。</li>
</ul>

<h3>Title: Intent Assurance using LLMs guided by Intent Drift</h3>
<ul>
<li><strong>Authors: </strong>Kristina Dzeparoska, Ali Tizghadam, Alberto Leon-Garcia</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.NI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00715">https://arxiv.org/abs/2402.00715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00715">https://arxiv.org/pdf/2402.00715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00715]] Intent Assurance using LLMs guided by Intent Drift(https://arxiv.org/abs/2402.00715)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner. However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents. To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states. In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs. To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.</li>
<li><strong>摘要：</strong>基于意图的网络 (IBN) 承诺以自动化方式将意图和业务目标与网络运营保持一致，从而实现了网络管理的范式转变。然而，其实际实现具有挑战性：1）处理意图，即翻译、分解和识别实现意图的逻辑，2）意图一致性，即考虑动态网络，逻辑应充分适应以确保意图。为了解决后者，意图保证的任务是持续验证和确认，包括采取必要的行动来调整操作状态和目标状态。在本文中，我们定义了一个保证框架，使我们能够在发生意图漂移时检测并采取行动。为此，我们利用由大型语言模型 (LLM) 生成的人工智能驱动的策略，这些策略可以快速学习必要的上下文要求，并协助实现和保证意图。</li>
</ul>

<h3>Title: Improving Semantic Control in Discrete Latent Spaces with Transformer  Quantized Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Yingji Zhang, Danilo S. Carvalho, Marco Valentino, Ian Pratt-Hartmann, Andre Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00723">https://arxiv.org/abs/2402.00723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00723">https://arxiv.org/pdf/2402.00723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00723]] Improving Semantic Control in Discrete Latent Spaces with Transformer  Quantized Variational Autoencoders(https://arxiv.org/abs/2402.00723)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon. Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism. To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities. Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and preservation of semantic information across different tasks such as auto-encoding of sentences and mathematical expressions, text transfer, and inference. Moreover, T5VQVAE exhibits improved inference capabilities, suggesting potential applications for downstream natural language and symbolic reasoning tasks.</li>
<li><strong>摘要：</strong>对变分自动编码器 (VAE) 的潜在空间实现精确的语义控制对于 NLP 中的下游任务具有重要价值，因为可以更好地本地化、解释和改进底层生成机制。然而，最近的研究一直难以获得一致的结果，这主要是由于变分瓶颈中语义信息不可避免的丢失以及对解码机制的控制有限。为了克服这些挑战，我们研究了矢量量化变分自动编码器 (VQVAE) 中的离散潜在空间，以改进基于 Transformer 的 VAE 中的语义控制和生成。特别是，我们提出了 T5VQVAE，这是一种利用 VQVAE 的可控性来指导 T5 代币级别的自注意力机制的新颖模型，充分利用其泛化能力。实验结果表明，T5VQVAE 在跨不同任务（例如句子和数学表达式的自动编码、文本传输和推理）的可控性和语义信息保存方面优于现有最先进的 VAE 模型（包括 Optimus）。此外，T5VQVAE 表现出改进的推理能力，表明下游自然语言和符号推理任务的潜在应用。</li>
</ul>

<h3>Title: Dropout-Based Rashomon Set Exploration for Efficient Predictive  Multiplicity Estimation</h3>
<ul>
<li><strong>Authors: </strong>Hsiang Hsu, Guihong Li, Shaohan Hu, Chun-Fu (Richard)Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00728">https://arxiv.org/abs/2402.00728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00728">https://arxiv.org/pdf/2402.00728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00728]] Dropout-Based Rashomon Set Exploration for Efficient Predictive  Multiplicity Estimation(https://arxiv.org/abs/2402.00728)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications. Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces. To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set. We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation. Numerical results show that our technique consistently outperforms baselines in terms of the effectiveness of predictive multiplicity metric estimation, with runtime speedup up to $20\times \sim 5000\times$. With efficient Rashomon set exploration and metric estimation, mitigation of predictive multiplicity is then achieved through dropout ensemble and model selection.</li>
<li><strong>摘要：</strong>预测多重性是指分类任务可能允许多个竞争模型实现几乎同样最佳的性能，但为各个样本生成相互冲突的输出的现象。这引起了重大关注，因为它可能会导致实际应用中的系统性排斥、莫名其妙的歧视和不公平。然而，测量和减轻预测多重性在计算上具有挑战性，因为需要在潜在的巨大假设空间中探索所有这些几乎同样最优的模型（称为罗生门集）。为了应对这一挑战，我们提出了一种新颖的框架，利用 dropout 技术来探索罗生门集中的模型。我们提供严格的理论推导，将 dropout 参数与罗生门集的属性联系起来，并通过广泛的实验对我们的框架进行实证评估。数值结果表明，我们的技术在预测多重性度量估计的有效性方面始终优于基线，运行时加速高达 $20\times \sim 5000\times$。通过有效的罗生门集探索和度量估计，然后通过 dropout 集成和模型选择来减轻预测多重性。</li>
</ul>

<h3>Title: FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum  Markov Game</h3>
<ul>
<li><strong>Authors: </strong>Guangzheng Hu, Yuanheng Zhu, Haoran Li, Dongbin Zhao</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00738">https://arxiv.org/abs/2402.00738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00738">https://arxiv.org/pdf/2402.00738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00738]] FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum  Markov Game(https://arxiv.org/abs/2402.00738)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years. However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability. In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with neural networks is proposed to implement FM3Q and obtain the deterministic and decentralized minimax policies for two-team players. A theoretical analysis is provided to prove the convergence of FM3Q. Empirically, we use three environments to evaluate the learning efficiency and final performance of FM3Q and show its superiority on 2t0sMGs.</li>
<li><strong>摘要：</strong>许多现实世界的应用程序涉及一些分为两个团队的代理，在同一团队中收益相等，但在对手团队中符号相反。近年来，所谓的两队零和马尔可夫博弈（2t0sMGs）可以通过强化学习来解决。然而，由于没有充分考虑团队内部的信用分配、数据利用和计算复杂性，现有方法效率低下。在本文中，我们提出了个体全局最小最大（IGMM）原则，以通过 2t0sMG 中的 Q 函数确保两队最小最大行为和个体贪婪行为之间的一致性。基于此，我们提出了一种新颖的多智能体强化学习框架，因子式多智能体 MiniMax Q-Learning (FM3Q)，它可以将联合极小极大 Q 函数分解为单独的函数，并迭代求解 IGMM 满足的极小极大 Q 函数2t0sMG。此外，提出了一种神经网络在线学习算法来实现 FM3Q 并获得两队玩家的确定性和分散的极小极大策略。理论分析证明了FM3Q的收敛性。根据经验，我们使用三种环境来评估 FM3Q 的学习效率和最终性能，并展示其在 2t0sMG 上的优越性。</li>
</ul>

<h3>Title: Transforming and Combining Rewards for Aligning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zihao Wang, Chirag Nagpal, Jonathan Berant, Jacob Eisenstein, Alex D'Amour, Sanmi Koyejo, Victor Veitch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00742">https://arxiv.org/abs/2402.00742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00742">https://arxiv.org/pdf/2402.00742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00742]] Transforming and Combining Rewards for Aligning Large Language Models(https://arxiv.org/abs/2402.00742)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model). Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise. Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach.</li>
<li><strong>摘要：</strong>将语言模型与人类偏好保持一致的常见方法是首先从偏好数据中学习奖励模型，然后使用该奖励模型来更新语言模型。我们研究了这种方法中出现的两个密切相关的问题。首先，奖励模型的任何单调变换都会保留偏好排名；有没有比其他选择“更好”的选择？其次，我们经常希望将语言模型与多个属性结合起来：我们应该如何组合多个奖励模型？使用对齐过程的概率解释，我们确定了从 Bradley-Terry 偏好模型中学习到的（常见情况）奖励的自然转换选择。这种导出的变换有两个重要的属性。首先，它强调改善表现不佳的产出，而不是已经得分良好的产出。这可以缓解欠拟合（某些提示未得到改进）和奖励黑客（模型学习利用奖励模型的错误指定）。其次，它通过将求和与逻辑连接联系起来，实现奖励的有原则的聚合：转换奖励的总和对应于输出在所有测量属性中“良好”的概率，从某种意义上说，我们是精确的。使用 RLHF 将语言模型调整为既有帮助又无害的实验表明，相对于基线（未转换）方法有显着改进。</li>
</ul>

<h3>Title: Benefits of Transformer: In-Context Learning in Linear Regression Tasks  with Unstructured Data</h3>
<ul>
<li><strong>Authors: </strong>Yue Xing, Xiaofeng Lin, Namjoon Suh, Qifan Song, Guang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00743">https://arxiv.org/abs/2402.00743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00743">https://arxiv.org/pdf/2402.00743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00743]] Benefits of Transformer: In-Context Learning in Linear Regression Tasks  with Unstructured Data(https://arxiv.org/abs/2402.00743)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token next to $x_i$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention.</li>
<li><strong>摘要：</strong>在实践中，我们发现基于 Transformer 的模型可以在推理阶段学习上下文中的概念。虽然现有文献，例如 \citet{zhang2023trained,huang2023context}，提供了这种上下文学习能力的理论解释，但它们假设每个样本的输入 $x_i$ 和输出 $y_i$ 嵌入在相同的标记中（即，结构化数据）。然而，实际上，它们以两种标记形式呈现（即非结构化数据\cite{wibisono2023role}）。在这种情况下，本文在线性回归任务中进行实验来研究 Transformer 架构的好处，并提供一些相应的理论直觉来解释为什么 Transformer 可以从非结构化数据中学习。我们研究变压器中促进上下文学习的确切组件。特别是，我们观察到（1）具有两层 softmax（自）注意力和前瞻注意力掩模的转换器可以从提示中学习每个示例中 $y_i$ 是否在 $x_i$ 旁边的标记中； (2)位置编码可以进一步提高性能； （3）具有高输入嵌入维数的多头注意力比单头注意力具有更好的预测性能。</li>
</ul>

<h3>Title: Enhancing Ethical Explanations of Large Language Models through  Iterative Symbolic Refinement</h3>
<ul>
<li><strong>Authors: </strong>Xin Quan, Marco Valentino, Louise A. Dennis, André Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00745">https://arxiv.org/abs/2402.00745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00745">https://arxiv.org/pdf/2402.00745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00745]] Enhancing Ethical Explanations of Large Language Models through  Iterative Symbolic Refinement(https://arxiv.org/abs/2402.00745)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.</li>
<li><strong>摘要：</strong>越来越多的自然语言推理（NLI）研究侧重于大型语言模型（LLM）及其推理能力的应用和评估。然而，尽管取得了成功，法学硕士在解释中仍然容易出现事实错误和不一致，为复杂领域的推理提供有限的控制和可解释性。在本文中，我们关注道德 NLI，研究混合神经符号技术如何增强法学硕士产生的道德解释的逻辑有效性和一致性。具体来说，我们提出了一个名为 Logic-Explainer 的溯因演绎框架，它将法学硕士与外部反向链接求解器集成在一起，以细化逐步的自然语言解释并共同验证其正确性，减少不完整性并最大限度地减少冗余。广泛的实证分析表明，Logic-Explainer 可以改进通过上下文学习方法和思想链 (CoT) 生成的解释，以应对具有挑战性的道德 NLI 任务，同时生成描述和支持模型推理的正式证明。由于道德 NLI 需要常识推理来识别潜在的道德违规行为，因此我们的结果表明神经符号方法在更广泛的多步骤 NLI 中的有效性，为增强法学硕士的逻辑一致性、可靠性和一致性提供了新的机会。</li>
</ul>

<h3>Title: Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Jin, Qinkai Yu, Chong Zhang, Dong Shu, Suiyuan Zhu, Mengnan Du, Yongfeng Zhang, Yanda Meng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00746">https://arxiv.org/abs/2402.00746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00746">https://arxiv.org/pdf/2402.00746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00746]] Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model(https://arxiv.org/abs/2402.00746)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease prediction. We have conducted disease prediction experiments on a large number of health reports to assess the effectiveness of Health-LLM. The results of the experiments indicate that the proposed method surpasses traditional methods and has the potential to revolutionize disease prediction and personalized health management. The code is available at https://github.com/jmyissb/HealthLLM.</li>
<li><strong>摘要：</strong>医疗领域的人工智能（AI）极大地促进了智能医疗的发展。然而，传统的智能医疗受到静态数据和统一标准的限制，无法与个人情况和其他挑战充分结合。因此，需要开发更专业、更细致的智能医疗方法。为此，我们提出了一个名为 Heath-LLM 的创新框架，该框架结合了大规模特征提取和医学知识权衡评分。与传统的健康管理方法相比，我们的方法具有三个主要优点。首先，我们的方法将健康报告集成到一个大模型中以提供详细的任务信息。其次，利用专业的医学知识来调整健康特征的加权分数。第三，我们使用半自动特征提取框架来增强语言模型的分析能力，并结合专家的见解来提高疾病预测的准确性。我们对大量的健康报告进行了疾病预测实验，以评估Health-LLM的有效性。实验结果表明，所提出的方法超越了传统方法，有可能彻底改变疾病预测和个性化健康管理。该代码可在 https://github.com/jmyissb/HealthLLM 获取。</li>
</ul>

<h3>Title: Unlearnable Algorithms for In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Andrei Muresanu, Anvith Thudi, Michael R. Zhang, Nicolas Papernot</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00751">https://arxiv.org/abs/2402.00751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00751">https://arxiv.org/pdf/2402.00751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00751]] Unlearnable Algorithms for In-context Learning(https://arxiv.org/abs/2402.00751)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.</li>
<li><strong>摘要：</strong>随着模型越来越多地部署在来源不明的数据上，机器取消学习是一种理想的操作。然而，实现精确的遗忘（当要遗忘的数据从未使用过时获得与模型分布相匹配的模型）具有挑战性或效率低下，通常需要大量的再训练。在本文中，我们重点关注预训练大语言模型（LLM）任务适应阶段的有效取消学习方法。我们观察到，法学硕士能够进行任务适应的上下文学习，从而可以有效地准确地忘却任务适应训练数据。我们提供了一种算法，用于选择少量训练示例，以添加到 LLM（用于任务适应）的提示之前，ERASE，其取消学习操作成本与模型和数据集大小无关，这意味着它可以扩展到大型模型和数据集。我们还比较了我们的方法和微调方法，并讨论了两种方法之间的权衡。这促使我们提出一种新的整体衡量忘却成本的方法，该方法考虑了不同的推理成本，并得出结论，上下文学习通常比对涉及忘却请求的部署进行微调更有利。</li>
</ul>

<h3>Title: Dense Reward for Free in Reinforcement Learning from Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Alex J. Chan, Hao Sun, Samuel Holt, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00782">https://arxiv.org/abs/2402.00782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00782">https://arxiv.org/pdf/2402.00782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00782]] Dense Reward for Free in Reinforcement Learning from Human Feedback(https://arxiv.org/abs/2402.00782)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many "actions" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima.</li>
<li><strong>摘要：</strong>人类反馈强化学习 (RLHF) 被认为是使大型语言模型 (LLM) 能够有效遵循指令并提供有用帮助的关键进步。传统上，这涉及到在使用单独的奖励模型为完整完成分配分数之前，从 LLM 生成完成以响应查询。作为一个自回归过程，LLM 必须采取许多“行动”（选择单个令牌），并且在一个情节结束时仅收到一个单一的、稀疏的奖励，众所周知，这种设置在传统强化学习中很难优化。在这项工作中，我们利用了这样一个事实：奖励模型包含的信息不仅仅是其标量输出，特别是，它计算了令牌上的注意力图作为变压器架构的一部分。我们使用这些注意力权重在整个完成过程中重新分配奖励，有效地致密信号并突出显示最重要的标记，所有这些都不会产生额外的计算成本或需要任何额外的建模。我们证明，从理论上讲，这种方法相当于基于潜力的奖励塑造，确保最优政策保持不变。根据经验，我们表明它可以稳定训练，加快学习速度，并且在实际情况下，可能会导致更好的局部最优。</li>
</ul>

<h3>Title: CroissantLLM: A Truly Bilingual French-English Language Model</h3>
<ul>
<li><strong>Authors: </strong>Manuel Faysse, Patrick Fernandes, Nuno Guerreiro, António Loison, Duarte Alves, Caio Corro, Nicolas Boizard, João Alves, Ricardo Rei, Pedro Martins, Antoni Bigata Casademunt, François Yvon, André Martins, Gautier Viaud, Céline Hudelot, Pierre Colombo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00786">https://arxiv.org/abs/2402.00786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00786">https://arxiv.org/pdf/2402.00786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00786]] CroissantLLM: A Truly Bilingual French-English Language Model(https://arxiv.org/abs/2402.00786)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, chat</a></li>
<li><strong>Abstract: </strong>We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models. We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives. This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models.</li>
<li><strong>摘要：</strong>我们推出了 CroissantLLM，这是一种在一组 3T 英语和法语代币上预训练的 1.3B 语言模型，为研究和工业界带来一个高性能、完全开源的双语模型，可以在消费级本地硬件上快速运行。为此，我们开创了使用 1:1 的英语与法语预训练数据比例、自定义分词器和双语微调数据集来训练本质上双语模型的方法。我们发布了训练数据集，特别是包含手动策划的高质量和多样化数据源的法国数据集。为了评估英语以外的性能，我们制定了一个新颖的基准——FrenchBench，它由一系列分类和生成任务组成，涵盖了法语模型性能的各个正交方面。此外，基于透明度并促进进一步的大型语言模型研究，我们发布了代码库和数十个跨不同模型大小、训练数据分布和训练步骤的检查点，以及微调的聊天模型和强大的翻译模型。我们通过 FMTI 框架评估我们的模型，并验证了 81% 的透明度标准，远远超出了大多数开放倡议的分数。这项工作丰富了 NLP 领域，摆脱了以前以英语为中心的工作，以加强我们对语言模型中多语言性的理解。</li>
</ul>

<h3>Title: Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective  State Spaces</h3>
<ul>
<li><strong>Authors: </strong>Chloe Wang, Oleksii Tsepa, Jun Ma, Bo Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00789">https://arxiv.org/abs/2402.00789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00789">https://arxiv.org/pdf/2402.00789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00789]] Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective  State Spaces(https://arxiv.org/abs/2402.00789)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance. Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption. The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba.</li>
<li><strong>摘要：</strong>注意力机制已被广泛用于捕获图转换器中节点之间的远程依赖关系。由于二次计算成本的瓶颈，注意力机制无法在大图中扩展。最近计算效率的提高主要是通过随机或基于启发式的图子采样的注意力稀疏化来实现的，这在数据依赖的上下文推理中存在不足。状态空间模型 (SSM)，例如 Mamba，因其在序列数据中的远程依赖关系建模方面的有效性和效率而受到关注。然而，使 SSM 适应非序列图数据提出了一个显着的挑战。在这项工作中，我们介绍了 Graph-Mamba，这是通过将 Mamba 块与依赖于输入的节点选择机制集成来增强图网络中的远程上下文建模的首次尝试。具体来说，我们制定了以图为中心的节点优先级和排列策略，以增强上下文感知推理，从而显着提高预测性能。对 10 个基准数据集进行的大量实验表明，Graph-Mamba 在远程图预测任务中的性能优于最先进的方法，而 FLOP 和 GPU 内存消耗的计算成本仅为其一小部分。代码和模型可在 https://github.com/bowang-lab/Graph-Mamba 上公开获取。</li>
</ul>

<h3>Title: Distinguishing the Indistinguishable: Human Expertise in Algorithmic  Prediction</h3>
<ul>
<li><strong>Authors: </strong>Rohan Alur, Manish Raghavan, Devavrat Shah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00793">https://arxiv.org/abs/2402.00793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00793">https://arxiv.org/pdf/2402.00793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00793]] Distinguishing the Indistinguishable: Human Expertise in Algorithmic  Prediction(https://arxiv.org/abs/2402.00793)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.</li>
<li><strong>摘要：</strong>我们引入了一种新颖的框架，将人类的专业知识融入算法预测中。我们的方法侧重于使用人类判断来区分与任何可行的预测算法“看起来相同”的输入。我们认为，这种框架澄清了预测任务中人类/人工智能协作的问题，因为专家通常可以访问未编码在算法训练数据中的信息，尤其是主观信息。我们利用这种洞察力开发了一套原则性算法，仅当人类反馈提高任何可行预测器的性能时才选择性地纳入人类反馈。我们根据经验发现，虽然平均而言，算法通常优于人类同行，但人类判断可以显着改善算法对特定实例（可以事前识别）的预测。在 X 射线分类任务中，我们发现该子集占患者群体的近 30%。我们的方法提供了一种发现这种异质性的自然方法，从而实现有效的人机协作。</li>
</ul>

<h3>Title: ReAGent: Towards A Model-agnostic Feature Attribution Method for  Generative Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhixue Zhao, Boxuan Shan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00794">https://arxiv.org/abs/2402.00794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00794">https://arxiv.org/pdf/2402.00794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00794]] ReAGent: Towards A Model-agnostic Feature Attribution Method for  Generative Language Models(https://arxiv.org/abs/2402.00794)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, agent</a></li>
<li><strong>Abstract: </strong>Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent). Our method updates the token importance distribution in a recursive manner. For each update, we compute the difference in the probability distribution over the vocabulary for predicting the next token between using the original input and using a modified version where a part of the input is replaced with RoBERTa predictions. Our intuition is that replacing an important token in the context should have resulted in a larger change in the model's confidence in predicting the token than replacing an unimportant token. Our method can be universally applied to any generative LM without accessing internal model weights or additional training and fine-tuning, as most other FAs require. We extensively compare the faithfulness of ReAGent with seven popular FAs across six decoder-only LMs of various sizes. The results show that our method consistently provides more faithful token importance distributions.</li>
<li><strong>摘要：</strong>特征归因方法（FA），例如梯度和注意力，是广泛采用的方法来推导所有输入特征对模型预测的重要性。自然语言处理领域的现有工作主要集中在分类任务中开发和测试仅编码器语言模型 (LM) 的 FA。然而，由于模型架构和任务设置之间的固有差异，尚不清楚在文本生成中是否忠实地使用这些 FA 进行仅解码器模型。此外，之前的工作已经证明，跨模型和任务不存在“一胜多负”的 FA。这使得大型 LM 的 FA 选择计算成本高昂，因为输入重要性推导通常需要多次前向和后向传递，包括梯度计算，即使访问大型计算也可能会令人望而却步。为了解决这些问题，我们提出了一种用于生成 LM 的与模型无关的 FA，称为递归归因生成器 (ReAGent)。我们的方法以递归方式更新令牌重要性分布。对于每次更新，我们都会计算使用原始输入和使用修改版本（其中部分输入被替换为 RoBERTa 预测）之间预测下一个标记的词汇表的概率分布差异。我们的直觉是，与替换不重要的标记相比，替换上下文中的重要标记应该会导致模型预测标记的置信度发生更大的变化。我们的方法可以普遍应用于任何生成式 LM，而无需像大多数其他 FA 那样访问内部模型权重或额外的训练和微调。我们广泛比较了 ReAGent 与不同大小的 6 个仅解码器 LM 中的 7 个流行 FA 的忠实度。结果表明，我们的方法始终提供更忠实的令牌重要性分布。</li>
</ul>

<h3>Title: LLMs learn governing principles of dynamical systems, revealing an  in-context neural scaling law</h3>
<ul>
<li><strong>Authors: </strong>Toni J.B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00795">https://arxiv.org/abs/2402.00795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00795">https://arxiv.org/pdf/2402.00795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00795]] LLMs learn governing principles of dynamical systems, revealing an  in-context neural scaling law(https://arxiv.org/abs/2402.00795)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.</li>
<li><strong>摘要：</strong>预训练的大型语言模型 (LLM) 在执行零样本任务（包括时间序列预测）方面出奇地有效。然而，由于模型的复杂性，理解这些功能背后的机制仍然非常具有挑战性。在本文中，我们研究了法学硕士推断动力系统行为的能力，该动力系统的演化受物理兴趣原理的支配。我们的结果表明，LLaMA 2（一种主要在文本上训练的语言模型）无需微调或即时工程即可实现动态系统时间序列的准确预测。此外，学习到的物理规则的准确性随着输入上下文窗口的长度而增加，揭示了神经缩放定律的上下文版本。在此过程中，我们提出了一种灵活高效的算法，用于直接从法学硕士中提取多位数的概率密度函数。</li>
</ul>

<h3>Title: Formal-LLM: Integrating Formal Language and Natural Language for  Controllable LLM-based Agents</h3>
<ul>
<li><strong>Authors: </strong>Zelong Li, Wenyue Hua, Hao Wang, He Zhu, Yongfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.FL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00798">https://arxiv.org/abs/2402.00798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00798">https://arxiv.org/pdf/2402.00798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00798]] Formal-LLM: Integrating Formal Language and Natural Language for  Controllable LLM-based Agents(https://arxiv.org/abs/2402.00798)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The work is open-sourced at https://github.com/agiresearch/Formal-LLM.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展使 AI 代理能够自动生成和执行多步骤计划来解决复杂任务。然而，由于LLM的内容生成过程难以控制，当前基于LLM的代理经常生成无效或不可执行的计划，这会危及生成计划的性能并损害用户对基于LLM的代理的信任。为此，本文通过整合自然语言的表达能力和形式语言的精确性，为基于 LLM 的代理提出了一种新颖的“Formal-LLM”框架。具体来说，该框架允许人类用户将他们对规划过程的要求或约束表达为自动机。然后在自动机的监督下进行基于堆栈的LLM计划生成过程，以确保生成的计划满足约束条件，使计划过程可控。我们对基准任务和实际任务进行了实验，我们的框架实现了超过 50% 的整体性能提升，验证了采用 Formal-LLM 指导智能体计划生成的可行性和有效性，防止智能体生成无效的规划和不成功的计划。此外，更可控的基于LLM的代理可以促进LLM在规划的高有效性至关重要的应用场景中得到更广泛的利用。这项工作已在 https://github.com/agiresearch/Formal-LLM 上开源。</li>
</ul>

<h3>Title: Distilling Conditional Diffusion Models for Offline Reinforcement  Learning through Trajectory Stitching</h3>
<ul>
<li><strong>Authors: </strong>Shangzhe Li, Xinhua Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00807">https://arxiv.org/abs/2402.00807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00807">https://arxiv.org/pdf/2402.00807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00807]] Distilling Conditional Diffusion Models for Offline Reinforcement  Learning through Trajectory Stitching(https://arxiv.org/abs/2402.00807)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Deep generative models have recently emerged as an effective approach to offline reinforcement learning. However, their large model size poses challenges in computation. We address this issue by proposing a knowledge distillation method based on data augmentation. In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator. Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks.</li>
<li><strong>摘要：</strong>深度生成模型最近已成为离线强化学习的有效方法。然而，它们的大模型尺寸给计算带来了挑战。我们通过提出一种基于数据增强的知识蒸馏方法来解决这个问题。特别是，高回报轨迹是从条件扩散模型生成的，并通过利用新奖励生成器的新颖拼接算法与原始轨迹混合。将所得数据集应用于行为克隆，学习到的浅层策略（其大小要小得多）在多个 D4RL 基准上优于或几乎匹配深度生成规划器。</li>
</ul>

<h3>Title: Leveraging Approximate Model-based Shielding for Probabilistic Safety  Guarantees in Continuous Environments</h3>
<ul>
<li><strong>Authors: </strong>Alexander W. Goodall, Francesco Belardinelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00816">https://arxiv.org/abs/2402.00816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00816">https://arxiv.org/pdf/2402.00816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00816]] Leveraging Approximate Model-based Shielding for Probabilistic Safety  Guarantees in Continuous Environments(https://arxiv.org/abs/2402.00816)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Shielding is a popular technique for achieving safe reinforcement learning (RL). However, classical shielding approaches come with quite restrictive assumptions making them difficult to deploy in complex environments, particularly those with continuous state or action spaces. In this paper we extend the more versatile approximate model-based shielding (AMBS) framework to the continuous setting. In particular we use Safety Gym as our test-bed, allowing for a more direct comparison of AMBS with popular constrained RL algorithms. We also provide strong probabilistic safety guarantees for the continuous setting. In addition, we propose two novel penalty techniques that directly modify the policy gradient, which empirically provide more stable convergence in our experiments.</li>
<li><strong>摘要：</strong>屏蔽是实现安全强化学习（RL）的流行技术。然而，经典的屏蔽方法具有相当严格的假设，这使得它们难以在复杂的环境中部署，特别是那些具有连续状态或动作空间的环境。在本文中，我们将更通用的基于模型的近似屏蔽（AMBS）框架扩展到连续设置。特别是，我们使用 Safety Gym 作为测试平台，可以更直接地将 AMBS 与流行的约束 RL 算法进行比较。我们还为连续设置提供强大的概率安全保证。此外，我们提出了两种直接修改策略梯度的新颖惩罚技术，这在我们的实验中凭经验提供了更稳定的收敛。</li>
</ul>

<h3>Title: SLIM: Skill Learning with Multiple Critics</h3>
<ul>
<li><strong>Authors: </strong>David Emukpere, Bingbing Wu, Julien Perez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00823">https://arxiv.org/abs/2402.00823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00823">https://arxiv.org/pdf/2402.00823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00823]] SLIM: Skill Learning with Multiple Critics(https://arxiv.org/abs/2402.00823)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop manipulation, we demonstrate the applicability of our novel skill discovery approach to acquire safe and efficient motor primitives in a hierarchical reinforcement learning fashion and leverage them through planning, surpassing the state-of-the-art approaches for skill discovery by a large margin.</li>
<li><strong>摘要：</strong>自我监督的技能学习旨在获得利用环境潜在动态的有用行为。基于互信息最大化的潜变量模型在这项任务中特别成功，但在机器人操作的背景下仍然举步维艰。由于它需要影响构成环境的可能较大的自由度，因此互信息最大化无法单独产生有用的操纵行为。为了解决这一限制，我们引入了 SLIM，这是一种用于技能发现的多批评学习方法，特别关注机器人操作。我们的主要见解是，在演员-批评家框架中利用多个批评家来优雅地组合多个奖励功能，可以显着改善机器人操作的潜在变量技能发现，同时克服奖励之间可能发生的干扰，从而阻碍有用技能的收敛。此外，在桌面操作的背景下，我们展示了我们新颖的技能发现方法的适用性，以分层强化学习方式获取安全高效的运动原语，并通过规划利用它们，超越了最先进的技能方法大幅发现。</li>
</ul>

<h3>Title: ALISON: Fast and Effective Stylometric Authorship Obfuscation</h3>
<ul>
<li><strong>Authors: </strong>Eric Xing, Saranya Venkatraman, Thai Le, Dongwon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00835">https://arxiv.org/abs/2402.00835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00835">https://arxiv.org/pdf/2402.00835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00835]] ALISON: Fast and Effective Stylometric Authorship Obfuscation(https://arxiv.org/abs/2402.00835)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, code, chat, rag</a></li>
<li><strong>Abstract: </strong>Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.</li>
<li><strong>摘要：</strong>作者归属（AA）和作者混淆（AO）是隐私研究中日益重要的两个相互竞争的任务。现代 AA 利用作者一致的写作风格，使用 AA 分类器将文本与其作者进行匹配。 AO 是相应的对抗性任务，旨在以保留其语义的方式修改文本，但 AA 模型无法正确推断其作者身份。为了解决最先进的 (SOTA) AA 方法引起的隐私问题，人们提出了新的 AO 方法，但由于其训练和混淆速度极慢，通常需要数小时，因此在很大程度上仍然不切实际。针对这一挑战，我们提出了一种实用的 AO 方法 ALISON，它 (1) 显着减少了训练/混淆时间，证明混淆速度比 SOTA AO 方法快 10 倍以上，(2) 通过攻击三种基于 Transformer 的 AA 方法，取得了更好的混淆成功率在两个基准数据集上，通常比竞争方法表现好 15%，(3) 在混淆过程中不需要来自目标 AA 分类器的直接信号，(4) 利用独特的风格特征，允许声音模型解释以进行可解释的混淆。我们还证明，ALISON 可以有效防止四种 SOTA AA 方法准确确定 ChatGPT 生成文本的作者，同时最小化原始文本语义的改变。为了确保我们的研究结果的可重复性，我们的代码和数据可从以下网址获取：https://github.com/EricX003/ALISON。</li>
</ul>

<h3>Title: OLMo: Accelerating the Science of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00838">https://arxiv.org/abs/2402.00838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00838">https://arxiv.org/pdf/2402.00838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00838]] OLMo: Accelerating the Science of Language Models(https://arxiv.org/abs/2402.00838)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.</li>
<li><strong>摘要：</strong>语言模型 (LM) 在 NLP 研究和商业产品中已经变得无处不在。随着其商业重要性的飙升，最强大的模型已被封闭，被封闭在专有接口后面，其训练数据、架构和开发的重要细节均未公开。考虑到这些细节在科学研究这些模型中的重要性，包括它们的偏差和潜在风险，我们认为研究界有必要获得强大的、真正开放的 LM。为此，本技术报告详细介绍了 OLMo 的第一个版本，这是一种最先进的、真正的开放语言模型及其用于构建和研究语言建模科学的框架。与之前大多数仅发布模型权重和推理代码的工作不同，我们发布了 OLMo 和整个框架，包括训练数据以及训练和评估代码。我们希望这个版本能够增强和加强开放研究社区，并激发新的创新浪潮。</li>
</ul>

<h3>Title: Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight  in the Real World for Meeting Summarization?</h3>
<ul>
<li><strong>Authors: </strong>Xue-Yong Fu, Md Tahmid Rahman Laskar, Elena Khasanova, Cheng Chen, Shashi Bhushan TN</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00841">https://arxiv.org/abs/2402.00841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00841">https://arxiv.org/pdf/2402.00841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00841]] Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight  in the Real World for Meeting Summarization?(https://arxiv.org/abs/2402.00841)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller. This makes compact LLMs like FLAN-T5 a suitable cost-efficient solution for real-world industrial deployment.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出令人印象深刻的能力，可以解决各种任务，而无需针对特定任务的数据集进行显式微调。然而，在现实世界中部署法学硕士并非易事，因为它需要大量的计算资源。在本文中，我们研究了较小、紧凑的法学硕士是否是相对较大的法学硕士的良好替代品2，以解决在现实世界中使用法学硕士相关的巨大成本。在这方面，我们研究了现实世界工业环境中的会议总结任务，并通过比较微调紧凑型 LLM（例如 FLAN-T5、TinyLLaMA、LiteLLaMA）与零样本较大 LLM（例如、LLaMA-2、GPT-3.5、PaLM-2）。我们观察到，大多数较小的法学硕士，即使经过微调，在满足摘要数据集方面也无法优于较大的零样本法学硕士。然而，一个值得注意的例外是 FLAN-T5（780M 参数），它的性能与许多零样本较大的 LLM（从 7B 到 70B 以上参数）相当甚至更好，但体积却小得多。这使得 FLAN-T5 等紧凑型法学硕士成为现实工业部署的合适且经济高效的解决方案。</li>
</ul>

<h3>Title: LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force  Fields</h3>
<ul>
<li><strong>Authors: </strong>Joshua A. Vita, Amit Samanta, Fei Zhou, Vincenzo Lordi</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00853">https://arxiv.org/abs/2402.00853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00853">https://arxiv.org/pdf/2402.00853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00853]] LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force  Fields(https://arxiv.org/abs/2402.00853)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep learning atomistic force fields. Despite this, widespread adoption of ensemble-based uncertainty quantification (UQ) techniques is limited by the high computational costs incurred by ensembles during both training and inference. In this work we leverage the cumulative distribution functions (CDFs) of per-sample errors obtained over the course of training to efficiently represent the model ensemble, and couple them with a distance-based similarity search in the model latent space. Using these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths of ensemble-based techniques without requiring the evaluation of multiple models during either training or inference. As an initial test, we apply our method towards estimating the epistemic uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to accurately predict test errors on multiple datasets from the literature. We then illustrate the utility of LTAU-FF in two practical applications: 1) tuning the training-validation gap for an example dataset, and 2) predicting errors in relaxation trajectories on the OC20 IS2RS task. Though in this work we focus on the use of LTAU with deep learning atomistic force fields, we emphasize that it can be readily applied to any regression task, or any ensemble-generation technique, to provide a reliable and easy-to-implement UQ metric.</li>
<li><strong>摘要：</strong>模型集成是估计深度学习原子力场的预测不确定性的简单而有效的工具。尽管如此，基于集成的不确定性量化（UQ）技术的广泛采用受到集成在训练和推理过程中产生的高计算成本的限制。在这项工作中，我们利用在训练过程中获得的每个样本误差的累积分布函数（CDF）来有效地表示模型集合，并将它们与模型潜在空间中基于距离的相似性搜索结合起来。使用这些工具，我们开发了一个简单的 UQ 指标（我们称之为 LTAU），该指标利用基于集成的技术的优势，而无需在训练或推理期间评估多个模型。作为初步测试，我们应用我们的方法来估计原子力场（LTAU-FF）的认知不确定性，并证明它可以轻松校准以准确预测文献中多个数据集的测试误差。然后，我们说明 LTAU-FF 在两个实际应用中的实用性：1）调整示例数据集的训练验证差距，2）预测 OC20 IS2RS 任务的松弛轨迹中的错误。尽管在这项工作中，我们重点关注 LTAU 与深度学习原子力场的使用，但我们强调它可以轻松应用于任何回归任务或任何集成生成技术，以提供可靠且易于实现的 UQ 度量。</li>
</ul>

<h3>Title: SymbolicAI: A framework for logic-based approaches combining generative  models and solvers</h3>
<ul>
<li><strong>Authors: </strong>Marius-Constantin Dinu, Claudiu Leoveanu-Condrei, Markus Holzleitner, Werner Zellinger, Sepp Hochreiter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SC, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00854">https://arxiv.org/abs/2402.00854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00854">https://arxiv.org/pdf/2402.00854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00854]] SymbolicAI: A framework for logic-based approaches combining generative  models and solvers(https://arxiv.org/abs/2402.00854)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag</a></li>
<li><strong>Abstract: </strong>We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. In turn, the framework facilitates the creation and evaluation of explainable computational graphs. We conclude by introducing a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the "Vector Embedding for Relational Trajectory Evaluation through Cross-similarity", or VERTEX score for short. The framework codebase and benchmark are linked below.</li>
<li><strong>摘要：</strong>我们引入了 SymbolicAI，这是一种多功能模块化框架，采用基于逻辑的方法来进行生成过程中的概念学习和流程管理。 SymbolicAI 通过将大型语言模型 (LLM) 视为基于自然和形式语言指令执行任务的语义解析器，实现了生成模型与各种求解器的无缝集成，从而弥合了符号推理和生成 AI 之间的差距。我们利用概率编程原理来处理复杂的任务，并利用可微分和经典编程范式及其各自的优势。该框架引入了一组用于数据流操作的多态、组合和自引用操作，使 LLM 输出与用户目标保持一致。因此，我们可以在具有零次和少次学习能力的各种基础模型的能力与精通解决特定问题的专门的、微调的模型或求解器之间进行转换。反过来，该框架有助于创建和评估可解释的计算图。最后，我们引入了用于评估这些计算图的质量衡量标准及其经验分数，并提出了一个基准来比较一组复杂工作流程中的各种最先进的法学硕士。我们将经验分数称为“通过交叉相似性进行关系轨迹评估的向量嵌入”，简称 VERTEX 分数。框架代码库和基准测试链接如下。</li>
</ul>

<h3>Title: Towards Efficient and Exact Optimization of Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie Tang, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00856">https://arxiv.org/abs/2402.00856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00856">https://arxiv.org/pdf/2402.00856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00856]] Towards Efficient and Exact Optimization of Language Model Alignment(https://arxiv.org/abs/2402.00856)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution. In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient optimization by circumventing the complexities associated with RL algorithms. We compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data.</li>
<li><strong>摘要：</strong>语言模型与人类偏好的一致性对于它们在现实世界任务中的应用至关重要。该问题被表述为优化模型的策略，以最大化反映人类偏好的预期奖励，同时与初始策略的偏差最小。虽然强化学习（RL）被认为是一种简单的解决方案，但它在策略更新方面存在很大的差异，这阻碍了有效的策略改进。最近，提出了直接偏好优化（DPO）来直接根据偏好数据优化策略。虽然实现简单，但 DPO 是基于最优策略推导出来的，但在实践中并不能保证实现，这会破坏其与预期解决方案的收敛性。在本文中，我们提出了对齐目标的高效精确优化（EXO）。我们证明，对于策略的任意参数化，EXO 保证在与 RL 算法相同的方向上渐进优化，同时通过规避与 RL 算法相关的复杂性来实现高效优化。我们通过理论和实证分析将我们的方法与 DPO 进行比较，并进一步证明我们的方法相对于现实人类偏好数据的现有方法的优势。</li>
</ul>

<h3>Title: Early Time Classification with Accumulated Accuracy Gap Control</h3>
<ul>
<li><strong>Authors: </strong>Liran Ringel, Regev Cohen, Daniel Freedman, Michael Elad, Yaniv Romano</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00857">https://arxiv.org/abs/2402.00857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00857">https://arxiv.org/pdf/2402.00857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00857]] Early Time Classification with Accumulated Accuracy Gap Control(https://arxiv.org/abs/2402.00857)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input. In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule. This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification. We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances. As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times. Numerical experiments demonstrate the effectiveness, applicability, and usefulness of our method. We show that our proposed early stopping mechanism reduces up to 94% of timesteps used for classification while achieving rigorous accuracy gap control.</li>
<li><strong>摘要：</strong>早期分类算法的目标是在不处理完整输入流的情况下标记特征流，同时保持与将分类器应用于整个输入所实现的精度相当的精度。在本文中，我们介绍了一个可应用于任何顺序分类器的统计框架，制定校准的停止规则。这种数据驱动的规则实现了对完整分类和早期分类之间的精度差距的有限样本、无分布控制。我们首先提出一种新颖的方法，该方法建立在学习然后测试校准框架的基础上，以稍微控制这种差距，平均超过独立同分布。实例。由于该算法往往会在早期停止时间产生过高的精度差距，因此我们的主要贡献是提出了一个控制更强误差概念的框架，其中精度差距是根据累积的停止时间有条件地控制的。数值实验证明了我们方法的有效性、适用性和实用性。我们表明，我们提出的早期停止机制可减少高达 94% 的用于分类的时间步长，同时实现严格的精度间隙控制。</li>
</ul>

<h3>Title: Can Large Language Models Understand Context?</h3>
<ul>
<li><strong>Authors: </strong>Yilun Zhu, Joel Ruben Antony Moniz, Shruti Bhargava, Jiarui Lu, Dhivya Piraviperumal, Site Li, Yuan Zhang, Hong Yu, Bo-Hsiang Tseng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00858">https://arxiv.org/abs/2402.00858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00858">https://arxiv.org/pdf/2402.00858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00858]] Can Large Language Models Understand Context?(https://arxiv.org/abs/2402.00858)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models' ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.</li>
<li><strong>摘要：</strong>理解上下文是理解人类语言的关键，人们越来越多地看到大型语言模型（LLM）在令人印象深刻的程度上展示了这种能力。然而，尽管法学硕士的评估涵盖了自然语言处理领域的各个领域，但对探讨其理解上下文特征的语言能力的关注有限。本文通过调整现有数据集以适应生成模型的评估，引入了上下文理解基准。该基准测试由四个不同的任务和九个数据集组成，所有任务都具有旨在评估模型理解上下文的能力的提示。首先，我们评估法学硕士在情境学习预训练场景下的表现。实验结果表明，与最先进的微调模型相比，预训练的密集模型很难理解更细微的上下文特征。其次，由于 LLM 压缩在研究和实际应用中都具有越来越重要的意义，我们评估了上下文学习环境下量化模型的上下文理解。我们发现 3 位训练后量化会导致我们的基准测试出现不同程度的性能下降。我们对这些场景进行了广泛的分析，以证实我们的实验结果。</li>
</ul>

<h3>Title: Evaluating Large Language Models for Generalization and Robustness via  Data Compression</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Li, Yunhao Guo, Frank Guerin, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.00861">https://arxiv.org/abs/2402.00861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.00861">https://arxiv.org/pdf/2402.00861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.00861]] Evaluating Large Language Models for Generalization and Robustness via  Data Compression(https://arxiv.org/abs/2402.00861)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code</a></li>
<li><strong>Abstract: </strong>Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such as Mistral and Llama-2 demonstrate a good balance between performance and robustness. Results also suggest that models struggle to generalize on news and code data, but work especially well on arXiv papers. We also find the context size and tokenization implementation have a big impact of on the overall compression performance.</li>
<li><strong>摘要：</strong>评估大型语言模型的现有方法面临着数据污染、对提示的敏感性以及基准创建成本高昂等挑战。为了解决这个问题，我们提出了一种基于无损数据压缩的评估方法，该方法测试模型的预测能力在训练截止后如何泛化。具体来说，我们收集了从2017年到2023年83个月的综合测试数据，并根据模型的训练数据截断将数据分为训练期和测试期。我们测量：1）测试期间的压缩性能，作为对未见数据的泛化程度的衡量； 2) 训练和测试期间的性能差距作为鲁棒性的衡量标准。我们的实验测试了 14 个具有不同规模的代表性大型语言模型，来源包括维基百科、新闻文章、代码、arXiv 论文和多模态数据。我们发现许多模型的压缩率在截止日期后显着降低，但 Mistral 和 Llama-2 等模型在性能和鲁棒性之间表现出良好的平衡。结果还表明，模型很难对新闻和代码数据进行泛化，但在 arXiv 论文上效果特别好。我们还发现上下文大小和标记化实现对整体压缩性能有很大影响。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
