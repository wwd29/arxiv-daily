<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-01</h1>
<h3>Title: A Latent Space Metric for Enhancing Prediction Confidence in Earth  Observation Data</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Pitsiorlas, Argyro Tsantalidou, George Arvanitakis, Marios Kountouris, Charalambos Kontoes</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17342">https://arxiv.org/abs/2401.17342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17342">https://arxiv.org/pdf/2401.17342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17342]] A Latent Space Metric for Enhancing Prediction Confidence in Earth  Observation Data(https://arxiv.org/abs/2401.17342)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>This study presents a new approach for estimating confidence in machine learning model predictions, specifically in regression tasks utilizing Earth Observation (EO) data, with a particular focus on mosquito abundance (MA) estimation. We take advantage of a Variational AutoEncoder architecture, to derive a confidence metric by the latent space representations of EO datasets. This methodology is pivotal in establishing a correlation between the Euclidean distance in latent representations and the Absolute Error (AE) in individual MA predictions. Our research focuses on EO datasets from the Veneto region in Italy and the Upper Rhine Valley in Germany, targeting areas significantly affected by mosquito populations. A key finding is a notable correlation of 0.46 between the AE of MA predictions and the proposed confidence metric. This correlation signifies a robust, new metric for quantifying the reliability and enhancing the trustworthiness of the AI model's predictions in the context of both EO data analysis and mosquito abundance studies.</li>
<li><strong>摘要：</strong>这项研究提出了一种估计机器学习模型预测置信度的新方法，特别是在利用地球观测（EO）数据的回归任务中，特别关注蚊子丰度（MA）估计。我们利用变分自动编码器架构，通过 EO 数据集的潜在空间表示导出置信度度量。该方法对于建立潜在表示中的欧几里得距离与单个 MA 预测中的绝对误差 (AE) 之间的相关性至关重要。我们的研究重点是意大利威尼托地区和德国莱茵河上游河谷的 EO 数据集，目标是受蚊子种群影响严重的地区。一个重要发现是 MA 预测的 AE 与所提出的置信度指标之间存在 0.46 的显着相关性。这种相关性意味着一种强大的新指标，可以在 EO 数据分析和蚊子丰度研究的背景下量化人工智能模型的预测可靠性并增强其可信度。</li>
</ul>

<h3>Title: Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for  Classifying Arabic Speech Acts on Twitter</h3>
<ul>
<li><strong>Authors: </strong>Khadejaa Alshehri, Areej Alhothali, Nahed Alowidi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17373">https://arxiv.org/abs/2401.17373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17373">https://arxiv.org/pdf/2401.17373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17373]] Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for  Classifying Arabic Speech Acts on Twitter(https://arxiv.org/abs/2401.17373)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Speech acts are a speakers actions when performing an utterance within a conversation, such as asking, recommending, greeting, or thanking someone, expressing a thought, or making a suggestion. Understanding speech acts helps interpret the intended meaning and actions behind a speakers or writers words. This paper proposes a Twitter dialectal Arabic speech act classification approach based on a transformer deep learning neural network. Twitter and social media, are becoming more and more integrated into daily life. As a result, they have evolved into a vital source of information that represents the views and attitudes of their users. We proposed a BERT based weighted ensemble learning approach to integrate the advantages of various BERT models in dialectal Arabic speech acts classification. We compared the proposed model against several variants of Arabic BERT models and sequence-based models. We developed a dialectal Arabic tweet act dataset by annotating a subset of a large existing Arabic sentiment analysis dataset (ASAD) based on six speech act categories. We also evaluated the models on a previously developed Arabic Tweet Act dataset (ArSAS). To overcome the class imbalance issue commonly observed in speech act problems, a transformer-based data augmentation model was implemented to generate an equal proportion of speech act categories. The results show that the best BERT model is araBERTv2-Twitter models with a macro-averaged F1 score and an accuracy of 0.73 and 0.84, respectively. The performance improved using a BERT-based ensemble method with a 0.74 and 0.85 averaged F1 score and accuracy on our dataset, respectively.</li>
<li><strong>摘要：</strong>言语行为是说话者在对话中执行话语时的行为，例如询问、推荐、问候或感谢某人，表达想法或提出建议。理解言语行为有助于解释说话者或作者话语背后的意图和行为。本文提出了一种基于 Transformer 深度学习神经网络的 Twitter 方言阿拉伯语言语行为分类方法。 Twitter 和社交媒体越来越融入日常生活。因此，它们已发展成为代表用户观点和态度的重要信息来源。我们提出了一种基于 BERT 的加权集成学习方法，以整合各种 BERT 模型在阿拉伯语方言语音行为分类中的优势。我们将所提出的模型与阿拉伯语 BERT 模型和基于序列的模型的几种变体进行了比较。我们通过基于六个言语行为类别注释现有大型阿拉伯语情感分析数据集 (ASAD) 的子集，开发了阿拉伯语方言推文行为数据集。我们还在之前开发的阿拉伯推文法案数据集 (ArSAS) 上评估了模型。为了克服言语行为问题中常见的类别不平衡问题，实现了基于变压器的数据增强模型来生成相等比例的言语行为类别。结果表明，最好的 BERT 模型是 araBERTv2-Twitter 模型，其宏观平均 F1 分数、准确度分别为 0.73 和 0.84。使用基于 BERT 的集成方法提高了性能，在我们的数据集上平均 F1 分数和准确度分别为 0.74 和 0.85。</li>
</ul>

<h3>Title: Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion  Tokens</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh Hajishirzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17377">https://arxiv.org/abs/2401.17377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17377">https://arxiv.org/pdf/2401.17377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17377]] Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion  Tokens(https://arxiv.org/abs/2401.17377)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM has fairly high accuracy for next-token prediction (47%), and can complement neural LLMs to greatly reduce their language modeling perplexities. When analyzing machine-generated text, we also observe irregularities in the machine--$\infty$-gram agreement level with respect to the suffix length, which indicates deficiencies in neural LLM pretraining and the positional embeddings of Transformers. We open-source our infini-gram engine in the hopes of enabling more study on how to best use verbatim information retrieved from large text corpora.</li>
<li><strong>摘要：</strong>n-gram 语言模型在这个神经大语言模型 (LLM) 时代仍然相关吗？我们的答案是肯定的，我们在文本分析和改进神经法学硕士方面展示了它们的价值。然而，这需要在两个方面对 n-gram 模型进行现代化改造。首先，我们以与神经 LLM 相同的数据规模（1.4 万亿个令牌）训练它们。这是有史以来最大的 n-gram 模型。其次，现有的n-gram模型使用较小的n，这阻碍了它们的性能；相反，我们通过引入带有退避功能的新 $\infty$-gram LM 来允许 n 任意大。我们开发了一个名为 infini-gram 的引擎（由后缀数组提供支持），而不是预先计算 n-gram 计数表（这会非常昂贵），它可以计算 $\infty$-gram （以及 n-gram）具有任意 n) 概率和毫秒级延迟。 $\infty$-gram 框架和 infini-gram 引擎使我们能够对人类编写的和机器生成的文本进行许多新颖且有趣的分析：我们发现 $\infty$-gram LM 对于下一个-令牌预测（47%），并且可以补充神经法学硕士，大大减少其语言建模的复杂性。在分析机器生成的文本时，我们还观察到机器-$\infty$-gram 在后缀长度方面的一致性水平存在不规则性，这表明神经 LLM 预训练和 Transformer 的位置嵌入存在缺陷。我们开源无限语法引擎，希望能够就如何最好地利用从大型文本语料库中检索到的逐字信息进行更多研究。</li>
</ul>

<h3>Title: Customizing Language Model Responses with Contrastive In-Context  Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiang Gao, Kamalika Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17390">https://arxiv.org/abs/2401.17390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17390">https://arxiv.org/pdf/2401.17390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17390]] Customizing Language Model Responses with Contrastive In-Context  Learning(https://arxiv.org/abs/2401.17390)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real-world datasets, including StackExchange and Reddit, and found that it significantly improves performance compared to standard few-shot prompting</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 对于机器学习应用程序变得越来越重要。然而，让法学硕士与我们的意图保持一致可能具有挑战性，特别是当我们想要生成比其他内容更好的内容时，或者当我们希望法学硕士以某种难以描述的风格或语气做出回应时。为了应对这一挑战，我们提出了一种使用对比示例来更好地描述我们的意图的方法。这包括提供说明真实意图的正面例子，以及显示我们希望法学硕士避免哪些特征的负面例子。负面例子可以从人类编写的标记数据中检索，也可以由法学硕士本身生成。在生成答案之前，我们要求模型分析示例，以教会自己应该避免什么。此推理步骤为模型提供了用户需求的适当表达，并指导其生成更好的答案。我们在合成数据集和真实数据集（包括 StackExchange 和 Reddit）上测试了我们的方法，发现与标准的几次提示相比，它显着提高了性能</li>
</ul>

<h3>Title: Fine-tuning Transformer-based Encoder for Turkish Language Understanding  Tasks</h3>
<ul>
<li><strong>Authors: </strong>Savas Yildirim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17396">https://arxiv.org/abs/2401.17396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17396">https://arxiv.org/pdf/2401.17396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17396]] Fine-tuning Transformer-based Encoder for Turkish Language Understanding  Tasks(https://arxiv.org/abs/2401.17396)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Deep learning-based and lately Transformer-based language models have been dominating the studies of natural language processing in the last years. Thanks to their accurate and fast fine-tuning characteristics, they have outperformed traditional machine learning-based approaches and achieved state-of-the-art results for many challenging natural language understanding (NLU) problems. Recent studies showed that the Transformer-based models such as BERT, which is Bidirectional Encoder Representations from Transformers, have reached impressive achievements on many tasks. Moreover, thanks to their transfer learning capacity, these architectures allow us to transfer pre-built models and fine-tune them to specific NLU tasks such as question answering. In this study, we provide a Transformer-based model and a baseline benchmark for the Turkish Language. We successfully fine-tuned a Turkish BERT model, namely BERTurk that is trained with base settings, to many downstream tasks and evaluated with a the Turkish Benchmark dataset. We showed that our studies significantly outperformed other existing baseline approaches for Named-Entity Recognition, Sentiment Analysis, Question Answering and Text Classification in Turkish Language. We publicly released these four fine-tuned models and resources in reproducibility and with the view of supporting other Turkish researchers and applications.</li>
<li><strong>摘要：</strong>过去几年，基于深度学习和最近基于 Transformer 的语言模型一直主导着自然语言处理的研究。由于其准确和快速的微调特性，它们的性能优于传统的基于机器学习的方法，并在许多具有挑战性的自然语言理解（NLU）问题上取得了最先进的结果。最近的研究表明，基于 Transformer 的模型（例如 BERT（Transformers 的双向编码器表示））在许多任务上取得了令人印象深刻的成就。此外，由于它们的迁移学习能力，这些架构允许我们迁移预构建的模型并将其微调到特定的 NLU 任务，例如回答问题。在这项研究中，我们提供了一个基于 Transformer 的模型和土耳其语言的基线基准。我们成功地对土耳其 BERT 模型（即 BERTurk）进行了微调，该模型使用基础设置进行训练，适用于许多下游任务，并使用土耳其基准数据集进行评估。我们表明，我们的研究在土耳其语命名实体识别、情感分析、问答和文本分类方面明显优于其他现有的基线方法。我们公开发布了这四个经过微调的模型和资源，以确保可重复性，并支持其他土耳其研究人员和应用程序。</li>
</ul>

<h3>Title: Can Large Language Models Replace Economic Choice Prediction Labs?</h3>
<ul>
<li><strong>Authors: </strong>Eilam Shapira, Omer Madmon, Roi Reichart, Moshe Tennenholtz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.GT, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17435">https://arxiv.org/abs/2401.17435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17435">https://arxiv.org/pdf/2401.17435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17435]] Can Large Language Models Replace Economic Choice Prediction Labs?(https://arxiv.org/abs/2401.17435)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predict human behavior in a language-based persuasion game, and can even outperform models trained on actual human data.</li>
<li><strong>摘要：</strong>经济选择预测是一项重要的挑战性任务，通常受到获取人类选择数据的困难的限制。事实上，实验经济学研究主要集中在简单的选择设置上。人工智能社区最近以两种方式为这一努力做出了贡献：考虑法学硕士是否可以在上述简单选择预测设置中替代人类，以及通过机器学习镜头进行更详细但仍然严格的实验经济学设置的研究，使用不完整的信息，重复游戏和自然语言交流，特别是基于语言的说服游戏。这给我们一个重大启发：法学硕士是否可以用来完全模拟经济环境并生成数据以进行高效的人类选择预测，从而替代复杂的经济实验室研究？我们开创了该课题的研究，证明了其可行性。特别是，我们表明，仅根据法学硕士生成的数据训练的模型可以有效地预测基于语言的说服游戏中的人类行为，甚至可以优于根据实际人类数据训练的模型。</li>
</ul>

<h3>Title: Liquid Democracy for Low-Cost Ensemble Pruning</h3>
<ul>
<li><strong>Authors: </strong>Ben Armstrong, Kate Larson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17443">https://arxiv.org/abs/2401.17443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17443">https://arxiv.org/pdf/2401.17443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17443]] Liquid Democracy for Low-Cost Ensemble Pruning(https://arxiv.org/abs/2401.17443)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We argue that there is a strong connection between ensemble learning and a delegative voting paradigm -- liquid democracy -- that can be leveraged to reduce ensemble training costs. We present an incremental training procedure that identifies and removes redundant classifiers from an ensemble via delegation mechanisms inspired by liquid democracy. Through both analysis and extensive experiments we show that this process greatly reduces the computational cost of training compared to training a full ensemble. By carefully selecting the underlying delegation mechanism, weight centralization in the classifier population is avoided, leading to higher accuracy than some boosting methods. Furthermore, this work serves as an exemplar of how frameworks from computational social choice literature can be applied to problems in nontraditional domains.</li>
<li><strong>摘要：</strong>我们认为，集成学习和委托投票范式（流动民主）之间存在密切联系，可以利用它来降低集成培训成本。我们提出了一种增量训练程序，通过受流动民主启发的委托机制，识别并删除集合中的冗余分类器。通过分析和广泛的实验，我们表明，与训练完整的集成相比，这个过程大大降低了训练的计算成本。通过仔细选择底层委托机制，可以避免分类器群体中的权重集中，从而比某些提升方法具有更高的准确性。此外，这项工作是如何将计算社会选择文献中的框架应用于非传统领域问题的范例。</li>
</ul>

<h3>Title: Synthetic Dialogue Dataset Generation using LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Yelaman Abdullin, Diego Molla-Aliod, Bahadorreza Ofoghi, John Yearwood, Qingyang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17461">https://arxiv.org/abs/2401.17461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17461">https://arxiv.org/pdf/2401.17461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17461]] Synthetic Dialogue Dataset Generation using LLM Agents(https://arxiv.org/abs/2401.17461)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Linear programming (LP) problems are pervasive in real-life applications. However, despite their apparent simplicity, an untrained user may find it difficult to determine the linear model of their specific problem. We envisage the creation of a goal-oriented conversational agent that will engage in conversation with the user to elicit all information required so that a subsequent agent can generate the linear model. In this paper, we present an approach for the generation of sample dialogues that can be used to develop and train such a conversational agent. Using prompt engineering, we develop two agents that "talk" to each other, one acting as the conversational agent, and the other acting as the user. Using a set of text descriptions of linear problems from NL4Opt available to the user only, the agent and the user engage in conversation until the agent has retrieved all key information from the original problem description. We also propose an extrinsic evaluation of the dialogues by assessing how well the summaries generated by the dialogues match the original problem descriptions. We conduct human and automatic evaluations, including an evaluation approach that uses GPT-4 to mimic the human evaluation metrics. The evaluation results show an overall good quality of the dialogues, though research is still needed to improve the quality of the GPT-4 evaluation metrics. The resulting dialogues, including the human annotations of a subset, are available to the research community. The conversational agent used for the generation of the dialogues can be used as a baseline.</li>
<li><strong>摘要：</strong>线性规划（LP）问题在现实生活应用中普遍存在。然而，尽管它们看起来很简单，但未经训练的用户可能会发现很难确定其特定问题的线性模型。我们设想创建一个面向目标的对话代理，它将与用户进行对话以获取所需的所有信息，以便后续代理可以生成线性模型。在本文中，我们提出了一种生成示例对话的方法，可用于开发和训练此类对话代理。使用即时工程，我们开发了两个相互“交谈”的代理，一个充当会话代理，另一个充当用户。使用 NL4Opt 中仅对用户可用的一组线性问题文本描述，代理和用户进行对话，直到代理从原始问题描述中检索到所有关键信息。我们还通过评估对话生成的摘要与原始问题描述的匹配程度，提出对对话的外在评估。我们进行人工和自动评估，包括使用 GPT-4 模仿人工评估指标的评估方法。评估结果显示对话的整体质量良好，但仍需要研究来提高 GPT-4 评估指标的质量。由此产生的对话，包括子集的人工注释，可供研究界使用。用于生成对话的对话代理可以用作基线。</li>
</ul>

<h3>Title: Efficient Tool Use with Chain-of-Abstraction Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Silin Gao, Jane Dwivedi-Yu, Ping Yu, Xiaoqing Ellen Tan, Ramakanth Pasunuru, Olga Golovneva, Koustuv Sinha, Asli Celikyilmaz, Antoine Bosselut, Tianlu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17464">https://arxiv.org/abs/2401.17464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17464">https://arxiv.org/pdf/2401.17464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17464]] Efficient Tool Use with Chain-of-Abstraction Reasoning(https://arxiv.org/abs/2401.17464)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average ~6% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs.</li>
<li><strong>摘要：</strong>为了实现符合人类期望的忠实推理，大型语言模型 (LLM) 需要将其推理建立在现实世界知识（例如网络事实、数学和物理规则）的基础上。工具可以帮助法学硕士访问这些外部知识，但微调法学硕士代理（例如 Toolformer）以在多步骤推理问题中调用工具仍然存在挑战，其中互连的工具调用需要全面且高效的工具使用规划。在这项工作中，我们提出了一种让法学硕士更好地利用多步推理工具的新方法。我们的方法抽象链（CoA）训练法学硕士首先用抽象占位符解码推理链，然后调用领域工具通过填充特定知识来具体化每个推理链。这种抽象链的规划使法学硕士能够学习更通用的推理策略，这些策略对于与不同推理问题相关的领域知识（例如数学结果）的变化具有鲁棒性。它还允许LLM并行执行外部工具的解码和调用，从而避免因等待工具响应而导致的推理延迟。在数学推理和 Wiki QA 领域，我们表明我们的方法在分布内和分布外测试集上始终优于先前的思想链和工具增强基线，平均绝对 QA 准确度提高约 6% 。使用我们的方法训练的 LLM 代理还显示出更有效的工具使用，推理速度平均比基线工具增强的 LLM 快约 1.4 倍。</li>
</ul>

<h3>Title: Detecting mental disorder on social media: a ChatGPT-augmented  explainable approach</h3>
<ul>
<li><strong>Authors: </strong>Loris Belcastro, Riccardo Cantini, Fabrizio Marozzo, Domenico Talia, Paolo Trunfio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17477">https://arxiv.org/abs/2401.17477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17477">https://arxiv.org/pdf/2401.17477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17477]] Detecting mental disorder on social media: a ChatGPT-augmented  explainable approach(https://arxiv.org/abs/2401.17477)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>In the digital era, the prevalence of depressive symptoms expressed on social media has raised serious concerns, necessitating advanced methodologies for timely detection. This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT. In our methodology, explanations are achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a novel self-explanatory model, namely BERT-XDD, capable of providing both classification and explanations via masked attention. The interpretability is further enhanced using ChatGPT to transform technical explanations into human-readable commentaries. By introducing an effective and modular approach for interpretable depression detection, our methodology can contribute to the development of socially responsible digital platforms, fostering early intervention and support for mental health challenges under the guidance of qualified healthcare professionals.</li>
<li><strong>摘要：</strong>在数字时代，社交媒体上表达的抑郁症状的普遍存在引起了严重关注，需要先进的方法来及时发现。本文通过提出一种新颖的方法来解决可解释的抑郁症检测的挑战，该方法有效地将大型语言模型 (LLM) 与可解释人工智能 (XAI) 和 ChatGPT 等会话代理结合起来。在我们的方法中，解释是通过将 BERTweet（BERT 的 Twitter 特定变体）集成到一种新颖的自解释模型（即 BERT-XDD）中来实现的，该模型能够通过屏蔽注意力提供分类和解释。使用 ChatGPT 将技术解释转化为人类可读的评论进一步增强了可解释性。通过引入有效的模块化方法来检测可解释的抑郁症，我们的方法可以有助于开发对社会负责的数字平台，在合格的医疗保健专业人员的指导下促进早期干预和支持心理健康挑战。</li>
</ul>

<h3>Title: Arrows of Time for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Vassilis Papadopoulos, Jérémie Wenger, Clément Hongler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17505">https://arxiv.org/abs/2401.17505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17505">https://arxiv.org/pdf/2401.17505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17505]] Arrows of Time for Large Language Models(https://arxiv.org/abs/2401.17505)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.</li>
<li><strong>摘要：</strong>我们从时间方向性的角度研究自回归大型语言模型执行的概率建模。我们凭经验发现此类模型在模拟自然语言的能力方面表现出时间不对称性：尝试预测下一个标记与尝试预测前一个标记时的平均对数困惑度存在差异。这种差异同时是微妙的，并且在各种模式（语言、模型大小、训练时间……）中非常一致。从理论上讲，这是令人惊讶的：从信息论的角度来看，不应该存在这样的差异。我们提供了一个理论框架来解释这种不对称性是如何从稀疏性和计算复杂性的考虑中出现的，并概述了我们的结果所带来的一些观点。</li>
</ul>

<h3>Title: FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Rheeya Uppaal, Yixuan Li, Junjie Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17514">https://arxiv.org/abs/2401.17514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17514">https://arxiv.org/pdf/2401.17514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17514]] FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation(https://arxiv.org/abs/2401.17514)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, rag</a></li>
<li><strong>Abstract: </strong>A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation. However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training. The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string. To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks. Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for classification. We conduct extensive experiments on 24 real-world domain pairs to show the effectiveness of our method over strong domain-invariant learning methods. Our analysis sheds light on why masked language modeling improves target-domain classification performance in prompt-based UDA. We discover that MLM helps the model learn both semantic and background knowledge of a domain, which are both beneficial for downstream classification.</li>
<li><strong>摘要：</strong>无监督域适应（UDA）方法的主线使用来自源域和目标域的未标记数据来学习域不变表示以进行适应。然而，这些方法存在一定的局限性，鼓励通过持续的预训练来使用自我监督学习。在基于提示的分类框架中，持续预训练或学习域不变表示的必要性仍不清楚，其中输入示例由模板修改，然后输入语言模型（LM）以生成标签字符串。为了在基于提示的设置中检查这种新的 UDA 范式，我们提出了一种极其简单的 UDA 方法 (FEUDA)，该方法使用两种不同的指令调整任务在未标记和标记的示例上训练自回归 LM。具体来说，第一个任务通过掩码语言模型 (MLM) 在来自两个领域的未标记文本上训练 LM，另一个任务对源标记数据使用监督指令调整来进行分类。我们对 24 个真实世界的域对进行了广泛的实验，以证明我们的方法相对于强大的域不变学习方法的有效性。我们的分析揭示了为什么掩码语言建模可以提高基于提示的 UDA 中的目标域分类性能。我们发现 MLM 帮助模型学习某个领域的语义和背景知识，这都有利于下游分类。</li>
</ul>

<h3>Title: Game-Theoretic Unlearnable Example Generator</h3>
<ul>
<li><strong>Authors: </strong>Shuang Liu, Yihan Wang, Xiao-Shan Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17523">https://arxiv.org/abs/2401.17523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17523">https://arxiv.org/pdf/2401.17523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17523]] Game-Theoretic Unlearnable Example Generator(https://arxiv.org/abs/2401.17523)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Unlearnable example attacks are data poisoning attacks aiming to degrade the clean test accuracy of deep learning by adding imperceptible perturbations to the training samples, which can be formulated as a bi-level optimization problem. However, directly solving this optimization problem is intractable for deep neural networks. In this paper, we investigate unlearnable example attacks from a game-theoretic perspective, by formulating the attack as a nonzero sum Stackelberg game. First, the existence of game equilibria is proved under the normal setting and the adversarial training setting. It is shown that the game equilibrium gives the most powerful poison attack in that the victim has the lowest test accuracy among all networks within the same hypothesis space, when certain loss functions are used. Second, we propose a novel attack method, called the Game Unlearnable Example (GUE), which has three main gradients. (1) The poisons are obtained by directly solving the equilibrium of the Stackelberg game with a first-order algorithm. (2) We employ an autoencoder-like generative network model as the poison attacker. (3) A novel payoff function is introduced to evaluate the performance of the poison. Comprehensive experiments demonstrate that GUE can effectively poison the model in various scenarios. Furthermore, the GUE still works by using a relatively small percentage of the training data to train the generator, and the poison generator can generalize to unseen data well. Our implementation code can be found at https://github.com/hong-xian/gue.</li>
<li><strong>摘要：</strong>不可学习的示例攻击是数据中毒攻击，旨在通过向训练样本添加难以察觉的扰动来降低深度学习的干净测试准确性，这可以表述为双层优化问题。然而，对于深度神经网络来说，直接解决这个优化问题是很困难的。在本文中，我们通过将攻击表述为非零和 Stackelberg 博弈，从博弈论的角度研究了不可学习的示例攻击。首先，证明了正常设置和对抗训练设置下博弈均衡的存在。结果表明，当使用某些损失函数时，博弈平衡给出了最强大的毒害攻击，因为受害者在同一假设空间内的所有网络中具有最低的测试精度。其次，我们提出了一种新颖的攻击方法，称为游戏不可学习示例（GUE），它具有三个主要梯度。 (1) 毒物是通过用一阶算法直接求解Stackelberg博弈的均衡而得到的。 (2) 我们采用类似自动编码器的生成网络模型作为毒攻击者。 (3)引入了一种新颖的支付函数来评估毒药的性能。综合实验表明，GUE可以在各种场景下有效地毒害模型。此外，GUE 仍然使用相对较小比例的训练数据来训练生成器，并且毒物生成器可以很好地泛化到未见过的数据。我们的实现代码可以在https://github.com/hong-xian/gue找到。</li>
</ul>

<h3>Title: Learning to Stop Cut Generation for Efficient Mixed-Integer Linear  Programming</h3>
<ul>
<li><strong>Authors: </strong>Haotian Ling, Zhihai Wang, Jie Wang</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17527">https://arxiv.org/abs/2401.17527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17527">https://arxiv.org/pdf/2401.17527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17527]] Learning to Stop Cut Generation for Efficient Mixed-Integer Linear  Programming(https://arxiv.org/abs/2401.17527)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Cutting planes (cuts) play an important role in solving mixed-integer linear programs (MILPs), as they significantly tighten the dual bounds and improve the solving performance. A key problem for cuts is when to stop cuts generation, which is important for the efficiency of solving MILPs. However, many modern MILP solvers employ hard-coded heuristics to tackle this problem, which tends to neglect underlying patterns among MILPs from certain applications. To address this challenge, we formulate the cuts generation stopping problem as a reinforcement learning problem and propose a novel hybrid graph representation model (HYGRO) to learn effective stopping strategies. An appealing feature of HYGRO is that it can effectively capture both the dynamic and static features of MILPs, enabling dynamic decision-making for the stopping strategies. To the best of our knowledge, HYGRO is the first data-driven method to tackle the cuts generation stopping problem. By integrating our approach with modern solvers, experiments demonstrate that HYGRO significantly improves the efficiency of solving MILPs compared to competitive baselines, achieving up to 31% improvement.</li>
<li><strong>摘要：</strong>割平面（割）在求解混合整数线性规划 (MILP) 中发挥着重要作用，因为它们显着收紧了对偶界限并提高了求解性能。割断的一个关键问题是何时停止割断生成，这对于求解 MILP 的效率非常重要。然而，许多现代 MILP 求解器采用硬编码启发式方法来解决这个问题，这往往会忽略某些应用程序中 MILP 之间的潜在模式。为了应对这一挑战，我们将削减生成停止问题表述为强化学习问题，并提出了一种新颖的混合图表示模型（HYGRO）来学习有效的停止策略。 HYGRO 的一个吸引人的特点是它可以有效地捕获 MILP 的动态和静态特征，从而实现停止策略的动态决策。据我们所知，HYGRO 是第一个解决削减发电停止问题的数据驱动方法。通过将我们的方法与现代求解器相结合，实验表明，与竞争基准相比，HYGRO 显着提高了求解 MILP 的效率，提高了高达 31%。</li>
</ul>

<h3>Title: Enhancing Score-Based Sampling Methods with Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Tobias Bischoff, Bryan Riel</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17539">https://arxiv.org/abs/2401.17539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17539">https://arxiv.org/pdf/2401.17539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17539]] Enhancing Score-Based Sampling Methods with Ensembles(https://arxiv.org/abs/2401.17539)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, rag</a></li>
<li><strong>Abstract: </strong>We introduce ensembles within score-based sampling methods to develop gradient-free approximate sampling techniques that leverage the collective dynamics of particle ensembles to compute approximate reverse diffusion drifts. We introduce the underlying methodology, emphasizing its relationship with generative diffusion models and the previously introduced F\"ollmer sampler. We demonstrate the efficacy of ensemble strategies through various examples, ranging from low- to medium-dimensionality sampling problems, including multi-modal and highly non-Gaussian probability distributions, and provide comparisons to traditional methods like NUTS. Our findings highlight the potential of ensemble strategies for modeling complex probability distributions in situations where gradients are unavailable. Finally, we showcase its application in the context of Bayesian inversion problems within the geophysical sciences.</li>
<li><strong>摘要：</strong>我们在基于分数的采样方法中引入了集成，以开发无梯度近似采样技术，该技术利用粒子集成的集体动力学来计算近似反向扩散漂移。我们介绍了基本方法，强调了它与生成扩散模型和之前介绍的 F\"ollmer 采样器的关系。我们通过各种示例证明了集成策略的有效性，范围从低维到中维采样问题，包括多模态和高度非高斯概率分布，并与 NUTS 等传统方法进行比较。我们的研究结果强调了集成策略在梯度不可用的情况下对复杂概率分布进行建模的潜力。最后，我们展示了其在贝叶斯反演问题中的应用地球物理科学。</li>
</ul>

<h3>Title: Data-Effective Learning: A Comprehensive Medical Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Yang, Weimin Tan, Yuqi Sun, Bo Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17542">https://arxiv.org/abs/2401.17542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17542">https://arxiv.org/pdf/2401.17542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17542]] Data-Effective Learning: A Comprehensive Medical Benchmark(https://arxiv.org/abs/2401.17542)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive experimental results show the baseline MedDEL can achieve performance comparable to the original large dataset with only 5% of the data. Establishing such an open data-effective learning benchmark is crucial for the medical AI research community because it facilitates efficient data use, promotes collaborative breakthroughs, and fosters the development of cost-effective, scalable, and impactful healthcare solutions. The project can be accessed at https://github.com/shadow2469/Data-Effective-Learning-A-Comprehensive-Medical-Benchmark.git.</li>
<li><strong>摘要：</strong>数据有效学习旨在以最有影响力的方式使用数据来训练人工智能模型，其中涉及注重数据质量而不是数量的策略，确保用于训练的数据具有较高的信息价值。数据有效学习在加速人工智能训练、降低计算成本和节省数据存储方面发挥着深远的作用，这在近年来医疗数据量增长超出许多人的预期的情况下非常重要。然而，由于缺乏标准和综合基准，医学数据有效学习的研究还很少。为了解决这一差距，我们的论文引入了一个专门用于评估医学领域数据有效学习的综合基准。该基准包括来自 31 个医疗中心的数百万数据样本的数据集 (DataDEL)、比较基线方法 (MedDEL) 和新的评估指标 (NormDEL)，以客观地衡量数据有效的学习绩效。我们广泛的实验结果表明，基线 MedDEL 仅需 5% 的数据即可实现与原始大型数据集相当的性能。建立这样一个开放的数据有效的学习基准对于医学人工智能研究社区至关重要，因为它有助于有效的数据使用，促进协作突破，并促进开发具有成本效益、可扩展和有影响力的医疗保健解决方案。该项目可以通过 https://github.com/shadow2469/Data-Effective-Learning-A-Compressive-Medical-Benchmark.git 访问。</li>
</ul>

<h3>Title: Rethinking Channel Dependence for Multivariate Time Series Forecasting:  Learning from Leading Indicators</h3>
<ul>
<li><strong>Authors: </strong>Lifan Zhao, Yanyan Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17548">https://arxiv.org/abs/2401.17548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17548">https://arxiv.org/pdf/2401.17548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17548]] Rethinking Channel Dependence for Multivariate Time Series Forecasting:  Learning from Leading Indicators(https://arxiv.org/abs/2401.17548)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments on six real-world datasets demonstrate that LIFT improves the state-of-the-art methods by 5.5% in average forecasting performance.</li>
<li><strong>摘要：</strong>最近，与通道无关的方法在多元时间序列（MTS）预测中取得了最先进的性能。尽管降低了过度拟合的风险，但这些方法错过了利用通道依赖性进行准确预测的潜在机会。我们认为变量之间存在局部固定的超前滞后关系，即一些滞后变量可能在短时间内跟随领先指标。利用这种渠道依赖性是有益的，因为领先指标提供了可用于降低滞后变量的预测难度的预先信息。在本文中，我们提出了一种名为 LIFT 的新方法，该方法首先有效地估计领先指标及其在每个时间步的领先步骤，然后明智地允许滞后变量利用领先指标的先进信息。 LIFT 作为一个插件，可以与任意时间序列预测方法无缝协作。对六个真实世界数据集的大量实验表明，LIFT 将最先进方法的平均预测性能提高了 5.5%。</li>
</ul>

<h3>Title: Scavenging Hyena: Distilling Transformers into Long Convolution Models</h3>
<ul>
<li><strong>Authors: </strong>Tokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad Sami Nur Islam, Wassim Jabbour, Laurence Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17574">https://arxiv.org/abs/2401.17574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17574">https://arxiv.org/pdf/2401.17574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17574]] Scavenging Hyena: Distilling Transformers into Long Convolution Models(https://arxiv.org/abs/2401.17574)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, long context, rag</a></li>
<li><strong>Abstract: </strong>The rapid evolution of Large Language Models (LLMs), epitomized by architectures like GPT-4, has reshaped the landscape of natural language processing. This paper introduces a pioneering approach to address the efficiency concerns associated with LLM pre-training, proposing the use of knowledge distillation for cross-architecture transfer. Leveraging insights from the efficient Hyena mechanism, our method replaces attention heads in transformer models by Hyena, offering a cost-effective alternative to traditional pre-training while confronting the challenge of processing long contextual information, inherent in quadratic attention mechanisms. Unlike conventional compression-focused methods, our technique not only enhances inference speed but also surpasses pre-training in terms of both accuracy and efficiency. In the era of evolving LLMs, our work contributes to the pursuit of sustainable AI solutions, striking a balance between computational power and environmental impact.</li>
<li><strong>摘要：</strong>以 GPT-4 等架构为代表的大型语言模型 (LLM) 的快速发展重塑了自然语言处理的格局。本文介绍了一种解决与 LLM 预训练相关的效率问题的开创性方法，提出使用知识蒸馏进行跨架构迁移。利用高效鬣狗机制的见解，我们的方法取代了鬣狗变压器模型中的注意力头，为传统预训练提供了一种经济高效的替代方案，同时应对二次注意力机制固有的处理长上下文信息的挑战。与传统的以压缩为重点的方法不同，我们的技术不仅提高了推理速度，而且在准确性和效率方面也超越了预训练。在法学硕士不断发展的时代，我们的工作有助于追求可持续的人工智能解决方案，在计算能力和环境影响之间取得平衡。</li>
</ul>

<h3>Title: Graph Contrastive Learning with Cohesive Subgraph Awareness</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Wu, Leye Wang, Xiao Han, Han-Jia Ye</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17580">https://arxiv.org/abs/2401.17580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17580">https://arxiv.org/pdf/2401.17580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17580]] Graph Contrastive Learning with Cohesive Subgraph Awareness(https://arxiv.org/abs/2401.17580)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy for learning representations of diverse graphs including social and biomedical networks. GCL widely uses stochastic graph topology augmentation, such as uniform node dropping, to generate augmented graphs. However, such stochastic augmentations may severely damage the intrinsic properties of a graph and deteriorate the following representation learning process. We argue that incorporating an awareness of cohesive subgraphs during the graph augmentation and learning processes has the potential to enhance GCL performance. To this end, we propose a novel unified framework called CTAug, to seamlessly integrate cohesion awareness into various existing GCL mechanisms. In particular, CTAug comprises two specialized modules: topology augmentation enhancement and graph learning enhancement. The former module generates augmented graphs that carefully preserve cohesion properties, while the latter module bolsters the graph encoder's ability to discern subgraph patterns. Theoretical analysis shows that CTAug can strictly improve existing GCL mechanisms. Empirical experiments verify that CTAug can achieve state-of-the-art performance for graph representation learning, especially for graphs with high degrees. The code is available at https://doi.org/10.5281/zenodo.10594093, or https://github.com/wuyucheng2002/CTAug.</li>
<li><strong>摘要：</strong>图对比学习（GCL）已成为学习各种图（包括社交和生物医学网络）表示的最先进策略。 GCL广泛使用随机图拓扑增强（例如统一节点丢弃）来生成增强图。然而，这种随机增强可能会严重损害图的内在属性，并恶化后续的表示学习过程。我们认为，在图增强和学习过程中结合对内聚子图的认识有可能提高 GCL 的性能。为此，我们提出了一个名为 CTAug 的新型统一框架，将内聚意识无缝集成到各种现有的 GCL 机制中。具体来说，CTAug 包括两个专门的模块：拓扑增强增强和图学习增强。前一个模块生成仔细保留内聚属性的增强图，而后一个模块增强了图编码器辨别子图模式的能力。理论分析表明CTAug可以严格改进现有的GCL机制。实证实验验证了 CTAug 可以在图表示学习方面实现最先进的性能，尤其是对于高阶图。该代码可在 https://doi.org/10.5281/zenodo.10594093 或 https://github.com/wuyu Cheng2002/CTAug 获取。</li>
</ul>

<h3>Title: Propagation and Pitfalls: Reasoning-based Assessment of Knowledge  Editing through Counterfactual Tasks</h3>
<ul>
<li><strong>Authors: </strong>Wenyue Hua, Jiang Guo, Mingwen Dong, Henghui Zhu, Patrick Ng, Zhiguo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17585">https://arxiv.org/abs/2401.17585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17585">https://arxiv.org/pdf/2401.17585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17585]] Propagation and Pitfalls: Reasoning-based Assessment of Knowledge  Editing through Counterfactual Tasks(https://arxiv.org/abs/2401.17585)</code><input type="text"></li>
<li><strong>Keywords: </strong>chain-of-thought</a></li>
<li><strong>Abstract: </strong>Current approaches of knowledge editing struggle to effectively propagate updates to interconnected facts. In this work, we delve into the barriers that hinder the appropriate propagation of updated knowledge within these models for accurate reasoning. To support our analysis, we introduce a novel reasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing dataset) -- which covers six common reasoning schemes in real world. We conduct a thorough analysis of existing knowledge editing techniques, including input augmentation, finetuning, and locate-and-edit. We found that all model editing methods show notably low performance on this dataset, especially in certain reasoning schemes. Our analysis over the chain-of-thought generation of edited models further uncover key reasons behind the inadequacy of existing knowledge editing methods from a reasoning standpoint, involving aspects on fact-wise editing, fact recall ability, and coherence in generation. We will make our benchmark publicly available.</li>
<li><strong>摘要：</strong>当前的知识编辑方法很难有效地将更新传播到相互关联的事实。在这项工作中，我们深入研究了阻碍这些模型中更新的知识适当传播的障碍，以进行准确的推理。为了支持我们的分析，我们引入了一种新颖的基于推理的基准——ReCoE（基于推理的反事实编辑数据集）——它涵盖了现实世界中的六种常见推理方案。我们对现有的知识编辑技术进行了彻底的分析，包括输入增强、微调以及定位和编辑。我们发现所有模型编辑方法在此数据集上的性能都明显较低，尤其是在某些推理方案中。我们对编辑模型的思想链生成的分析进一步从推理的角度揭示了现有知识编辑方法不足的关键原因，涉及事实编辑、事实回忆能力和生成连贯性等方面。我们将公开我们的基准。</li>
</ul>

<h3>Title: Local and Global Contexts for Conversation</h3>
<ul>
<li><strong>Authors: </strong>Zuoquan Lin, Xinyi Shen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17588">https://arxiv.org/abs/2401.17588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17588">https://arxiv.org/pdf/2401.17588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17588]] Local and Global Contexts for Conversation(https://arxiv.org/abs/2401.17588)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>The context in conversation is the dialog history crucial for multi-turn dialogue. Learning from the relevant contexts in dialog history for grounded conversation is a challenging problem. Local context is the most neighbor and more sensitive to the subsequent response, and global context is relevant to a whole conversation far beyond neighboring utterances. Currently, pretrained transformer models for conversation challenge capturing the correlation and connection between local and global contexts. We introduce a local and global conversation model (LGCM) for general-purpose conversation in open domain. It is a local-global hierarchical transformer model that excels at accurately discerning and assimilating the relevant contexts necessary for generating responses. It employs a local encoder to grasp the local context at the level of individual utterances and a global encoder to understand the broader context at the dialogue level. The seamless fusion of these locally and globally contextualized encodings ensures a comprehensive comprehension of the conversation. Experiments on popular datasets show that LGCM outperforms the existing conversation models on the performance of automatic metrics with significant margins.</li>
<li><strong>摘要：</strong>对话中的上下文是对多轮对话至关重要的对话历史。从对话历史中的相关上下文中学习基础对话是一个具有挑战性的问题。局部上下文是最邻近的，对后续响应更敏感，而全局上下文与整个对话的相关性远远超出邻近的话语。目前，用于对话的预训练 Transformer 模型面临捕获本地和全局上下文之间的相关性和连接的挑战。我们引入了一种本地和全局对话模型（LGCM），用于开放域中的通用对话。它是一种局部-全局分层变压器模型，擅长准确识别和同化生成响应所需的相关上下文。它使用本地编码器来掌握个体话语级别的本地上下文，并使用全局编码器来理解对话级别的更广泛的上下文。这些本地和全局上下文编码的无缝融合确保了对对话的全面理解。对流行数据集的实验表明，LGCM 在自动指标的性能上明显优于现有的对话模型。</li>
</ul>

<h3>Title: SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization</h3>
<ul>
<li><strong>Authors: </strong>Sangwoo Cho, Kaiqiang Song, Chao Zhao, Xiaoyang Wang, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17597">https://arxiv.org/abs/2401.17597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17597">https://arxiv.org/pdf/2401.17597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17597]] SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization(https://arxiv.org/abs/2401.17597)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context, rag</a></li>
<li><strong>Abstract: </strong>Multi-turn dialogues are characterized by their extended length and the presence of turn-taking conversations. Traditional language models often overlook the distinct features of these dialogues by treating them as regular text. In this paper, we propose a speaker-enhanced pre-training method for long dialogue summarization, which leverages the inherent structure of multiple-turn dialogues. To support our study, we curate a diverse dataset that includes transcripts from real-world scenarios, movie or TV show transcripts, and dialogues generated by a Large Language Model. We then perform a pre-training, which encompasses the detection of speaker changes, and masked utterance generation. Experimental results of fine-tuned models demonstrate that our model achieves state-of-the-art performance on downstream benchmarks with long context, surpassing baseline models and highlighting the effectiveness of our approach. Our findings highlight the importance of curating pre-training datasets that exhibit diversity and variations in length distribution to ensure effective alignment with downstream datasets.</li>
<li><strong>摘要：</strong>多轮对话的特点是长度较长且存在轮流对话。传统的语言模型经常将这些对话视为常规文本，从而忽略了它们的独特特征。在本文中，我们提出了一种用于长对话摘要的说话人增强预训练方法，该方法利用了多轮对话的固有结构。为了支持我们的研究，我们整理了一个多样化的数据集，其中包括现实世界场景的转录本、电影或电视节目的转录本以及大型语言模型生成的对话。然后，我们进行预训练，其中包括检测说话者变化和屏蔽话语生成。微调模型的实验结果表明，我们的模型在具有长上下文的下游基准上实现了最先进的性能，超越了基线模型并突出了我们方法的有效性。我们的研究结果强调了整理预训练数据集的重要性，这些数据集表现出长度分布的多样性和变化，以确保与下游数据集的有效对齐。</li>
</ul>

<h3>Title: Good at captioning, bad at counting: Benchmarking GPT-4V on Earth  observation data</h3>
<ul>
<li><strong>Authors: </strong>Chenhui Zhang, Sherrie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17600">https://arxiv.org/abs/2401.17600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17600">https://arxiv.org/pdf/2401.17600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17600]] Good at captioning, bad at counting: Benchmarking GPT-4V on Earth  observation data(https://arxiv.org/abs/2401.17600)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting tasks. Our benchmark will be made publicly available at https://vleo.danielz.ch/ and on Hugging Face at https://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70 for easy model evaluation.</li>
<li><strong>摘要：</strong>大型视觉语言模型 (VLM) 在涉及自然语言指令视觉输入的复杂任务中表现出了令人印象深刻的性能。然而，目前尚不清楚自然图像在多大程度上能够转移到地球观测（EO）数据，这些数据主要是卫星和航空图像，在 VLM 训练数据中不太常见。在这项工作中，我们提出了一个全面的基准，通过评估 VLM 在场景理解、定位和计数以及变化检测任务方面的能力，来衡量 VLM 成为 EO 数据有用工具的进展。在现实世界应用的推动下，我们的基准包括城市监控、救灾、土地利用和保护等场景。我们发现，尽管像 GPT-4V 这样最先进的 VLM 拥有广泛的世界知识，可以在位置理解和图像字幕等开放式任务上表现出色，但它们糟糕的空间推理能力限制了对象定位和计数任务的实用性。我们的基准测试将在 https://vleo.danielz.ch/ 和 Hugging Face 上公开发布：https://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70，以便于模型评估。</li>
</ul>

<h3>Title: Assertion Detection Large Language Model In-context Learning LoRA  Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yuelyu Ji, Zeshui Yu, Yanshan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17602">https://arxiv.org/abs/2401.17602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17602">https://arxiv.org/pdf/2401.17602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17602]] Assertion Detection Large Language Model In-context Learning LoRA  Fine-tuning(https://arxiv.org/abs/2401.17602)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, rag</a></li>
<li><strong>Abstract: </strong>In this study, we aim to address the task of assertion detection when extracting medical concepts from clinical notes, a key process in clinical natural language processing (NLP). Assertion detection in clinical NLP usually involves identifying assertion types for medical concepts in the clinical text, namely certainty (whether the medical concept is positive, negated, possible, or hypothetical), temporality (whether the medical concept is for present or the past history), and experiencer (whether the medical concept is described for the patient or a family member). These assertion types are essential for healthcare professionals to quickly and clearly understand the context of medical conditions from unstructured clinical texts, directly influencing the quality and outcomes of patient care. Although widely used, traditional methods, particularly rule-based NLP systems and machine learning or deep learning models, demand intensive manual efforts to create patterns and tend to overlook less common assertion types, leading to an incomplete understanding of the context. To address this challenge, our research introduces a novel methodology that utilizes Large Language Models (LLMs) pre-trained on a vast array of medical data for assertion detection. We enhanced the current method with advanced reasoning techniques, including Tree of Thought (ToT), Chain of Thought (CoT), and Self-Consistency (SC), and refine it further with Low-Rank Adaptation (LoRA) fine-tuning. We first evaluated the model on the i2b2 2010 assertion dataset. Our method achieved a micro-averaged F-1 of 0.89, with 0.11 improvements over the previous works. To further assess the generalizability of our approach, we extended our evaluation to a local dataset that focused on sleep concept extraction. Our approach achieved an F-1 of 0.74, which is 0.31 higher than the previous method.</li>
<li><strong>摘要：</strong>在本研究中，我们的目标是解决从临床笔记中提取医学概念时的断言检测任务，这是临床自然语言处理（NLP）的关键过程。临床NLP中的断言检测通常涉及识别临床文本中医学概念的断言类型，即确定性（医学概念是肯定的、否定的、可能的还是假设的）、时间性（医学概念是针对现在的还是过去的历史）和体验者（无论是为患者还是家庭成员描述医学概念）。这些断言类型对于医疗保健专业人员从非结构化临床文本中快速、清晰地了解医疗状况的背景至关重要，直接影响患者护理的质量和结果。尽管广泛使用，但传统方法，特别是基于规则的 NLP 系统和机器学习或深度学习模型，需要大量的手动工作来创建模式，并且往往会忽略不太常见的断言类型，从而导致对上下文的不完整理解。为了应对这一挑战，我们的研究引入了一种新颖的方法，该方法利用在大量医学数据上预先训练的大型语言模型 (LLM) 进行断言检测。我们使用先进的推理技术增强了当前的方法，包括思想树（ToT）、思想链（CoT）和自洽（SC），并通过低阶适应（LoRA）微调进一步完善。我们首先在 i2b2 2010 断言数据集上评估了模型。我们的方法实现了 0.89 的微平均 F-1，比之前的方法提高了 0.11。为了进一步评估我们方法的普遍性，我们将评估扩展到专注于睡眠概念提取的本地数据集。我们的方法实现了 0.74 的 F-1，比之前的方法高了 0.31。</li>
</ul>

<h3>Title: Graph Multi-Similarity Learning for Molecular Property Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hao Xu, Zhengyang Zhou, Pengyu Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17615">https://arxiv.org/abs/2401.17615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17615">https://arxiv.org/pdf/2401.17615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17615]] Graph Multi-Similarity Learning for Molecular Property Prediction(https://arxiv.org/abs/2401.17615)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Effective molecular representation learning is essential for molecular property prediction. Contrastive learning, a prominent self-supervised approach for molecular representation learning, relies on establishing positive and negative pairs. However, this binary similarity categorization oversimplifies the nature of complex molecular relationships and overlooks the degree of relative similarities among molecules, posing challenges to the effectiveness and generality of representation learning. In response to this challenge, we propose the Graph Multi-Similarity Learning for Molecular Property Prediction (GraphMSL) framework. GraphMSL incorporates a generalized multi-similarity metric in a continuous scale, capturing self-similarity and relative similarities. The unimodal multi-similarity metrics are derived from various chemical modalities, and the fusion of these metrics into a multimodal form significantly enhances the effectiveness of GraphMSL. In addition, the flexibility of fusion function can reshape the focus of the model to convey different chemical semantics. GraphMSL proves effective in drug discovery evaluations through various downstream tasks and post-hoc analysis of learnt representations. Its notable performance suggests significant potential for the exploration of new drug candidates.</li>
<li><strong>摘要：</strong>有效的分子表示学习对于分子特性预测至关重要。对比学习是一种重要的分子表征学习自我监督方法，依赖于建立正负对。然而，这种二元相似性分类过于简单化了复杂分子关系的本质，忽视了分子之间的相对相似程度，对表征学习的有效性和通用性提出了挑战。为了应对这一挑战，我们提出了分子特性预测的图多重相似性学习（GraphMSL）框架。 GraphMSL 在连续尺度上结合了广义的多重相似性度量，捕获自相似性和相对相似性。单峰多相似性度量源自各种化学模态，将这些度量融合为多峰形式显着增强了 GraphMSL 的有效性。此外，融合函数的灵活性可以重塑模型的焦点以传达不同的化学语义。通过各种下游任务和学习表征的事后分析，GraphMSL 被证明在药物发现评估中是有效的。其显着的表现表明探索新候选药物的巨大潜力。</li>
</ul>

<h3>Title: Neighboring Perturbations of Knowledge Editing on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jun-Yu Ma, Jia-Chen Gu, Ningyu Zhang, Zhen-Hua Ling</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17623">https://arxiv.org/abs/2401.17623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17623">https://arxiv.org/pdf/2401.17623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17623]] Neighboring Perturbations of Knowledge Editing on Large Language Models(https://arxiv.org/abs/2401.17623)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play framework termed Appending via Preservation and Prevention (APP) is proposed to mitigate the neighboring perturbation by maintaining the integrity of the answer list. Experiments demonstrate the effectiveness of APP coupling with four editing methods on three LLMs.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 具有卓越的功能，但由于错误或过时的知识，很容易生成意外的文本。鉴于法学硕士再培训的资源密集型性质，知识编辑的发展显着增加。然而，当前的方法和评估很少探索编辑邻近知识的扰动。本文研究了法学硕士更新新知识是否会扰乱其中封装的邻近知识。具体来说，我们试图弄清楚将新答案添加到事实问题的答案列表中是否会导致灾难性地忘记该列表中的原始正确答案，以及无意中包含错误答案。引入了可加性度量，并构建了称为附加知识扰动评估（PEAK）的基准来评估附加新知识时对邻近知识的扰动程度。此外，提出了一种称为通过保存和预防追加（APP）的即插即用框架，通过维护答案列表的完整性来减轻邻近扰动。实验证明了 APP 与四种编辑方法耦合在三个法学硕士上的有效性。</li>
</ul>

<h3>Title: What Do Self-Supervised Speech and Speaker Models Learn? New Findings  From a Cross Model Layer-Wise Analysis</h3>
<ul>
<li><strong>Authors: </strong>Takanori Ashihara, Marc Delcroix, Takafumi Moriya, Kohei Matsuura, Taichi Asami, Yusuke Ijima</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17632">https://arxiv.org/abs/2401.17632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17632">https://arxiv.org/pdf/2401.17632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17632]] What Do Self-Supervised Speech and Speaker Models Learn? New Findings  From a Cross Model Layer-Wise Analysis(https://arxiv.org/abs/2401.17632)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has attracted increased attention for learning meaningful speech representations. Speech SSL models, such as WavLM, employ masked prediction training to encode general-purpose representations. In contrast, speaker SSL models, exemplified by DINO-based models, adopt utterance-level training objectives primarily for speaker representation. Understanding how these models represent information is essential for refining model efficiency and effectiveness. Unlike the various analyses of speech SSL, there has been limited investigation into what information speaker SSL captures and how its representation differs from speech SSL or other fully-supervised speaker models. This paper addresses these fundamental questions. We explore the capacity to capture various speech properties by applying SUPERB evaluation probing tasks to speech and speaker SSL models. We also examine which layers are predominantly utilized for each task to identify differences in how speech is represented. Furthermore, we conduct direct comparisons to measure the similarities between layers within and across models. Our analysis unveils that 1) the capacity to represent content information is somewhat unrelated to enhanced speaker representation, 2) specific layers of speech SSL models would be partly specialized in capturing linguistic information, and 3) speaker SSL models tend to disregard linguistic information but exhibit more sophisticated speaker representation.</li>
<li><strong>摘要：</strong>自监督学习（SSL）对于学习有意义的语音表示越来越受到关注。语音 SSL 模型（例如 WavLM）采用屏蔽预测训练来编码通用表示。相比之下，说话人 SSL 模型（以基于 DINO 的模型为例）采用主要针对说话人表示的话语级训练目标。了解这些模型如何表示信息对于提高模型效率和有效性至关重要。与语音 SSL 的各种分析不同，对于说话者 SSL 捕获的信息以及其表示与语音 SSL 或其他完全监督的说话者模型有何不同的研究有限。本文解决了这些基本问题。我们通过将 SUPERB 评估探测任务应用于语音和说话人 SSL 模型来探索捕获各种语音属性的能力。我们还检查了每个任务主要使用哪些层，以识别语音表示方式的差异。此外，我们进行直接比较，以衡量模型内和模型间各层之间的相似性。我们的分析表明，1）表示内容信息的能力与增强的说话人表示有些无关，2）特定的语音层 SSL 模型将部分专门用于捕获语言信息，3）说话人 SSL 模型倾向于忽略语言信息，但表现出更复杂的说话者表示。</li>
</ul>

<h3>Title: Navigating the OverKill in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Shi, Xiao Wang, Qiming Ge, Songyang Gao, Xianjun Yang, Tao Gui, Qi Zhang, Xuanjing Huang, Xun Zhao, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17633">https://arxiv.org/abs/2401.17633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17633">https://arxiv.org/pdf/2401.17633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17633]] Navigating the OverKill in Large Language Models(https://arxiv.org/abs/2401.17633)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, rag</a></li>
<li><strong>Abstract: </strong>Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to an over-attention of harmful words like 'kill' and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such over-attention by amplifying the difference in the model's output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the over-attention from the model via contrastive decoding. Empirical results indicate that our method has achieved an average reduction of the refusal rate by 20\% while having almost no impact on safety.</li>
<li><strong>摘要：</strong>大型语言模型经过精心调整，既有帮助又无害。然而，最近的研究指出了潜在的杀伤力，这意味着模型可能会拒绝回答良性查询。在本文中，我们通过探索模型如何处理和确定查询的安全性来调查过度杀伤的因素。我们的研究结果揭示了模型中存在捷径，导致过度关注“杀戮”等有害词语，而强调安全性的提示会加剧过度杀伤。基于这些见解，我们引入了自对比解码（Self-CD），这是一种免训练且与模型无关的策略，以缓解这种现象。我们首先通过在响应包括或省略对安全性的强调的系统提示时放大模型输出分布的差异来消除这种过度关注。然后，我们通过对比解码淡化模型的过度关注来确定最终的下一个标记预测。实证结果表明，我们的方法实现了拒绝率平均降低 20%，同时对安全性几乎没有影响。</li>
</ul>

<h3>Title: Document Structure in Long Document Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jan Buchmann, Max Eichler, Jan-Micha Bodensohn, Ilia Kuznetsov, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17658">https://arxiv.org/abs/2401.17658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17658">https://arxiv.org/pdf/2401.17658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17658]] Document Structure in Long Document Transformers(https://arxiv.org/abs/2401.17658)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Long documents often exhibit structure with hierarchically organized elements of different functions, such as section headers and paragraphs. Despite the omnipresence of document structure, its role in natural language processing (NLP) remains opaque. Do long-document Transformer models acquire an internal representation of document structure during pre-training? How can structural information be communicated to a model after pre-training, and how does it influence downstream performance? To answer these questions, we develop a novel suite of probing tasks to assess structure-awareness of long-document Transformers, propose general-purpose structure infusion methods, and evaluate the effects of structure infusion on QASPER and Evidence Inference, two challenging long-document NLP tasks. Results on LED and LongT5 suggest that they acquire implicit understanding of document structure during pre-training, which can be further enhanced by structure infusion, leading to improved end-task performance. To foster research on the role of document structure in NLP modeling, we make our data and code publicly available.</li>
<li><strong>摘要：</strong>长文档通常表现出具有不同功能的分层组织元素的结构，例如节标题和段落。尽管文档结构无处不在，但它在自然语言处理 (NLP) 中的作用仍然不透明。长文档 Transformer 模型在预训练期间是否获取文档结构的内部表示？预训练后如何将结构信息传递给模型，以及它如何影响下游性能？为了回答这些问题，我们开发了一套新颖的探测任务来评估长文档 Transformer 的结构感知，提出通用结构注入方法，并评估结构注入对 QASPER 和证据推理（两个具有挑战性的长文档）的影响NLP 任务。 LED 和 LongT5 的结果表明，它们在预训练期间获得了对文档结构的隐式理解，这可以通过结构注入进一步增强，从而提高最终任务性能。为了促进对文档结构在 NLP 建模中的作用的研究，我们公开了我们的数据和代码。</li>
</ul>

<h3>Title: Contextual Feature Extraction Hierarchies Converge in Large Language  Models and the Brain</h3>
<ul>
<li><strong>Authors: </strong>Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta, Nima Mesgarani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17671">https://arxiv.org/abs/2401.17671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17671">https://arxiv.org/pdf/2401.17671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17671]] Contextual Feature Extraction Hierarchies Converge in Large Language  Models and the Brain(https://arxiv.org/abs/2401.17671)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in artificial intelligence have sparked interest in the parallels between large language models (LLMs) and human neural processing, particularly in language comprehension. While prior research has established similarities in the representation of LLMs and the brain, the underlying computational principles that cause this convergence, especially in the context of evolving LLMs, remain elusive. Here, we examined a diverse selection of high-performance LLMs with similar parameter sizes to investigate the factors contributing to their alignment with the brain's language processing mechanisms. We find that as LLMs achieve higher performance on benchmark tasks, they not only become more brain-like as measured by higher performance when predicting neural responses from LLM embeddings, but also their hierarchical feature extraction pathways map more closely onto the brain's while using fewer layers to do the same encoding. We also compare the feature extraction pathways of the LLMs to each other and identify new ways in which high-performing models have converged toward similar hierarchical processing mechanisms. Finally, we show the importance of contextual information in improving model performance and brain similarity. Our findings reveal the converging aspects of language processing in the brain and LLMs and offer new directions for developing models that align more closely with human cognitive processing.</li>
<li><strong>摘要：</strong>人工智能的最新进展引发了人们对大型语言模型（LLM）和人类神经处理之间相似之处的兴趣，特别是在语言理解方面。虽然先前的研究已经确定了法学硕士和大脑表示的相似性，但导致这种融合的基本计算原理，特别是在不断发展的法学硕士的背景下，仍然难以捉摸。在这里，我们检查了具有相似参数大小的多种高性能法学硕士，以研究有助于它们与大脑语言处理机制保持一致的因素。我们发现，随着 LLM 在基准任务上取得更高的性能，它们不仅变得更像大脑（通过预测 LLM 嵌入的神经响应时的更高性能来衡量），而且它们的分层特征提取路径在使用更少的层时更接近地映射到大脑的路径进行相同的编码。我们还对法学硕士的特征提取路径进行了相互比较，并确定了高性能模型向类似的分层处理机制收敛的新方法。最后，我们展示了上下文信息在提高模型性能和大脑相似性方面的重要性。我们的研究结果揭示了大脑和法学硕士语言处理的融合方面，并为开发与人类认知处理更紧密结合的模型提供了新方向。</li>
</ul>

<h3>Title: Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought  Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Tinghui Zhu, Kai Zhang, Jian Xie, Yu Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17686">https://arxiv.org/abs/2401.17686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17686">https://arxiv.org/pdf/2401.17686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17686]] Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought  Reasoning(https://arxiv.org/abs/2401.17686)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves DBS's capability of detecting diverse and subtle reasoning errors and robustness on different model scales.</li>
<li><strong>摘要：</strong>最近的进展通过各种方法，特别是思想链（CoT）推理，显着增强了大型语言模型（LLM）的推理能力。然而，以前的方法无法解决中间步骤中的推理错误，从而导致累积错误。在本文中，我们提出了演绎波束搜索（DBS），它将CoT和演绎推理与LLM的逐步波束搜索无缝集成。我们的方法部署了一个验证器，验证推理步骤及其前提的可推论性，从而减轻错误累积。此外，我们引入了一种可扩展且无需人工的数据构建方法，以增强我们模型的验证能力。大量实验表明，我们的方法显着增强了不同规模（7B、13B、70B 和 ChatGPT）的 LLM 在来自 3 个不同推理流派（包括算术、常识和符号）的 8 个推理数据集上的基本性能。此外，我们的分析证明了 DBS 具有检测各种细微推理错误的能力以及在不同模型规模上的鲁棒性。</li>
</ul>

<h3>Title: Mitigating the Problem of Strong Priors in LMs with Context  Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Raymond Douglas, Andis Draguns, Tomáš Gavenčiak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17692">https://arxiv.org/abs/2401.17692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17692">https://arxiv.org/pdf/2401.17692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17692]] Mitigating the Problem of Strong Priors in LMs with Context  Extrapolation(https://arxiv.org/abs/2401.17692)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Language models (LMs) have become important tools in a variety of applications, from data processing to the creation of instruction-following assistants. But despite their advantages, LMs have certain idiosyncratic limitations such as the problem of `strong priors', where a model learns to output typical continuations in response to certain, usually local, portions of the input regardless of any earlier instructions. For example, prompt injection attacks can induce models to ignore explicit directives. In some cases, larger models have been shown to be more susceptible to these problems than similar smaller models, an example of the phenomenon of `inverse scaling'. We develop a new technique for mitigating the problem of strong priors: we take the original set of instructions, produce a weakened version of the original prompt that is even more susceptible to the strong priors problem, and then extrapolate the continuation away from the weakened prompt. This lets us infer how the model would continue a hypothetical strengthened set of instructions. Our technique conceptualises LMs as mixture models which combine a family of data generation processes, reinforcing the desired elements of the mixture. Our approach works at inference time, removing any need for retraining. We apply it to eleven models including GPT-2, GPT-3, Llama 2, and Mistral on four tasks, and find improvements in 41/44. Across all 44 combinations the median increase in proportion of tasks completed is 40%.</li>
<li><strong>摘要：</strong>语言模型 (LM) 已成为各种应用中的重要工具，从数据处理到创建遵循指令的助手。但尽管有其优点，LM 仍具有某些特殊的局限性，例如“强先验”问题，即模型学习输出典型的延续来响应输入的某些（通常是本地的）部分，而不管任何早期指令如何。例如，提示注入攻击可能会导致模型忽略显式指令。在某些情况下，较大的模型比类似的较小模型更容易受到这些问题的影响，这是“逆缩放”现象的一个例子。我们开发了一种新技术来缓解强先验问题：我们采用原始指令集，生成原始提示的弱化版本，该版本更容易受到强先验问题的影响，然后从弱化提示中推断延续。这让我们可以推断模型将如何继续假设的强化指令集。我们的技术将 LM 概念化为混合模型，它结合了一系列数据生成过程，增强了混合物中所需的元素。我们的方法在推理时起作用，无需再训练。我们将其应用于包括 GPT-2、GPT-3、Llama 2 和 Mistral 在内的 11 个模型的 4 项任务，并在 41/44 中找到了改进。在所有 44 种组合中，已完成任务比例的中位数增长为 40%。</li>
</ul>

<h3>Title: Datacube segmentation via Deep Spectral Clustering</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Bombini, Fernando García-Avello Bofías, Caterina Bracci, Michele Ginolfi, Chiara Ruberto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, physics.app-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17695">https://arxiv.org/abs/2401.17695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17695">https://arxiv.org/pdf/2401.17695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17695]] Datacube segmentation via Deep Spectral Clustering(https://arxiv.org/abs/2401.17695)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Extended Vision techniques are ubiquitous in physics. However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube. Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering of data-cube's spectra, performed in a suitably defined low-dimensional embedding space. To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels. A statistical dimensional reduction is performed by an ad hoc trained (Variational) AutoEncoder, in charge of mapping spectra into lower dimensional metric spaces, while the clustering process is performed by a (learnable) iterative K-Means clustering algorithm. We apply this technique to two different use cases, of different physical origins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data on pictorial artworks, and a dataset of simulated astrophysical observations.</li>
<li><strong>摘要：</strong>扩展视觉技术在物理学中无处不在。然而，由于从构成数据立方体的光谱中辨别相关信息的内在困难，从这种分析中产生的数据立方体常常对它们的解释提出挑战。此外，数据立方体光谱的巨大维数在其统计解释中提出了复杂的任务；然而，这种复杂性包含大量的统计信息，可以以无监督的方式利用这些信息来概述当前案例研究的一些基本属性，例如，可以通过数据立方体的（深度）聚类来获得图像分割光谱，在适当定义的低维嵌入空间中执行。为了解决这个主题，我们探索在编码空间中应用无监督聚类方法的可能性，即对数据立方体像素的光谱属性执行深度聚类。统计降维由专门训练的（变分）自动编码器执行，负责将频谱映射到较低维的度量空间，而聚类过程由（可学习的）迭代 K 均值聚类算法执行。我们将该技术应用于具有不同物理起源的两个不同用例：一组关于绘画艺术品的宏观映射 X 射线荧光 (MA-XRF) 合成数据，以及一组模拟天体物理观测数据。</li>
</ul>

<h3>Title: WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Pardis Sadat Zahraei, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17703">https://arxiv.org/abs/2401.17703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17703">https://arxiv.org/pdf/2401.17703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17703]] WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts(https://arxiv.org/abs/2401.17703)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>The Winograd Schema Challenge (WSC) serves as a prominent benchmark for evaluating machine understanding. While Large Language Models (LLMs) excel at answering WSC questions, their ability to generate such questions remains less explored. In this work, we propose Tree-of-Experts (ToE), a novel prompting method which enhances the generation of WSC instances (50% valid cases vs. 10% in recent methods). Using this approach, we introduce WSC+, a novel dataset comprising 3,026 LLM-generated sentences. Notably, we extend the WSC framework by incorporating new 'ambiguous' and 'offensive' categories, providing a deeper insight into model overconfidence and bias. Our analysis reveals nuances in generation-evaluation consistency, suggesting that LLMs may not always outperform in evaluating their own generated questions when compared to those crafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves an accuracy of 68.7%, significantly below the human benchmark of 95.1%.</li>
<li><strong>摘要：</strong>Winograd Schema Challenge (WSC) 是评估机器理解的重要基准。虽然大型语言模型 (LLM) 擅长回答 WSC 问题，但它们生成此类问题的能力仍然很少被探索。在这项工作中，我们提出了专家树（ToE），这是一种新颖的提示方法，可以增强 WSC 实例的生成（有效案例为 50%，而最近的方法为 10%）。使用这种方法，我们引入了 WSC+，这是一个包含 3,026 个 LLM 生成的句子的新颖数据集。值得注意的是，我们通过纳入新的“模糊”和“攻击性”类别来扩展 WSC 框架，从而更深入地了解模型的过度自信和偏见。我们的分析揭示了生成评估一致性的细微差别，这表明与其他模型提出的问题相比，法学硕士在评估自己生成的问题方面可能并不总是表现出色。在 WSC+ 上，表现最好的法学硕士 GPT-4 的准确率达到 68.7%，明显低于人类基准的 95.1%。</li>
</ul>

<h3>Title: Aesthetic Preference Prediction in Interior Design: Fuzzy Approach</h3>
<ul>
<li><strong>Authors: </strong>Ayana Adilova, Pakizar Shamoi</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17710">https://arxiv.org/abs/2401.17710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17710">https://arxiv.org/pdf/2401.17710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17710]] Aesthetic Preference Prediction in Interior Design: Fuzzy Approach(https://arxiv.org/abs/2401.17710)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Interior design is all about creating spaces that look and feel good. However, the subjective nature of aesthetic preferences presents a significant challenge in defining and quantifying what makes an interior design visually appealing. The current paper addresses this gap by introducing a novel methodology for quantifying and predicting aesthetic preferences in interior design. Our study combines fuzzy logic with image processing techniques. We collected a dataset of interior design images from social media platforms, focusing on essential visual attributes such as color harmony, lightness, and complexity. We integrate these features using weighted average to compute a general aesthetic score. Our approach considers individual color preferences in calculating the overall aesthetic preference. We initially gather user ratings for primary colors like red, brown, and others to understand their preferences. Then, we use the pixel count of the top five dominant colors in the image to get the color scheme preference. The color scheme preference and the aesthetic score are then passed as inputs to the fuzzy inference system to calculate an overall preference score. This score represents a comprehensive measure of the user's preference for a particular interior design, considering their color choices and general aesthetic appeal. We used the 2AFC (Two-Alternative Forced Choice) method to validate our methodology, achieving a notable hit rate of 0.7. This study can help designers and professionals better understand and meet people's interior design preferences, especially in a world that relies heavily on digital media.</li>
<li><strong>摘要：</strong>室内设计就是创造外观和感觉良好的空间。然而，审美偏好的主观性质在定义和量化室内设计视觉吸引力方面提出了重大挑战。本文通过引入一种量化和预测室内设计中的审美偏好的新颖方法来解决这一差距。我们的研究将模糊逻辑与图像处理技术相结合。我们从社交媒体平台收集了室内设计图像的数据集，重点关注基本的视觉属性，例如色彩和谐、亮度和复杂性。我们使用加权平均来整合这些特征来计算一般的审美分数。我们的方法在计算整体审美偏好时考虑了个人的颜色偏好。我们最初收集用户对红色、棕色等原色的评分，以了解他们的偏好。然后，我们使用图像中前五种主色的像素数来获得配色方案偏好。然后，将配色方案偏好和审美得分作为输入传递给模糊推理系统，以计算总体偏好得分。该分数代表了用户对特定室内设计的偏好的综合衡量，考虑了他们的颜色选择和总体审美吸引力。我们使用 2AFC（两种选择强制选择）方法来验证我们的方法，达到了 0.7 的显着命中率。这项研究可以帮助设计师和专业人士更好地了解和满足人们的室内设计偏好，特别是在一个严重依赖数字媒体的世界中。</li>
</ul>

<h3>Title: Enhancing Large Language Model with Decomposed Reasoning for Emotion  Cause Pair Extraction</h3>
<ul>
<li><strong>Authors: </strong>Jialiang Wu, Yi Shen, Ziheng Zhang, Longjun Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17716">https://arxiv.org/abs/2401.17716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17716">https://arxiv.org/pdf/2401.17716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17716]] Enhancing Large Language Model with Decomposed Reasoning for Emotion  Cause Pair Extraction(https://arxiv.org/abs/2401.17716)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Emotion-Cause Pair Extraction (ECPE) involves extracting clause pairs representing emotions and their causes in a document. Existing methods tend to overfit spurious correlations, such as positional bias in existing benchmark datasets, rather than capturing semantic features. Inspired by recent work, we explore leveraging large language model (LLM) to address ECPE task without additional training. Despite strong capabilities, LLMs suffer from uncontrollable outputs, resulting in mediocre performance. To address this, we introduce chain-of-thought to mimic human cognitive process and propose the Decomposed Emotion-Cause Chain (DECC) framework. Combining inducing inference and logical pruning, DECC guides LLMs to tackle ECPE task. We further enhance the framework by incorporating in-context learning. Experiment results demonstrate the strength of DECC compared to state-of-the-art supervised fine-tuning methods. Finally, we analyze the effectiveness of each component and the robustness of the method in various scenarios, including different LLM bases, rebalanced datasets, and multi-pair extraction.</li>
<li><strong>摘要：</strong>情感-原因对提取 (ECPE) 涉及提取文档中表示情感及其原因的子句对。现有方法往往会过度拟合虚假相关性，例如现有基准数据集中的位置偏差，而不是捕获语义特征。受最近工作的启发，我们探索利用大语言模型 (LLM) 来解决 ECPE 任务，而无需额外培训。尽管法学硕士能力很强，但其产出却无法控制，导致表现平平。为了解决这个问题，我们引入思维链来模仿人类认知过程，并提出分解情绪因果链（DECC）框架。 DECC结合归纳推理和逻辑剪枝，指导法学硕士解决ECPE任务。我们通过结合情境学习进一步增强该框架。实验结果证明了 DECC 与最先进的监督微调方法相比的优势。最后，我们分析了每个组件的有效性以及该方法在各种场景下的鲁棒性，包括不同的LLM基础、重新平衡的数据集和多对提取。</li>
</ul>

<h3>Title: Algorithmic Robust Forecast Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Yongkang Guo, Jason D. Hartline, Zhihuan Huang, Yuqing Kong, Anant Shah, Fang-Yi Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17743">https://arxiv.org/abs/2401.17743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17743">https://arxiv.org/pdf/2401.17743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17743]] Algorithmic Robust Forecast Aggregation(https://arxiv.org/abs/2401.17743)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Forecast aggregation combines the predictions of multiple forecasters to improve accuracy. However, the lack of knowledge about forecasters' information structure hinders optimal aggregation. Given a family of information structures, robust forecast aggregation aims to find the aggregator with minimal worst-case regret compared to the omniscient aggregator. Previous approaches for robust forecast aggregation rely on heuristic observations and parameter tuning. We propose an algorithmic framework for robust forecast aggregation. Our framework provides efficient approximation schemes for general information aggregation with a finite family of possible information structures. In the setting considered by Arieli et al. (2018) where two agents receive independent signals conditioned on a binary state, our framework also provides efficient approximation schemes by imposing Lipschitz conditions on the aggregator or discrete conditions on agents' reports. Numerical experiments demonstrate the effectiveness of our method by providing a nearly optimal aggregator in the setting considered by Arieli et al. (2018).</li>
<li><strong>摘要：</strong>预测聚合结合了多个预测者的预测以提高准确性。然而，缺乏对预报员信息结构的了解阻碍了最佳聚合。给定一系列信息结构，稳健的预测聚合旨在找到与全知聚合器相比最坏情况遗憾最小的聚合器。以前的稳健预测聚合方法依赖于启发式观察和参数调整。我们提出了一个用于稳健预测聚合的算法框架。我们的框架为具有有限可能信息结构族的一般信息聚合提供了有效的近似方案。在 Arieli 等人考虑的设置中。 （2018）当两个代理接收以二元状态为条件的独立信号时，我们的框架还通过对聚合器施加 Lipschitz 条件或对代理报告施加离散条件来提供有效的近似方案。数值实验通过在 Arieli 等人考虑的设置中提供近乎最优的聚合器来证明我们的方法的有效性。 （2018）。</li>
</ul>

<h3>Title: SwarmBrain: Embodied agent for real-time strategy game StarCraft II via  large language models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Shao, Weifu Jiang, Fei Zuo, Mengqing Liu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17749">https://arxiv.org/abs/2401.17749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17749">https://arxiv.org/pdf/2401.17749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17749]] SwarmBrain: Embodied agent for real-time strategy game StarCraft II via  large language models(https://arxiv.org/abs/2401.17749)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, rag, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently garnered significant accomplishments in various exploratory tasks, even surpassing the performance of traditional reinforcement learning-based methods that have historically dominated the agent-based field. The purpose of this paper is to investigate the efficacy of LLMs in executing real-time strategy war tasks within the StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an embodied agent leveraging LLM for real-time strategy implementation in the StarCraft II game environment. The SwarmBrain comprises two key components: 1) a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed to orchestrate macro-level strategies from a high-level perspective. This matrix emulates the overarching consciousness of the Zerg intelligence brain, synthesizing strategic foresight with the aim of allocating resources, directing expansion, and coordinating multi-pronged assaults. 2) a Swarm ReflexNet, which is agile counterpart to the calculated deliberation of the Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the Swarm ReflexNet employs a condition-response state machine framework, enabling expedited tactical responses for fundamental Zerg unit maneuvers. In the experimental setup, SwarmBrain is in control of the Zerg race in confrontation with an Computer-controlled Terran adversary. Experimental results show the capacity of SwarmBrain to conduct economic augmentation, territorial expansion, and tactical formulation, and it shows the SwarmBrain is capable of achieving victory against Computer players set at different difficulty levels.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）最近在各种探索性任务中取得了重大成就，甚至超越了历史上主导基于代理领域的基于强化学习的传统方法的性能。本文的目的是研究法学硕士在星际争霸 II 游戏环境中执行实时战略战争任务的有效性。在本文中，我们介绍了 SwarmBrain，这是一种利用 LLM 在《星际争霸 II》游戏环境中实现实时策略的具体代理。 SwarmBrain 包含两个关键组件：1）由最先进的法学硕士提供支持的主宰智能矩阵，旨在从高层角度协调宏观战略。这个矩阵模拟了虫族智能大脑的总体意识，综合了战略远见，旨在分配资源、指导扩张和协调多管齐下的攻击。 2）Swarm ReflexNet，它是与主宰智能矩阵的计算深思熟虑相对应的敏捷对应物。由于 LLM 推理中固有的延迟，Swarm ReflexNet 采用条件响应状态机框架，能够对基本的 Zerg 单位机动进行快速战术响应。在实验设置中，SwarmBrain 控制着虫族种族，与计算机控制的人族对手对抗。实验结果显示了SwarmBrain进行经济增强、领土扩张和战术制定的能力，并且表明SwarmBrain能够在不同难度级别的计算机玩家中取得胜利。</li>
</ul>

<h3>Title: A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with  Uniform PAC Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Toshinori Kitamura, Tadashi Kozuno, Masahiro Kato, Yuki Ichihara, Soichiro Nishimori, Akiyoshi Sannai, Sho Sonoda, Wataru Kumagai, Yutaka Matsuo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17780">https://arxiv.org/abs/2401.17780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17780">https://arxiv.org/pdf/2401.17780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17780]] A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with  Uniform PAC Guarantees(https://arxiv.org/abs/2401.17780)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We study a primal-dual reinforcement learning (RL) algorithm for the online constrained Markov decision processes (CMDP) problem, wherein the agent explores an optimal policy that maximizes return while satisfying constraints. Despite its widespread practical use, the existing theoretical literature on primal-dual RL algorithms for this problem only provides sublinear regret guarantees and fails to ensure convergence to optimal policies. In this paper, we introduce a novel policy gradient primal-dual algorithm with uniform probably approximate correctness (Uniform-PAC) guarantees, simultaneously ensuring convergence to optimal policies, sublinear regret, and polynomial sample complexity for any target accuracy. Notably, this represents the first Uniform-PAC algorithm for the online CMDP problem. In addition to the theoretical guarantees, we empirically demonstrate in a simple CMDP that our algorithm converges to optimal policies, while an existing algorithm exhibits oscillatory performance and constraint violation.</li>
<li><strong>摘要：</strong>我们研究了一种用于在线约束马尔可夫决策过程（CMDP）问题的原对偶强化学习（RL）算法，其中代理探索在满足约束的同时最大化回报的最优策略。尽管其实际应用广泛，但现有的关于该问题的原对偶强化学习算法的理论文献仅提供了次线性遗憾保证，无法确保收敛到最优策略。在本文中，我们引入了一种新颖的策略梯度原始对偶算法，具有统一大概近似正确性（Uniform-PAC）保证，同时确保收敛到最优策略、亚线性遗憾和任意目标精度的多项式样本复杂性。值得注意的是，这代表了第一个用于在线 CMDP 问题的 Uniform-PAC 算法。除了理论保证之外，我们还在简单的 CMDP 中凭经验证明我们的算法收敛于最优策略，而现有算法则表现出振荡性能和约束违规。</li>
</ul>

<h3>Title: RADIN: Souping on a Budget</h3>
<ul>
<li><strong>Authors: </strong>Thibaut Menes, Olivier Risser-Maroix</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17790">https://arxiv.org/abs/2401.17790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17790">https://arxiv.org/pdf/2401.17790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17790]] RADIN: Souping on a Budget(https://arxiv.org/abs/2401.17790)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>Model Soups, extending Stochastic Weights Averaging (SWA), combine models fine-tuned with different hyperparameters. Yet, their adoption is hindered by computational challenges due to subset selection issues. In this paper, we propose to speed up model soups by approximating soups performance using averaged ensemble logits performances. Theoretical insights validate the congruence between ensemble logits and weight averaging soups across any mixing ratios. Our Resource ADjusted soups craftINg (RADIN) procedure stands out by allowing flexible evaluation budgets, enabling users to adjust his budget of exploration adapted to his resources while increasing performance at lower budget compared to previous greedy approach (up to 4% on ImageNet).</li>
<li><strong>摘要：</strong>Model Soups 扩展了随机权重平均 (SWA)，结合了使用不同超参数进行微调的模型。然而，由于子集选择问题，它们的采用受到计算挑战的阻碍。在本文中，我们建议通过使用平均集成 Logits 性能来近似 soups 性能来加速模型 soups。理论见解验证了任何混合比下的整体逻辑与重量平均汤之间的一致性。我们的资源调整汤制作 (RADIN) 程序的突出之处在于允许灵活的评估预算，使用户能够调整适合其资源的探索预算，同时与之前的贪婪方法相比（在 ImageNet 上高达 4%）以较低的预算提高性能。</li>
</ul>

<h3>Title: SWEA: Changing Factual Knowledge in Large Language Models via Subject  Word Embedding Altering</h3>
<ul>
<li><strong>Authors: </strong>Xiaopeng Li, Shasha Li, Bin Ji, Shezheng Song, Xi Wang, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Weimin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17809">https://arxiv.org/abs/2401.17809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17809">https://arxiv.org/pdf/2401.17809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17809]] SWEA: Changing Factual Knowledge in Large Language Models via Subject  Word Embedding Altering(https://arxiv.org/abs/2401.17809)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS method for editing factual knowledge in LLMs. We demonstrate the state-of-the-art performance of SWEAOS on the COUNTERFACT and zsRE datasets. To further validate the reasoning ability of SWEAOS in editing knowledge, we evaluate it on the more complex RIPPLEEDITS benchmark. The results on two subdatasets demonstrate that our SWEAOS possesses state-of-the-art reasoning ability.</li>
<li><strong>摘要：</strong>模型编辑最近受到广泛关注。目前的模型编辑方法主要涉及修改模型参数或向现有模型添加附加模块。然而，前者会对 LLM 造成不可逆转的损害，而后者会产生额外的推理开销，并且模糊向量匹配并不总是可靠。为了解决这些问题，我们提出了一个可扩展的主题词嵌入改变（SWEA）框架，该框架修改主题的表示并实现在推理阶段编辑知识的目标。 SWEA 在模型外部使用精确的键匹配并执行可靠的主题词嵌入更改，从而在不增加推理开销的情况下保护模型的原始权重。然后，我们提出先优化后抑制融合方法，该方法首先优化编辑目标的嵌入向量，然后抑制知识嵌入维度（KED）以获得最终的融合嵌入。因此，我们提出了用于编辑法学硕士事实知识的 SWEAOS 方法。我们在 COUNTERFACT 和 zsRE 数据集上展示了 SWEAOS 的最先进性能。为了进一步验证 SWEAOS 在编辑知识方面的推理能力，我们在更复杂的 RIPPLEEDITS 基准上对其进行评估。两个子数据集的结果表明我们的 SWEAOS 拥有最先进的推理能力。</li>
</ul>

<h3>Title: Privacy-preserving data release leveraging optimal transport and  particle gradient descent</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Donhauser, Javier Abad, Neha Hulkund, Fanny Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17823">https://arxiv.org/abs/2401.17823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17823">https://arxiv.org/pdf/2401.17823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17823]] Privacy-preserving data release leveraging optimal transport and  particle gradient descent(https://arxiv.org/abs/2401.17823)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints.</li>
<li><strong>摘要：</strong>我们提出了一种受保护的表格数据集的差分隐私数据合成的新方法，这是医疗保健和政府等高度敏感领域的相关任务。当前最先进的方法主要使用基于边际的方法，其中数据集是根据边际的私人估计生成的。在本文中，我们介绍了 PrivPGD，这是一种基于边际的私有数据合成的新一代方法，利用了最佳传输和粒子梯度下降的工具。我们的算法在大量数据集上优于现有方法，同时具有高度可扩展性，并提供了合并其他特定领域约束的灵活性。</li>
</ul>

<h3>Title: A Survey of Pre-trained Language Models for Processing Scientific Text</h3>
<ul>
<li><strong>Authors: </strong>Xanh Ho, Anh Khoa Duong Nguyen, An Tuan Dao, Junfeng Jiang, Yuki Chida, Kaito Sugimoto, Huy Quoc To, Florian Boudin, Akiko Aizawa</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17824">https://arxiv.org/abs/2401.17824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17824">https://arxiv.org/pdf/2401.17824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17824]] A Survey of Pre-trained Language Models for Processing Scientific Text(https://arxiv.org/abs/2401.17824)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The number of Language Models (LMs) dedicated to processing scientific text is on the rise. Keeping pace with the rapid growth of scientific LMs (SciLMs) has become a daunting task for researchers. To date, no comprehensive surveys on SciLMs have been undertaken, leaving this issue unaddressed. Given the constant stream of new SciLMs, appraising the state-of-the-art and how they compare to each other remain largely unknown. This work fills that gap and provides a comprehensive review of SciLMs, including an extensive analysis of their effectiveness across different domains, tasks and datasets, and a discussion on the challenges that lie ahead.</li>
<li><strong>摘要：</strong>专用于处理科学文本的语言模型 (LM) 的数量正在增加。跟上科学语言模型 (SciLM) 的快速发展已经成为研究人员面临的一项艰巨任务。迄今为止，尚未对 SciLM 进行全面调查，因此该问题尚未得到解决。鉴于新的 SciLM 不断涌现，评估最先进的技术以及它们如何相互比较仍然很大程度上未知。这项工作填补了这一空白，并对 SciLM 进行了全面回顾，包括对其在不同领域、任务和数据集上的有效性进行了广泛分析，并对未来的挑战进行了讨论。</li>
</ul>

<h3>Title: Predicting the Future with Simple World Models</h3>
<ul>
<li><strong>Authors: </strong>Tankred Saanum, Peter Dayan, Eric Schulz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17835">https://arxiv.org/abs/2401.17835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17835">https://arxiv.org/pdf/2401.17835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17835]] Predicting the Future with Simple World Models(https://arxiv.org/abs/2401.17835)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>World models can represent potentially high-dimensional pixel observations in compact latent spaces, making it tractable to model the dynamics of the environment. However, the latent dynamics inferred by these models may still be highly complex. Abstracting the dynamics of the environment with simple models can have several benefits. If the latent dynamics are simple, the model may generalize better to novel transitions, and discover useful latent representations of environment states. We propose a regularization scheme that simplifies the world model's latent dynamics. Our model, the Parsimonious Latent Space Model (PLSM), minimizes the mutual information between latent states and the dynamics that arise between them. This makes the dynamics softly state-invariant, and the effects of the agent's actions more predictable. We combine the PLSM with three different model classes used for i) future latent state prediction, ii) video prediction, and iii) planning. We find that our regularization improves accuracy, generalization, and performance in downstream tasks.</li>
<li><strong>摘要：</strong>世界模型可以表示紧凑潜在空间中潜在的高维像素观察，从而可以轻松地对环境动态进行建模。然而，这些模型推断的潜在动态可能仍然非常复杂。用简单的模型抽象环境的动态可以有几个好处。如果潜在的动态很简单，模型可以更好地推广到新的转变，并发现环境状态的有用的潜在表示。我们提出了一种正则化方案，可以简化世界模型的潜在动态。我们的模型，简约潜在空间模型（PLSM），最大限度地减少了潜在状态之间的互信息以及它们之间出现的动态。这使得动力学具有柔和的状态不变性，并且代理行为的效果更加可预测。我们将 PLSM 与三种不同的模型类相结合，用于 i) 未来潜在状态预测、ii) 视频预测和 iii) 规划。我们发现我们的正则化提高了下游任务的准确性、泛化性和性能。</li>
</ul>

<h3>Title: A Cross-View Hierarchical Graph Learning Hypernetwork for Skill  Demand-Supply Joint Prediction</h3>
<ul>
<li><strong>Authors: </strong>Wenshuo Chao, Zhaopeng Qiu, Likang Wu, Zhuoning Guo, Zhi Zheng, Hengshu Zhu, Hao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17838">https://arxiv.org/abs/2401.17838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17838">https://arxiv.org/pdf/2401.17838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17838]] A Cross-View Hierarchical Graph Learning Hypernetwork for Skill  Demand-Supply Joint Prediction(https://arxiv.org/abs/2401.17838)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>The rapidly changing landscape of technology and industries leads to dynamic skill requirements, making it crucial for employees and employers to anticipate such shifts to maintain a competitive edge in the labor market. Existing efforts in this area either rely on domain-expert knowledge or regarding skill evolution as a simplified time series forecasting problem. However, both approaches overlook the sophisticated relationships among different skills and the inner-connection between skill demand and supply variations. In this paper, we propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH) framework for joint skill demand-supply prediction. Specifically, CHGH is an encoder-decoder network consisting of i) a cross-view graph encoder to capture the interconnection between skill demand and supply, ii) a hierarchical graph encoder to model the co-evolution of skills from a cluster-wise perspective, and iii) a conditional hyper-decoder to jointly predict demand and supply variations by incorporating historical demand-supply gaps. Extensive experiments on three real-world datasets demonstrate the superiority of the proposed framework compared to seven baselines and the effectiveness of the three modules.</li>
<li><strong>摘要：</strong>技术和行业格局的快速变化导致了动态的技能要求，因此员工和雇主必须预测这种变化以保持劳动力市场的竞争优势。该领域的现有工作要么依赖于领域专家知识，要么将技能演化视为简化的时间序列预测问题。然而，这两种方法都忽视了不同技能之间的复杂关系以及技能需求和供给变化之间的内在联系。在本文中，我们提出了一种用于联合技能需求供给预测的跨视图分层图学习超网络（CHGH）框架。具体来说，CHGH 是一个编码器-解码器网络，由以下部分组成：i) 一个跨视图图编码器，用于捕获技能需求和供给之间的互连；ii) 一个分层图编码器，用于从集群角度对技能的共同进化进行建模， iii) 条件超级解码器，通过结合历史供需缺口来共同预测需求和供应变化。对三个真实世界数据集的广泛实验证明了所提出的框架相对于七个基线的优越性以及三个模块的有效性。</li>
</ul>

<h3>Title: Global-Liar: Factuality of LLMs over Time and Geographic Regions</h3>
<ul>
<li><strong>Authors: </strong>Shujaat Mirza, Bruno Coelho, Yuyuan Cui, Christina Pöpper, Damon McCoy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17839">https://arxiv.org/abs/2401.17839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17839">https://arxiv.org/pdf/2401.17839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17839]] Global-Liar: Factuality of LLMs over Time and Geographic Regions(https://arxiv.org/abs/2401.17839)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The increasing reliance on AI-driven solutions, particularly Large Language Models (LLMs) like the GPT series, for information retrieval highlights the critical need for their factuality and fairness, especially amidst the rampant spread of misinformation and disinformation online. Our study evaluates the factual accuracy, stability, and biases in widely adopted GPT models, including GPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated information dissemination. We introduce 'Global-Liar,' a dataset uniquely balanced in terms of geographic and temporal representation, facilitating a more nuanced evaluation of LLM biases. Our analysis reveals that newer iterations of GPT models do not always equate to improved performance. Notably, the GPT-4 version from March demonstrates higher factual accuracy than its subsequent June release. Furthermore, a concerning bias is observed, privileging statements from the Global North over the Global South, thus potentially exacerbating existing informational inequities. Regions such as Africa and the Middle East are at a disadvantage, with much lower factual accuracy. The performance fluctuations over time suggest that model updates may not consistently benefit all regions equally. Our study also offers insights into the impact of various LLM configuration settings, such as binary decision forcing, model re-runs and temperature, on model's factuality. Models constrained to binary (true/false) choices exhibit reduced factuality compared to those allowing an 'unclear' option. Single inference at a low temperature setting matches the reliability of majority voting across various configurations. The insights gained highlight the need for culturally diverse and geographically inclusive model training and evaluation. This approach is key to achieving global equity in technology, distributing AI benefits fairly worldwide.</li>
<li><strong>摘要：</strong>信息检索越来越依赖人工智能驱动的解决方案，特别是像 GPT 系列这样的大型语言模型 (LLM)，这突显了对其真实性和公平性的迫切需求，尤其是在网上错误信息和虚假信息猖獗的情况下。我们的研究评估了广泛采用的 GPT 模型（包括 GPT-3.5 和 GPT-4）的事实准确性、稳定性和偏差，有助于提高人工智能介导的信息传播的可靠性和完整性。我们引入了“Global-Liar”，这是一个在地理和时间表示方面独特平衡的数据集，有助于对 LLM 偏差进行更细致的评估。我们的分析表明，GPT 模型的更新迭代并不总是等同于性能的提高。值得注意的是，3 月份的 GPT-4 版本比随后的 6 月份版本表现出更高的事实准确性。此外，还观察到了一种令人担忧的偏见，即北半球国家的言论比南半球国家的言论享有特权，从而可能加剧现有的信息不平等。非洲和中东等地区处于劣势，事实准确性要低得多。随着时间的推移，性能波动表明模型更新可能无法始终平等地使所有区域受益。我们的研究还深入了解了各种 LLM 配置设置（例如二元决策强制、模型重新运行和温度）对模型真实性的影响。与允许“不清楚”选项的模型相比，受限于二元（真/假）选择的模型表现出较低的真实性。低温设置下的单一推理与各种配置中多数投票的可靠性相匹配。获得的见解凸显了文化多样性和地理包容性模型培训和评估的必要性。这种方法是实现全球技术公平、在全球范围内公平分配人工智能利益的关键。</li>
</ul>

<h3>Title: Probing Language Models' Gesture Understanding for Enhanced Human-AI  Interaction</h3>
<ul>
<li><strong>Authors: </strong>Philipp Wicke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17858">https://arxiv.org/abs/2401.17858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17858">https://arxiv.org/pdf/2401.17858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17858]] Probing Language Models' Gesture Understanding for Enhanced Human-AI  Interaction(https://arxiv.org/abs/2401.17858)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models (LLMs) has affected various disciplines that got beyond mere text generation. Going beyond their textual nature, this project proposal aims to investigate the interaction between LLMs and non-verbal communication, specifically focusing on gestures. The proposal sets out a plan to examine the proficiency of LLMs in deciphering both explicit and implicit non-verbal cues within textual prompts and their ability to associate these gestures with various contextual factors. The research proposes to test established psycholinguistic study designs to construct a comprehensive dataset that pairs textual prompts with detailed gesture descriptions, encompassing diverse regional variations, and semantic labels. To assess LLMs' comprehension of gestures, experiments are planned, evaluating their ability to simulate human behaviour in order to replicate psycholinguistic experiments. These experiments consider cultural dimensions and measure the agreement between LLM-identified gestures and the dataset, shedding light on the models' contextual interpretation of non-verbal cues (e.g. gestures).</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的兴起影响了各个学科，而不仅仅是文本生成。除了文本性质之外，该项目提案旨在研究法学硕士与非语言交流之间的互动，特别关注手势。该提案制定了一项计划，旨在检查法学硕士在破译文本提示中明确和隐含的非语言线索方面的熟练程度，以及他们将这些手势与各种上下文因素相关联的能力。该研究建议测试已建立的心理语言学研究设计，以构建一个综合数据集，将文本提示与详细的手势描述配对，涵盖不同的区域变化和语义标签。为了评估法学硕士对手势的理解，计划进行实验，评估他们模拟人类行为的能力，以复制心理语言学实验。这些实验考虑了文化维度，并测量了法学硕士识别的手势与数据集之间的一致性，揭示了模型对非语言线索（例如手势）的上下文解释。</li>
</ul>

<h3>Title: Efficient Subseasonal Weather Forecast using Teleconnection-informed  Transformers</h3>
<ul>
<li><strong>Authors: </strong>Shan Zhao, Zhitong Xiong, Xiao Xiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17870">https://arxiv.org/abs/2401.17870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17870">https://arxiv.org/pdf/2401.17870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17870]] Efficient Subseasonal Weather Forecast using Teleconnection-informed  Transformers(https://arxiv.org/abs/2401.17870)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enhances predictability on four surface and five upper-level atmospheric variables at a two-week lead time. Furthermore, the teleconnection-filtered features improve the spatial granularity of outputs significantly, indicating their potential physical consistency. Our research underscores the importance of atmospheric and oceanic teleconnections in driving future weather conditions. Besides, it presents a resource-efficient pathway for researchers to leverage existing foundation models on versatile downstream tasks.</li>
<li><strong>摘要：</strong>次季节预报对于农业、水资源管理和灾害预警至关重要，但由于大气的混乱性质而面临挑战。机器学习 (ML) 的最新进展通过为数值模型提供具有竞争力的预测技能，彻底改变了天气预报。然而，训练此类基础模型需要数千个 GPU 天，这会导致大量碳排放并限制其更广泛的适用性。此外，机器学习模型倾向于通过产生缺乏物理一致性和气象意义的平滑结果来欺骗像素误差分数。为了解决上述问题，我们提出了一种远程连接通知变压器。我们的架构利用预训练的盘古模型来实现良好的初始权重，并集成远程连接通知的时间模块以提高扩展时间范围内的可预测性。值得注意的是，通过调整盘古模式1.1%的参数，我们的方法在两周的准备时间内增强了四个地表和五个高层大气变量的可预测性。此外，遥连接过滤的特征显着提高了输出的空间粒度，表明它们潜在的物理一致性。我们的研究强调了大气和海洋遥相关在推动未来天气状况方面的重要性。此外，它为研究人员提供了一种资源高效的途径，可以利用现有的基础模型来完成多种下游任务。</li>
</ul>

<h3>Title: I Think, Therefore I am: Awareness in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan, Lichao Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17882">https://arxiv.org/abs/2401.17882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17882">https://arxiv.org/pdf/2401.17882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17882]] I Think, Therefore I am: Awareness in Large Language Models(https://arxiv.org/abs/2401.17882)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Do large language models (LLMs) exhibit any forms of awareness similar to humans? In this paper, we introduce the concept of awareness to LLMs, arguing that awareness is an essential aspect of trustworthiness for LLMs to enhance their interaction with humans while ensuring ethical responses. We define awareness in LLMs as the ability to perceive and understand themselves as AI models and to exhibit social intelligence. We identify four key dimensions of awareness: capability, mission, emotion, and perspective. To assess LLMs on these dimensions, we introduce a specialized dataset, AwareLLM dataset. Our findings reveal that LLMs demonstrate a decent degree of awareness, though they still lack substantial capability awareness.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）是否表现出与人类类似的任何形式的意识？在本文中，我们向法学硕士介绍了意识的概念，认为意识是法学硕士值得信赖的一个重要方面，可以增强他们与人类的互动，同时确保道德反应。我们将法学硕士的意识定义为感知和理解自己作为人工智能模型并展示社交智能的能力。我们确定了意识的四个关键维度：能力、使命、情感和视角。为了在这些维度上评估法学硕士，我们引入了一个专门的数据集，AwareLLM 数据集。我们的研究结果表明，法学硕士表现出了相当程度的意识，尽管他们仍然缺乏实质性的能力意识。</li>
</ul>

<h3>Title: Employing Label Models on ChatGPT Answers Improves Legal Text Entailment  Performance</h3>
<ul>
<li><strong>Authors: </strong>Chau Nguyen, Le-Minh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17897">https://arxiv.org/abs/2401.17897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17897">https://arxiv.org/pdf/2401.17897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17897]] Employing Label Models on ChatGPT Answers Improves Legal Text Entailment  Performance(https://arxiv.org/abs/2401.17897)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat, rag</a></li>
<li><strong>Abstract: </strong>The objective of legal text entailment is to ascertain whether the assertions in a legal query logically follow from the information provided in one or multiple legal articles. ChatGPT, a large language model, is robust in many natural language processing tasks, including legal text entailment: when we set the temperature = 0 (the ChatGPT answers are deterministic) and prompt the model, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperforms the previous SOTA of 67.89%. On the other hand, if the temperature is larger than zero, ChatGPT answers are not deterministic, leading to inconsistent answers and fluctuating results. We propose to leverage label models (a fundamental component of weak supervision techniques) to integrate the provisional answers by ChatGPT into consolidated labels. By that way, we treat ChatGPT provisional answers as noisy predictions which can be consolidated by label models. The experimental results demonstrate that this approach can attain an accuracy of 76.15%, marking a significant improvement of 8.26% over the prior state-of-the-art benchmark. Additionally, we perform an analysis of the instances where ChatGPT produces incorrect answers, then we classify the errors, offering insights that could guide potential enhancements for future research endeavors.</li>
<li><strong>摘要：</strong>法律文本蕴涵的目的是确定法律查询中的断言是否逻辑上源自一篇或多篇法律文章中提供的信息。 ChatGPT 是一个大型语言模型，在许多自然语言处理任务中都很稳健，包括法律文本蕴涵：当我们设置温度 = 0（ChatGPT 答案是确定性的）并提示模型时，它在 COLIEE 2022 数据集上达到了 70.64% 的准确率，优于之前的 SOTA 67.89%。另一方面，如果温度大于零，ChatGPT 答案就不稳定，导致答案不一致和结果波动。我们建议利用标签模型（弱监督技术的基本组成部分）将 ChatGPT 的临时答案集成到合并标签中。通过这种方式，我们将 ChatGPT 临时答案视为噪声预测，可以通过标签模型进行整合。实验结果表明，该方法的准确率可以达到 76.15%，比之前的最先进基准显着提高 8.26%。此外，我们对 ChatGPT 产生错误答案的实例进行分析，然后对错误进行分类，提供可以指导未来研究工作的潜在改进的见解。</li>
</ul>

<h3>Title: LOCOST: State-Space Models for Long Document Abstractive Summarization</h3>
<ul>
<li><strong>Authors: </strong>Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17919">https://arxiv.org/abs/2401.17919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17919">https://arxiv.org/pdf/2401.17919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17919]] LOCOST: State-Space Models for Long Document Abstractive Summarization(https://arxiv.org/abs/2401.17919)</code><input type="text"></li>
<li><strong>Keywords: </strong>long context, code</a></li>
<li><strong>Abstract: </strong>State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.</li>
<li><strong>摘要：</strong>状态空间模型是 Transformer 的低复杂度替代方案，用于编码长序列和捕获长期依赖性。我们提出 LOCOST：一种基于状态空间模型的编码器-解码器架构，用于具有长上下文输入的条件文本生成。该架构的计算复杂度为 $O(L \log L)$，可以处理比基于稀疏注意力模式的最先进模型更长的序列。我们在一系列长文档抽象总结任务上评估我们的模型。该模型的性能水平与相同大小的顶级稀疏变换器相当，达到 93-96% 的性能水平，同时在训练期间节省高达 50% 的内存，在推理期间节省高达 87% 的内存。此外，LOCOST 在推理时有效处理超过 600K 标记的输入文本，为全书摘要设定了新的最先进结果，并为长时间输入处理开辟了新视角。</li>
</ul>

<h3>Title: [Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference  Annotation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rebecca M. M. Hicke, David Mimno</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17922">https://arxiv.org/abs/2401.17922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17922">https://arxiv.org/pdf/2401.17922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17922]] [Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference  Annotation with LLMs(https://arxiv.org/abs/2401.17922)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Coreference annotation and resolution is a vital component of computational literary studies. However, it has previously been difficult to build high quality systems for fiction. Coreference requires complicated structured outputs, and literary text involves subtle inferences and highly varied language. New language-model-based seq2seq systems present the opportunity to solve both these problems by learning to directly generate a copy of an input sentence with markdown-like annotations. We create, evaluate, and release several trained models for coreference, as well as a workflow for training new models.</li>
<li><strong>摘要：</strong>共指注释和解析是计算文学研究的重要组成部分。然而，以前很难为小说构建高质量的系统。共指需要复杂的结构化输出，而文学文本涉及微妙的推论和高度多样化的语言。新的基于语言模型的 seq2seq 系统通过学习直接生成带有类似 markdown 注释的输入句子的副本，提供了解决这两个问题的机会。我们创建、评估和发布几个经过训练的共指模型，以及训练新模型的工作流程。</li>
</ul>

<h3>Title: GUMsley: Evaluating Entity Salience in Summarization for 12 English  Genres</h3>
<ul>
<li><strong>Authors: </strong>Jessica Lin, Amir Zeldes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17974">https://arxiv.org/abs/2401.17974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17974">https://arxiv.org/pdf/2401.17974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17974]] GUMsley: Evaluating Entity Salience in Summarization for 12 English  Genres(https://arxiv.org/abs/2401.17974)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>As NLP models become increasingly capable of understanding documents in terms of coherent entities rather than strings, obtaining the most salient entities for each document is not only an important end task in itself but also vital for Information Retrieval (IR) and other downstream applications such as controllable summarization. In this paper, we present and evaluate GUMsley, the first entity salience dataset covering all named and non-named salient entities for 12 genres of English text, aligned with entity types, Wikification links and full coreference resolution annotations. We promote a strict definition of salience using human summaries and demonstrate high inter-annotator agreement for salience based on whether a source entity is mentioned in the summary. Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries. We also show that predicting or providing salient entities to several model architectures enhances performance and helps derive higher-quality summaries by alleviating the entity hallucination problem in existing abstractive summarization.</li>
<li><strong>摘要：</strong>随着 NLP 模型越来越能够根据连贯实体而不是字符串来理解文档，获取每个文档最显着的实体不仅本身就是一项重要的最终任务，而且对于信息检索 (IR) 和其他下游应用程序（例如可控概括。在本文中，我们提出并评估了 GUMsley，这是第一个实体显着性数据集，涵盖 12 种英语文本类型的所有命名和非命名显着实体，与实体类型、维基百科链接和完整的共指解析注释保持一致。我们提倡使用人工摘要对显着性进行严格定义，并根据摘要中是否提及源实体来证明注释者之间对显着性的高度一致性。我们的评估显示，预训练的 SOTA 摘要模型和零样本 LLM 在捕获生成摘要中的显着实体方面表现不佳。我们还表明，预测或向多个模型架构提供显着实体可以提高性能，并通过减轻现有抽象摘要中的实体幻觉问题来帮助得出更高质量的摘要。</li>
</ul>

<h3>Title: Understanding polysemanticity in neural networks through coding theory</h3>
<ul>
<li><strong>Authors: </strong>Simon C. Marshall, Jan H. Kirchner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17975">https://arxiv.org/abs/2401.17975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17975">https://arxiv.org/pdf/2401.17975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17975]] Understanding polysemanticity in neural networks through coding theory(https://arxiv.org/abs/2401.17975)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes. We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix. Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is. This same framework explains the advantages of polysemantic neurons to learning performance and explains trends found in recent results by Elhage et al.~(2022). Our approach advances the pursuit of interpretability in neural networks, providing insights into their underlying structure and suggesting new avenues for circuit-level interpretability.</li>
<li><strong>摘要：</strong>尽管付出了巨大的努力，神经网络的可解释性仍然是一个难以捉摸的目标，之前的研究未能对大多数单个神经元对网络输出的影响提供简洁的解释。这种限制是由于大多数神经元的多语义性质造成的，即给定的神经元涉及多个不相关的网络状态，从而使该神经元的解释变得复杂。在本文中，我们应用神经科学和信息论中开发的工具，提出了一种新颖的网络可解释性实用方法以及对多语义性和代码密度的理论见解。我们通过检查激活协方差矩阵的特征谱来推断网络代码中的冗余级别。此外，我们还展示了随机投影如何揭示网络是否表现出平滑或不可微的代码，以及代码的可解释性。同样的框架解释了多语义神经元对学习表现的优势，并解释了 Elhage 等人（2022）最近结果中发现的趋势。我们的方法推进了对神经网络可解释性的追求，提供了对其底层结构的见解，并为电路级可解释性提出了新途径。</li>
</ul>

<h3>Title: Entity Linking in the Job Market Domain</h3>
<ul>
<li><strong>Authors: </strong>Mike Zhang, Rob van der Goot, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.17979">https://arxiv.org/abs/2401.17979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.17979">https://arxiv.org/pdf/2401.17979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.17979]] Entity Linking in the Job Market Domain(https://arxiv.org/abs/2401.17979)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>In Natural Language Processing, entity linking (EL) has centered around Wikipedia, but yet remains underexplored for the job market domain. Disambiguating skill mentions can help us get insight into the current labor market demands. In this work, we are the first to explore EL in this domain, specifically targeting the linkage of occupational skills to the ESCO taxonomy (le Vrang et al., 2014). Previous efforts linked coarse-grained (full) sentences to a corresponding ESCO skill. In this work, we link more fine-grained span-level mentions of skills. We tune two high-performing neural EL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et al., 2021), on a synthetically generated mention--skill pair dataset and evaluate them on a human-annotated skill-linking benchmark. Our findings reveal that both models are capable of linking implicit mentions of skills to their correct taxonomy counterparts. Empirically, BLINK outperforms GENRE in strict evaluation, but GENRE performs better in loose evaluation (accuracy@$k$).</li>
<li><strong>摘要：</strong>在自然语言处理中，实体链接（EL）以维基百科为中心，但在就业市场领域的探索仍然不足。消除技能提及的歧义可以帮助我们深入了解当前的劳动力市场需求。在这项工作中，我们是第一个在该领域探索 EL 的人，特别针对职业技能与 ESCO 分类法的联系（le Vrang 等人，2014）。之前的努力将粗粒度（完整）句子与相应的 ESCO 技能联系起来。在这项工作中，我们链接了更细粒度的跨级别技能提及。我们在综合生成的提及技能对数据集上调整了两个高性能神经 EL 模型，即双编码器（Wu 等人，2020）和自回归模型（Cao 等人，2021），并在人工注释的技能链接基准。我们的研究结果表明，这两种模型都能够将隐式提及的技能与其正确的分类对应项联系起来。根据经验，BLINK 在严格评估中优于 GENRE，但 GENRE 在宽松评估中表现更好 (accuracy@$k$)。</li>
</ul>

<h3>Title: Prompt-Driven LLM Safeguarding via Directed Representation Optimization</h3>
<ul>
<li><strong>Authors: </strong>Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, Nanyun Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18018">https://arxiv.org/abs/2401.18018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18018">https://arxiv.org/pdf/2401.18018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18018]] Prompt-Driven LLM Safeguarding via Directed Representation Optimization(https://arxiv.org/abs/2401.18018)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts as continuous, trainable embeddings and learns to move the representations of harmful/harmless queries along/opposite the direction in which the model's refusal probability increases. We demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts and outperforms strong baselines, as evaluated on out-of-domain benchmarks, without compromising the general model capability.</li>
<li><strong>摘要：</strong>在模型输入前添加安全提示是保护大型语言模型 (LLM) 免于遵守包含有害意图的查询的常见做法。然而，安全提示的工作机制尚未被完全理解，这阻碍了自动优化它们以提高 LLM 安全性的潜力。受这个问题的启发，我们从模型表示的角度研究了安全提示的影响。我们发现，在模型的表示空间中，可以在很大程度上区分有害和无害的查询，但这并没有通过安全提示得到明显增强。相反，查询的表示会通过不同的安全提示向相似的方向移动，即使查询是无害的，模型也更容易拒绝（即拒绝提供帮助）。受这些发现的启发，我们提出了一种称为 DRO（定向表示优化）的方法，用于自动安全提示优化。 DRO 将安全提示视为连续的、可训练的嵌入，并学习沿着/向模型拒绝概率增加的方向移动有害/无害查询的表示。我们证明，根据域外基准评估，DRO 显着提高了人工安全提示的防护性能，并且优于强大的基线，而不会影响一般模型的能力。</li>
</ul>

<h3>Title: Supporting Anticipatory Governance using LLMs: Evaluating and Aligning  Large Language Models with the News Media to Anticipate the Negative Impacts  of AI</h3>
<ul>
<li><strong>Authors: </strong>Mowafak Allaham, Nicholas Diakopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18028">https://arxiv.org/abs/2401.18028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18028">https://arxiv.org/pdf/2401.18028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18028]] Supporting Anticipatory Governance using LLMs: Evaluating and Aligning  Large Language Models with the News Media to Anticipate the Negative Impacts  of AI(https://arxiv.org/abs/2401.18028)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag</a></li>
<li><strong>Abstract: </strong>Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development. An understudied approach to such anticipation is the use of LLMs to enhance and guide this process. Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks. Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating. In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against. By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts. We then evaluate both instruction-based (GPT-4 and Mistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3) using a sample from this baseline. We find that the generated impacts using Mistral-7B, fine-tuned on impacts from the news media, tend to be qualitatively on par with impacts generated using a larger scale model such as GPT-4. Moreover, we find that these LLMs generate impacts that largely reflect the taxonomy of negative impacts identified in the news media, however the impacts produced by instruction-based models had gaps in the production of certain categories of impacts in comparison to fine-tuned models. This research highlights a potential bias in state-of-the-art LLMs when used for anticipating impacts and demonstrates the advantages of aligning smaller LLMs with a diverse range of impacts, such as those reflected in the news media, to better reflect such impacts during anticipatory exercises.</li>
<li><strong>摘要：</strong>预测新兴人工智能技术的负面影响是一项挑战，尤其是在发展的早期阶段。实现这种预期的一种尚未得到充分研究的方法是使用法学硕士来增强和指导这一过程。尽管法学硕士和评估指标取得了进步，可以解决生成文本中的偏差，但尚不清楚这些模型在预期任务中的表现如何。具体来说，使用法学硕士来预测人工智能的影响引发了关于这些模型能够产生的负面影响的质量和类别范围的问题。在本文中，我们利用新闻媒体（一个富含新兴技术规范评估的多样化数据源）来制定影响分类法，作为比较的基准。通过计算分析全球数百个在线新闻领域发布的数千篇新闻文章，我们开发了一个由十类人工智能影响组成的分类法。然后，我们使用此基线中的样本评估基于指令的模型（GPT-4 和 Mistral-7B-Instruct）和微调完成模型（Mistral-7B 和 GPT-3）。我们发现，使用 Mistral-7B 生成的影响（根据新闻媒体的影响进行微调）在质量上往往与使用更大规模的模型（例如 GPT-4）生成的影响相当。此外，我们发现这些法学硕士产生的影响在很大程度上反映了新闻媒体中识别的负面影响的分类，但是与微调模型相比，基于教学的模型产生的影响在某些类别的影响的产生方面存在差距。这项研究强调了最先进的法学硕士在用于预测影响时的潜在偏见，并证明了将较小的法学硕士与各种影响（例如新闻媒体中反映的影响）结合起来的优势，以更好地反映在预期练习。</li>
</ul>

<h3>Title: Paramanu: A Family of Novel Efficient Indic Generative Foundation  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mitodru Niyogi, Arnab Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18034">https://arxiv.org/abs/2401.18034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18034">https://arxiv.org/pdf/2401.18034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18034]] Paramanu: A Family of Novel Efficient Indic Generative Foundation  Language Models(https://arxiv.org/abs/2401.18034)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>We present Gyan AI Paramanu ("atom"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the "curse of multi-linguality" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics for Bangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit models outperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B, GPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite being smaller in size by 66 to 20 times compared to standard 7B LLMs. To run inference on our pretrained models, CPU is enough, and GPU is not needed. We also instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu models on 23k instructions in respective languages. Our pretrained and instruction-tuned models which are first of its kind, most powerful efficient small generative language models ever developed for Indic languages, and the various results lead to the conclusion that high quality generative language models are possible without high amount of compute power and humongous number of parameters. We plan to release our models at https://www.bharatgpts.com.</li>
<li><strong>摘要：</strong>我们推出了 Gyan AI Paramanu（“atom”），这是一系列印度语言的新颖语言模型。它是在单个 GPU 上从头开始针对 10 种印度语言（阿萨姆语、孟加拉语、印地语、孔卡尼语、迈蒂利语、马拉地语、奥迪亚语、梵语、泰米尔语、泰卢固语）进行预训练的自回归单语、双语和多语印度语言模型的集合。 5 种脚本（孟加拉文、天城文、奥迪亚文、泰米尔文、泰卢固文），大小各异，范围从 13.29M 到 367.5M。模型在单个 GPU 上使用 1024 的上下文大小进行预训练。这些模型非常高效、小巧、快速且功能强大。我们还开发了一种高效、最先进的印度语分词器，甚至可以对看不见的语言进行分词。为了避免我们的多语言 mParamanu 模型中的“多语言诅咒”，我们使用相同的脚本通过类型分组对可比较的语料库进行预训练。我们对用于生成开放式文本的预训练模型进行了人工评估，评估了孟加拉语、印地语和梵语的语法、连贯性、创造力和事实性指标。我们的孟加拉语、印地语和梵语模型的性能优于 GPT-3.5-Turbo (ChatGPT)、Bloom 7B、LLaMa-2 7B、OPT 6.7B、GPT-J 6B、GPTNeo 1.3B、GPT2-XL 大语言模型 (LLM)尽管与标准 7B LLM 相比规模小 66 至 20 倍，但利润仍然很大。要在我们的预训练模型上运行推理，CPU 就足够了，不需要 GPU。我们还根据各自语言的 23k 指令对预训练的孟加拉语、印地语、马拉地语、泰米尔语和泰卢固语模型进行了指令调整。我们的预训练和指令调整模型是有史以来为印度语言开发的同类中第一个、最强大、最高效的小型生成语言模型，各种结果得出这样的结论：高质量的生成语言模型无需大量的计算能力和参数数量巨大。我们计划在 https://www.bharatgpts.com 上发布我们的模型。</li>
</ul>

<h3>Title: Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic  Motivation Reinforcement Learning Algorithms for Improved Training and  Adaptability</h3>
<ul>
<li><strong>Authors: </strong>Navin Kamuni, Hardik Shah, Sathishkumar Chintala, Naveen Kunchakuri, Sujatha Alla Old Dominion</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18040">https://arxiv.org/abs/2401.18040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18040">https://arxiv.org/pdf/2401.18040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18040]] Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic  Motivation Reinforcement Learning Algorithms for Improved Training and  Adaptability(https://arxiv.org/abs/2401.18040)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag, agent</a></li>
<li><strong>Abstract: </strong>End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, show that intrinsic motivation-based debate systems outperform policies that depend on extrinsic incentives. By adopting random network distillation, for example, which is trained using semantic similarity between user-system dialogues, an astounding average success rate of 73% is achieved. This is a significant improvement over the baseline Proximal Policy Optimization (PPO), which has an average success rate of 60%. In addition, performance indicators such as booking rates and completion rates show a 10% rise over the baseline. Furthermore, these intrinsic incentive models help improve the system's policy's resilience in an increasing amount of domains. This implies that they could be useful in scaling up to settings that cover a wider range of domains.</li>
<li><strong>摘要：</strong>端到端多任务对话系统通常设计有单独的对话管道模块。其中，策略模块对于决定如何响应用户输入至关重要。该策略是通过强化学习算法利用代理接收奖励信号形式的反馈的环境来训练的。然而，当前的对话系统仅提供微薄且简单的奖励。研究内在动机强化学习算法是本研究的目标。通过这种方式，智能体可以通过教授内部激励系统来快速加速训练并提高其判断其行为质量的能力。特别是，我们采用随机网络蒸馏和好奇心驱动的强化学习技术来测量状态访问的频率，并通过使用话语之间的语义相似性来鼓励探索。 MultiWOZ（一个异构数据集）上的实验结果表明，基于内在动机的辩论系统优于依赖外在激励的政策。例如，通过采用随机网络蒸馏，利用用户系统对话之间的语义相似性进行训练，可以实现 73% 的惊人平均成功率。与平均成功率为 60% 的基准近端策略优化 (PPO) 相比，这是一个显着的改进。此外，预订率和完成率等绩效指标显示比基线增长了 10%。此外，这些内在激励模型有助于提高系统政策在越来越多领域的弹性。这意味着它们可以用于扩展到覆盖更广泛领域的设置。</li>
</ul>

<h3>Title: SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition</h3>
<ul>
<li><strong>Authors: </strong>Yihan Wu, Soumi Maiti, Yifan Peng, Wangyou Zhang, Chenda Li, Yuyue Wang, Xihua Wang, Shinji Watanabe, Ruihua Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18045">https://arxiv.org/abs/2401.18045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18045">https://arxiv.org/pdf/2401.18045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18045]] SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition(https://arxiv.org/abs/2401.18045)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code</a></li>
<li><strong>Abstract: </strong>Recent advancements in language models have significantly enhanced performance in multiple speech-related tasks. Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model. However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task. In this work, we propose a novel decoder-only speech language model, SpeechComposer, that can unify common speech tasks by composing a fixed set of prompt tokens. Built upon four primary tasks -- speech synthesis, speech recognition, speech language modeling, and text language modeling -- SpeechComposer can easily extend to more speech tasks via compositions of well-designed prompt tokens, like voice conversion and speech enhancement. The unification of prompt tokens also makes it possible for knowledge sharing among different speech tasks in a more structured manner. Experimental results demonstrate that our proposed SpeechComposer can improve the performance of both primary tasks and composite tasks, showing the effectiveness of the shared prompt tokens. Remarkably, the unified decoder-only model achieves a comparable and even better performance than the baselines which are expert models designed for single tasks.</li>
<li><strong>摘要：</strong>语言模型的最新进展显着提高了多种语音相关任务的性能。现有的语音语言模型通常利用任务相关的提示标记将各种语音任务统一在单个模型中。然而，这种设计忽略了不同语音任务之间的内在联系，这可能会提高每个任务的性能。在这项工作中，我们提出了一种新颖的仅解码器语音语言模型，SpeechComposer，它可以通过组成一组固定的提示标记来统一常见的语音任务。基于四个主要任务——语音合成、语音识别、语音语言建模和文本语言建模——SpeechComposer 可以通过精心设计的提示标记的组合（例如语音转换和语音增强）轻松扩展到更多语音任务。提示标记的统一还使得不同语音任务之间以更加结构化的方式共享知识成为可能。实验结果表明，我们提出的 SpeechComposer 可以提高主要任务和复合任务的性能，显示出共享提示标记的有效性。值得注意的是，统一的仅解码器模型实现了与为单一任务设计的专家模型相当的甚至更好的性能。</li>
</ul>

<h3>Title: Multipath parsing in the brain</h3>
<ul>
<li><strong>Authors: </strong>Berta Franzluebbers, Donald Dunagan, Miloš Stanojević, Jan Buys, John T. Hale</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18046">https://arxiv.org/abs/2401.18046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18046">https://arxiv.org/pdf/2401.18046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18046]] Multipath parsing in the brain(https://arxiv.org/abs/2401.18046)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus.</li>
<li><strong>摘要：</strong>人类按照听到的顺序逐字理解句子。这种增量需要解决有关句法关系的临时歧义。我们通过将增量生成依赖解析器的预测与在听有声读物时接受功能神经成像的人的时间过程数据相关联来研究人类如何处理这些句法歧义。特别是，我们比较了关于逐字理解过程中进行句法分析的数量的相互竞争的假设：一个与多个。这种比较涉及根据现有的 fMRI 数据集评估具有 LLM 适应编码的最先进的依存解析器的句法惊喜。在英文和中文数据中，我们都找到了多路径解析的证据。与这种多路径效应相关的大脑区域包括双侧颞上回。</li>
</ul>

<h3>Title: Rank Supervised Contrastive Learning for Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Qianying Ren, Dongsheng Luo, Dongjin Song</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18057">https://arxiv.org/abs/2401.18057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18057">https://arxiv.org/pdf/2401.18057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18057]] Rank Supervised Contrastive Learning for Time Series Classification(https://arxiv.org/abs/2401.18057)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Recently, various contrastive learning techniques have been developed to categorize time series data and exhibit promising performance. A general paradigm is to utilize appropriate augmentations and construct feasible positive samples such that the encoder can yield robust and discriminative representations by mapping similar data points closer together in the feature space while pushing dissimilar data points farther apart. Despite its efficacy, the fine-grained relative similarity (e.g., rank) information of positive samples is largely ignored, especially when labeled samples are limited. To this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform time series classification. Different from conventional contrastive learning frameworks, RankSCL augments raw data in a targeted way in the embedding space and adopts certain filtering rules to select more informative positive and negative pairs of samples. Moreover, a novel rank loss is developed to assign different weights for different levels of positive samples, enable the encoder to extract the fine-grained information of the same class, and produce a clear boundary among different classes. Thoroughly empirical studies on 128 UCR datasets and 30 UEA datasets demonstrate that the proposed RankSCL can achieve state-of-the-art performance compared to existing baseline methods.</li>
<li><strong>摘要：</strong>最近，已经开发了各种对比学习技术来对时间序列数据进行分类并表现出有希望的性能。一般范例是利用适当的增强并构造可行的正样本，以便编码器可以通过将相似的数据点映射到特征空间中更靠近在一起，同时将不相似的数据点推得更远，从而产生鲁棒且有辨别力的表示。尽管它很有效，但阳性样本的细粒度相对相似性（例如排名）信息在很大程度上被忽略，特别是当标记样本有限时。为此，我们提出了排名监督对比学习（RankSCL）来执行时间序列分类。与传统的对比学习框架不同，RankSCL在嵌入空间中有针对性地增强原始数据，并采用一定的过滤规则来选择信息量更大的正负样本对。此外，还开发了一种新颖的秩损失，为不同级别的正样本分配不同的权重，使编码器能够提取同一类别的细粒度信息，并在不同类别之间产生清晰的边界。对 128 个 UCR 数据集和 30 个 UEA 数据集的彻底实证研究表明，与现有基线方法相比，所提出的 RankSCL 可以实现最先进的性能。</li>
</ul>

<h3>Title: LongAlign: A Recipe for Long Context Alignment of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18058">https://arxiv.org/abs/2401.18058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18058">https://arxiv.org/pdf/2401.18058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18058]] LongAlign: A Recipe for Long Context Alignment of Large Language Models(https://arxiv.org/abs/2401.18058)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, code, chat</a></li>
<li><strong>Abstract: </strong>Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\%, while also maintaining their proficiency in handling short, generic tasks. The code, data, and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.</li>
<li><strong>摘要：</strong>扩展大型语言模型以有效处理长上下文需要对相似长度的输入序列进行指令微调。为了解决这个问题，我们提出了 LongAlign——长上下文对齐的指令数据、训练和评估的配方。首先，我们使用 Self-Instruct 构建一个长指令跟踪数据集。为了确保数据多样性，它涵盖了来自各种长上下文源的广泛任务。其次，我们采用打包和排序批处理策略来加速对不同长度分布的数据的监督微调。此外，我们开发了一种损失加权方法来平衡打包训练期间不同序列对损失的贡献。第三，我们引入了 LongBench-Chat 基准，用于评估长度为 10k-100k 的查询的指令跟踪能力。实验表明，LongAlign 在长上下文任务中的性能比法学硕士的现有方法高出 30%，同时还保持了处理短通用任务的熟练程度。代码、数据和长对齐模型在 https://github.com/THUDM/LongAlign 上开源。</li>
</ul>

<h3>Title: RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18059">https://arxiv.org/abs/2401.18059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18059">https://arxiv.org/pdf/2401.18059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18059]] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval(https://arxiv.org/abs/2401.18059)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.</li>
<li><strong>摘要：</strong>检索增强语言模型可以更好地适应世界状态的变化并融入长尾知识。然而，大多数现有方法仅从检索语料库中检索短的连续块，限制了对整个文档上下文的整体理解。我们引入了递归嵌入、聚类和总结文本块的新颖方法，从下到上构建具有不同摘要级别的树。在推理时，我们的 RAPTOR 模型从这棵树中检索，在不同抽象级别集成冗长文档中的信息。对照实验表明，在多项任务上，递归摘要检索比传统检索增强型语言模型有显着改进。在涉及复杂、多步骤推理的问答任务中，我们展示了最先进的结果；例如，通过将 RAPTOR 检索与 GPT-4 结合使用，我们可以将 QuALITY 基准的最佳性能绝对准确率提高 20%。</li>
</ul>

<h3>Title: Do Language Models Exhibit the Same Cognitive Biases in Problem Solving  as Human Learners?</h3>
<ul>
<li><strong>Authors: </strong>Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan Cotterell, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18070">https://arxiv.org/abs/2401.18070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18070">https://arxiv.org/pdf/2401.18070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18070]] Do Language Models Exhibit the Same Cognitive Biases in Problem Solving  as Human Learners?(https://arxiv.org/abs/2401.18070)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not during the final step which relies on the problem's arithmetic expressions (solution execution).</li>
<li><strong>摘要：</strong>人们越来越有兴趣采用大型语言模型（LLM）作为认知模型。为此，了解哪些认知属性可以被法学硕士很好地建模，哪些不能很好地建模是至关重要的。在这项工作中，我们研究了法学硕士在解决算术应用题时相对于儿童已知的偏见。通过调查学习科学文献，我们认为解决问题的过程可以分为三个不同的步骤：文本理解、解决方案规划和解决方案执行。我们为每个过程构建测试，以了解当前最先进的法学硕士可以忠实地模拟该过程的哪些部分。我们使用神经符号方法为每个测试生成一组新颖的应用题，该方法可以对问题特征进行细粒度控制。我们发现证据表明，无论有没有指令调整，法学硕士在解决过程的文本理解和解决方案规划步骤中都表现出类似人类的偏见，但在依赖于问题算术表达式的最后一步中却没有表现出类似人类的偏见（解决方案）。执行）。</li>
</ul>

<h3>Title: KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache  Quantization</h3>
<ul>
<li><strong>Authors: </strong>Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.18079">https://arxiv.org/abs/2401.18079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.18079">https://arxiv.org/pdf/2401.18079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.18079]] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache  Quantization(https://arxiv.org/abs/2401.18079)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges; and (v) Q-Norm, where we normalize quantization centroids in order to mitigate distribution shift, providing additional benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving the LLaMA-7B model with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.</li>
<li><strong>摘要：</strong>法学硕士越来越多地用于需要大型上下文窗口的文档分析和摘要等应用程序，并且在这些大型上下文窗口中，KV 缓存激活表面成为推理过程中内存消耗的主要贡献者。量化是一种很有前途的压缩 KV 缓存激活的方法；然而，现有的解决方案无法以超低精度（例如低于 4 位）准确表示激活。在这项工作中，我们提出了 KVQuant，它通过结合量化缓存 KV 激活的新方法来解决这个问题，包括：（i）每通道密钥量化，我们调整量化密钥激活的维度以更好地匹配分布; (ii) RoPE 前密钥量化，我们在旋转位置嵌入之前量化密钥激活，以减轻其对量化的影响； (iii) 非均匀 KV 缓存量化，我们导出每层敏感度加权的非均匀数据类型，更好地表示分布； (iv) 每向量密集和稀疏量化，我们分别隔离每个向量的异常值，以最大限度地减少量化范围中的偏差； (v) Q-Norm，我们对量化质心进行归一化以减轻分布偏移，为 2 位量化提供额外的好处。通过将我们的方法应用于 LLaMA、LLaMA-2 和 Mistral 模型，我们在 Wikitext-2 和 C4 上通过 3 位量化实现了 $<0.1$ 的困惑度降低，优于现有方法。我们的方法能够在单个 A100-80GB GPU 上为 LLaMA-7B 模型提供高达 100 万个上下文长度的服务，在 8-GPU 系统上为高达 1000 万个上下文长度提供服务。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
