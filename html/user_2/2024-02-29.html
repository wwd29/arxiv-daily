<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-29</h1>
<h3>Title: Stepwise Self-Consistent Mathematical Reasoning with Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Zilong Zhao, Yao Rong, Dongyang Guo, Emek Gözlüklü, Emir Gülboy, Enkelejda Kasneci</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17786">https://arxiv.org/abs/2402.17786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17786">https://arxiv.org/pdf/2402.17786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17786]] Stepwise Self-Consistent Mathematical Reasoning with Large Language  Models(https://arxiv.org/abs/2402.17786)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions. To address these issues, we introduce a novel algorithm, namely Stepwise Self-Consistent Chain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting intermediate steps based on the intersection of various reasoning chains. Additionally, SSC-CoT enables the model to discover critical intermediate steps by querying a knowledge graph comprising relevant domain knowledge. To validate SSC-CoT, we present a new dataset, TriMaster100, tailored for complex trigonometry problems. This dataset contains 100 questions, with each solution broken down into scored intermediate steps, facilitating a comprehensive evaluation of the mathematical reasoning process. On TriMaster100, SSC-CoT triples the effectiveness of the state-of-the-art methods. Furthermore, we benchmark SSC-CoT on the widely recognized complex mathematical question dataset, MATH level 5, and it surpasses the second-best method by 7.2% in accuracy. Code and the TriMaster100 dataset can be found at: https://github.com/zhao-zilong/ssc-cot.</li>
<li><strong>摘要：</strong>使用大型语言模型进行复杂的数学推理很困难，这主要是由于多步骤推理的复杂性。该过程的主要挑战包括（1）选择关键的中间结果来推进该过程，以及（2）对潜在解决方案的探索有限。为了解决这些问题，我们引入了一种新颖的算法，即逐步自洽思想链（SSC-CoT）。 SSC-CoT 采用基于各种推理链的交集选择中间步骤的策略。此外，SSC-CoT 使模型能够通过查询包含相关领域知识的知识图来发现关键的中间步骤。为了验证 SSC-CoT，我们提出了一个新的数据集 TriMaster100，专为复杂的三角学问题而定制。该数据集包含 100 个问题，每个解决方案都分为评分的中间步骤，有助于对数学推理过程进行全面评估。在 TriMaster100 上，SSC-CoT 使最先进方法的有效性提高了三倍。此外，我们在广泛认可的复杂数学问题数据集 MATH level 5 上对 SSC-CoT 进行了基准测试，它的准确率超过了第二好的方法 7.2%。代码和TriMaster100数据集可以在：https://github.com/zhao-zilong/ssc-cot找到。</li>
</ul>

<h3>Title: A Surprising Failure? Multimodal LLMs and the NLVR Challenge</h3>
<ul>
<li><strong>Authors: </strong>Anne Wu, Kianté Brantley, Yoav Artzi</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17793">https://arxiv.org/abs/2402.17793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17793">https://arxiv.org/pdf/2402.17793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17793]] A Surprising Failure? Multimodal LLMs and the NLVR Challenge(https://arxiv.org/abs/2402.17793)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and the open-source model IDEFICS -- on the compositional natural language vision reasoning task NLVR. Given a human-written sentence paired with a synthetic image, this task requires the model to determine the truth value of the sentence with respect to the image. Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.</li>
<li><strong>摘要：</strong>这项研究在组合自然语言视觉推理任务 NLVR 上评估了三种最先进的 MLLM——GPT-4V、Gemini Pro 和开源模型 IDEFICS。给定一个与合成图像配对的人类书写的句子，该任务要求模型确定该句子相对于图像的真值。尽管这些模型表现出了强大的性能，但我们观察到它们在 NLVR 上表现不佳，NLVR 的构建需要组合和空间推理，并且对语义和系统偏差具有鲁棒性。</li>
</ul>

<h3>Title: TruthX: Alleviating Hallucinations by Editing Large Language Models in  Truthful Space</h3>
<ul>
<li><strong>Authors: </strong>Shaolei Zhang, Tian Yu, Yang Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17811">https://arxiv.org/abs/2402.17811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17811">https://arxiv.org/pdf/2402.17811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17811]] TruthX: Alleviating Hallucinations by Editing Large Language Models in  Truthful Space(https://arxiv.org/abs/2402.17811)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge. In this paper, we propose TruthX, an inference-time method to elicit the truthfulness of LLMs by editing their internal representations in truthful space. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLMs. Experiments show that TruthX effectively improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that the truthful space acquired by TruthX plays a pivotal role in controlling LLM to produce truthful or hallucinatory responses.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种任务中表现出了卓越的能力。然而，他们有时会产生幻觉，特别是在尽管拥有正确的知识但仍可能产生不真实反应的情况下。在本文中，我们提出了 TruthX，这是一种推理时间方法，通过编辑真实空间中的内部表示来得出法学硕士的真实性。 TruthX 采用自动编码器将 LLM 的表示分别映射到语义和真实潜在空间，并应用对比学习来识别真实空间内的真实编辑方向。在推理过程中，通过编辑LLM在真实空间中的内部表示，TruthX有效地增强了LLM的真实性。实验表明，TruthX 在 TruthfulQA 基准上有效地将 13 个高级法学硕士的真实性平均提高了 20%。进一步分析表明，TruthX获得的真实空间在控制LLM产生真实或幻觉反应方面发挥着关键作用。</li>
</ul>

<h3>Title: DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping  Backward Propagation</h3>
<ul>
<li><strong>Authors: </strong>Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Sejung Kwon, Dongsuk Jeon, Dongsoo Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17812">https://arxiv.org/abs/2402.17812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17812">https://arxiv.org/pdf/2402.17812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17812]] DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping  Backward Propagation(https://arxiv.org/abs/2402.17812)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropagation. Specifically, utilizing DropBP in QLoRA reduces training time by 44%, increases the convergence speed to the identical loss level by 1.5$\times$, and enables training with a 6.2$\times$ larger sequence length on a single NVIDIA-A100 80GiB GPU in LLaMA2-70B. The code is available at https://github.com/WooSunghyeon/dropbp.</li>
<li><strong>摘要：</strong>训练深度神经网络通常在前向和后向传播过程中涉及大量的计算成本。传统的层丢弃技术在训练期间丢弃某些层以减少计算负担。然而，在前向传播过程中丢弃层会降低准确性，从而对训练过程产生不利影响。在本文中，我们提出了丢弃反向传播（DropBP），这是一种旨在降低计算成本同时保持准确性的新颖方法。 DropBP在后向传播过程中随机丢弃层，这不会偏离前向传播。此外，DropBP计算每一层的敏感性来分配适当的下降率，从而稳定训练过程。 DropBP 旨在通过反向传播提高训练过程的效率，从而能够使用反向传播加速完全微调和参数高效的微调。具体来说，在 QLoRA 中使用 DropBP 可减少 44% 的训练时间，将相同损失水平的收敛速度提高 1.5$\times$，并且可以在单个 NVIDIA-A100 80GiB GPU 上以大 6.2$\times$ 的序列长度进行训练在 LLaMA2-70B 中。代码可在 https://github.com/WooSunghyeon/dropbp 获取。</li>
</ul>

<h3>Title: Prediction-Powered Ranking of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ivi Chatzi, Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY, cs.HC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17826">https://arxiv.org/abs/2402.17826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17826">https://arxiv.org/pdf/2402.17826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17826]] Prediction-Powered Ranking of Large Language Models(https://arxiv.org/abs/2402.17826)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provides a rank-set -- a set of possible ranking positions -- for each of the models under comparison. Moreover, it guarantees that, with a probability greater than or equal to a user-specified value, the rank-sets cover the true ranking consistent with (the distribution of) human pairwise preferences. Our framework is computationally efficient, easy to use, and does not make any assumption about the distribution of human preferences nor about the degree of alignment between the pairwise comparisons by the humans and the strong large language model.</li>
<li><strong>摘要：</strong>大型语言模型通常根据其与人类偏好的一致程度进行排名——如果一个模型的输出更常被人类偏好，则该模型比其他模型更好。引发人类偏好的最流行方法之一是利用不同模型提供的输出与相同输入之间的成对比较。然而，由于人类收集成对比较既昂贵又耗时，因此通过强大的大型语言模型（与人类偏好密切相关的模型）收集成对比较已成为一种非常常见的做法。令人惊讶的是，从业者目前无法衡量人类和模型偏好之间的任何不匹配可能在构建的排名中引入的不确定性。在这项工作中，我们开发了一个统计框架来弥补这一差距。给定一小部分人类的成对比较和大量模型的成对比较，我们的框架为每个比较的模型提供了一个排名集——一组可能的排名位置。此外，它保证以大于或等于用户指定值的概率，排名集覆盖与人类成对偏好（的分布）一致的真实排名。我们的框架计算效率高，易于使用，并且不对人类偏好的分布或人类与强大的大语言模型之间的成对比较之间的对齐程度做出任何假设。</li>
</ul>

<h3>Title: Stable LM 2 1.6B Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, Meng Lee, Emad Mostaque, Michael Pieler, Nikhil Pinnaparju, Paulo Rocha, Harry Saini, Hannah Teufel, Niccolo Zanichelli, Carlos Riquelme</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17834">https://arxiv.org/abs/2402.17834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17834">https://arxiv.org/pdf/2402.17834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17834]] Stable LM 2 1.6B Technical Report(https://arxiv.org/abs/2402.17834)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce StableLM 2 1.6B, the first in a new generation of our language model series. In this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of StableLM 2 1.6B. The weights for both models are available via Hugging Face for anyone to download and use. The report contains thorough evaluations of these models, including zero- and few-shot benchmarks, multilingual benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of publishing this report, StableLM 2 1.6B was the state-of-the-art open model under 2B parameters by a significant margin. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.</li>
<li><strong>摘要：</strong>我们推出了 StableLM 2 1.6B，这是我们新一代语言模型系列中的第一个。在这份技术报告中，我们详细介绍了 StableLM 2 1.6B 的基础版本和指令调整版本的数据和训练过程。两种模型的权重均可通过 Hugging Face 提供，任何人都可以下载和使用。该报告包含对这些模型的全面评估，包括零样本和少样本基准、多语言基准以及专注于多轮对话的 MT 基准。在发布本报告时，StableLM 2 1.6B 是 2B 参数下最先进的开放模型，遥遥领先。鉴于其较小的尺寸，我们还提供了许多边缘设备的吞吐量测量。此外，我们还开源了几个量化检查点，并提供了它们与原始模型相比的性能指标。</li>
</ul>

<h3>Title: Follow My Instruction and Spill the Beans: Scalable Data Extraction from  Retrieval-Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Zhenting Qi, Hanlin Zhang, Eric Xing, Sham Kakade, Himabindu Lakkaraju</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17840">https://arxiv.org/abs/2402.17840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17840">https://arxiv.org/pdf/2402.17840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17840]] Follow My Instruction and Spill the Beans: Scalable Data Extraction from  Retrieval-Augmented Generation Systems(https://arxiv.org/abs/2402.17840)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 通过在测试时结合外部知识来改进预训练模型，以实现定制适应。我们研究上下文检索 RAG 语言模型 (LM) 中数据存储泄漏的风险。我们表明，对手可以利用 LM 的指令跟踪功能，通过提示注入轻松地从使用指令调整 LM 构建的 RAG 系统的数据存储中逐字提取文本数据。该漏洞存在于 Llama2、Mistral/Mixtral、Vicuna、SOLAR、WizardLM、Qwen1.5 和 Platypus2 等多种现代 LM 中，并且随着模型规模的扩大，可利用性会加剧。将我们的研究扩展到生产 RAG 模型 GPT，我们设计了一种攻击，该攻击可以导致数据存储泄漏，在 25 个随机选择的自定义 GPT 上最多进行 2 次查询，成功率为 100%，并且我们以 41% 的速度逐字提取文本数据。通过仅用自己生成的 100 个查询来提示 GPT，可以从包含 77,000 个单词的书中提取 1,569,000 个单词的语料库中的 3%。</li>
</ul>

<h3>Title: Automated Statistical Model Discovery with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Y. Li, Emily B. Fox, Noah D. Goodman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17879">https://arxiv.org/abs/2402.17879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17879">https://arxiv.org/pdf/2402.17879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17879]] Automated Statistical Model Discovery with Language Models(https://arxiv.org/abs/2402.17879)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Statistical model discovery involves a challenging search over a vast space of models subject to domain-specific modeling constraints. Efficiently searching over this space requires human expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, key restrictions of previous systems. We evaluate our method in three common settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving classic models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method matches the performance of previous systems, identifies models on par with human expert designed models, and extends classic models in interpretable ways. Our results highlight the promise of LM driven model discovery.</li>
<li><strong>摘要：</strong>统计模型发现涉及对受特定领域建模约束的大量模型进行具有挑战性的搜索。有效地搜索这个空间需要人类在建模和问题领域方面的专业知识。在大型语言模型（LM）的领域知识和编程能力的推动下，我们引入了一种语言模型驱动的自动统计模型发现方法。我们在 Box 循环的框架内构建我们的自动化程序：LM 在提出表示为概率程序的统计模型（充当建模者）和批评这些模型（充当领域专家）之间进行迭代。通过利用语言模型，我们不必定义特定于领域的模型语言或设计手工搜索过程（这是以前系统的关键限制）。我们在概率建模的三种常见设置中评估我们的方法：在有限的模型空间内搜索、在开放式空间上搜索以及在自然语言约束下改进经典模型（例如，该模型应该可由生态学家解释）。我们的方法与以前系统的性能相匹配，识别与人类专家设计的模型相当的模型，并以可解释的方式扩展经典模型。我们的结果凸显了 LM 驱动模型发现的前景。</li>
</ul>

<h3>Title: BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in  Relational Algebra</h3>
<ul>
<li><strong>Authors: </strong>Parker Glenn, Parag Pravin Dakle, Liang Wang, Preethi Raghavan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17882">https://arxiv.org/abs/2402.17882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17882">https://arxiv.org/pdf/2402.17882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17882]] BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in  Relational Algebra(https://arxiv.org/abs/2402.17882)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Many existing end-to-end systems for hybrid question answering tasks can often be boiled down to a "prompt-and-pray" paradigm, where the user has limited control and insight into the intermediate reasoning steps used to achieve the final result. Additionally, due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting. We introduce BlendSQL, a superset of SQLite to act as a unified dialect for orchestrating reasoning across both unstructured and structured data. For hybrid question answering tasks involving multi-hop reasoning, we encode the full decomposed reasoning roadmap into a single interpretable BlendSQL query. Notably, we show that BlendSQL can scale to massive datasets and improve the performance of end-to-end systems while using 35% fewer tokens. Our code is available and installable as a package at https://github.com/parkervg/blendsql.</li>
<li><strong>摘要：</strong>许多现有的用于混合问答任务的端到端系统通常可以归结为“提示和祈祷”范例，其中用户对用于实现最终结果的中间推理步骤的控制和洞察力有限。此外，由于许多基于 Transformer 的 LLM 的上下文大小限制，期望完整的结构化和非结构化上下文能够适应零样本设置中的给定提示通常是不合理的，更不用说少数样本设置了。我们引入 BlendSQL，它是 SQLite 的超集，充当跨非结构化和结构化数据编排推理的统一方言。对于涉及多跳推理的混合问答任务，我们将完整分解的推理路线图编码为单个可解释的 BlendSQL 查询。值得注意的是，我们表明 BlendSQL 可以扩展到海量数据集并提高端到端系统的性能，同时使用的令牌减少 35%。我们的代码可作为包在 https://github.com/parkervg/blendsql 上获取和安装。</li>
</ul>

<h3>Title: Independent Learning in Constrained Markov Potential Games</h3>
<ul>
<li><strong>Authors: </strong>Philip Jordan, Anas Barakat, Niao He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17885">https://arxiv.org/abs/2402.17885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17885">https://arxiv.org/pdf/2402.17885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17885]] Independent Learning in Constrained Markov Potential Games(https://arxiv.org/abs/2402.17885)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Constrained Markov games offer a formal mathematical framework for modeling multi-agent reinforcement learning problems where the behavior of the agents is subject to constraints. In this work, we focus on the recently introduced class of constrained Markov Potential Games. While centralized algorithms have been proposed for solving such constrained games, the design of converging independent learning algorithms tailored for the constrained setting remains an open question. We propose an independent policy gradient algorithm for learning approximate constrained Nash equilibria: Each agent observes their own actions and rewards, along with a shared state. Inspired by the optimization literature, our algorithm performs proximal-point-like updates augmented with a regularized constraint set. Each proximal step is solved inexactly using a stochastic switching gradient algorithm. Notably, our algorithm can be implemented independently without a centralized coordination mechanism requiring turn-based agent updates. Under some technical constraint qualification conditions, we establish convergence guarantees towards constrained approximate Nash equilibria. We perform simulations to illustrate our results.</li>
<li><strong>摘要：</strong>约束马尔可夫博弈提供了一个正式的数学框架，用于建模多智能体强化学习问题，其中智能体的行为受到约束。在这项工作中，我们重点关注最近引入的一类约束马尔可夫势博弈。虽然已经提出了集中式算法来解决此类受限博弈，但针对受限环境定制的收敛独立学习算法的设计仍然是一个悬而未决的问题。我们提出了一种独立的策略梯度算法，用于学习近似约束纳什均衡：每个代理观察自己的行为和奖励，以及共享状态。受优化文献的启发，我们的算法执行使用正则化约束集增强的近端点更新。使用随机切换梯度算法不精确地求解每个近端步骤。值得注意的是，我们的算法可以独立实现，无需需要轮流代理更新的集中协调机制。在某些技术约束条件下，我们建立了对约束近似纳什均衡的收敛保证。我们进行模拟来说明我们的结果。</li>
</ul>

<h3>Title: JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning  and Professional Question Answering Capability</h3>
<ul>
<li><strong>Authors: </strong>Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17887">https://arxiv.org/abs/2402.17887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17887">https://arxiv.org/pdf/2402.17887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17887]] JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning  and Professional Question Answering Capability(https://arxiv.org/abs/2402.17887)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering questions. Our experimental results demonstrate that JMLR-13B (81.2% on Amboos, 61.3% on MedQA) outperforms models using conventional pre-training and fine-tuning Meditron-70B (76.4% on AMBOSS, 60.3% on MedQA). For models of the same 7B scale, JMLR-7B(68.7% on Amboos, 51.7% on MedQA) significantly outperforms other public models (Meditron-7B: 50.1%, 47.9%), proving its superiority in terms of cost (our training time: 37 hours, traditional method: 144 hours), efficiency, and effectiveness in medical question-answering tasks. Through this work, we provide a new and efficient knowledge enhancement tool for healthcare, demonstrating the great potential of integrating IR and LLM training in precision medical information retrieval and question-answering systems.</li>
<li><strong>摘要：</strong>随着医疗数据的爆发式增长和人工智能技术的快速发展，精准医疗已成为提升医疗服务质量和效率的关键。在这种背景下，大型语言模型（LLM）在医学知识获取和问答系统中发挥着越来越重要的作用。为了进一步提高这些系统在医学领域的性能，我们引入了一种创新方法，在微调阶段联合训练信息检索（IR）系统和法学硕士。这种方法，我们称之为联合医学法学硕士和检索训练（JMLR），旨在克服传统模型在处理医学问答任务时面临的挑战。通过采用同步训练机制，JMLR 减少了对计算资源的需求，并增强了模型利用医学知识进行推理和回答问题的能力。我们的实验结果表明，JMLR-13B（Amboos 上为 81.2%，MedQA 上为 61.3%）优于使用传统预训练和微调 Meditron-70B 的模型（AMBOSS 上为 76.4%，MedQA 上为 60.3%）。对于相同 7B 规模的模型，JMLR-7B（Amboos 上为 68.7%，MedQA 上为 51.7%）显着优于其他公共模型（Meditron-7B：50.1%、47.9%），证明了其在成本（我们的训练时间）方面的优越性：37小时，传统方法：144小时），医疗问答任务的效率和效果。通过这项工作，我们为医疗保健提供了一种新的、高效的知识增强工具，展示了将 IR 和 LLM 培训整合到精准医疗信息检索和问答系统中的巨大潜力。</li>
</ul>

<h3>Title: Researchy Questions: A Dataset of Multi-Perspective, Decompositional  Questions for LLM Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Corby Rosset, Ho-Lam Chung, Guanghui Qin, Ethan C. Chau, Zhuo Feng, Ahmed Awadallah, Jennifer Neville, Nikhil Rao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17896">https://arxiv.org/abs/2402.17896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17896">https://arxiv.org/pdf/2402.17896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17896]] Researchy Questions: A Dataset of Multi-Perspective, Decompositional  Questions for LLM Web Agents(https://arxiv.org/abs/2402.17896)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Existing question answering (QA) datasets are no longer challenging to most powerful Large Language Models (LLMs). Traditional QA benchmarks like TriviaQA, NaturalQuestions, ELI5 and HotpotQA mainly study ``known unknowns'' with clear indications of both what information is missing, and how to find it to answer the question. Hence, good performance on these benchmarks provides a false sense of security. A yet unmet need of the NLP community is a bank of non-factoid, multi-perspective questions involving a great deal of unclear information needs, i.e. ``unknown uknowns''. We claim we can find such questions in search engine logs, which is surprising because most question-intent queries are indeed factoid. We present Researchy Questions, a dataset of search engine queries tediously filtered to be non-factoid, ``decompositional'' and multi-perspective. We show that users spend a lot of ``effort'' on these questions in terms of signals like clicks and session length, and that they are also challenging for GPT-4. We also show that ``slow thinking'' answering techniques, like decomposition into sub-questions shows benefit over answering directly. We release $\sim$ 100k Researchy Questions, along with the Clueweb22 URLs that were clicked.</li>
<li><strong>摘要：</strong>现有的问答 (QA) 数据集不再对最强大的大型语言模型 (LLM) 构成挑战。 TriviaQA、NaturalQuestions、ELI5 和 HotpotQA 等传统的 QA 基准主要研究“已知的未知数”，并明确指出缺少哪些信息以及如何找到它来回答问题。因此，这些基准测试的良好表现会带来一种错误的安全感。 NLP 社区尚未满足的需求是一系列非事实的、多视角的问题，涉及大量不明确的信息需求，即“未知的未知数”。我们声称我们可以在搜索引擎日志中找到此类问题，这令人惊讶，因为大多数问题意图查询确实是事实。我们提出了研究问题，这是一个搜索引擎查询的数据集，经过繁琐的过滤，成为非事实、“分解”和多视角的。我们表明，用户在这些问题上花费了大量的“努力”，从点击次数和会话长度等信号来看，他们对 GPT-4 也提出了挑战。我们还表明，“缓慢思考”的回答技巧（例如分解为子问题）比直接回答更有好处。我们发布了 $\sim$ 100k 研究问题，以及被点击的 Clueweb22 URL。</li>
</ul>

<h3>Title: A Language Model based Framework for New Concept Placement in Ontologies</h3>
<ul>
<li><strong>Authors: </strong>Hang Dong, Jiaoyan Chen, Yuan He, Yongsheng Gao, Ian Horrocks</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17897">https://arxiv.org/abs/2402.17897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17897">https://arxiv.org/pdf/2402.17897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17897]] A Language Model based Framework for New Concept Placement in Ontologies(https://arxiv.org/abs/2402.17897)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our framework use fine-tuned PLM for search and a multi-label Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate for the task, and we proposed explainable instruction tuning of LLMs for improved performance. Our study shows the advantages of PLMs and highlights the encouraging performance of LLMs that motivates future studies.</li>
<li><strong>摘要：</strong>我们研究使用语言模型将从文本中提取的新概念插入到本体中的任务。我们探索了一种包含三个步骤的方法：边缘搜索，即找到一组要插入的候选位置（即概念之间的包含），边缘形成和丰富，利用本体结构来生成和增强边缘候选，以及边缘选择最终找到要放入的边缘。在所有步骤中，我们建议利用神经方法，其中我们应用基于嵌入的方法和与预训练语言模型（PLM）的对比学习，例如用于边缘搜索的 BERT，并采用基于 BERT 微调的多标签边缘- 交叉编码器和大型语言模型 (LLM)，例如 GPT 系列、FLAN-T5 和 Llama 2，用于边缘选择。我们评估了使用 SNOMED CT 本体和 MedMentions 实体链接基准创建的最新数据集的方法。我们框架中的最佳设置使用经过微调的 PLM 进行搜索，并使用多标签交叉编码器进行选择。 LLM 的零样本提示仍然不足以完成任务，我们提出了 LLM 的可解释指令调整以提高性能。我们的研究展示了 PLM 的优势，并强调了 LLM 令人鼓舞的表现，这激励了未来的研究。</li>
</ul>

<h3>Title: LLM-Resistant Math Word Problem Generation via Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17916">https://arxiv.org/abs/2402.17916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17916">https://arxiv.org/pdf/2402.17916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17916]] LLM-Resistant Math Word Problem Generation via Adversarial Attacks(https://arxiv.org/abs/2402.17916)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis on math problems and investigate the cause of failure to guide future research on LLM's mathematical capability.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）极大地改变了教育格局。由于当前的抄袭检测工具难以跟上法学硕士的快速发展，教育界面临着在法学硕士存在的情况下评估学生真正解决问题的能力的挑战。在这项工作中，我们探索了一种确保公平评估的新范式——生成对抗性示例，这些示例保留了旨在评估的原始问题的结构和难度，但法学硕士无法解决。专注于数学应用题领域，我们利用抽象语法树在结构上生成对抗性示例，这些示例导致法学硕士通过简单地编辑问题中的数值来产生错误的答案。我们对各种开源和闭源法学硕士进行了实验，定量和定性地证明我们的方法显着降低了他们解决数学问题的能力。我们识别了法学硕士之间的共同漏洞，并提出了一种经济高效的方法来攻击高成本模型。此外，我们对数学问题进行自动分析，并调查失败的原因，以指导未来LLM数学能力的研究。</li>
</ul>

<h3>Title: Pragmatic Instruction Following and Goal Assistance via Cooperative  Language-Guided Inverse Planning</h3>
<ul>
<li><strong>Authors: </strong>Tan Zhi-Xuan, Lance Ying, Vikash Mansinghka, Joshua B. Tenenbaum</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17930">https://arxiv.org/abs/2402.17930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17930">https://arxiv.org/pdf/2402.17930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17930]] Pragmatic Instruction Following and Goal Assistance via Cooperative  Language-Guided Inverse Planning(https://arxiv.org/abs/2402.17930)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>People often give instructions whose meaning is ambiguous without further context, expecting that their actions or goals will disambiguate their intentions. How can we build assistive agents that follow such instructions in a flexible, context-sensitive manner? This paper introduces cooperative language-guided inverse plan search (CLIPS), a Bayesian agent architecture for pragmatic instruction following and goal assistance. Our agent assists a human by modeling them as a cooperative planner who communicates joint plans to the assistant, then performs multimodal Bayesian inference over the human's goal from actions and language, using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan. Given this posterior, our assistant acts to minimize expected goal achievement cost, enabling it to pragmatically follow ambiguous instructions and provide effective assistance even when uncertain about the goal. We evaluate these capabilities in two cooperative planning domains (Doors, Keys & Gems and VirtualHome), finding that CLIPS significantly outperforms GPT-4V, LLM-based literal instruction following and unimodal inverse planning in both accuracy and helpfulness, while closely matching the inferences and assistive judgments provided by human raters.</li>
<li><strong>摘要：</strong>人们经常在没有进一步上下文的情况下发出含义模糊的指令，期望他们的行为或目标能够消除他们的意图的歧义。我们如何构建以灵活、上下文敏感的方式遵循此类指令的辅助代理？本文介绍了协作语言引导逆向计划搜索（CLIPS），这是一种用于语用指令跟踪和目标辅助的贝叶斯代理架构。我们的代理通过将人类建模为合作规划者来协助人类，该规划者将联合计划传达给助手，然后根据行为和语言对人类的目标进行多模态贝叶斯推理，使用大型语言模型 (LLM) 来评估给定指令的可能性。假设的计划。考虑到这个后验，我们的助手会采取行动，尽量减少预期目标实现成本，使其能够务实地遵循模糊的指令，即使在目标不确定的情况下也能提供有效的帮助。我们在两个合作规划领域（Doors、Keys & Gems 和 VirtualHome）中评估了这些功能，发现 CLIPS 在准确性和有用性方面显着优于 GPT-4V、基于 LLM 的文字指令跟踪和单峰逆向规划，同时与推论和帮助紧密匹配。由人类评估者提供的辅助判断。</li>
</ul>

<h3>Title: Multitask Multilingual Model Adaptation with Featurized Low-Rank  Mixtures</h3>
<ul>
<li><strong>Authors: </strong>Chu-Cheng Lin, Xinyi Wang, Jonathan H. Clark, Han Lu, Yun Zhu, Chenxi Whitehouse, Hongkun Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17934">https://arxiv.org/abs/2402.17934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17934">https://arxiv.org/pdf/2402.17934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17934]] Multitask Multilingual Model Adaptation with Featurized Low-Rank  Mixtures(https://arxiv.org/abs/2402.17934)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads to significant improvements over a variety of tasks for both supervised learning and zero-shot settings using different training data mixtures.</li>
<li><strong>摘要：</strong>将预训练的大型语言模型 (LLM) 应用于数十或数百种人类语言的各种下游任务在计算上是昂贵的。参数高效微调（PEFT）通过仅调整少量参数来显着降低适应成本。然而，由于有限的参数容量和不同数据集之间的负面干扰，直接将 LoRA（Hu et al., 2022）等 PEFT 方法应用于不同的数据集混合可能会导致性能不佳。在这项工作中，我们提出了 Featurized Low-rank Mixtures (FLix)，这是一种新颖的 PEFT 方法，专为有效的多任务多语言调优而设计。 FLix 将每个独特的数据集特征（例如数据集的语言或任务）与其自己的低秩权重更新参数相关联。通过为每个数据集组合特定于特征的参数，FLix 可以适应不同的数据集混合，并更好地泛化到未见过的数据集。我们的实验表明，FLix 使用不同的训练数据混合对监督学习和零样本设置的各种任务带来了显着的改进。</li>
</ul>

<h3>Title: Acquiring Linguistic Knowledge from Multimodal Input</h3>
<ul>
<li><strong>Authors: </strong>Theodor Amariucai, Alex Warstadt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17936">https://arxiv.org/abs/2402.17936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17936">https://arxiv.org/pdf/2402.17936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17936]] Acquiring Linguistic Knowledge from Multimodal Input(https://arxiv.org/abs/2402.17936)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In contrast to children, language models (LMs) exhibit considerably inferior data efficiency when acquiring language. In this submission to the BabyLM Challenge (Warstadt et al., 2023), we test the hypothesis that this data efficiency gap is partly caused by a lack of multimodal input and grounding in the learning environment of typical language models. Although previous work looking into this question found that multimodal training can even harm language-only performance, we speculate that these findings can be attributed to catastrophic forgetting of complex language due to fine-tuning on captions data. To test our hypothesis, we perform an ablation study on FLAVA (Singh et al., 2022), a multimodal vision-and-language model, independently varying the volume of text and vision input to quantify how much text data (if any) can be offset by vision at different data scales. We aim to limit catastrophic forgetting through a multitask pretraining regime that includes unimodal text-only tasks and data sampled from WiT, the relatively diverse Wikipedia-based dataset (Srinivasan et al., 2021). Our results are largely negative: Multimodal pretraining does not harm our models' language performance but does not consistently help either. That said, our conclusions are limited by our having been able to conduct only a small number of runs. While we must leave open the possibility that multimodal input explains some of the gap in data efficiency between LMs and humans, positive evidence for this hypothesis will require better architectures and techniques for multimodal training.</li>
<li><strong>摘要：</strong>与儿童相比，语言模型（LM）在获取语言时表现出相当低的数据效率。在提交给 BabyLM 挑战赛（Warstadt 等人，2023）的文章中，我们测试了这样一个假设：这种数据效率差距部分是由于缺乏多模态输入和典型语言模型学习环境的基础造成的。尽管之前研究这个问题的工作发现多模式训练甚至会损害纯语言的表现，但我们推测这些发现可以归因于由于字幕数据的微调而导致的复杂语言的灾难性遗忘。为了检验我们的假设，我们对 FLAVA（Singh 等人，2022）进行了消融研究，FLAVA 是一种多模态视觉和语言模型，独立地改变文本和视觉输入的量，以量化有多少文本数据（如果有）可以被不同数据尺度的视觉所抵消。我们的目标是通过多任务预训练机制来限制灾难性遗忘，该机制包括单模式纯文本任务和从 WiT（相对多样化的基于维基百科的数据集）采样的数据（Srinivasan 等人，2021）。我们的结果在很大程度上是负面的：多模式预训练不会损害我们模型的语言性能，但也不能始终如一地有所帮助。也就是说，我们的结论受到我们只能进行少量运行的限制。虽然我们必须保留多模态输入解释 LM 和人类之间数据效率差距的可能性，但这一假设的积极证据将需要更好的多模态训练架构和技术。</li>
</ul>

<h3>Title: Large Language Models on Tabular Data -- A Survey</h3>
<ul>
<li><strong>Authors: </strong>Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, Christos Faloutsos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17944">https://arxiv.org/abs/2402.17944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17944">https://arxiv.org/pdf/2402.17944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17944]] Large Language Models on Tabular Data -- A Survey(https://arxiv.org/abs/2402.17944)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.</li>
<li><strong>摘要：</strong>大语言建模的最新突破促进了对其在与表格数据建模相关的各种任务中的应用的严格探索，例如预测、表格数据合成、问答和表格理解。每项任务都带来独特的挑战和机遇。然而，目前缺乏对该研究领域的关键技术、指标、数据集、模型和优化方法进行总结和比较的全面综述。本调查旨在通过巩固这些领域的最新进展，对所使用的数据集、指标和方法进行全面的调查和分类来解决这一差距。它确定了现有文献中的优势、局限性、未探索的领域和差距，同时为这个重要且快速发展的领域的未来研究方向提供了一些见解。它还提供了相关代码和数据集参考。通过这次全面的审查，我们希望为感兴趣的读者提供相关的参考资料和富有洞察力的观点，为他们提供必要的工具和知识，以有效地应对和解决该领域普遍存在的挑战。</li>
</ul>

<h3>Title: Gradient-Free Adaptive Global Pruning for Pre-trained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guangji Bai, Yijiang Li, Chen Ling, Kibaek Kim, Liang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17946">https://arxiv.org/abs/2402.17946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17946">https://arxiv.org/pdf/2402.17946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17946]] Gradient-Free Adaptive Global Pruning for Pre-trained Language Models(https://arxiv.org/abs/2402.17946)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods.</li>
<li><strong>摘要：</strong>LLaMA 和 GPT 等大型语言模型 (LLM) 对自然语言处理的变革性影响与其过高的计算需求相抵消。剪枝已成为一种关键的压缩策略，通过引入稀疏性来提高内存和计算效率。然而，由于可扩展性问题，传统的全局修剪对于法学硕士来说是不切实际的，而局部修剪尽管效率很高，但会导致解决方案次优。为了应对这些挑战，我们提出了自适应全局剪枝（AdaGP），这是一种新颖的框架，它将全局剪枝过程重新定义为可管理的、协调的子问题，从而实现具有全局最优性的资源高效优化。 AdaGP 的方法将 LLM 概念化为一系列模块化函数，并利用辅助变量进行问题分解，不仅促进了 LLM 的实用应用，而且还展示了显着的性能改进，特别是在高稀疏性情况下，它超越了当前的状态-艺术方法。</li>
</ul>

<h3>Title: An Iterative Associative Memory Model for Empathetic Response Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhou Yang, Zhaochun Ren, Yufeng Wang, Chao Chen, Haizhou Sun, Xiaofei Zhu, Xiangwen Liao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17959">https://arxiv.org/abs/2402.17959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17959">https://arxiv.org/pdf/2402.17959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17959]] An Iterative Associative Memory Model for Empathetic Response Generation(https://arxiv.org/abs/2402.17959)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Empathetic response generation is to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Meanwhile, variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression.</li>
<li><strong>摘要：</strong>共情反应的产生是理解对话话语中的认知和情感状态并产生适当的反应。心理学理论认为，理解情绪和认知状态需要迭代地捕捉和理解对话话语中的相关单词。然而，现有的方法将对话话语视为长序列或独立的话语来进行理解，这很容易忽略它们之间的关联词。为了解决这个问题，我们提出了一种迭代联想记忆模型（IAMM）来生成同理心响应。具体来说，我们采用一种新颖的二阶交互注意机制来迭代捕获对话话语和情境、对话历史和记忆模块（用于存储关联词）之间的重要关联词，从而准确而细致地理解话语。我们在 Empathetic-Dialogue 数据集上进行了实验。自动评估和人工评估都验证了模型的有效性。同时，法学硕士的变体实验也表明，关注相关单词可以提高同理心理解和表达能力。</li>
</ul>

<h3>Title: Sample-Efficient Preference-based Reinforcement Learning with Dynamics  Aware Rewards</h3>
<ul>
<li><strong>Authors: </strong>Katherine Metcalf, Miguel Sarabia, Natalie Mackraz, Barry-John Theobald</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17975">https://arxiv.org/abs/2402.17975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17975">https://arxiv.org/pdf/2402.17975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17975]] Sample-Efficient Preference-based Reinforcement Learning with Dynamics  Aware Rewards(https://arxiv.org/abs/2402.17975)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Preference-based reinforcement learning (PbRL) aligns a robot behavior with human preferences via a reward function learned from binary feedback over agent behaviors. We show that dynamics-aware reward functions improve the sample efficiency of PbRL by an order of magnitude. In our experiments we iterate between: (1) learning a dynamics-aware state-action representation (z^{sa}) via a self-supervised temporal consistency task, and (2) bootstrapping the preference-based reward function from (z^{sa}), which results in faster policy learning and better final policy performance. For example, on quadruped-walk, walker-walk, and cheetah-run, with 50 preference labels we achieve the same performance as existing approaches with 500 preference labels, and we recover 83\% and 66\% of ground truth reward policy performance versus only 38\% and 21\%. The performance gains demonstrate the benefits of explicitly learning a dynamics-aware reward model. Repo: \texttt{https://github.com/apple/ml-reed}.</li>
<li><strong>摘要：</strong>基于偏好的强化学习 (PbRL) 通过从代理行为的二元反馈中学习到的奖励函数，使机器人行为与人类偏好保持一致。我们表明，动态感知奖励函数将 PbRL 的样本效率提高了一个数量级。在我们的实验中，我们迭代：（1）通过自我监督的时间一致性任务学习动态感知的状态动作表示（z^{sa}），以及（2）从（z^）引导基于偏好的奖励函数{sa}），这会导致更快的策略学习和更好的最终策略性能。例如，在四足行走、步行者行走和猎豹奔跑上，使用 50 个偏好标签，我们实现了与使用 500 个偏好标签的现有方法相同的性能，并且我们恢复了 83% 和 66% 的地面真实奖励策略性能而只有 38% 和 21%。性能提升证明了显式学习动态感知奖励模型的好处。仓库：\texttt{https://github.com/apple/ml-reed}。</li>
</ul>

<h3>Title: Imagine, Initialize, and Explore: An Effective Exploration Method in  Multi-Agent Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zeyang Liu, Lipeng Wan, Xinrui Yang, Zhuoran Chen, Xingyu Chen, Xuguang Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17978">https://arxiv.org/abs/2402.17978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17978">https://arxiv.org/pdf/2402.17978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17978]] Imagine, Initialize, and Explore: An Effective Exploration Method in  Multi-Agent Reinforcement Learning(https://arxiv.org/abs/2402.17978)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, agent</a></li>
<li><strong>Abstract: </strong>Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively. The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models.</li>
<li><strong>摘要：</strong>有效的探索对于发现复杂协调任务中多智能体强化学习（MARL）的最佳策略至关重要。现有方法主要利用内在奖励来实现致力于探索或使用基于角色的学习来分解联合动作空间，而不是直接在整个动作观察空间中进行集体搜索。然而，他们经常面临获得特定联合行动序列以在长期任务中达到成功状态的挑战。为了解决这一限制，我们提出了想象、初始化和探索（IIE），这是一种新颖的方法，为复杂场景中的高效多智能体探索提供了一种有前景的解决方案。 IIE 采用变压器模型来想象智能体如何达到可以影响彼此转换函数的临界状态。然后，我们在探索阶段之前使用模拟器初始化此状态下的环境。我们将想象力表述为一个序列建模问题，其中状态、观察、提示、动作和奖励都是自回归预测的。提示由时间步长、返回时间、影响值和一次性演示组成，指定所需的状态和轨迹，并指导动作生成。通过在关键状态下初始化代理，IIE 显着增加了发现潜在重要的未开发区域的可能性。尽管很简单，但实证结果表明，我们的方法在星际争霸多智能体挑战（SMAC）和 SMACv2 环境中优于多智能体探索基线。特别是，IIE 在稀疏奖励 SMAC 任务中表现出改进的性能，并且比其他生成方法（例如 CVAE-GAN 和扩散模型）在初始化状态上生成更有效的课程。</li>
</ul>

<h3>Title: Collaborative decoding of critical tokens for boosting factuality of  large language models</h3>
<ul>
<li><strong>Authors: </strong>Lifeng Jin, Baolin Peng, Linfeng Song, Haitao Mi, Ye Tian, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17982">https://arxiv.org/abs/2402.17982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17982">https://arxiv.org/pdf/2402.17982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17982]] Collaborative decoding of critical tokens for boosting factuality of  large language models(https://arxiv.org/abs/2402.17982)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>The most common training pipeline for large language models includes pretraining, finetuning and aligning phases, with their respective resulting models, such as the pretrained model and the finetuned model. Finetuned and aligned models show improved abilities of instruction following and safe generation, however their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination. In this work, we introduce a collaborative decoding framework to harness the high factuality within pretrained models through the concept of critical tokens. We first design a critical token classifier to decide which model to use for the next token, and subsequently generates the next token using different decoding strategies. Experiments with different models and datasets show that our decoding framework is able to reduce model hallucination significantly, showcasing the importance of the collaborative decoding framework.</li>
<li><strong>摘要：</strong>大型语言模型最常见的训练流程包括预训练、微调和对齐阶段，及其各自的结果模型，例如预训练模型和微调模型。经过微调和对齐的模型显示出更高的指令跟随和安全生成能力，但是它们对世界保持真实态度的能力受到微调过程的影响。此外，在生成过程中使用采样的常见做法也会增加产生幻觉的机会。在这项工作中，我们引入了一种协作解码框架，通过关键令牌的概念来利用预训练模型中的高真实性。我们首先设计一个关键的令牌分类器来决定下一个令牌使用哪个模型，然后使用不同的解码策略生成下一个令牌。对不同模型和数据集的实验表明，我们的解码框架能够显着减少模型幻觉，展示了协作解码框架的重要性。</li>
</ul>

<h3>Title: FlattenQuant: Breaking Through the Inference Compute-bound for Large  Language Models with Per-tensor Quantization</h3>
<ul>
<li><strong>Authors: </strong>Yi Zhang, Fei Yang, Shuang Peng, Fangyu Wang, Aimin Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17985">https://arxiv.org/abs/2402.17985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17985">https://arxiv.org/pdf/2402.17985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17985]] FlattenQuant: Breaking Through the Inference Compute-bound for Large  Language Models with Per-tensor Quantization(https://arxiv.org/abs/2402.17985)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated state-of-the-art performance across various tasks. However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance. Recently, there have been some efficient attempts to quantize LLMs, yet inference with large batch size or long sequence still has the issue of being compute-bound. Fine-grained quantization methods have showcased their proficiency in achieving low-bit quantization for LLMs, while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with large batch size or long sequence. In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the large channels in the tensor, to achieve low bit per-tensor quantization with minimal accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in LLMs, with the remaining layers using 8 bits. The 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by large matrix calculation. Our work achieves up to 2$\times$ speedup and 2.3$\times$ memory reduction for LLMs with negligible loss in accuracy.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种任务中都展示了最先进的性能。然而，LLM 的推理延迟和大量 GPU 内存消耗限制了其部署性能。最近，已经有一些量化 LLM 的有效尝试，但大批量或长序列的推理仍然存在计算限制的问题。细粒度量化方法展示了它们在实现 LLM 低位量化方面的熟练程度，同时需要 FP16 数据类型进行线性层计算，这在处理大批量或长序列时非常耗时。在本文中，我们介绍了一种称为 FlattenQuant 的方法，该方法通过展平张量中的大通道来显着降低张量的最大值，以实现每张量的低比特量化，同时精度损失最小。我们的实验表明，FlattenQuant 可以直接使用 4 位来实现 LLM 中 48.29% 的线性层计算，其余层使用 8 位。 FlattenQuant方法中引入的4位矩阵乘法可以有效解决大矩阵计算带来的计算限制。我们的工作为法学硕士实现了高达 2$\times$ 的加速和 2.3$\times$ 的内存减少，而准确性损失可以忽略不计。</li>
</ul>

<h3>Title: Exploring Multi-Document Information Consolidation for Scientific  Sentiment Summarization</h3>
<ul>
<li><strong>Authors: </strong>Miao Li, Jey Han Lau, Eduard Hovy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18005">https://arxiv.org/abs/2402.18005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18005">https://arxiv.org/pdf/2402.18005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18005]] Exploring Multi-Document Information Consolidation for Scientific  Sentiment Summarization(https://arxiv.org/abs/2402.18005)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information. To make scientific sentiment summarization more grounded, we hypothesize that in peer review human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews and it represents the logic of summarizing scientific sentiments in meta-review generation. The framework is validated via human annotation. Based on the framework, we propose evaluation metrics to assess the quality of generated meta-reviews, and we find that the hypothesis of the sentiment consolidation framework works out empirically when we incorporate it as prompts for LLMs to generate meta-reviews in extensive experiments.</li>
<li><strong>摘要：</strong>具有法学硕士的现代自然语言生成系统具有生成多个文档的合理摘要的能力；然而，模型是否真正具有信息整合生成摘要的能力尚不确定，特别是对于那些含有固执己见信息的源文档。为了使科学情感总结更有根据，我们假设在同行评审中，人类元审稿人遵循情感巩固的三层框架来撰写元评论，这代表了元评论生成中总结科学情感的逻辑。该框架通过人工注释进行验证。基于该框架，我们提出了评估指标来评估生成的元评论的质量，并且我们发现，当我们将其作为法学硕士在广泛的实验中生成元评论的提示时，情绪巩固框架的假设在经验上是有效的。</li>
</ul>

<h3>Title: A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, Ying Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18013">https://arxiv.org/abs/2402.18013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18013">https://arxiv.org/pdf/2402.18013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18013]] A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems(https://arxiv.org/abs/2402.18013)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.</li>
<li><strong>摘要：</strong>本次调查对多轮对话系统的研究进行了全面的回顾，特别关注基于大语言模型（LLM）的多轮对话系统。本文旨在 (a) 总结现有的法学硕士以及使法学硕士适应下游任务的方法； (b) 阐述多轮对话系统的最新进展，涵盖基于法学硕士的开放域对话（ODD）和面向任务的对话（TOD）系统，以及数据集和评估指标； (c) 讨论由于法学硕士的发展以及对多轮对话系统日益增长的需求而产生的一些未来重点和近期研究问题。</li>
</ul>

<h3>Title: Do Large Language Models Mirror Cognitive Language Processing?</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Ren, Renren Jin, Tongxuan Zhang, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18023">https://arxiv.org/abs/2402.18023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18023">https://arxiv.org/pdf/2402.18023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18023]] Do Large Language Models Mirror Cognitive Language Processing?(https://arxiv.org/abs/2402.18023)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks. As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing. Or to what extend LLMs resemble cognitive language processing? In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment. Experimental results indicate that model scaling is positively correlated with LLM-brain similarity, and alignment training can significantly improve LLM-brain similarity. Additionally, the performance of a wide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated with the LLM-brain similarity.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在文本理解和逻辑推理方面表现出了卓越的能力，在众多认知任务中达到甚至超越了人类水平。由于法学硕士是根据人类语言认知的大量文本输出进行训练的，因此很自然地会问法学硕士是否反映了认知语言处理。或者法学硕士在何种程度上类似于认知语言处理？在本文中，我们提出了一种连接 LLM 表示和人类认知信号的新方法，以评估 LLM 模拟认知语言处理的有效性。我们采用表征相似性分析 (RSA) 来测量 16 个主流 LLM 与大脑 fMRI 信号之间的一致性。我们凭经验研究了各种因素（例如模型缩放、对齐训练、指令附加）对这种 LLM-大脑对齐的影响。实验结果表明，模型缩放与LLM-大脑相似度呈正相关，对齐训练可以显着提高LLM-大脑相似度。此外，各种 LLM 评估（例如 MMLU、Chatbot Arena）的表现与 LLM 大脑相似性高度相关。</li>
</ul>

<h3>Title: Hire a Linguist!: Learning Endangered Languages with In-Context  Linguistic Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Kexun Zhang, Yee Man Choi, Zhenqiao Song, Taiqi He, William Yang Wang, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18025">https://arxiv.org/abs/2402.18025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18025">https://arxiv.org/pdf/2402.18025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18025]] Hire a Linguist!: Learning Endangered Languages with In-Context  Linguistic Descriptions(https://arxiv.org/abs/2402.18025)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>How can large language models (LLMs) process and translate endangered languages? Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages. On the contrary, we observe that 2000 endangered languages, though without a large corpus, have a grammar book or a dictionary. We propose LINGOLLM, a training-free approach to enable an LLM to process unseen languages that hardly occur in its pre-training. Our key insight is to demonstrate linguistic knowledge of an unseen language in an LLM's prompt, including a dictionary, a grammar book, and morphologically analyzed input text. We implement LINGOLLM on top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks across 8 endangered or low-resource languages. Our results show that LINGOLLM elevates translation capability from GPT-4's 0 to 10.5 BLEU for 10 language directions. Our findings demonstrate the tremendous value of linguistic knowledge in the age of LLMs for endangered languages. Our data, code, and model generations can be found at https://github.com/LLiLab/llm4endangeredlang.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 如何处理和翻译濒危语言？许多语言缺乏大型语料库来培养像样的法学硕士；因此，现有的法学硕士很少能在未见过的濒危语言方面表现良好。相反，我们观察到2000种濒临灭绝的语言虽然没有大型语料库，但却有一本语法书或一本词典。我们提出了 LINGOLLM，这是一种免训练的方法，使法学硕士能够处理在预训练中几乎不会出现的未见过的语言。我们的主要见解是在法学硕士的提示中展示一种看不见的语言的语言知识，包括字典、语法书和形态分析的输入文本。我们在 GPT-4 和 Mixtral 这两个模型之上实现 LINGOLLM，并评估它们在 8 种濒危或资源匮乏语言的 5 项任务上的性能。我们的结果表明，LINGOLLM 将 10 种语言方向的翻译能力从 GPT-4 的 0 BLEU 提升到 10.5 BLEU。我们的研究结果证明了语言知识在法学硕士时代对于濒危语言的巨大价值。我们的数据、代码和模型生成可以在 https://github.com/LLiLab/llm4endangeredlang 找到。</li>
</ul>

<h3>Title: ResLoRA: Identity Residual Mapping in Low-Rank Adaption</h3>
<ul>
<li><strong>Authors: </strong>Shuhua Shi, Shaohan Huang, Minghui Song, Zhoujun Li, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18039">https://arxiv.org/abs/2402.18039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18039">https://arxiv.org/pdf/2402.18039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18039]] ResLoRA: Identity Residual Mapping in Low-Rank Adaption(https://arxiv.org/abs/2402.18039)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora .</li>
<li><strong>摘要：</strong>作为最流行的参数高效微调（PEFT）方法之一，低秩适应（LoRA）通常应用于微调大型语言模型（LLM）。然而，由于原始模型中的计算路径较长，有效且快速地更新 LoRA 块的权重具有挑战性。为了解决这个问题，我们提出了 ResLoRA，这是 LoRA 的改进框架。与 LoRA 相比，通过在训练期间添加剩余路径并使用合并方法在推理期间消除这些额外路径，我们的方法可以在更少的训练步骤中获得更好的结果，而无需任何额外的可训练参数或推理成本。 NLG、NLU 和文本到图像任务的实验证明了我们方法的有效性。据我们所知，ResLoRA是第一个将残差路径与LoRA相结合的工作。我们方法的代码可在 https://github.com/microsoft/LMOps/tree/main/reslora 获取。</li>
</ul>

<h3>Title: Automated Discovery of Integral with Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxin Yin</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18040">https://arxiv.org/abs/2402.18040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18040">https://arxiv.org/pdf/2402.18040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18040]] Automated Discovery of Integral with Deep Learning(https://arxiv.org/abs/2402.18040)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in the realm of deep learning, particularly in the development of large language models (LLMs), have demonstrated AI's ability to tackle complex mathematical problems or solving programming challenges. However, the capability to solve well-defined problems based on extensive training data differs significantly from the nuanced process of making scientific discoveries. Trained on almost all human knowledge available, today's sophisticated LLMs basically learn to predict sequences of tokens. They generate mathematical derivations and write code in a similar way as writing an essay, and do not have the ability to pioneer scientific discoveries in the manner a human scientist would do. In this study we delve into the potential of using deep learning to rediscover a fundamental mathematical concept: integrals. By defining integrals as area under the curve, we illustrate how AI can deduce the integral of a given function, exemplified by inferring $\int_{0}^{x} t^2 dt = \frac{x^3}{3}$ and $\int_{0}^{x} ae^{bt} dt = \frac{a}{b} e^{bx} - \frac{a}{b}$. Our experiments show that deep learning models can approach the task of inferring integrals either through a sequence-to-sequence model, akin to language translation, or by uncovering the rudimentary principles of integration, such as $\int_{0}^{x} t^n dt = \frac{x^{n+1}}{n+1}$.</li>
<li><strong>摘要：</strong>深度学习领域的最新进展，特别是大型语言模型 (LLM) 的开发，已经证明了人工智能处理复杂数学问题或解决编程挑战的能力。然而，基于大量训练数据解决明确问题的能力与做出科学发现的微妙过程有很大不同。当今复杂的法学硕士接受了几乎所有人类可用知识的培训，基本上学会了预测标记序列。他们以类似于写论文的方式生成数学推导并编写代码，并且不具备以人类科学家的方式开拓科学发现的能力。在这项研究中，我们深入研究了使用深度学习重新发现基本数学概念：积分的潜力。通过将积分定义为曲线下的面积，我们说明了 AI 如何推导给定函数的积分，例如推断 $\int_{0}^{x} t^2 dt = \frac{x^3}{3} $ 和 $\int_{0}^{x} ae^{bt} dt = \frac{a}{b} e^{bx} - \frac{a}{b}$。我们的实验表明，深度学习模型可以通过序列到序列模型（类似于语言翻译）或通过揭示积分的基本原理来完成推断积分的任务，例如 $\int_{0}^{x} t^n dt = \frac{x^{n+1}}{n+1}$。</li>
</ul>

<h3>Title: Datasets for Large Language Models: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18041">https://arxiv.org/abs/2402.18041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18041">https://arxiv.org/pdf/2402.18041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18041]] Datasets for Large Language Models: A Comprehensive Survey(https://arxiv.org/abs/2402.18041)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.</li>
<li><strong>摘要：</strong>本文着手探索大型语言模型（LLM）数据集，该数据集在 LLM 的显着进步中发挥着至关重要的作用。这些数据集作为基础设施，类似于维持和培育法学硕士发展的根系统。因此，对这些数据集的检查成为研究中的一个关键主题。为了解决目前LLM数据集缺乏全面概述和深入分析的问题，深入了解LLM数据集的现状和未来趋势，本次调查从五个角度对LLM数据集的基本方面进行了整合和分类：（1）预训练语料库； (2)指令微调数据集； (3) 偏好数据集； (4) 评估数据集； (5) 传统自然语言处理（NLP）数据集。该调查揭示了当前的挑战，并指出了未来调查的潜在途径。此外，还提供了对现有可用数据集资源的全面回顾，包括来自 444 个数据集的统计数据，涵盖 8 个语言类别，跨越 32 个领域。 20个维度的信息被纳入数据集统计中。调查的预训练语料库总数据量超过 774.5 TB，其他数据集的总数据量超过 7 亿个实例。我们的目标是展示法学硕士文本数据集的整体概况，为该领域的研究人员提供全面的参考，并为未来的研究做出贡献。相关资源可访问：https://github.com/lmmlzn/Awesome-LLMs-Datasets。</li>
</ul>

<h3>Title: Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using  FActScore</h3>
<ul>
<li><strong>Authors: </strong>Sheikh Shafayat, Eunsu Kim, Juhyun Oh, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18045">https://arxiv.org/abs/2402.18045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18045">https://arxiv.org/pdf/2402.18045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18045]] Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using  FActScore(https://arxiv.org/abs/2402.18045)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge. While extensive research has addressed this in English, little is known about multilingual LLMs. This paper systematically evaluates multilingual LLMs' factual accuracy across languages and geographic regions. We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages. Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts. Furthermore, multilingual models demonstrate a bias towards factual information from Western continents. These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in LLMs' fact generation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 很容易出现事实幻觉，生成与既定知识相矛盾的文本。虽然广泛的研究已经用英语解决了这个问题，但人们对多语言法学硕士知之甚少。本文系统地评估了多语言法学硕士跨语言和地理区域的事实准确性。我们引入了一种用于多语言事实性评估的新颖管道，针对不同语言采用 FActScore（Min 等人，2023）。我们对九种语言的分析表明，英语在事实准确性和生成事实的数量方面始终优于其他语言。此外，多语言模型表现出对来自西方大陆的事实信息的偏见。这些发现强调了改进多语言事实评估的必要性，并强调了法学硕士事实生成中的地域偏见。</li>
</ul>

<h3>Title: Characterizing Truthfulness in Large Language Model Generations with  Local Intrinsic Dimension</h3>
<ul>
<li><strong>Authors: </strong>Fan Yin, Jayanth Srinivasa, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18048">https://arxiv.org/abs/2402.18048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18048">https://arxiv.org/pdf/2402.18048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18048]] Characterizing Truthfulness in Large Language Model Generations with  Local Intrinsic Dimension(https://arxiv.org/abs/2402.18048)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness ohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs.</li>
<li><strong>摘要：</strong>我们研究如何表征和预测大型语言模型（LLM）生成的文本的真实性，这是在人类和 LLM 之间建立信任的关键一步。尽管已经提出了几种基于熵或语言不确定性的方法来校准模型预测，但这些方法通常很棘手，对超参数敏感，并且在应用于法学硕士的生成任务时可靠性较差。在本文中，我们建议研究内部激活并使用模型激活的局部内在维度（LID）来量化 LLM 的真实性。通过对四个问答（QA）数据集的实验，我们证明了我们提出的方法的有效性。此外，我们研究了法学硕士的内在维度及其与模型层、自回归语言建模和法学硕士训练的关系，揭示了内在维度可以成为理解法学硕士的强大方法。</li>
</ul>

<h3>Title: MEGAnno+: A Human-LLM Collaborative Annotation System</h3>
<ul>
<li><strong>Authors: </strong>Hannah Kim, Kushan Mitra, Rafael Li Chen, Sajjadur Rahman, Dan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18050">https://arxiv.org/abs/2402.18050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18050">https://arxiv.org/pdf/2402.18050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18050]] MEGAnno+: A Human-LLM Collaborative Annotation System(https://arxiv.org/abs/2402.18050)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.</li>
<li><strong>摘要：</strong>对于各种 NLP 任务，大型​​语言模型 (LLM) 可以比人类更快、更便宜地标记数据。尽管法学硕士实力雄厚，但他们可能无法理解复杂的、社会文化的或特定领域的背景，从而可能导致错误的注释。因此，我们提倡人类和法学硕士共同努力生产可靠和高质量的标签的协作方法。我们推出 MEGAnno+，一个人与 LLM 协作注释系统，提供有效的 LLM 代理和注释管理、方便且强大的 LLM 注释以及人类对 LLM 标签的探索性验证。</li>
</ul>

<h3>Title: Token-Specific Watermarking with Enhanced Detectability and Semantic  Coherence for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mingjia Huo, Sai Ashish Somayajula, Youwei Liang, Ruisi Zhang, Farinaz Koushanfar, Pengtao Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18059">https://arxiv.org/abs/2402.18059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18059">https://arxiv.org/pdf/2402.18059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18059]] Token-Specific Watermarking with Enhanced Detectability and Semantic  Coherence for Large Language Models(https://arxiv.org/abs/2402.18059)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark .</li>
<li><strong>摘要：</strong>大型语言模型会生成带有潜在错误信息的高质量响应，这凸显了通过区分人工智能生成的文本和人类编写的文本进行监管的必要性。水印在这种情况下至关重要，它涉及在 LLM 推理阶段在文本中嵌入隐藏标记，而人类无法察觉。然而，当前的水印算法面临着同时实现插入水印的可检测性和生成文本的语义完整性的挑战，其中增强一方面往往会破坏另一方面。为了克服这个问题，我们引入了一种新颖的多目标优化（MOO）水印方法，该方法利用轻量级网络生成特定于令牌的水印逻辑和分割比率。通过利用 MOO 优化检测和语义目标函数，我们的方法同时实现了可检测性和语义完整性。实验结果表明，我们的方法在增强 LLM 生成的文本的可检测性同时保持其语义一致性方面优于当前的水印技术。我们的代码可在 https://github.com/mignonjia/TS_watermark 获取。</li>
</ul>

<h3>Title: Benchmarking Large Language Models on Answering and Explaining  Challenging Medical Questions</h3>
<ul>
<li><strong>Authors: </strong>Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18060">https://arxiv.org/abs/2402.18060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18060">https://arxiv.org/pdf/2402.18060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18060]] Benchmarking Large Language Models on Answering and Explaining  Challenging Medical Questions(https://arxiv.org/abs/2402.18060)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>LLMs have demonstrated impressive performance in answering medical questions, such as passing medical licensing examinations. However, most existing benchmarks rely on board exam questions or general medical questions, falling short in capturing the complexity of realistic clinical cases. Moreover, the lack of reference explanations for answers hampers the evaluation of model explanations, which are crucial to supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style clinical questions. Both datasets are structured as multiple-choice question-answering tasks, where each question is accompanied by an expert-written explanation. We evaluate four LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. The inconsistency between automatic and human evaluations of model-generated explanations highlights the need to develop new metrics to support future research on explainable medical QA.</li>
<li><strong>摘要：</strong>法学硕士在回答医学问题（例如通过医疗执照考试）方面表现出了令人印象深刻的表现。然而，大多数现有基准依赖于委员会考试问题或一般医学问题，无法捕捉现实临床病例的复杂性。此外，缺乏答案的参考解释阻碍了模型解释的评估，而模型解释对于支持医生做出复杂的医疗决策至关重要。为了应对这些挑战，我们构建了两个新的数据集：JAMA Clinical Challenge 和 Medbullets。 JAMA 临床挑战赛由基于挑战性临床病例的问题组成，而 Medbullets 则由 USMLE 第 2 步和第 3 步风格的临床问题组成。这两个数据集的结构都是多项选择题回答任务，其中每个问题都附有专家编写的解释。我们使用各种提示在两个数据集上评估四个法学硕士。实验表明我们的数据集比以前的基准更难。对模型生成的解释的自动评估和人工评估之间的不一致凸显了开发新指标以支持可解释的医学 QA 未来研究的必要性。</li>
</ul>

<h3>Title: No Token Left Behind: Reliable KV Cache Compression via Importance-Aware  Mixed Precision Quantization</h3>
<ul>
<li><strong>Authors: </strong>June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18096">https://arxiv.org/abs/2402.18096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18096">https://arxiv.org/pdf/2402.18096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18096]] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware  Mixed Precision Quantization(https://arxiv.org/abs/2402.18096)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.</li>
<li><strong>摘要：</strong>键值 (KV) 缓存已成为加速生成式大型语言模型 (LLM) 的推理速度和吞吐量的重要技术。然而，KV 缓存的内存占用在 LLM 部署中构成了关键瓶颈，因为缓存大小随着批量大小和序列长度而增长，甚至常常超过模型本身的大小。尽管最近提出了从缓存中选择和驱逐不重要的 KV 对以减少内存消耗的方法，但驱逐对生成过程的潜在影响仍有待彻底检查。在本文中，我们研究了缓存驱逐的有害影响，并观察到，当 KV 对中包含的信息被彻底丢弃时，会出现不可预见的风险，从而导致安全漏洞、幻觉和上下文丢失。令人惊讶的是，我们发现通过降低精度的量化保留被驱逐的 KV 对中包含的少量信息基本上可以恢复所发生的退化。另一方面，我们观察到重要的 KV 对必须保持相对较高的精度以保证生成质量。受这些观察的启发，我们提出了 \textit{混合精度 KV 缓存}~(MiKV)，这是一种可靠的缓存压缩方法，通过以低精度保留被逐出的 KV 对来同时保留上下文细节，并通过保留重要的 KV 对来确保生成质量高精度KV对。对不同基准和 LLM 主干的实验表明，与其他基准相比，我们提出的方法在压缩比和性能之间提供了最先进的权衡。</li>
</ul>

<h3>Title: Editing Factual Knowledge and Explanatory Ability of Medical Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Derong Xu, Ziheng Zhang, Zhihong Zhu, Zhenxi Lin, Qidong Liu, Xian Wu, Tong Xu, Xiangyu Zhao, Yefeng Zheng, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18099">https://arxiv.org/abs/2402.18099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18099">https://arxiv.org/pdf/2402.18099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18099]] Editing Factual Knowledge and Explanatory Ability of Medical Large  Language Models(https://arxiv.org/abs/2402.18099)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge. To evaluate the editing impact, we build two benchmark datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting irrelevant knowledge that is not edited.</li>
<li><strong>摘要：</strong>模型编辑旨在精确修改大型语言模型（LLM）对特定知识的行为，同时保持不相关知识不变。事实证明，它可以有效解决法学硕士的幻觉和过时问题。因此，它可以促进法学硕士在许多不能容忍幻觉的关键领域（例如医学领域）的应用。在本文中，我们提出了两种模型编辑研究，并在医学领域进行了验证：（1）直接编辑事实医学知识和（2）编辑对事实的解释。同时，我们观察到当前的模型编辑方法与医学知识的专业化和复杂性相矛盾。因此，我们提出了 MedLaSA，一种用于医学模型编辑的新型分层可扩展适配器策略。它采用因果追踪来识别神经元中知识的精确位置，然后将可扩展的适配器引入到法学硕士的密集层中。这些适配器根据相应的特定知识分配缩放值。为了评估编辑影响，我们构建了两个基准数据集并引入了一系列具有挑战性的综合指标。对医学法学硕士的大量实验证明了 MedLaSA 的编辑效率，不会影响未编辑的无关知识。</li>
</ul>

<h3>Title: Small But Funny: A Feedback-Driven Approach to Humor Distillation</h3>
<ul>
<li><strong>Authors: </strong>Sahithya Ravi, Patrick Huber, Akshat Shrivastava, Aditya Sagar, Ahmed Aly, Vered Shwartz, Arash Einolghozati</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18113">https://arxiv.org/abs/2402.18113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18113">https://arxiv.org/pdf/2402.18113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18113]] Small But Funny: A Feedback-Driven Approach to Humor Distillation(https://arxiv.org/abs/2402.18113)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a "teacher" generating data, as well as a "critic" evaluating the student's performance. Our experiments on humor generation reveal that the incorporation of feedback significantly narrows the performance gap between SLMs and their larger counterparts compared to merely relying on imitation. As a result, our research highlights the potential of using feedback as an additional dimension to data when transferring complex language abilities via distillation.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的出现带来了有前途的语言生成能力，特别是在执行复杂推理和创造性写作等任务方面。因此，通过模仿教师反应进行提炼已成为一种流行的技术，将知识从法学硕士转移到更容易理解的小语言模型（SLM）。虽然这对于简单的任务来说效果很好，但在需要复杂的语言理解和创造力的任务（例如幽默生成）上，存在很大的性能差距。我们假设这种差距可能源于这样一个事实：创造性任务可能很难仅通过模仿来学习，并探索一种涉及教师补充指导的方法是否可以产生更高的表现。为了解决这个问题，我们研究了为法学硕士分配双重角色的效果——作为生成数据的“教师”，以及评估学生表现的“评论家”。我们关于幽默生成的实验表明，与仅仅依靠模仿相比，反馈的结合显着缩小了 SLM 与其更大的同类之间的性能差距。因此，我们的研究强调了在通过蒸馏转移复杂语言能力时使用反馈作为数据的附加维度的潜力。</li>
</ul>

<h3>Title: Exploring Multilingual Human Value Concepts in Large Language Models: Is  Value Alignment Consistent, Transferable and Controllable across Languages?</h3>
<ul>
<li><strong>Authors: </strong>Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18120">https://arxiv.org/abs/2402.18120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18120">https://arxiv.org/pdf/2402.18120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18120]] Exploring Multilingual Human Value Concepts in Large Language Models: Is  Value Alignment Consistent, Transferable and Controllable across Languages?(https://arxiv.org/abs/2402.18120)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Prior research in representation engineering has revealed that LLMs encode concepts within their representation spaces, predominantly centered around English. In this study, we extend this philosophy to a multilingual scenario, delving into multilingual human value concepts in LLMs. Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 LLM series with distinct multilinguality, we empirically substantiate the existence of multilingual human values in LLMs. Further cross-lingual analysis on these concepts discloses 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts. Additionally, we validate the feasibility of cross-lingual control over value alignment capabilities of LLMs, leveraging the dominant language as a source language. Drawing from our findings on multilingual value alignment, we prudently provide suggestions on the composition of multilingual data for LLMs pre-training: including a limited number of dominant languages for cross-lingual alignment transfer while avoiding their excessive prevalence, and keeping a balanced distribution of non-dominant languages. We aspire that our findings would contribute to enhancing the safety and utility of multilingual AI.</li>
<li><strong>摘要：</strong>先前对表示工程的研究表明，法学硕士在其表示空间内编码概念，主要以英语为中心。在这项研究中，我们将这一理念扩展到多语言场景，深入研究法学硕士中的多语言人类价值概念。通过对7种人类价值观、16种语言和3个具有鲜明多语言性的法学硕士系列的全面探索，我们实证证实了法学硕士中多语言人类价值观的存在。对这些概念的进一步跨语言分析揭示了语言资源差异带来的三个特征：跨语言不一致、扭曲的语言关系以及高资源语言和低资源语言之间的单向跨语言迁移，所有这些都是从人类价值概念的角度来看的。此外，我们还验证了法学硕士价值调整能力的跨语言控制的可行性，利用主导语言作为源语言。根据我们对多语言价值对齐的研究结果，我们审慎地对法学硕士预训练的多语言数据构成提出建议：包括有限数量的主导语言以进行跨语言对齐迁移，同时避免其过度流行，并保持数据的均衡分布。非主导语言。我们希望我们的研究结果有助于提高多语言人工智能的安全性和实用性。</li>
</ul>

<h3>Title: Saving the legacy of Hero Ibash: Evaluating Four Language Models for  Aminoacian</h3>
<ul>
<li><strong>Authors: </strong>Yunze Xiao, Yiyang Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18121">https://arxiv.org/abs/2402.18121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18121">https://arxiv.org/pdf/2402.18121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18121]] Saving the legacy of Hero Ibash: Evaluating Four Language Models for  Aminoacian(https://arxiv.org/abs/2402.18121)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.</li>
<li><strong>摘要：</strong>这项研究评估了尚未开发的氨基语言中的四种前沿语言模型。通过评估，它仔细检查它们在文本生成、语义连贯性和上下文理解方面的适应性、有效性和局限性。这项研究揭示了这些模型在资源匮乏的语言中的表现，开创了弥合语言差距的途径。通过提供基准和理解挑战，它为自然语言处理的未来进步奠定了基础，旨在提高语言模型在类似语言环境中的适用性，标志着语言技术的包容性和进步迈出了重要一步。</li>
</ul>

<h3>Title: Cause and Effect: Can Large Language Models Truly Understand Causality?</h3>
<ul>
<li><strong>Authors: </strong>Swagata Ashwani, Kshiteesh Hegde, Nishith Reddy Mannuru, Mayank Jindal, Dushyant Singh Sengar, Krishna Chaitanya Rao Kathala, Dishant Banga, Vinija Jain, Aman Chadha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18139">https://arxiv.org/abs/2402.18139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18139">https://arxiv.org/pdf/2402.18139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18139]] Cause and Effect: Can Large Language Models Truly Understand Causality?(https://arxiv.org/abs/2402.18139)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multiple causal reasoning tasks such as causal discovery, causal identification and counterfactual reasoning. The counterfactual sentences add explicit knowledge of the not caused by scenarios. By combining these powerful modules, our model aims to provide a deeper understanding of causal relationships, enabling enhanced interpretability. Evaluation of benchmark datasets shows improved performance across all metrics, such as accuracy, precision, recall, and F1 scores. We also introduce CausalNet, a new dataset accompanied by our code, to facilitate further research in this domain.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的兴起，了解它们在破译和解释语言所涉及的复杂因果关系网络方面的能力和局限性变得至关重要。当前的方法使用显式或隐式因果推理，但强烈需要一种将两者结合起来的统一方法，以更有效地处理各种因果关系。本研究提出了一种称为上下文感知推理增强与反事实分析（CARE CA）框架的新颖架构，以增强因果推理和可解释性。所提出的框架结合了带有 ConceptNet 和反事实陈述的显式因果检测模块，以及通过法学硕士的隐式因果检测。我们的框架更进一步，提供了一层反事实解释，以强调法学硕士对因果关系的理解。 ConceptNet 的知识增强了多种因果推理任务的性能，例如因果发现、因果识别和反事实推理。反事实句子增加了非场景引起的明确知识。通过结合这些强大的模块，我们的模型旨在提供对因果关系的更深入的理解，从而增强可解释性。对基准数据集的评估显示所有指标的性能都有所提高，例如准确性、精确度、召回率和 F1 分数。我们还引入了 CausalNet，一个带有我们代码的新数据集，以促进该领域的进一步研究。</li>
</ul>

<h3>Title: Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a  Large Language Model Based on Group-Level Demographic Information</h3>
<ul>
<li><strong>Authors: </strong>Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung Lee, Bernard J. Jansen, Jang Hyun Kim</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18144">https://arxiv.org/abs/2402.18144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18144">https://arxiv.org/pdf/2402.18144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18144]] Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a  Large Language Model Based on Group-Level Demographic Information(https://arxiv.org/abs/2402.18144)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models exhibit societal biases associated with demographic information, including race, gender, and others. Endowing such language models with personalities based on demographic data can enable generating opinions that align with those of humans. Building on this idea, we propose "random silicon sampling," a method to emulate the opinions of the human population sub-group. Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions. Through random silicon sampling and using only group-level demographic information, we discovered that language models can generate response distributions that are remarkably similar to the actual U.S. public opinion polls. Moreover, we found that the replicability of language models varies depending on the demographic group and topic of the question, and this can be attributed to inherent societal biases in the models. Our findings demonstrate the feasibility of mirroring a group's opinion using only demographic distribution and elucidate the effect of social biases in language models on such simulations.</li>
<li><strong>摘要：</strong>大型语言模型表现出与人口统计信息相关的社会偏见，包括种族、性别等。赋予此类语言模型基于人口统计数据的个性，可以生成与人类一致的观点。基于这个想法，我们提出了“随机硅采样”，这是一种模仿人群子群体意见的方法。我们的研究分析了 1) 一种语言模型，该模型仅根据人口统计分布生成与人类群体相对应的调查答复；2) 我们的方法在各种人口统计亚组和主题问题中的适用性。通过随机硅抽样并仅使用群体级别的人口统计信息，我们发现语言模型可以生成与实际的美国民意调查非常相似的响应分布。此外，我们发现语言模型的可复制性根据人口群体和问题主题的不同而变化，这可以归因于模型中固有的社会偏见。我们的研究结果证明了仅使用人口统计分布来反映群体意见的可行性，并阐明了语言模型中的社会偏见对此类模拟的影响。</li>
</ul>

<h3>Title: Unsupervised Information Refinement Training of Large Language Models  for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18150">https://arxiv.org/abs/2402.18150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18150">https://arxiv.org/pdf/2402.18150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18150]] Unsupervised Information Refinement Training of Large Language Models  for Retrieval-Augmented Generation(https://arxiv.org/abs/2402.18150)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39\% relative points. InFO-RAG also shows advantages in in-context learning and robustness of RAG.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 通过合并检索中的附加信息来增强大型语言模型 (LLM)。然而，研究表明，法学硕士在有效利用检索到的信息方面仍然面临挑战，甚至忽视它或被它误导。关键原因是法学硕士的培训并没有明确让法学硕士学会如何利用不同质量的输入检索文本。在本文中，我们提出了一个新颖的视角，将 RAG 中法学硕士的角色视为“信息提炼者”，这意味着无论检索到的文本的正确性、完整性或有用性如何，法学硕士都可以始终如一地将知识整合到检索到的文本中，并模型参数来生成比检索到的文本更简洁、准确和完整的文本。为此，我们提出了一种名为 InFO-RAG 的信息细化训练方法，以无监督的方式优化 RAG 的 LLM。 InFO-RAG 成本低廉，并且适用于各种任务。在问答、槽填充、语言建模、对话和代码生成等不同任务中对 11 个数据集进行零样本预测的大量实验表明，InFO-RAG 将 LLaMA2 的性能平均提高了 9.39% 相对点。 InFO-RAG 还显示了 RAG 的上下文学习和鲁棒性方面的优势。</li>
</ul>

<h3>Title: Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and  Mitigating Knowledge Conflicts in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18154">https://arxiv.org/abs/2402.18154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18154">https://arxiv.org/pdf/2402.18154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18154]] Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and  Mitigating Knowledge Conflicts in Language Models(https://arxiv.org/abs/2402.18154)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs. In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point. We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context. Moreover, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads. Inspired by the insights, we propose a novel method called Pruning Head via PatH PatcHing (PH3), which can efficiently mitigate knowledge conflicts by pruning conflicting attention heads without updating model parameters. PH3 can flexibly control eight LMs to use internal memory ($\uparrow$ 44.0%) or external context ($\uparrow$ 38.5%). Moreover, PH3 can also improve the performance of LMs on open-domain QA tasks. We also conduct extensive experiments to demonstrate the cross-model, cross-relation, and cross-format generalization of our method.</li>
<li><strong>摘要：</strong>最近，检索增强和工具增强已经证明了通过提供外部上下文来扩展语言模型（LM）内部记忆边界的卓越能力。然而，内部记忆和外部环境不可避免地发生冲突，导致语言模型内部的知识冲突。本文旨在通过信息流的视角解读知识冲突的机制，进而通过关键点的精准干预来缓解冲突。我们发现在后面的层中有一些具有相反效果的注意力头，其中记忆头可以从内部记忆中回忆知识，上下文头可以从外部上下文中检索知识。此外，我们揭示了 LM 中出现知识冲突的关键点是记忆头和上下文头不一致的信息流的整合。受这些见解的启发，我们提出了一种名为 Pruning Head via PathH PatchHing (PH3) 的新方法，它可以通过修剪冲突的注意力头而无需更新模型参数来有效地缓解知识冲突。 PH3可以灵活控制8个LM使用内部存储器（$\uparrow$ 44.0%）或外部上下文（$\uparrow$ 38.5%）。此外，PH3 还可以提高 LM 在开放域 QA 任务上的性能。我们还进行了广泛的实验来证明我们的方法的跨模型、交叉关系和跨格式泛化。</li>
</ul>

<h3>Title: From Summary to Action: Enhancing Large Language Models for Complex  Tasks with Open World APIs</h3>
<ul>
<li><strong>Authors: </strong>Yulong Liu, Yunlong Yuan, Chunwei Wang, Jianhua Han, Yongqiang Ma, Li Zhang, Nanning Zheng, Hang Xu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18157">https://arxiv.org/abs/2402.18157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18157">https://arxiv.org/pdf/2402.18157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18157]] From Summary to Action: Enhancing Large Language Models for Complex  Tasks with Open World APIs(https://arxiv.org/abs/2402.18157)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations. Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence. Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of LLMs. The first approach emphasizes the construction of relevant datasets for model fine-tuning. The second approach, in contrast, aims to fully exploit the inherent reasoning abilities of LLMs through in-context learning strategies. In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs. This pipeline mirrors the human task-solving process, addressing complicated real-life user queries. At each step, we guide LLMs to summarize the achieved results and determine the next course of action. We term this pipeline `from Summary to action', Sum2Act for short. Empirical evaluations of our Sum2Act pipeline on the ToolBench benchmark show significant performance improvements, outperforming established methods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in enhancing LLMs for complex real-world tasks.</li>
<li><strong>摘要：</strong>人类与动物的区别在于人类使用和创造工具的独特能力。工具使人类能够克服生理限制，促进创造伟大的文明。同样，启用具有学习外部工具使用能力的大型语言模型（LLM）等基础模型可能是实现通用人工智能的关键一步。该领域之前的研究主要采用两种不同的方法来增强法学硕士的工具调用能力。第一种方法强调构建用于模型微调的相关数据集。相比之下，第二种方法旨在通过情境学习策略充分利用法学硕士固有的推理能力。在这项工作中，我们引入了一种新颖的工具调用管道，旨在控制大量现实世界的 API。该管道反映了人工任务解决过程，解决了现实生活中复杂的用户查询。在每一步中，我们都会指导法学硕士总结所取得的成果并确定下一步的行动方针。我们将这个管道称为“从摘要到行动”，简称 Sum2Act。我们在 ToolBench 基准上对 Sum2Act 管道进行的实证评估显示出显着的性能改进，优于 ReAct 和 DFSDT 等既定方法。这凸显了 Sum2Act 在增强法学硕士应对复杂现实任务方面的有效性。</li>
</ul>

<h3>Title: Evaluating Quantized Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18158">https://arxiv.org/abs/2402.18158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18158">https://arxiv.org/pdf/2402.18158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18158]] Evaluating Quantized Large Language Models(https://arxiv.org/abs/2402.18158)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their applicability. Based on the extensive experiments, we systematically summarize the effect of quantization, provide recommendations to apply quantization techniques, and point out future directions.</li>
<li><strong>摘要：</strong>训练后量化 (PTQ) 已成为降低大型语言模型 (LLM) 成本的一项有前景的技术。具体来说，PTQ 可以有效减少 LLM 中的内存消耗并减少计算开销。为了满足不同场景下高效性能的要求，对量化LLM进行综合评估对于指导量化方法的选择至关重要。本文通过评估 PTQ 对 11 个模型系列（包括 OPT、LLaMA2、Falcon、Bloomz、Mistral、ChatGLM、Vicuna、LongChat、StableLM、Gemma 和Mamba，参数范围从125M到180B。评估包括五类任务：基础NLP、涌现能力、可信度、对话和长上下文任务。此外，我们还评估了最先进的（SOTA）量化方法以证明其适用性。基于大量的实验，我们系统地总结了量化的效果，提供了应用量化技术的建议，并指出了未来的方向。</li>
</ul>

<h3>Title: MIKO: Multimodal Intention Knowledge Distillation from Large Language  Models for Social-Media Commonsense Discovery</h3>
<ul>
<li><strong>Authors: </strong>Feihong Lu, Weiqi Wang, Yangyifei Luo, Ziqin Zhu, Qingyun Sun, Baixuan Xu, Haochen Shi, Shiqi Gao, Qian Li, Yangqiu Song, Jianxin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18169">https://arxiv.org/abs/2402.18169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18169">https://arxiv.org/pdf/2402.18169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18169]] MIKO: Multimodal Intention Knowledge Distillation from Large Language  Models for Social-Media Commonsense Discovery(https://arxiv.org/abs/2402.18169)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Social media has become a ubiquitous tool for connecting with others, staying updated with news, expressing opinions, and finding entertainment. However, understanding the intention behind social media posts remains challenging due to the implicitness of intentions in social media posts, the need for cross-modality understanding of both text and images, and the presence of noisy information such as hashtags, misspelled words, and complicated abbreviations. To address these challenges, we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions. Specifically, we use an MLLM to interpret the image and an LLM to extract key information from the text and finally instruct the LLM again to generate intentions. By applying MIKO to publicly available social media datasets, we construct an intention knowledge base featuring 1,372K intentions rooted in 137,287 posts. We conduct a two-stage annotation to verify the quality of the generated knowledge and benchmark the performance of widely used LLMs for intention generation. We further apply MIKO to a sarcasm detection dataset and distill a student model to demonstrate the downstream benefits of applying intention knowledge.</li>
<li><strong>摘要：</strong>社交媒体已成为与他人联系、了解最新新闻、表达观点和寻找娱乐的普遍工具。然而，由于社交媒体帖子中意图的隐含性、对文本和图像的跨模态理解的需要，以及诸如标签、拼写错误的单词和复杂的噪音信息的存在，理解社交媒体帖子背后的意图仍然具有挑战性。缩写。为了应对这些挑战，我们提出了 MIKO，这是一个多模态意图知识蒸馏框架，它协作利用大语言模型 (LLM) 和多模态大语言模型 (MLLM) 来揭示用户的意图。具体来说，我们使用 MLLM 来解释图像，并使用 LLM 从文本中提取关键信息，最后再次指示 LLM 生成意图。通过将 MIKO 应用于公开的社交媒体数据集，我们构建了一个意图知识库，其中包含源自 137,287 个帖子的 1,372K 意图。我们进行两阶段注释来验证生成知识的质量，并对广泛使用的法学硕士的意图生成性能进行基准测试。我们进一步将 MIKO 应用于讽刺检测数据集，并提炼出学生模型，以展示应用意图知识的下游好处。</li>
</ul>

<h3>Title: Clustering and Ranking: Diversity-preserved Instruction Selection  through Expert-aligned Quality Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Hongxia Ma, Li Zhang, Hao Yang, Tong Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18191">https://arxiv.org/abs/2402.18191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18191">https://arxiv.org/pdf/2402.18191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18191]] Clustering and Ranking: Diversity-preserved Instruction Selection  through Expert-aligned Quality Estimation(https://arxiv.org/abs/2402.18191)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>With contributions from the open-source community, a vast amount of instruction tuning (IT) data has emerged. Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for instruction data selection have limitations such as relying on fragile external APIs, being affected by biases in GPT models, or reducing the diversity of the selected instruction dataset. In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved instruction data selection method: Clustering and Ranking (CaR). CaR consists of two steps. The first step involves ranking instruction pairs using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25%). The second step involves preserving dataset diversity through a clustering process.In our experiment, CaR selected a subset containing only 1.96% of Alpaca's IT data, yet the underlying AlpaCaR model trained on this subset outperforms Alpaca by an average of 32.1% in GPT-4 evaluations. Furthermore, our method utilizes small models (355M parameters) and requires only 11.2% of the monetary cost compared to existing methods, making it easily deployable in industrial scenarios.</li>
<li><strong>摘要：</strong>在开源社区的贡献下，出现了大量的指令调优（IT）数据。鉴于训练和评估模型需要大量的资源分配，拥有一种有效的方法来选择高质量的 IT 数据是有利的。然而，现有的指令数据选择方法存在局限性，例如依赖脆弱的外部 API、受 GPT 模型偏差的影响或降低所选指令数据集的多样性。在本文中，我们提出了一种工业友好型、专家对齐且保留多样性的指令数据选择方法：聚类和排序（CaR）。 CaR 由两个步骤组成。第一步涉及使用与专家偏好非常一致的评分模型对指令对进行排名（准确度达到 84.25%）。第二步是通过聚类过程保留数据集多样性。在我们的实验中，CaR 选择了仅包含 Alpaca IT 数据 1.96% 的子集，但在该子集上训练的底层 AlpaCaR 模型在 GPT-4 中平均优于 Alpaca 32.1%评价。此外，我们的方法利用小模型（3.55 亿个参数），与现有方法相比仅需要 11.2% 的货币成本，使其可以轻松部署在工业场景中。</li>
</ul>

<h3>Title: LLM Task Interference: An Initial Study on the Impact of Task-Switch in  Conversational History</h3>
<ul>
<li><strong>Authors: </strong>Akash Gupta, Ivaxi Sheth, Vyas Raina, Mark Gales, Mario Fritz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18216">https://arxiv.org/abs/2402.18216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18216">https://arxiv.org/pdf/2402.18216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18216]] LLM Task Interference: An Initial Study on the Impact of Task-Switch in  Conversational History(https://arxiv.org/abs/2402.18216)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>With the recent emergence of powerful instruction-tuned large language models (LLMs), various helpful conversational Artificial Intelligence (AI) systems have been deployed across many applications. When prompted by users, these AI systems successfully perform a wide range of tasks as part of a conversation. To provide some sort of memory and context, such approaches typically condition their output on the entire conversational history. Although this sensitivity to the conversational history can often lead to improved performance on subsequent tasks, we find that performance can in fact also be negatively impacted, if there is a task-switch. To the best of our knowledge, our work makes the first attempt to formalize the study of such vulnerabilities and interference of tasks in conversational LLMs caused by task-switches in the conversational history. Our experiments across 5 datasets with 15 task switches using popular LLMs reveal that many of the task-switches can lead to significant performance degradation.</li>
<li><strong>摘要：</strong>随着最近出现的强大的指令调整大语言模型 (LLM)，各种有用的对话式人工智能 (AI) 系统已在许多应用程序中部署。当用户提示时，这些人工智能系统可以作为对话的一部分成功执行各种任务。为了提供某种记忆和上下文，此类方法通常会根据整个对话历史记录来调整其输出。尽管这种对对话历史记录的敏感性通常可以提高后续任务的性能，但我们发现，如果存在任务切换，性能实际上也会受到负面影响。据我们所知，我们的工作首次尝试对会话历史中的任务切换引起的会话法学硕士中的此类漏洞和任务干扰进行正式研究。我们使用流行的 LLM 在 5 个数据集上进行了 15 个任务切换的实验，结果表明许多任务切换可能会导致性能显着下降。</li>
</ul>

<h3>Title: Improving Open-Ended Text Generation via Adaptive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Wenhong Zhu, Hongkun Hao, Zhiwei He, Yiming Ai, Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18223">https://arxiv.org/abs/2402.18223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18223">https://arxiv.org/pdf/2402.18223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18223]] Improving Open-Ended Text Generation via Adaptive Decoding(https://arxiv.org/abs/2402.18223)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at https://github.com/zwhong714/adaptive_decoding.</li>
<li><strong>摘要：</strong>当前的语言模型根据概率分布逐个令牌解码文本令牌，确定下一个令牌的适当候选者对于确保生成质量至关重要。这项研究引入了自适应解码，这是一种使语言模型能够在生成过程中动态确定合理候选集的机制。具体来说，我们引入了一种称为置信度的基于熵的度量，并将确定最佳候选集概念化为置信度增加的过程。通过利用置信度的增量来评估在候选集中包含令牌的合理性，使模型能够自适应地确定最合适的候选集。实验结果表明，我们的方法在故事生成任务中实现了更高的 MAUVE 和多样性，并保持了一定的连贯性，凸显了其相对于现有算法的优越性。代码可在 https://github.com/zwhong714/adaptive_decoding 获取。</li>
</ul>

<h3>Title: CogBench: a large language model walks into a psychology lab</h3>
<ul>
<li><strong>Authors: </strong>Julian Coda-Forno, Marcel Binz, Jane X. Wang, Eric Schulz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18225">https://arxiv.org/abs/2402.18225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18225">https://arxiv.org/pdf/2402.18225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18225]] CogBench: a large language model walks into a psychology lab(https://arxiv.org/abs/2402.18225)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）极大地推进了人工智能领域的发展。然而，全面评估它们仍然具有挑战性。我们认为，这部分是由于大多数基准测试中主要关注性能指标。本文介绍了 CogBench，这是一个基准，包括来自七个认知心理学实验的十个行为指标。这种新颖的方法为法学硕士行为的表型分析提供了一个工具包。我们将 CogBench 应用于 35 个法学硕士，产生了丰富多样的数据集。我们使用统计多级建模技术分析这些数据，解释特定法学硕士微调版本之间的嵌套依赖关系。我们的研究强调了模型大小和基于人类反馈的强化学习 (RLHF) 在提高性能和与人类行为保持一致方面的关键作用。有趣的是，我们发现开源模型比专有模型更不容易出现风险，并且对代码的微调并不一定会增强法学硕士的行为。最后，我们探讨了即时工程技术的效果。我们发现，思维链提示可以改善概率推理，而后退一步提示则可以促进基于模型的行为。</li>
</ul>

<h3>Title: Learning or Self-aligning? Rethinking Instruction Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Mengjie Ren, Boxi Cao, Hongyu Lin, Liu Cao, Xianpei Han, Ke Zeng, Guanglu Wan, Xunliang Cai, Le Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18243">https://arxiv.org/abs/2402.18243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18243">https://arxiv.org/pdf/2402.18243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18243]] Learning or Self-aligning? Rethinking Instruction Fine-tuning(https://arxiv.org/abs/2402.18243)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instruction Fine-tuning~(IFT) is a critical phase in building large language models~(LLMs). Previous works mainly focus on the IFT's role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors. Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works.</li>
<li><strong>摘要：</strong>指令微调（IFT）是构建大型语言模型（LLM）的关键阶段。之前的工作主要集中于 IFT 在行为规范迁移和额外世界知识学习中的作用。然而，对 IFT 潜在机制的理解仍然非常有限。在本文中，我们设计了一个知识干预框架来解耦IFT的潜在潜在因素，从而能够对不同因素进行个体分析。令人惊讶的是，我们的实验表明，尝试通过 IFT 学习额外的世界知识往往难以产生积极影响，甚至可能导致明显的负面影响。此外，我们发现在 IFT 前后保持内部知识一致性是实现成功 IFT 的关键因素。我们的研究结果揭示了 IFT 的基本机制，并为一些最近和未来潜在的工作提供了强有力的支持。</li>
</ul>

<h3>Title: Towards Generalist Prompting for Large Language Models by Mental Models</h3>
<ul>
<li><strong>Authors: </strong>Haoxiang Guan, Jiyan He, Shuxin Zheng, En-Hong Chen, Weiming Zhang, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18252">https://arxiv.org/abs/2402.18252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18252">https://arxiv.org/pdf/2402.18252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18252]] Towards Generalist Prompting for Large Language Models by Mental Models(https://arxiv.org/abs/2402.18252)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive performance on many tasks. However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks. In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems. Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting. MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select the most suitable mental models for the problem, achieving or being near to the state-of-the-art results on diverse tasks such as STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We hope that the insights presented herein will stimulate further exploration of generalist prompting methods for LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在许多任务上都表现出了令人印象深刻的性能。然而，为了达到最佳性能，仍然需要专门设计的提示方法。这些方法要么依赖于特定于任务的少数样本，需要一定水平的领域知识，要么设计得很简单，但只在少数类型的任务上表现良好。在这项工作中，我们尝试引入通才提示的概念，它的设计原则是在广泛的任务上实现最佳或接近最佳的性能，同时消除针对特定问题的手动选择和定制提示的需要。此外，我们提出了MeMo（心理模型），这是一种设计简单但有效满足通才提示标准的创新提示方法。 MeMo将各种提示方法的核心提炼成个人的心智模型，让法学硕士能够自主选择最适合问题的心智模型，在STEM、逻辑推理等多种任务上达到或接近最先进的结果，以及零样本设置中的常识推理。我们希望本文提出的见解能够激发对法学硕士通才激励方法的进一步探索。</li>
</ul>

<h3>Title: Retrieval-based Full-length Wikipedia Generation for Emergent Events</h3>
<ul>
<li><strong>Authors: </strong>Jiebin Zhang, Eugene J. Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xiaoguang Li, Qun Liu, Sujian Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18264">https://arxiv.org/abs/2402.18264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18264">https://arxiv.org/pdf/2402.18264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18264]] Retrieval-based Full-length Wikipedia Generation for Emergent Events(https://arxiv.org/abs/2402.18264)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In today's fast-paced world, the growing demand to quickly generate comprehensive and accurate Wikipedia documents for emerging events is both crucial and challenging. However, previous efforts in Wikipedia generation have often fallen short of meeting real-world requirements. Some approaches focus solely on generating segments of a complete Wikipedia document, while others overlook the importance of faithfulness in generation or fail to consider the influence of the pre-training corpus. In this paper, we simulate a real-world scenario where structured full-length Wikipedia documents are generated for emergent events using input retrieved from web sources. To ensure that Large Language Models (LLMs) are not trained on corpora related to recently occurred events, we select events that have taken place recently and introduce a new benchmark Wiki-GenBen, which consists of 309 events paired with their corresponding retrieved web pages for generating evidence. Additionally, we design a comprehensive set of systematic evaluation metrics and baseline methods, to evaluate the capability of LLMs in generating factual full-length Wikipedia documents. The data and code are open-sourced at WikiGenBench.</li>
<li><strong>摘要：</strong>在当今快节奏的世界中，针对新兴事件快速生成全面且准确的维基百科文档的需求不断增长，这一需求既至关重要又充满挑战。然而，之前维基百科生成的努力常常无法满足现实世界的要求。一些方法仅专注于生成完整维基百科文档的片段，而另一些方法则忽略了生成中忠实性的重要性或未能考虑预训练语料库的影响。在本文中，我们模拟了一个现实场景，其中使用从网络源检索的输入为紧急事件生成结构化的全长维基百科文档。为了确保大型语言模型 (LLM) 不会在与最近发生的事件相关的语料库上进行训练，我们选择最近发生的事件并引入一个新的基准 Wiki-GenBen，其中包含 309 个事件及其相应的检索网页。产生证据。此外，我们设计了一套全面的系统评估指标和基线方法，以评估法学硕士生成事实完整维基百科文档的能力。数据和代码在 WikiGenBench 上开源。</li>
</ul>

<h3>Title: Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the  Key?</h3>
<ul>
<li><strong>Authors: </strong>Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18272">https://arxiv.org/abs/2402.18272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18272">https://arxiv.org/pdf/2402.18272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18272]] Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the  Key?(https://arxiv.org/abs/2402.18272)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.</li>
<li><strong>摘要：</strong>法学硕士讨论的最新进展表明，多智能体讨论提高了法学硕士的推理能力。在这项工作中，我们通过系统实验重新评估了这一主张，提出了一种新颖的小组讨论框架来丰富讨论机制。有趣的是，我们的结果表明，具有强提示的单代理法学硕士可以在各种推理任务和骨干法学硕士上获得与现有最佳讨论方法几乎相同的性能。我们观察到，仅当提示中没有演示时，多代理讨论的性能才优于单代理。进一步的研究揭示了法学硕士在讨论过程中常见的互动机制。</li>
</ul>

<h3>Title: Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of  Pre-trained Language Models with Proximal Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Shuo Yang, Gjergji Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18284">https://arxiv.org/abs/2402.18284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18284">https://arxiv.org/pdf/2402.18284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18284]] Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of  Pre-trained Language Models with Proximal Policy Optimization(https://arxiv.org/abs/2402.18284)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation shows that our ranking results exhibit a remarkably high consistency with that of humans. This research significantly reduces training costs of proximal policy-guided models and demonstrates the potential for self-correction of language models.</li>
<li><strong>摘要：</strong>ChatGPT 的广泛使用凸显了根据人类反馈进行强化学习的潜力。然而，其训练流程依赖于手动排名，这是一个资源密集型过程。为了降低劳动力成本，我们提出了一种自我监督的文本排名方法，用于应用邻近策略优化来微调语言模型，同时消除对人类注释者的需求。我们的方法从概率采样开始，以鼓励语言模型为每个输入生成不同的响应。然后，我们使用 TextRank 和 ISODATA 算法根据这些响应的语义对它们进行排名和聚类。随后，我们构建了一个奖励模型来学习排名并优化我们的生成策略。我们在三项任务上使用两种语言模型进行的实验结果表明，通过我们的方法训练的模型在 BLEU、GLEU 和 METEOR 分数方面明显优于基线。此外，我们的手动评估表明，我们的排名结果与人类的排名结果表现出非常高的一致性。这项研究显着降低了近端策略引导模型的训练成本，并展示了语言模型自我纠正的潜力。</li>
</ul>

<h3>Title: How to think step-by-step: A mechanistic understanding of  chain-of-thought reasoning</h3>
<ul>
<li><strong>Authors: </strong>Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18312">https://arxiv.org/abs/2402.18312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18312">https://arxiv.org/pdf/2402.18312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18312]] How to think step-by-step: A mechanistic understanding of  chain-of-thought reasoning(https://arxiv.org/abs/2402.18312)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context taking over abruptly in the later half. This internal phase shift manifests in different functional components: attention heads that write the answer token predominantly appear in the later half, attention heads that move information along ontological relationships appear exclusively in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs.</li>
<li><strong>摘要：</strong>尽管具有思想链 (CoT) 提示的大型语言模型 (LLM) 展示了卓越的推理能力，但人们对促进 CoT 生成的模型的内部机制普遍缺乏了解。这项工作研究了法学硕士内的神经子结构，从机械的角度体现了 CoT 推理。通过对应用于虚拟本体多步推理的 LLaMA-2 7B 的分析，我们证明了法学硕士部署了多个并行的答案生成路径来进行逐步推理。这些并行路径从输入问题上下文以及生成的 CoT 中提供顺序答案。我们观察到法学硕士中间层存在显着的功能裂痕。前半部分的令牌表示仍然强烈偏向于预训练之前，而上下文在后半部分突然接管。这种内部相移体现在不同的功能组件中：编写答案标记的注意力头主要出现在后半部分，沿着本体关系移动信息的注意力头只出现在前半部分，等等。据我们所知，这是法学硕士 CoT 推理机制研究的首次尝试。</li>
</ul>

<h3>Title: Learning to Generate Instruction Tuning Datasets for Zero-Shot Task  Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Nihal V. Nayak, Yiyang Nan, Avi Trost, Stephen H. Bach</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18334">https://arxiv.org/abs/2402.18334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18334">https://arxiv.org/pdf/2402.18334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18334]] Learning to Generate Instruction Tuning Datasets for Zero-Shot Task  Adaptation(https://arxiv.org/abs/2402.18334)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains. The model, dataset, and code are available at https://github.com/BatsResearch/bonito.</li>
<li><strong>摘要：</strong>我们介绍 Bonito，一个用于条件任务生成的开源模型：将未注释的文本转换为特定于任务的训练数据集以进行指令调整的任务。我们的目标是在用户的专业私有数据上实现大型语言模型的零样本任务适应。我们在一个新的大规模数据集上训练 Bonito，该数据集包含 165 万个示例，这些示例是通过将现有指令调整数据集重新混合到元模板中而创建的。数据集的元模板生成训练示例，其中输入是未注释的文本和任务属性，输出由指令和响应组成。我们使用 Bonito 为来自专业领域的七个数据集生成综合任务，涉及三种任务类型——是非问答、提取式问答和自然语言推理——并调整语言模型。我们表明，Bonito 在事实上的自监督基线上显着提高了预训练和指令调整模型的平均性能。例如，采用 Mistral-Instruct-v2 以及 Mistral 和 Llama2 与 Bonito 的指令调整变体将强大的零样本性能提高了 22.1 F1 点，而下一个单词预测目标则抵消了指令调整的一些好处，并将平均性能降低了0.8 F1 点。我们使用 Bonito 进行了额外的实验，以了解域的影响、训练集的大小以及替代合成任务生成器的选择。总的来说，我们表明使用合成指令调整数据集进行学习是使语言模型适应新领域的有效方法。模型、数据集和代码可在 https://github.com/BatsResearch/bonito 获取。</li>
</ul>

<h3>Title: Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems  in Commonsense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18344">https://arxiv.org/abs/2402.18344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18344">https://arxiv.org/pdf/2402.18344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18344]] Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems  in Commonsense Reasoning(https://arxiv.org/abs/2402.18344)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%).</li>
<li><strong>摘要：</strong>大型语言模型表现出高级常识推理能力，尤其是使用思想链（CoT）等增强方法。然而，我们发现这些类似 CoT 的方法会导致相当多的原本正确的答案变成错误的，我们将其定义为有毒 CoT 问题。为了解释和缓解这个问题，我们首先利用归因追踪和因果追踪方法来探究LLM在CoT推理过程中的内部工作机制。通过比较，我们证明该模型在生成基本原理或答案时，会在浅层注意力层上表现出问题的信息丢失。基于探索结果，我们设计了一种称为 RIDERS（剩余解码和串行位置交换）的新颖方法，它从解码和串行位置角度补偿了模型中的信息缺陷。通过对多个常识推理基准的大量实验，我们验证了该方法不仅显着消除了 Toxic CoT 问题（下降了 23.6%），而且有效提高了模型的整体常识推理性能（上升了 5.5%）。</li>
</ul>

<h3>Title: VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning  with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seoyeon Kim, Kwangwook Seo, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18374">https://arxiv.org/abs/2402.18374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18374">https://arxiv.org/pdf/2402.18374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18374]] VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning  with Large Language Models(https://arxiv.org/abs/2402.18374)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing NER methods using knowledge and revises them into more faithful predictions. Our framework leverages the reasoning abilities of large language models to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can successfully verify errors from existing models as a model-agnostic approach. Further analyses on out-of-domain and low-resource settings show the usefulness of VerifiNER on real-world applications.</li>
<li><strong>摘要：</strong>特定领域命名实体识别 (NER) 的最新方法（例如生物医学 NER）已显示出显着的进步。然而，他们仍然缺乏忠诚度，产生了错误的预测。我们假设实体的知识有助于验证预测的正确性。尽管知识很有用，但用知识解决此类错误并非易事，因为知识本身并不直接指示真实标签。为此，我们提出了 VerifiNER，这是一个事后验证框架，可以使用知识识别现有 NER 方法中的错误，并将其修改为更忠实的预测。我们的框架利用大型语言模型的推理能力，在验证过程中充分基于知识和上下文信息。我们通过对生物医学数据集进行广泛的实验来验证 VerifiNER 的有效性。结果表明，VerifiNER 作为一种与模型无关的方法，可以成功验证现有模型的错误。对域外和低资源设置的进一步分析显示了 VerifiNER 在实际应用中的有用性。</li>
</ul>

<h3>Title: Tokenization Is More Than Compression</h3>
<ul>
<li><strong>Authors: </strong>Craig W. Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter, Chris Tanner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18376">https://arxiv.org/abs/2402.18376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18376">https://arxiv.org/pdf/2402.18376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18376]] Tokenization Is More Than Compression(https://arxiv.org/abs/2402.18376)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.</li>
<li><strong>摘要：</strong>标记化是自然语言处理 (NLP) 任务的基础步骤，连接原始文本和语言模型。诸如字节对编码（BPE）之类的现有标记化方法起源于数据压缩领域，并且有人认为 BPE 的有效性源于其将文本压缩为相对较少数量的标记的能力。我们通过引入 PathPiece 来测试更少的标记会带来更好的下游性能的假设，PathPiece 是一种新的标记生成器，可将文档的文本分割为给定词汇表的最小数量的标记。通过大量的实验，我们发现这一假设并非如此，这让我们对有效标记化的原因产生了怀疑。为了检查哪些其他因素发挥了作用，我们评估了标记化所有三个阶段的设计决策：预标记化、词汇构建和分段，为有效标记化器的设计提供了新的见解。具体来说，我们说明了预标记化的重要性以及使用 BPE 初始化词汇表构建的好处。我们训练了 64 种具有不同标记化的语言模型，参数大小从 350M 到 2.4B 不等，所有这些模型都是公开的。</li>
</ul>

<h3>Title: Large Language Models As Evolution Strategies</h3>
<ul>
<li><strong>Authors: </strong>Robert Tjarko Lange, Yingtao Tian, Yujin Tang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18381">https://arxiv.org/abs/2402.18381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18381">https://arxiv.org/pdf/2402.18381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18381]] Large Language Models As Evolution Strategies(https://arxiv.org/abs/2402.18381)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms. These include gradient descent, classification, sequence completion, transformation, and improvement. In this work, we investigate whether large language models (LLMs), which never explicitly encountered the task of black-box optimization, are in principle capable of implementing evolutionary optimization algorithms. While previous works have solely focused on language-based task specification, we move forward and focus on the zero-shot application of LLMs to black-box optimization. We introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic, i.e. perform a type of black-box recombination operation. Empirically, we find that our setup allows the user to obtain an LLM-based evolution strategy, which we call `EvoLLM', that robustly outperforms baseline algorithms such as random search and Gaussian Hill Climbing on synthetic BBOB functions as well as small neuroevolution tasks. Hence, LLMs can act as `plug-in' in-context recombination operators. We provide several comparative studies of the LLM's model size, prompt strategy, and context construction. Finally, we show that one can flexibly improve EvoLLM's performance by providing teacher algorithm information via instruction fine-tuning on previously collected teacher optimization trajectories.</li>
<li><strong>摘要：</strong>大型 Transformer 模型能够实现大量所谓的上下文学习算法。这些包括梯度下降、分类、序列完成、转换和改进。在这项工作中，我们研究了从未明确遇到黑盒优化任务的大型语言模型（LLM）原则上是否能够实现进化优化算法。虽然之前的工作只关注基于语言的任务规范，但我们继续前进并专注于法学硕士在黑盒优化中的零样本应用。我们引入了一种新颖的提示策略，包括对离散总体成员进行从最小到最大的排序，并查询 LLM 以提出对均值统计的改进，即执行一种黑盒重组操作。根据经验，我们发现我们的设置允许用户获得基于 LLM 的进化策略，我们称之为“EvoLLM”，该策略在合成 BBOB 函数以及小型神经进化任务上远远优于随机搜索和高斯爬山等基线算法。因此，LLM 可以充当上下文重组运算符的“插件”。我们提供了一些关于法学硕士模型规模、提示策略和背景构建的比较研究。最后，我们表明，通过对先前收集的教师优化轨迹进行指令微调来提供教师算法信息，可以灵活地提高 EvoLLM 的性能。</li>
</ul>

<h3>Title: The First Place Solution of WSDM Cup 2024: Leveraging Large Language  Models for Conversational Multi-Doc QA</h3>
<ul>
<li><strong>Authors: </strong>Yiming Li, Zhao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18385">https://arxiv.org/abs/2402.18385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18385">https://arxiv.org/pdf/2402.18385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18385]] The First Place Solution of WSDM Cup 2024: Leveraging Large Language  Models for Conversational Multi-Doc QA(https://arxiv.org/abs/2402.18385)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations. In this paper, we introduce our winning approach for the "Conversational Multi-Doc QA" challenge in WSDM Cup 2024, which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs). We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data. Moreover, an advanced text embedding model is adopted to filter out potentially irrelevant documents and several approaches are designed and compared for the model ensemble. Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a large extent. The source codes have been released at https://github.com/zhangzhao219/WSDM-Cup-2024.</li>
<li><strong>摘要：</strong>会话式多文档问答旨在根据检索到的文档以及上下文对话来回答特定问题。在本文中，我们介绍了 WSDM Cup 2024 中“对话式多文档 QA”挑战赛的获胜方法，该方法利用了大型语言模型 (LLM) 卓越的自然语言理解和生成能力。我们首先使法学硕士适应该任务，然后设计一种混合训练策略，以充分利用领域内的未标记数据。此外，采用先进的文本嵌入模型来过滤掉潜在的不相关文档，并为模型集成设计和比较了几种方法。凭借所有这些技术，我们的解决方案最终在 WSDM Cup 2024 中获得第一名，很大程度上超越了竞争对手。源代码已发布于https://github.com/zhangzhao219/WSDM-Cup-2024。</li>
</ul>

<h3>Title: Decomposed Prompting: Unveiling Multilingual Linguistic Structure  Knowledge in English-Centric Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ercong Nie, Shuzhou Yuan, Bolei Ma, Helmut Schmid, Michael Färber, Frauke Kreuter, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18397">https://arxiv.org/abs/2402.18397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18397">https://arxiv.org/pdf/2402.18397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18397]] Decomposed Prompting: Unveiling Multilingual Linguistic Structure  Knowledge in English-Centric Large Language Models(https://arxiv.org/abs/2402.18397)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and the use of instructions in prompts. Our multilingual investigation shows that English-centric language models perform better on average than multilingual models. Our study offers insights into the multilingual transferability of English-centric LLMs, contributing to the understanding of their multilingual linguistic knowledge.</li>
<li><strong>摘要：</strong>尽管英语在训练数据中占主导地位，但像 GPT-3 和 LLaMA 这样以英语为中心的大语言模型 (LLM) 却表现出了执行多语言任务的卓越能力，这引发了人们对其跨语言能力的深度和性质的质疑。本文介绍了分解提示方法来探讨这些法学硕士在序列标记任务中的语言结构理解。与单个文本到文本的提示不同，我们的方法为输入句子的每个标记生成一个单独的提示，询问其语言标签。我们利用以英语为中心的多语言法学硕士，在 38 种语言的通用依赖词性标记数据集上评估了我们的方法。我们的研究结果表明，在零次和少次设置下，分解提示的功效和效率超过了迭代提示基线。进一步的分析揭示了评估方法和提示中指令的使用的影响。我们的多语言调查表明，以英语为中心的语言模型平均表现优于多语言模型。我们的研究提供了对以英语为中心的法学硕士的多语言可转移性的见解，有助于理解他们的多语言语言知识。</li>
</ul>

<h3>Title: A Cognitive Evaluation Benchmark of Image Reasoning and Description for  Large Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiujie Song, Mengyue Wu, Kenny Q. Zhu, Chunhao Zhang, Yanyi Chen</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18409">https://arxiv.org/abs/2402.18409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18409">https://arxiv.org/pdf/2402.18409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18409]] A Cognitive Evaluation Benchmark of Image Reasoning and Description for  Large Vision Language Models(https://arxiv.org/abs/2402.18409)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.</li>
<li><strong>摘要：</strong>大视觉语言模型（LVLM）尽管最近取得了成功，但其认知能力却很难得到全面测试。受人类认知测试中普遍使用的“Cookie Theft”任务的启发，我们提出了一种新颖的评估基准，使用具有丰富语义的图像来评估 LVLM 的高级认知能力。它定义了八种推理能力，由图像描述任务和视觉问答任务组成。我们对著名的 LVLM 的评估表明，LVLM 与人类的认知能力仍然存在很大差距。</li>
</ul>

<h3>Title: Can GPT Improve the State of Prior Authorization via Guideline Based  Automated Question Answering?</h3>
<ul>
<li><strong>Authors: </strong>Shubham Vatsal, Ayush Singh, Shabnam Tafreshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18419">https://arxiv.org/abs/2402.18419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18419">https://arxiv.org/pdf/2402.18419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18419]] Can GPT Improve the State of Prior Authorization via Guideline Based  Automated Question Answering?(https://arxiv.org/abs/2402.18419)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt</a></li>
<li><strong>Abstract: </strong>Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage. For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task. One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc. In this work, we evaluate whether GPT can validate numerous key factors, in turn helping health plans reach a decision drastically faster. We frame it as a question answering task, prompting GPT to answer a question from patient electronic health record. We experiment with different conventional prompting techniques as well as introduce our own novel prompting technique. Moreover, we report qualitative assessment by humans on the natural language generation outputs from our approach. Results show that our method achieves superior performance with the mean weighted F1 score of 0.61 as compared to its standard counterparts.</li>
<li><strong>摘要：</strong>健康保险公司有一个称为事先授权 (PA) 的明确流程，这是一个健康计划成本控制流程，要求医生和其他医疗保健专业人员在对患者执行特定程序之前提前获得健康计划的许可，以便有资格获得付款保险。对于健康保险公司来说，批准医疗领域患者的 PA 请求是一项耗时且具有挑战性的任务。这些关键挑战之一是验证请求是否符合年龄、性别等特定标准。在这项工作中，我们评估 GPT 是否可以验证众多关键因素，从而帮助健康计划更快地做出决策。我们将其定义为一个问答任务，促使 GPT 回答患者电子健康记录中的问题。我们尝试不同的传统提示技术，并引入我们自己的新颖的提示技术。此外，我们报告了人类对我们方法的自然语言生成输出的定性评估。结果表明，与标准方法相比，我们的方法实现了卓越的性能，平均加权 F1 得分为 0.61。</li>
</ul>

<h3>Title: Beyond Natural Language: LLMs Leveraging Alternative Formats for  Enhanced Reasoning and Communication</h3>
<ul>
<li><strong>Authors: </strong>Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18439">https://arxiv.org/abs/2402.18439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18439">https://arxiv.org/pdf/2402.18439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18439]] Beyond Natural Language: LLMs Leveraging Alternative Formats for  Enhanced Reasoning and Communication(https://arxiv.org/abs/2402.18439)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \url{https://github.com/thunlp/AutoForm}.</li>
<li><strong>摘要：</strong>自然语言（NL）长期以来一直是人类认知和交流的主要格式，并且推而广之，在大型语言模型（LLM）的开发和应用中也同样至关重要。然而，除了 NL 之外，法学硕士在预训练期间还看到了各种非 NL 格式，例如代码和逻辑表达式。 NL 作为 LLM 最佳格式的地位，特别是在单 LLM 推理和多代理通信方面，尚未得到彻底检验。在这项工作中，我们通过探索非 NL 格式在这些上下文中的实用性来挑战 NL 的默认使用。我们表明，允许 LLM 在推理或通信之前自主选择最合适的格式，可以使不同 LLM 的推理效率提高 3.3 到 5.7%，并且多代理通信中的令牌使用量减少高达 72.7%。同时保持沟通的有效性。我们的综合分析进一步表明，法学硕士可以从有限的任务指令中设计出一种格式，并且所设计的格式可以在不同的法学硕士之间有效地转移。有趣的是，法学硕士决定的结构化通信格式与已建立的代理通信语言具有显着的相似之处，这表明代理通信中向高效、结构化通信的自然演变。我们的代码发布于 \url{https://github.com/thunlp/AutoForm}。</li>
</ul>

<h3>Title: LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Md Hafizur Rahman, Prabuddha Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18443">https://arxiv.org/abs/2402.18443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18443">https://arxiv.org/pdf/2402.18443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18443]] LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs(https://arxiv.org/abs/2402.18443)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Building efficient neural network architectures can be a time-consuming task requiring extensive expert knowledge. This task becomes particularly challenging for edge devices because one has to consider parameters such as power consumption during inferencing, model size, inferencing speed, and CO2 emissions. In this article, we introduce a novel framework designed to automatically discover new neural network architectures based on user-defined parameters, an expert system, and an LLM trained on a large amount of open-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be used by non-AI experts, does not require a predetermined neural architecture search space, and considers a large set of edge device-specific parameters. We implement and validate this proposed neural architecture discovery framework using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo and Gemini as the LLM component. We observe that the proposed framework can rapidly (within hours) discover intricate neural network models that perform extremely well across a diverse set of application settings defined by the user.</li>
<li><strong>摘要：</strong>构建高效的神经网络架构可能是一项耗时的任务，需要广泛的专业知识。这项任务对于边缘设备来说尤其具有挑战性，因为必须考虑推理过程中的功耗、模型大小、推理速度和二氧化碳排放等参数。在本文中，我们介绍了一种新颖的框架，旨在基于用户定义的参数、专家系统和经过大量开放领域知识训练的法学硕士自动发现新的神经网络架构。引入的框架（LeMo-NADe）专为非人工智能专家使用而定制，不需要预定的神经架构搜索空间，并考虑大量边缘设备特定的参数。我们使用 CIFAR-10、CIFAR-100 和 ImageNet16-120 数据集实现并验证了这个提出的神经架构发现框架，同时使用 GPT-4 Turbo 和 Gemini 作为 LLM 组件。我们观察到，所提出的框架可以快速（在数小时内）发现复杂的神经网络模型，这些模型在用户定义的各种应用程序设置中表现得非常好。</li>
</ul>

<h3>Title: Meta-Task Prompting Elicits Embedding from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, Andrew Yates</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18458">https://arxiv.org/abs/2402.18458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18458">https://arxiv.org/pdf/2402.18458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18458]] Meta-Task Prompting Elicits Embedding from Large Language Models(https://arxiv.org/abs/2402.18458)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios.</li>
<li><strong>摘要：</strong>在这项工作中，我们引入了一种新的无监督嵌入方法，即具有显式单字限制的元任务提示（MetaEOL），用于从大型语言模型（LLM）生成高质量的句子嵌入，而不需要模型微调或任务-特定工程。利用元任务提示，MetaEOL 指导法学硕士通过一系列精心设计的提示来生成嵌入，这些提示解决了多个表征方面的问题。我们的综合实验表明，从各种元任务中平均得到的嵌入在语义文本相似性（STS）基准上产生了有竞争力的性能，并且在下游任务中表现出色，超越了对比训练模型。我们的研究结果提出了一种新的嵌入生成缩放法则，为跨不同以句子为中心的场景进行嵌入提取提供了一种多功能、资源高效的方法。</li>
</ul>

<h3>Title: Language Models Represent Beliefs of Self and Others</h3>
<ul>
<li><strong>Authors: </strong>Wentao Zhu, Zhining Zhang, Yizhou Wang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18496">https://arxiv.org/abs/2402.18496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18496">https://arxiv.org/pdf/2402.18496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18496]] Language Models Represent Beliefs of Self and Others(https://arxiv.org/abs/2402.18496)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.</li>
<li><strong>摘要：</strong>理解和归因心理状态，称为心理理论 (ToM)，是人类社会推理的基本能力。虽然大型语言模型 (LLM) 似乎拥有某些 ToM 功能，但这些功能背后的机制仍然难以捉摸。在这项研究中，我们发现可以通过语言模型的神经激活从不同主体的角度线性解码信念状态，表明自我和他人信念的内部表征的存在。通过操纵这些表示，我们观察到模型 ToM 性能的巨大变化，强调了它们在社会推理过程中的关键作用。此外，我们的研究结果扩展到涉及不同因果推理模式的各种社会推理任务，表明这些表示的潜在普遍性。</li>
</ul>

<h3>Title: Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware  Classification</h3>
<ul>
<li><strong>Authors: </strong>Garima Chhikara, Anurag Sharma, Kripabandhu Ghosh, Abhijnan Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18502">https://arxiv.org/abs/2402.18502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18502">https://arxiv.org/pdf/2402.18502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18502]] Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware  Classification(https://arxiv.org/abs/2402.18502)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Employing Large Language Models (LLM) in various downstream applications such as classification is crucial, especially for smaller companies lacking the expertise and resources required for fine-tuning a model. Fairness in LLMs helps ensure inclusivity, equal representation based on factors such as race, gender and promotes responsible AI deployment. As the use of LLMs has become increasingly prevalent, it is essential to assess whether LLMs can generate fair outcomes when subjected to considerations of fairness. In this study, we introduce a framework outlining fairness regulations aligned with various fairness definitions, with each definition being modulated by varying degrees of abstraction. We explore the configuration for in-context learning and the procedure for selecting in-context demonstrations using RAG, while incorporating fairness rules into the process. Experiments conducted with different LLMs indicate that GPT-4 delivers superior results in terms of both accuracy and fairness compared to other models. This work is one of the early attempts to achieve fairness in prediction tasks by utilizing LLMs through in-context learning.</li>
<li><strong>摘要：</strong>在分类等各种下游应用中采用大型语言模型 (LLM) 至关重要，特别是对于缺乏微调模型所需的专业知识和资源的小型公司。法学硕士的公平性有助于确保基于种族、性别等因素的包容性和平等代表性，并促进负责任的人工智能部署。随着法学硕士的使用变得越来越普遍，有必要评估法学硕士在考虑公平性时是否能够产生公平的结果。在这项研究中，我们引入了一个框架，概述了与各种公平定义相一致的公平法规，每个定义都通过不同程度的抽象进行了调整。我们探索情境学习的配置以及使用 RAG 选择情境演示的过程，同时将公平规则纳入该过程。对不同法学硕士进行的实验表明，与其他模型相比，GPT-4 在准确性和公平性方面均提供了出色的结果。这项工作是通过情境学习利用法学硕士实现预测任务公平性的早期尝试之一。</li>
</ul>

<h3>Title: Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Karami, Ali Ghodsi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18508">https://arxiv.org/abs/2402.18508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18508">https://arxiv.org/pdf/2402.18508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18508]] Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling(https://arxiv.org/abs/2402.18508)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of deep learning, the quest for models that balance expressivity with computational efficiency has never been more critical. This paper introduces Orchid, a novel architecture that reimagines sequence modeling by incorporating a new data-dependent convolution mechanism. Orchid is designed to address the inherent limitations of traditional attention mechanisms, particularly their quadratic complexity, without compromising the ability to capture long-range dependencies and in-context learning. At the core of Orchid lies the data-dependent convolution layer, which dynamically adjusts its kernel conditioned on input data using a dedicated conditioning neural network. We design two simple conditioning networks that maintain shift equivariance in the adaptive convolution operation. The dynamic nature of data-dependent convolution kernel, coupled with gating operations, grants Orchid high expressivity while maintaining efficiency and quasilinear scalability for long sequences. We rigorously evaluate Orchid across multiple domains, including language modeling and image classification, to showcase its performance and generality. Our experiments demonstrate that Orchid architecture not only outperforms traditional attention-based architectures such as BERT and Vision Transformers with smaller model sizes, but also extends the feasible sequence length beyond the limitations of the dense attention layers. This achievement represents a significant step towards more efficient and scalable deep learning models for sequence modeling.</li>
<li><strong>摘要：</strong>在快速发展的深度学习领域，寻求平衡表达能力和计算效率的模型从未如此重要。本文介绍了 Orchid，这是一种新颖的架构，它通过结合新的数据相关卷积机制来重新构想序列建模。 Orchid 旨在解决传统注意力机制的固有局限性，特别是其二次复杂性，而不影响捕获远程依赖性和上下文学习的能力。 Orchid 的核心在于数据相关的卷积层，它使用专用的调节神经网络根据输入数据动态调整其内核。我们设计了两个简单的条件网络，在自适应卷积运算中保持移位等方差。数据相关卷积核的动态特性与门控操作相结合，赋予 Orchid 高表达能力，同时保持长序列的效率和准线性可扩展性。我们跨多个领域严格评估 Orchid，包括语言建模和图像分类，以展示其性能和通用性。我们的实验表明，Orchid 架构不仅优于传统的基于注意力的架构，例如模型尺寸较小的 BERT 和 Vision Transformers，而且还扩展了可行的序列长度，超出了密集注意力层的限制。这一成就代表着朝着更高效、可扩展的序列建模深度学习模型迈出了重要一步。</li>
</ul>

<h3>Title: RNNs are not Transformers (Yet): The Key Bottleneck on In-context  Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Kaiyue Wen, Xingyu Dang, Kaifeng Lyu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18510">https://arxiv.org/abs/2402.18510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18510">https://arxiv.org/pdf/2402.18510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18510]] RNNs are not Transformers (Yet): The Key Bottleneck on In-context  Retrieval(https://arxiv.org/abs/2402.18510)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, retrieval-augmented generation, chain-of-thought</a></li>
<li><strong>Abstract: </strong>This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG) and adding a single Transformer layer, can elevate RNNs to be capable of solving all polynomial-time solvable problems with CoT, hence closing the representation gap with Transformers.</li>
<li><strong>摘要：</strong>本文研究了循环神经网络 (RNN) 和 Transformer 在解决算法问题方面的表示能力差距。我们专注于了解以处理长序列的内存效率而闻名的 RNN 是否可以与 Transformer 的性能相匹配，特别是在通过思想链 (CoT) 提示进行增强时。我们的理论分析表明，CoT 改进了 RNN，但不足以缩小与 Transformer 的差距。一个关键瓶颈在于 RNN 无法完美地从上下文中检索信息，即使使用 CoT：对于显式或隐式需要这种能力的几个任务，例如联想回忆和确定图是否是树，我们证明 RNN表达能力不足以解决任务，而变形金刚可以轻松解决它们。相反，我们证明，采用技术来增强 RNN 的上下文检索能力，包括检索增强生成（RAG）和添加单个 Transformer 层，可以使 RNN 能够通过 CoT 解决所有多项式时间可解决的问题，从而缩小了与变形金刚的代表性差距。</li>
</ul>

<h3>Title: Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt  Templates</h3>
<ul>
<li><strong>Authors: </strong>Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, Sanjeev Arora</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18540">https://arxiv.org/abs/2402.18540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18540">https://arxiv.org/pdf/2402.18540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18540]] Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt  Templates(https://arxiv.org/abs/2402.18540)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research. These models underwent alignment training and were considered safe. Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the "Pure Tuning, Safe Testing" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost eliminates them in some cases.</li>
<li><strong>摘要：</strong>Llama 2-Chat 等公共法学硕士推动了法学硕士研究的巨大活动。这些模型接受了对齐训练并被认为是安全的。最近齐等人。 （2023）报告说，即使是良性的微调（例如，在看似安全的数据集上）也可能会导致模型中出现不安全行为。当前的论文是关于减轻这种一致性损失的方法和最佳实践。通过对多种聊天模型（Meta 的 Llama 2-Chat、Mistral AI 的 Mistral 7B Instruct v0.2 和 OpenAI 的 GPT-3.5 Turbo）的大量实验，本文发现微调和推理过程中使用的提示模板在保持安全一致性，并提出“纯粹调整，安全测试”（PTST）原则——在没有安全提示的情况下微调模型，但在测试时包含它。在 GSM8K、ChatDoctor 和 OpenOrca 上的微调实验表明，PTST 显着减少了不安全行为的出现，在某些情况下甚至几乎消除了不安全行为。</li>
</ul>

<h3>Title: Implicit Bias of Next-Token Prediction</h3>
<ul>
<li><strong>Authors: </strong>Christos Thrampoulidis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18551">https://arxiv.org/abs/2402.18551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18551">https://arxiv.org/pdf/2402.18551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18551]] Implicit Bias of Next-Token Prediction(https://arxiv.org/abs/2402.18551)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that the parameters of GD projected onto an appropriate data subspace converge to the unique solution of a system of linear equations, which requires the logits' difference of in-support tokens to be equal to the log-ratio of their respective probabilities. Meanwhile, on the orthogonal subspace, the parameters diverge and converge in the direction of the solution of a max-margin quadratic program, minimizing the Euclidean norm of parameters satisfying the \NTP-separability conditions. Akin to prior research on implicit bias of one-hot classification, our work opens exciting avenues for future research that can lead to better understanding optimization, generalization and robustness principles of models trained with NTP.</li>
<li><strong>摘要：</strong>下一个标记预测 (NTP) 是训练大型语言模型的首选训练范例，涉及预测序列中的下一个标记。与传统的 one-hot 分类不同，在 NTP 中，每个给定的上下文遵循不同频率的多个标记。这项工作将 NTP 训练框架定义为不同上下文中的交叉熵最小化，每个上下文都与有限词汇中的稀疏经验概率向量相关联。然后它解决了以下问题：当 NTP 训练损失达到其下限（熵）时，基于梯度的优化器是否会表现出对具有特定结构的解决方案的偏见？具体来说，对于使用梯度下降（GD）训练的线性 NTP 模型，我们做出以下贡献：首先，我们确定数据上的 NTP 可分离性条件，在该条件下 GD 可以达到其下限。我们还证明了这些条件在过度参数化的情况下成立。其次，我们确定投影到适当数据子空间上的 GD 参数收敛于线性方程组的唯一解，这要求支持标记的 logits 差等于它们各自概率的对数比。同时，在正交子空间上，参数在最大边际二次规划解的方向上发散和收敛，从而最小化满足\NTP-可分离性条件的参数的欧几里德范数。类似于之前对单热分类隐式偏差的研究，我们的工作为未来的研究开辟了令人兴奋的途径，可以更好地理解用 NTP 训练的模型的优化、泛化和鲁棒性原理。</li>
</ul>

<h3>Title: Approaching Human-Level Forecasting with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Danny Halawi, Fred Zhang, Chen Yueh-Han, Jacob Steinhardt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18563">https://arxiv.org/abs/2402.18563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18563">https://arxiv.org/pdf/2402.18563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18563]] Approaching Human-Level Forecasting with Language Models(https://arxiv.org/abs/2402.18563)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.</li>
<li><strong>摘要：</strong>预测未来事件对于政策和决策非常重要。在这项工作中，我们研究语言模型（LM）是否可以达到与人类预测者竞争的水平。为了实现这一目标，我们开发了一种检索增强的 LM 系统，旨在自动搜索相关信息、生成预测和聚合预测。为了促进我们的研究，我们从竞争性预测平台收集了大量问题数据集。根据 LM 知识截止后发布的测试集，我们根据人类预测的汇总来评估我们系统的端到端性能。平均而言，该系统接近竞争性预测者的人群总数，并且在某些情况下甚至超过了它。我们的工作表明，使用语言模型来预测未来可以提供大规模的准确预测，并有助于为机构决策提供信息。</li>
</ul>

<h3>Title: Diffusion Language Models Are Versatile Protein Learners</h3>
<ul>
<li><strong>Authors: </strong>Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18567">https://arxiv.org/abs/2402.18567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18567">https://arxiv.org/pdf/2402.18567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18567]] Diffusion Language Models Are Versatile Protein Learners(https://arxiv.org/abs/2402.18567)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative self-supervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways: (1) conditioning on partial peptide sequences, e.g., generating scaffolds for functional motifs with high success rate; (2) incorporating other modalities as conditioner, e.g., structure-conditioned generation for inverse folding; and (3) steering sequence generation towards desired properties, e.g., satisfying specified secondary structures, through a plug-and-play classifier guidance.</li>
<li><strong>摘要：</strong>本文介绍了扩散蛋白语言模型（DPLM），这是一种多功能的蛋白质语言模型，展示了强大的蛋白质序列生成和预测能力。我们首先在生成式自监督离散扩散概率框架内根据进化规模的蛋白质序列预训练可扩展的 DPLM，该框架以原则性的方式概括了蛋白质的语言模型。经过预训练后，DPLM 表现出生成结构合理、新颖且多样化的蛋白质序列以进行无条件生成的能力。我们进一步证明了所提出的扩散生成预训练使 DPLM 能够更好地理解蛋白质，使其成为一种卓越的表示学习器，可以针对各种预测任务进行微调，与 ESM2 相比具有优势（Lin 等人，2022）。此外，DPLM可以针对各种需求进行定制，这在几个方面展示了其条件生成的能力：（1）对部分肽序列进行条件化，例如以高成功率生成功能基序的支架； （2）结合其他方式作为调节器，例如用于反向折叠的结构调节生成； (3)通过即插即用的分类器指导，引导序列生成朝向所需的属性，例如，满足指定的二级结构。</li>
</ul>

<h3>Title: Arithmetic Control of LLMs for Diverse User Preferences: Directional  Preference Alignment with Multi-Objective Rewards</h3>
<ul>
<li><strong>Authors: </strong>Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18571">https://arxiv.org/abs/2402.18571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18571">https://arxiv.org/pdf/2402.18571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18571]] Arithmetic Control of LLMs for Diverse User Preferences: Directional  Preference Alignment with Multi-Objective Rewards(https://arxiv.org/abs/2402.18571)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO).</li>
<li><strong>摘要：</strong>对大型语言模型 (LLM) 的细粒度控制仍然是一个重大挑战，阻碍了它们对不同用户需求的适应性。虽然人类反馈强化学习 (RLHF) 在协调法学硕士方面显示出希望，但其对标量奖励的依赖往往限制了其在现实应用程序中捕获不同用户偏好的能力。为了解决这个限制，我们引入了方向偏好对齐（DPA）框架。与标量奖励 RLHF 不同，DPA 结合了多目标奖励模型来表示不同的偏好概况。此外，DPA 将用户偏好建模为奖励空间中的方向（即单位向量），以实现依赖于用户的偏好控制。我们的方法包括训练多目标奖励模型，然后使用拒绝采样微调 (RSF) 的偏好条件变体对 LLM 进行微调，RSF 是 Llama 2 采用的 RLHF 方法。该方法在各种不同的条件下具有更好的性能权衡。奖励目标。与标量奖励 RLHF 相比，DPA 为用户提供了对 LLM 生成的直观控制：他们可以通过算术指定他们想要的权衡（例如，以更少的冗长提供更多帮助）。我们还通过 Mistral-7B 上的真实对齐实验验证了 DPA 的有效性。我们的方法提供了对有用性和冗长性之间权衡的直接算术控制，同时通过直接偏好优化（DPO）等强大的基线保持竞争性能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
