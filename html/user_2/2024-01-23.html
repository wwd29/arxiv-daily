<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-23</h1>
<h3>Title: A Review of Findings from Neuroscience and Cognitive Psychology as  Possible Inspiration for the Path to Artificial General Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Florin Leon</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10904">https://arxiv.org/abs/2401.10904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10904">https://arxiv.org/pdf/2401.10904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10904]] A Review of Findings from Neuroscience and Cognitive Psychology as  Possible Inspiration for the Path to Artificial General Intelligence(https://arxiv.org/abs/2401.10904)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>This review aims to contribute to the quest for artificial general intelligence by examining neuroscience and cognitive psychology methods for potential inspiration. Despite the impressive advancements achieved by deep learning models in various domains, they still have shortcomings in abstract reasoning and causal understanding. Such capabilities should be ultimately integrated into artificial intelligence systems in order to surpass data-driven limitations and support decision making in a way more similar to human intelligence. This work is a vertical review that attempts a wide-ranging exploration of brain function, spanning from lower-level biological neurons, spiking neural networks, and neuronal ensembles to higher-level concepts such as brain anatomy, vector symbolic architectures, cognitive and categorization models, and cognitive architectures. The hope is that these concepts may offer insights for solutions in artificial general intelligence.</li>
<li><strong>摘要：</strong>这篇综述旨在通过研究神经科学和认知心理学方法来寻找潜在的灵感，为通用人工智能的探索做出贡献。尽管深度学习模型在各个领域取得了令人印象深刻的进步，但它们在抽象推理和因果理解方面仍然存在缺陷。这些能力最终应该集成到人工智能系统中，以超越数据驱动的限制，并以更类似于人类智能的方式支持决策。这项工作是一项垂直综述，试图对大脑功能进行广泛的探索，从较低级别的生物神经元、尖峰神经网络和神经元集合到较高级别的概念，如大脑解剖学、矢量符号架构、认知和分类模型和认知架构。希望这些概念可以为通用人工智能的解决方案提供见解。</li>
</ul>

<h3>Title: MacroSwarm: A Field-based Compositional Framework for Swarm Programming</h3>
<ul>
<li><strong>Authors: </strong>Gianluca Aguzzi, Roberto Casadei, Mirko Viroli</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LO, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10969">https://arxiv.org/abs/2401.10969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10969">https://arxiv.org/pdf/2401.10969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10969]] MacroSwarm: A Field-based Compositional Framework for Swarm Programming(https://arxiv.org/abs/2401.10969)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Swarm behaviour engineering is an area of research that seeks to investigate methods and techniques for coordinating computation and action within groups of simple agents to achieve complex global goals like pattern formation, collective movement, clustering, and distributed sensing. Despite recent progress in the analysis and engineering of swarms (of drones, robots, vehicles), there is still a need for general design and implementation methods and tools that can be used to define complex swarm behaviour in a principled way. To contribute to this quest, this article proposes a new field-based coordination approach, called MacroSwarm, to design and program swarm behaviour in terms of reusable and fully composable functional blocks embedding collective computation and coordination. Based on the macroprogramming paradigm of aggregate computing, MacroSwarm builds on the idea of expressing each swarm behaviour block as a pure function mapping sensing fields into actuation goal fields, e.g. including movement vectors. In order to demonstrate the expressiveness, compositionality, and practicality of MacroSwarm as a framework for collective intelligence, we perform a variety of simulations covering common patterns of flocking, morphogenesis, and collective decision-making.</li>
<li><strong>摘要：</strong>群体行为工程是一个研究领域，旨在研究协调简单代理组内的计算和行动的方法和技术，以实现复杂的全局目标，如模式形成、集体运动、集群和分布式传感。尽管最近在群体（无人机、机器人、车辆）的分析和工程方面取得了进展，但仍然需要可用于以原则性方式定义复杂群体行为的通用设计和实现方法和工具。为了实现这一目标，本文提出了一种新的基于现场的协调方法，称为 MacroSwarm，根据嵌入集体计算和协调的可重用和完全可组合的功能块来设计和编程群体行为。基于聚合计算的宏编程范式，MacroSwarm 建立在将每个群体行为块表达为将感知场映射到驱动目标场的纯函数的思想之上，例如包括运动矢量。为了证明 MacroSwarm 作为集体智能框架的表现力、组合性和实用性，我们进行了各种模拟，涵盖了集群、形态发生和集体决策的常见模式。</li>
</ul>

<h3>Title: The Radiation Oncology NLP Database</h3>
<ul>
<li><strong>Authors: </strong>Zhengliang Liu, Jason Holmes, Wenxiong Liao, Chenbin Liu, Lian Zhang, Hongying Feng, Peilong Wang, Muhammad Ali Elahi, Hongmin Cai, Lichao Sun, Quanzheng Li, Xiang Li, Tianming Liu, Jiajian Shen, Wei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10995">https://arxiv.org/abs/2401.10995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10995">https://arxiv.org/pdf/2401.10995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10995]] The Radiation Oncology NLP Database(https://arxiv.org/abs/2401.10995)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora, chat</a></li>
<li><strong>Abstract: </strong>We present the Radiation Oncology NLP Database (ROND), the first dedicated Natural Language Processing (NLP) dataset for radiation oncology, an important medical specialty that has received limited attention from the NLP community in the past. With the advent of Artificial General Intelligence (AGI), there is an increasing need for specialized datasets and benchmarks to facilitate research and development. ROND is specifically designed to address this gap in the domain of radiation oncology, a field that offers many opportunities for NLP exploration. It encompasses various NLP tasks including Logic Reasoning, Text Classification, Named Entity Recognition (NER), Question Answering (QA), Text Summarization, and Patient-Clinician Conversations, each with a distinct focus on radiation oncology concepts and application cases. In addition, we have developed an instruction-tuning dataset consisting of over 20k instruction pairs (based on ROND) and trained a large language model, CancerChat. This serves to demonstrate the potential of instruction-tuning large language models within a highly-specialized medical domain. The evaluation results in this study could serve as baseline results for future research. ROND aims to stimulate advancements in radiation oncology and clinical NLP by offering a platform for testing and improving algorithms and models in a domain-specific context. The ROND dataset is a joint effort of multiple U.S. health institutions. The data is available at https://github.com/zl-liu/Radiation-Oncology-NLP-Database.</li>
<li><strong>摘要：</strong>我们推出了放射肿瘤学 NLP 数据库 (ROND)，这是第一个专门用于放射肿瘤学的自然语言处理 (NLP) 数据集，放射肿瘤学是一个重要的医学专业，过去受到 NLP 社区的关注有限。随着通用人工智能 (AGI) 的出现，对专门数据集和基准来促进研究和开发的需求不断增加。 ROND 专为解决放射肿瘤学领域的这一空白而设计，该领域为 NLP 探索提供了许多机会。它涵盖各种 NLP 任务，包括逻辑推理、文本分类、命名实体识别 (NER)、问答 (QA)、文本摘要和患者与临床医生对话，每个任务都明确关注放射肿瘤学概念和应用案例。此外，我们还开发了一个由超过 20k 指令对组成的指令调优数据集（基于 ROND），并训练了一个大型语言模型 CancerChat。这证明了在高度专业化的医学领域内对大型语言模型进行指令调整的潜力。本研究的评估结果可以作为未来研究的基线结果。 ROND 旨在通过提供一个在特定领域环境中测试和改进算法和模型的平台来刺激放射肿瘤学和临床 NLP 的进步。 ROND 数据集是美国多个卫生机构的共同努力。数据可在 https://github.com/zl-liu/Radiation-Oncology-NLP-Database 获取。</li>
</ul>

<h3>Title: Communication Efficient and Provable Federated Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Youming Tao, Cheng-Long Wang, Miao Pan, Dongxiao Yu, Xiuzhen Cheng, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11018">https://arxiv.org/abs/2401.11018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11018">https://arxiv.org/pdf/2401.11018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11018]] Communication Efficient and Provable Federated Unlearning(https://arxiv.org/abs/2401.11018)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We study federated unlearning, a novel problem to eliminate the impact of specific clients or data points on the global model learned via federated learning (FL). This problem is driven by the right to be forgotten and the privacy challenges in FL. We introduce a new framework for exact federated unlearning that meets two essential criteria: \textit{communication efficiency} and \textit{exact unlearning provability}. To our knowledge, this is the first work to tackle both aspects coherently. We start by giving a rigorous definition of \textit{exact} federated unlearning, which guarantees that the unlearned model is statistically indistinguishable from the one trained without the deleted data. We then pinpoint the key property that enables fast exact federated unlearning: total variation (TV) stability, which measures the sensitivity of the model parameters to slight changes in the dataset. Leveraging this insight, we develop a TV-stable FL algorithm called \texttt{FATS}, which modifies the classical \texttt{\underline{F}ed\underline{A}vg} algorithm for \underline{T}V \underline{S}tability and employs local SGD with periodic averaging to lower the communication round. We also design efficient unlearning algorithms for \texttt{FATS} under two settings: client-level and sample-level unlearning. We provide theoretical guarantees for our learning and unlearning algorithms, proving that they achieve exact federated unlearning with reasonable convergence rates for both the original and unlearned models. We empirically validate our framework on 6 benchmark datasets, and show its superiority over state-of-the-art methods in terms of accuracy, communication cost, computation cost, and unlearning efficacy.</li>
<li><strong>摘要：</strong>我们研究联邦取消学习，这是一个新问题，旨在消除特定客户端或数据点对通过联邦学习（FL）学习的全局模型的影响。这个问题是由佛罗里达州的被遗忘权和隐私挑战造成的。我们引入了一个新的精确联合遗忘框架，它满足两个基本标准：\textit{通信效率}和\textit{精确遗忘证明性}。据我们所知，这是第一个连贯地解决这两个方面的工作。我们首先给出 \textit{exact} 联合取消学习的严格定义，它保证了未学习的模型与没有删除数据的训练模型在统计上无法区分。然后，我们查明实现快速精确联合取消学习的关键属性：总变分 (TV) 稳定性，它衡量模型参数对数据集中微小变化的敏感性。利用这一见解，我们开发了一种名为 \texttt{FATS} 的电视稳定 FL 算法，该算法将经典的 \texttt{\underline{F}ed\underline{A}vg} 算法修改为 \underline{T}V \underline{ S}稳定性并采用本地 SGD 和周期性平均来降低通信轮数。我们还在两种设置下为 \texttt{FATS} 设计了高效的遗忘算法：客户端级和样本级遗忘。我们为我们的学习和取消学习算法提供了理论保证，证明它们能够以合理的收敛速度为原始模型和未学习模型实现精确的联合取消学习。我们在 6 个基准数据集上实证验证了我们的框架，并在准确性、通信成本、计算成本和忘却效率方面展示了其相对于最先进方法的优越性。</li>
</ul>

<h3>Title: FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for  Large Language Models' Training?</h3>
<ul>
<li><strong>Authors: </strong>Shaina Raza, Shardul Ghuge, Chen Ding, Deval Pandya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11033">https://arxiv.org/abs/2401.11033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11033">https://arxiv.org/pdf/2401.11033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11033]] FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for  Large Language Models' Training?(https://arxiv.org/abs/2401.11033)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Advancements in Large Language Models (LLMs) highlight the need for ethical practices and data integrity. We introduce a framework that embeds FAIR (Findable, Accessible, Interoperable, Reusable) data principles into LLM training. This approach marks a shift towards practices compliant with FAIR standards. Our framework presents guidelines for integrating FAIR data principles into LLM training. This initiative includes a checklist for researchers and developers. We also demonstrate its practical application through a case study focused on bias identification and mitigation in our FAIR-compliant dataset. This work is a significant contribution to AI ethics and data science, advocating for balanced and ethical training methods in LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的进步凸显了对道德实践和数据完整性的需求。我们引入了一个框架，将公平（可查找、可访问、可互操作、可重用）数据原则嵌入到法学硕士培训中。这种方法标志着向符合 FAIR 标准的做法的转变。我们的框架提供了将公平数据原则整合到法学硕士培训中的指南。该计划包括一份针对研究人员和开发人员的清单。我们还通过一个案例研究展示了其实际应用，该案例研究重点关注我们符合 FAIR 的数据集中的偏差识别和缓解。这项工作对人工智能伦理和数据科学做出了重大贡献，倡导法学硕士的平衡和道德培训方法。</li>
</ul>

<h3>Title: PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical  Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Chih-Hsuan Wei, Alexis Allot, Po-Ting Lai, Robert Leaman, Shubo Tian, Ling Luo, Qiao Jin, Zhizheng Wang, Qingyu Chen, Zhiyong Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11048">https://arxiv.org/abs/2401.11048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11048">https://arxiv.org/pdf/2401.11048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11048]] PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical  Knowledge(https://arxiv.org/abs/2401.11048)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art AI techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive set of features and tools that allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery.</li>
<li><strong>摘要：</strong>PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) 是一个生物医学文献资源，使用最先进的 AI 技术为蛋白质、遗传等关键概念提供语义和关系搜索。变异、疾病和化学物质。目前，它提供了超过 10 亿个实体和关系注释，涵盖约 3600 万篇 PubMed 摘要和来自 PMC 开放获取子集的 600 万篇全文文章，每周更新一次。 PubTator 3.0 的在线界面和 API 利用这些预先计算的实体关系和同义词来提供高级搜索功能并实现大规模分析，从而简化许多复杂的信息需求。我们使用一系列实体对查询来展示 PubTator 3.0 的检索质量，证明 PubTator 3.0 检索的文章数量比 PubMed 或 Google Scholar 多，并且前 20 个结果的精度更高。我们进一步表明，将 ChatGPT (GPT-4) 与 PubTator API 集成可以显着提高其响应的真实性和可验证性。总之，PubTator 3.0 提供了一套全面的功能和工具，使研究人员能够浏览不断丰富的生物医学文献，加快研究速度并为科学发现释放有价值的见解。</li>
</ul>

<h3>Title: Mining experimental data from Materials Science literature with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Luca Foppiano, Guillaume Lambard, Toshiyuki Amagasa, Masashi Ishii</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11052">https://arxiv.org/abs/2401.11052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11052">https://arxiv.org/pdf/2401.11052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11052]] Mining experimental data from Materials Science literature with Large  Language Models(https://arxiv.org/abs/2401.11052)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study is dedicated to evaluating the capabilities of advanced large language models (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in the extraction of structured information from scientific documents within the field of materials science. We introduce a novel methodology for the comparative analysis of intricate material expressions, emphasising the standardisation of chemical formulas to tackle the complexities inherent in materials science information assessment. To this end, we primarily focus on two critical tasks of information extraction: (i) a named entity recognition (NER) of studied materials and physical properties and (ii) a relation extraction (RE) between these entities. The performance of LLMs in executing these tasks is benchmarked against traditional models based on the BERT architecture and rule-based approaches. For NER, LLMs fail to outperform the baseline with zero-shot prompting and exhibit only limited improvement with few-shot prompting. However, for RE, a GPT-3.5-Turbo fine-tuned with the appropriate strategy outperforms all models, including the baseline. Without any fine-tuning, GPT-4 and GPT-4-Turbo display remarkable reasoning and relationship extraction capabilities after being provided with merely a couple of examples, surpassing the baseline. Overall, the results suggest that although LLMs demonstrate relevant reasoning skills in connecting concepts, for tasks requiring extracting complex domain-specific entities like materials, specialised models are currently a better choice.</li>
<li><strong>摘要：</strong>本研究致力于评估先进大语言模型 (LLM)（例如 GPT-3.5-Turbo、GPT-4 和 GPT-4-Turbo）从材料科学领域的科学文档中提取结构化信息的能力。我们引入了一种新的方法来对复杂的材料表达进行比较分析，强调化学式的标准化，以解决材料科学信息评估中固有的复杂性。为此，我们主要关注信息提取的两个关键任务：（i）研究材料和物理属性的命名实体识别（NER）和（ii）这些实体之间的关系提取（RE）。法学硕士在执行这些任务时的表现以基于 BERT 架构和基于规则的方法的传统模型为基准。对于 NER，LLM 在零样本提示下无法超越基线，并且在少样本提示下仅表现出有限的改进。然而，对于 RE，使用适当策略进行微调的 GPT-3.5-Turbo 优于所有模型，包括基线。在没有任何微调的情况下，GPT-4 和 GPT-4-Turbo 在仅提供几个示例后就显示出卓越的推理和关系提取能力，超越了基线。总体而言，结果表明，尽管法学硕士在连接概念方面表现出相关的推理技能，但对于需要提取复杂的特定领域实体（如材料）的任务，专业模型目前是更好的选择。</li>
</ul>

<h3>Title: On The Temporal Domain of Differential Equation Inspired Graph Neural  Networks</h3>
<ul>
<li><strong>Authors: </strong>Moshe Eliasof, Eldad Haber, Eran Treister, Carola-Bibiane Schönlieb</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11074">https://arxiv.org/abs/2401.11074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11074">https://arxiv.org/pdf/2401.11074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11074]] On The Temporal Domain of Differential Equation Inspired Graph Neural  Networks(https://arxiv.org/abs/2401.11074)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated remarkable success in modeling complex relationships in graph-structured data. A recent innovation in this field is the family of Differential Equation-Inspired Graph Neural Networks (DE-GNNs), which leverage principles from continuous dynamical systems to model information flow on graphs with built-in properties such as feature smoothing or preservation. However, existing DE-GNNs rely on first or second-order temporal dependencies. In this paper, we propose a neural extension to those pre-defined temporal dependencies. We show that our model, called TDE-GNN, can capture a wide range of temporal dynamics that go beyond typical first or second-order methods, and provide use cases where existing temporal models are challenged. We demonstrate the benefit of learning the temporal dependencies using our method rather than using pre-defined temporal dynamics on several graph benchmarks.</li>
<li><strong>摘要：</strong>图神经网络（GNN）在对图结构数据中的复杂关系进行建模方面取得了显着的成功。该领域最近的一项创新是受微分方程启发的图神经网络 (DE-GNN) 系列，它利用连续动态系统的原理，对具有特征平滑或保留等内置属性的图上的信息流进行建模。然而，现有的 DE-GNN 依赖于一阶或二阶时间依赖性。在本文中，我们提出了对这些预定义时间依赖性的神经扩展。我们证明，我们的模型（称为 TDE-GNN）可以捕获超出典型一阶或二阶方法的广泛时间动态，并提供现有时间模型受到挑战的用例。我们展示了使用我们的方法学习时间依赖性的好处，而不是在几个图基准上使用预定义的时间动态。</li>
</ul>

<h3>Title: TypeDance: Creating Semantic Typographic Logos from Image through  Personalized Generation</h3>
<ul>
<li><strong>Authors: </strong>Shishi Xiao, Liangwei Wang, Xiaojuan Ma, Wei Zeng</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11094">https://arxiv.org/abs/2401.11094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11094">https://arxiv.org/pdf/2401.11094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11094]] TypeDance: Creating Semantic Typographic Logos from Image through  Personalized Generation(https://arxiv.org/abs/2401.11094)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Semantic typographic logos harmoniously blend typeface and imagery to represent semantic concepts while maintaining legibility. Conventional methods using spatial composition and shape substitution are hindered by the conflicting requirement for achieving seamless spatial fusion between geometrically dissimilar typefaces and semantics. While recent advances made AI generation of semantic typography possible, the end-to-end approaches exclude designer involvement and disregard personalized design. This paper presents TypeDance, an AI-assisted tool incorporating design rationales with the generative model for personalized semantic typographic logo design. It leverages combinable design priors extracted from uploaded image exemplars and supports type-imagery mapping at various structural granularity, achieving diverse aesthetic designs with flexible control. Additionally, we instantiate a comprehensive design workflow in TypeDance, including ideation, selection, generation, evaluation, and iteration. A two-task user evaluation, including imitation and creation, confirmed the usability of TypeDance in design across different usage scenarios</li>
<li><strong>摘要：</strong>语义印刷徽标和谐地融合了字体和图像，以表示语义概念，同时保持易读性。使用空间合成和形状替换的传统方法受到在几何上不同的字体和语义之间实现无缝空间融合的冲突要求的阻碍。虽然最近的进展使得人工智能生成语义排版成为可能，但端到端方法排除了设计师的参与并忽视了个性化设计。本文介绍了 TypeDance，这是一种人工智能辅助工具，将设计原理与个性化语义印刷标志设计的生成模型相结合。它利用从上传的图像样本中提取的可组合设计先验，并支持各种结构粒度的类型图像映射，通过灵活的控制实现多样化的美学设计。此外，我们在 TypeDance 中实例化了一个全面的设计工作流程，包括构思、选择、生成、评估和迭代。通过模仿和创作两项任务的用户评估，证实了TypeDance在不同使用场景的设计中的可用性</li>
</ul>

<h3>Title: Exploiting Duality in Open Information Extraction with Predicate Prompt</h3>
<ul>
<li><strong>Authors: </strong>Zhen Chen, Jingping Liu, Deqing Yang, Yanghua Xiao, Huimin Xu, Zongyu Wang, Rui Xie, Yunsen Xian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11107">https://arxiv.org/abs/2401.11107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11107">https://arxiv.org/pdf/2401.11107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11107]] Exploiting Duality in Open Information Extraction with Predicate Prompt(https://arxiv.org/abs/2401.11107)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, rag</a></li>
<li><strong>Abstract: </strong>Open information extraction (OpenIE) aims to extract the schema-free triplets in the form of (\emph{subject}, \emph{predicate}, \emph{object}) from a given sentence. Compared with general information extraction (IE), OpenIE poses more challenges for the IE models, {especially when multiple complicated triplets exist in a sentence. To extract these complicated triplets more effectively, in this paper we propose a novel generative OpenIE model, namely \emph{DualOIE}, which achieves a dual task at the same time as extracting some triplets from the sentence, i.e., converting the triplets into the sentence.} Such dual task encourages the model to correctly recognize the structure of the given sentence and thus is helpful to extract all potential triplets from the sentence. Specifically, DualOIE extracts the triplets in two steps: 1) first extracting a sequence of all potential predicates, 2) then using the predicate sequence as a prompt to induce the generation of triplets. Our experiments on two benchmarks and our dataset constructed from Meituan demonstrate that DualOIE achieves the best performance among the state-of-the-art baselines. Furthermore, the online A/B test on Meituan platform shows that 0.93\% improvement of QV-CTR and 0.56\% improvement of UV-CTR have been obtained when the triplets extracted by DualOIE were leveraged in Meituan's search system.</li>
<li><strong>摘要：</strong>开放信息提取 (OpenIE) 旨在从给定句子中提取 (\emph{subject}, \emph{predicate}, \emph{object}) 形式的无模式三元组。与一般信息提取（IE）相比，OpenIE 对 IE 模型提出了更多挑战，{特别是当句子中存在多个复杂的三元组时。为了更有效地提取这些复杂的三元组，在本文中，我们提出了一种新的生成式 OpenIE 模型，即 \emph{DualOIE}，它在从句子中提取一些三元组的同时实现了双重任务，即将三元组转换为这种双重任务鼓励模型正确识别给定句子的结构，从而有助于从句子中提取所有潜在的三元组。具体来说，DualOIE 分两步提取三元组：1）首先提取所有潜在谓词的序列，2）然后使用谓词序列作为提示来诱导三元组的生成。我们对两个基准的实验以及从美团构建的数据集表明，DualOIE 在最先进的基准中实现了最佳性能。此外，美团平台在线A/B测试表明，将DualOIE提取的三元组应用于美团搜索系统时，QV-CTR提高了0.93%，UV-CTR提高了0.56%。</li>
</ul>

<h3>Title: Meta Reinforcement Learning for Strategic IoT Deployments Coverage in  Disaster-Response UAV Swarms</h3>
<ul>
<li><strong>Authors: </strong>Marwan Dhuheir, Aiman Erbad, Ala Al-Fuqaha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11118">https://arxiv.org/abs/2401.11118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11118">https://arxiv.org/pdf/2401.11118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11118]] Meta Reinforcement Learning for Strategic IoT Deployments Coverage in  Disaster-Response UAV Swarms(https://arxiv.org/abs/2401.11118)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In the past decade, Unmanned Aerial Vehicles (UAVs) have grabbed the attention of researchers in academia and industry for their potential use in critical emergency applications, such as providing wireless services to ground users and collecting data from areas affected by disasters, due to their advantages in terms of maneuverability and movement flexibility. The UAVs' limited resources, energy budget, and strict mission completion time have posed challenges in adopting UAVs for these applications. Our system model considers a UAV swarm that navigates an area collecting data from ground IoT devices focusing on providing better service for strategic locations and allowing UAVs to join and leave the swarm (e.g., for recharging) in a dynamic way. In this work, we introduce an optimization model with the aim of minimizing the total energy consumption and provide the optimal path planning of UAVs under the constraints of minimum completion time and transmit power. The formulated optimization is NP-hard making it not applicable for real-time decision making. Therefore, we introduce a light-weight meta-reinforcement learning solution that can also cope with sudden changes in the environment through fast convergence. We conduct extensive simulations and compare our approach to three state-of-the-art learning models. Our simulation results prove that our introduced approach is better than the three state-of-the-art algorithms in providing coverage to strategic locations with fast convergence.</li>
<li><strong>摘要：</strong>在过去的十年中，无人机（UAV）因其在关键紧急应用中的潜在用途而引起了学术界和工业界研究人员的关注，例如为地面用户提供无线服务和从受灾害影响的地区收集数据。在机动性和运动灵活性方面具有优势。无人机有限的资源、能源预算和严格的任务完成时间给在这些应用中采用无人机带来了挑战。我们的系统模型考虑了一个无人机群，它在一个区域中导航，从地面物联网设备收集数据，重点是为战略位置提供更好的服务，并允许无人机以动态方式加入和离开群（例如，充电）。在这项工作中，我们引入了一种优化模型，旨在最小化总能耗，并在最小完成时间和发射功率的约束下提供无人机的最优路径规划。公式化的优化是 NP 困难的，因此不适用于实时决策。因此，我们引入了一种轻量级的元强化学习解决方案，它也可以通过快速收敛来应对环境的突然变化。我们进行了广泛的模拟，并将我们的方法与三种最先进的学习模型进行了比较。我们的模拟结果证明，我们引入的方法在以快速收敛的方式提供对战略位置的覆盖方面优于三种最先进的算法。</li>
</ul>

<h3>Title: Enhancing Large Language Models for Clinical Decision Support by  Incorporating Clinical Practice Guidelines</h3>
<ul>
<li><strong>Authors: </strong>David Oniani, Xizhi Wu, Shyam Visweswaran, Sumit Kapoor, Shravan Kooragayalu, Katelyn Polanska, Yanshan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11120">https://arxiv.org/abs/2401.11120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11120">https://arxiv.org/pdf/2401.11120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11120]] Enhancing Large Language Models for Clinical Decision Support by  Incorporating Clinical Practice Guidelines(https://arxiv.org/abs/2401.11120)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Background Large Language Models (LLMs), enhanced with Clinical Practice Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS). However, methods for incorporating CPGs into LLMs are not well studied. Methods We develop three distinct methods for incorporating CPGs into LLMs: Binary Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of the proposed methods, we create a set of synthetic patient descriptions and conduct both automatic and human evaluation of the responses generated by four LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was used as the baseline method. We focus on CDS for COVID-19 outpatient treatment as the case study. Results All four LLMs exhibit improved performance when enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP and PAGC in automatic evaluation. All of the proposed methods demonstrated high performance in human evaluation. Conclusion LLMs enhanced with CPGs demonstrate superior performance, as compared to plain LLMs with ZSP, in providing accurate recommendations for COVID-19 outpatient treatment, which also highlights the potential for broader applications beyond the case study.</li>
<li><strong>摘要：</strong>背景 大语言模型 (LLM) 通过临床实践指南 (CPG) 得到增强，可以显着改善临床决策支持 (CDS)。然而，将 CPG 纳入 LLM 的方法尚未得到充分研究。方法 我们开发了三种不同的方法将 CPG 合并到 LLM 中：二元决策树 (BDT)、程序辅助图构建 (PAGC) 和思想链少镜头提示 (CoT-FSP)。为了评估所提出方法的有效性，我们创建了一组综合患者描述，并对四个 LLM 生成的响应进行自动和人工评估：GPT-4、GPT-3.5 Turbo、LLaMA 和 PaLM 2。 零射击使用提示（ZSP）作为基线方法。我们重点关注 CDS 用于 COVID-19 门诊治疗作为案例研究。结果 与基线 ZSP 相比，所有四个法学硕士在使用 CPG 增强后都表现出更高的性能。 BDT 在自动评估方面优于 CoT-FSP 和 PAGC。所有提出的方法都在人类评估中表现出了高性能。结论 与使用 ZSP 的普通法学硕士相比，使用 CPG 增强的法学硕士在为 COVID-19 门诊治疗提供准确建议方面表现出优越的性能，这也凸显了案例研究之外更广泛应用的潜力。</li>
</ul>

<h3>Title: Identification and Estimation of Conditional Average Partial Causal  Effects via Instrumental Variable</h3>
<ul>
<li><strong>Authors: </strong>Yuta Kawakami, Manabu Kuroki, Jin Tian</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11130">https://arxiv.org/abs/2401.11130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11130">https://arxiv.org/pdf/2401.11130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11130]] Identification and Estimation of Conditional Average Partial Causal  Effects via Instrumental Variable(https://arxiv.org/abs/2401.11130)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>There has been considerable recent interest in estimating heterogeneous causal effects. In this paper, we introduce conditional average partial causal effects (CAPCE) to reveal the heterogeneity of causal effects with continuous treatment. We provide conditions for identifying CAPCE in an instrumental variable setting. We develop three families of CAPCE estimators: sieve, parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze their statistical properties. We illustrate the proposed CAPCE estimators on synthetic and real-world data.</li>
<li><strong>摘要：</strong>最近人们对估计异质因果效应产生了相当大的兴趣。在本文中，我们引入条件平均部分因果效应（CAPCE）来揭示连续治疗因果效应的异质性。我们提供了在工具变量设置中识别 CAPCE 的条件。我们开发了三个系列的 CAPCE 估计器：筛式估计器、参数估计器和基于再生核希尔伯特空间 (RKHS) 的估计器，并分析了它们的统计特性。我们在合成数据和真实数据上说明了所提出的 CAPCE 估计器。</li>
</ul>

<h3>Title: How the Advent of Ubiquitous Large Language Models both Stymie and  Turbocharge Dynamic Adversarial Question Generation</h3>
<ul>
<li><strong>Authors: </strong>Yoo Yeon Sung, Ishani Mondal, Jordan Boyd-Graber</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11185">https://arxiv.org/abs/2401.11185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11185">https://arxiv.org/pdf/2401.11185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11185]] How the Advent of Ubiquitous Large Language Models both Stymie and  Turbocharge Dynamic Adversarial Question Generation(https://arxiv.org/abs/2401.11185)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Dynamic adversarial question generation, where humans write examples to stump a model, aims to create examples that are realistic and informative. However, the advent of large language models (LLMs) has been a double-edged sword for human authors: more people are interested in seeing and pushing the limits of these models, but because the models are so much stronger an opponent, they are harder to defeat. To understand how these models impact adversarial question writing process, we enrich the writing guidance with LLMs and retrieval models for the authors to reason why their questions are not adversarial. While authors could create interesting, challenging adversarial questions, they sometimes resort to tricks that result in poor questions that are ambiguous, subjective, or confusing not just to a computer but also to humans. To address these issues, we propose new metrics and incentives for eliciting good, challenging questions and present a new dataset of adversarially authored questions.</li>
<li><strong>摘要：</strong>动态对抗性问题生成是人类编写示例来难倒模型的过程，旨在创建现实且信息丰富的示例。然而，大型语言模型（LLM）的出现对于人类作者来说是一把双刃剑：越来越多的人有兴趣看到并突破这些模型的极限，但由于这些模型的对手强大得多，因此它们变得更加困难打败。为了了解这些模型如何影响对抗性问题的写作过程，我们通过法学硕士和检索模型丰富了写作指导，以便作者推理为什么他们的问题不是对抗性的。虽然作者可以创造有趣的、具有挑战性的对抗性问题，但他们有时会采取一些技巧，导致提出的问题不明确、主观或不仅对计算机而且对人类来说都是模糊的、主观的或令人困惑的。为了解决这些问题，我们提出了新的指标和激励措施来引发好的、具有挑战性的问题，并提出了一个新的对抗性问题数据集。</li>
</ul>

<h3>Title: PartIR: Composing SPMD Partitioning Strategies for Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Sami Alabed, Bart Chrzaszcz, Juliana Franco, Dominik Grewe, Dougal Maclaurin, James Molloy, Tom Natan, Tamara Norman, Xiaoyue Pan, Adam Paszke, Norman A. Rink, Michael Schaarschmidt, Timur Sitdikov, Agnieszka Swietlik, Dimitrios Vytiniotis, Joel Wee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11202">https://arxiv.org/abs/2401.11202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11202">https://arxiv.org/pdf/2401.11202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11202]] PartIR: Composing SPMD Partitioning Strategies for Machine Learning(https://arxiv.org/abs/2401.11202)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..</li>
<li><strong>摘要：</strong>现代大型神经网络 (NN) 的训练需要结合包括数据、模型或优化器分片在内的并行化策略。当策略的复杂性增加时，分区工具就必须具有以下特点：1）具有表现力，允许组合更简单的策略；2）可预测，以分析方式估计性能。我们展示了 PartIR，我们的神经网络分区系统设计。 PartIR 专注于增量重写方法，并且与硬件和运行时无关。我们提供了一个简单但功能强大的 API，用于编写分片策略和一个模拟器来验证它们。该过程由高级程序员发布的分区策略驱动，可以是手动的，也可以是自动的。重要的是，策略是与模型代码分开指定的，因此很容易更改。我们在几种不同的模型上评估 PartIR，以证明其可预测性、可表达性以及达到峰值性能的能力。</li>
</ul>

<h3>Title: InferAligner: Inference-Time Alignment for Harmlessness through  Cross-Model Guidance</h3>
<ul>
<li><strong>Authors: </strong>Pengyu Wang, Dong Zhang, Linyang Li, Chenkun Tan, Xinghao Wang, Ke Ren, Botian Jiang, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11206">https://arxiv.org/abs/2401.11206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11206">https://arxiv.org/pdf/2401.11206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11206]] InferAligner: Inference-Time Alignment for Harmlessness through  Cross-Model Guidance(https://arxiv.org/abs/2401.11206)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the rapid development of large language models (LLMs), they are not only used as general-purpose AI assistants but are also customized through further fine-tuning to meet the requirements of different applications. A pivotal factor in the success of current LLMs is the alignment process. Current alignment methods, such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), focus on training-time alignment and are often complex and cumbersome to implement. Therefore, we develop \textbf{InferAligner}, a novel inference-time alignment method that utilizes cross-model guidance for harmlessness alignment. InferAligner utilizes safety steering vectors extracted from safety-aligned model to modify the activations of the target model when responding to harmful inputs, thereby guiding the target model to provide harmless responses. Experimental results show that our method can be very effectively applied to domain-specific models in finance, medicine, and mathematics, as well as to multimodal large language models (MLLMs) such as LLaVA. It significantly diminishes the Attack Success Rate (ASR) of both harmful instructions and jailbreak attacks, while maintaining almost unchanged performance in downstream tasks.</li>
<li><strong>摘要：</strong>随着大语言模型（LLM）的快速发展，它们不仅被用作通用的人工智能助手，而且还通过进一步微调进行定制，以满足不同应用的需求。当前法学硕士成功的关键因素是调整过程。当前的对齐方法，例如监督微调（SFT）和来自人类反馈的强化学习（RLHF），专注于训练时间对齐，并且实施起来通常很复杂且繁琐。因此，我们开发了 \textbf{InferAligner}，一种新颖的推理时间对齐方法，利用跨模型指导进行无害对齐。 InferAligner利用从安全对齐模型中提取的安全引导向量来修改目标模型在响应有害输入时的激活，从而引导目标模型提供无害响应。实验结果表明，我们的方法可以非常有效地应用于金融、医学和数学领域的特定模型，以及 LLaVA 等多模态大语言模型（MLLM）。它显着降低了有害指令和越狱攻击的攻击成功率（ASR），同时保持下游任务的性能几乎不变。</li>
</ul>

<h3>Title: Unfair TOS: An Automated Approach using Customized BERT</h3>
<ul>
<li><strong>Authors: </strong>Bathini Sai Akash, Akshara Kupireddy, Lalita Bhanu Murthy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11207">https://arxiv.org/abs/2401.11207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11207">https://arxiv.org/pdf/2401.11207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11207]] Unfair TOS: An Automated Approach using Customized BERT(https://arxiv.org/abs/2401.11207)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Terms of Service (ToS) form an integral part of any agreement as it defines the legal relationship between a service provider and an end-user. Not only do they establish and delineate reciprocal rights and responsibilities, but they also provide users with information on essential aspects of contracts that pertain to the use of digital spaces. These aspects include a wide range of topics, including limitation of liability, data protection, etc. Users tend to accept the ToS without going through it before using any application or service. Such ignorance puts them in a potentially weaker situation in case any action is required. Existing methodologies for the detection or classification of unfair clauses are however obsolete and show modest performance. In this research paper, we present SOTA(State of The Art) results on unfair clause detection from ToS documents based on unprecedented Fine-tuning BERT in integration with SVC(Support Vector Classifier). The study shows proficient performance with a macro F1-score of 0.922 at unfair clause detection, and superior performance is also shown in the classification of unfair clauses by each tag. Further, a comparative analysis is performed by answering research questions on the Transformer models utilized. In order to further research and experimentation the code and results are made available on https://github.com/batking24/Unfair-TOS-An-Automated-Approach-based-on-Fine-tuning-BERT-in-conjunction-with-ML.</li>
<li><strong>摘要：</strong>服务条款 (ToS) 是任何协议的组成部分，因为它定义了服务提供商和最终用户之间的法律关系。它们不仅建立和界定相互的权利和责任，而且还为用户提供与数字空间使用相关的合同基本方面的信息。这些方面涵盖了广泛的主题，包括责任限制、数据保护等。用户倾向于在使用任何应用程序或服务之前不经过阅读就接受 ToS。如果需要采取任何行动，这种无知会使他们处于潜在的弱势地位。然而，用于检测或分类不公平条款的现有方法已经过时，并且表现不佳。在这篇研究论文中，我们展示了基于前所未有的微调 BERT 与 SVC（支持向量分类器）集成的 ToS 文档中不公平条款检测的 SOTA（最先进）结果。研究表明，在不公平条款检测方面，其宏观 F1 得分为 0.922，表现出色，并且在按每个标签对不公平条款进行分类方面也表现出优异的性能。此外，通过回答有关所使用的 Transformer 模型的研究问题来进行比较分析。为了进一步研究和实验，代码和结果可在 https://github.com/batking24/Unfair-TOS-An-Automated-Approach-based-on-Fine-tuning-BERT-in-conjunction-with 上获取-ML。</li>
</ul>

<h3>Title: TreeMIL: A Multi-instance Learning Framework for Time Series Anomaly  Detection with Inexact Supervision</h3>
<ul>
<li><strong>Authors: </strong>Chen Liu, Shibo He, Haoyu Liu, Shizhong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11235">https://arxiv.org/abs/2401.11235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11235">https://arxiv.org/pdf/2401.11235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11235]] TreeMIL: A Multi-instance Learning Framework for Time Series Anomaly  Detection with Inexact Supervision(https://arxiv.org/abs/2401.11235)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection (TSAD) plays a vital role in various domains such as healthcare, networks, and industry. Considering labels are crucial for detection but difficult to obtain, we turn to TSAD with inexact supervision: only series-level labels are provided during the training phase, while point-level anomalies are predicted during the testing phase. Previous works follow a traditional multi-instance learning (MIL) approach, which focuses on encouraging high anomaly scores at individual time steps. However, time series anomalies are not only limited to individual point anomalies, they can also be collective anomalies, typically exhibiting abnormal patterns over subsequences. To address the challenge of collective anomalies, in this paper, we propose a tree-based MIL framework (TreeMIL). We first adopt an N-ary tree structure to divide the entire series into multiple nodes, where nodes at different levels represent subsequences with different lengths. Then, the subsequence features are extracted to determine the presence of collective anomalies. Finally, we calculate point-level anomaly scores by aggregating features from nodes at different levels. Experiments conducted on seven public datasets and eight baselines demonstrate that TreeMIL achieves an average 32.3% improvement in F1- score compared to previous state-of-the-art methods. The code is available at https://github.com/fly-orange/TreeMIL.</li>
<li><strong>摘要：</strong>时间序列异常检测 (TSAD) 在医疗保健、网络和工业等各个领域发挥着至关重要的作用。考虑到标签对于检测至关重要但难以获取，我们转向监督不精确的 TSAD：在训练阶段仅提供系列级标签，而在测试阶段预测点级异常。以前的工作遵循传统的多实例学习（MIL）方法，该方法侧重于鼓励单个时间步长的高异常分数。然而，时间序列异常不仅限于单个点异常，它们也可以是集体异常，通常在子序列上表现出异常模式。为了应对集体异常的挑战，在本文中，我们提出了一种基于树的 MIL 框架（TreeMIL）。我们首先采用N叉树结构将整个序列划分为多个节点，其中不同级别的节点代表不同长度的子序列。然后，提取子序列特征以确定集体异常的存在。最后，我们通过聚合不同级别节点的特征来计算点级别异常分数。在 7 个公共数据集和 8 个基线上进行的实验表明，与之前最先进的方法相比，TreeMIL 的 F1 分数平均提高了 32.3%。代码可在 https://github.com/fly-orange/TreeMIL 获取。</li>
</ul>

<h3>Title: Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented  Generation in Niche Domains, Exemplified by Korean Medicine</h3>
<ul>
<li><strong>Authors: </strong>Bongsu Kang, Jundong Kim, Tae-Rim Yun, Chang-Eop Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11246">https://arxiv.org/abs/2401.11246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11246">https://arxiv.org/pdf/2401.11246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11246]] Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented  Generation in Niche Domains, Exemplified by Korean Medicine(https://arxiv.org/abs/2401.11246)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>We propose a natural language prompt-based retrieval augmented generation (Prompt-RAG), a novel approach to enhance the performance of generative large language models (LLMs) in niche domains. Conventional RAG methods mostly require vector embeddings, yet the suitability of generic LLM-based embedding representations for specialized domains remains uncertain. To explore and exemplify this point, we compared vector embeddings from Korean Medicine (KM) and Conventional Medicine (CM) documents, finding that KM document embeddings correlated more with token overlaps and less with human-assessed document relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from conventional RAG models, operates without the need for embedding vectors. Its performance was assessed through a Question-Answering (QA) chatbot application, where responses were evaluated for relevance, readability, and informativeness. The results showed that Prompt-RAG outperformed existing models, including ChatGPT and conventional vector embedding-based RAGs, in terms of relevance and informativeness. Despite challenges like content structuring and response latency, the advancements in LLMs are expected to encourage the use of Prompt-RAG, making it a promising tool for other domains in need of RAG methods.</li>
<li><strong>摘要：</strong>我们提出了一种基于自然语言提示的检索增强生成（Prompt-RAG），这是一种增强利基领域中生成大语言模型（LLM）性能的新方法。传统的 RAG 方法大多需要向量嵌入，但基于通用 LLM 的嵌入表示对于专业领域的适用性仍然不确定。为了探索和举例说明这一点，我们比较了韩国医学 (KM) 和传统医学 (CM) 文档的向量嵌入，发现与 CM 嵌入相比，KM 文档嵌入与标记重叠相关性更大，与人类评估的文档相关性相关性较小。 Prompt-RAG 与传统的 RAG 模型不同，无需嵌入向量即可运行。其性能通过问答 (QA) 聊天机器人应用程序进行评估，其中评估响应的相关性、可读性和信息量。结果表明，Prompt-RAG 在相关性和信息量方面优于现有模型，包括 ChatGPT 和传统的基于向量嵌入的 RAG。尽管存在内容结构和响应延迟等挑战，法学硕士的进步预计将鼓励 Prompt-RAG 的使用，使其成为需要 RAG 方法的其他领域的有前途的工具。</li>
</ul>

<h3>Title: AFS-BM: Enhancing Model Performance through Adaptive Feature Selection  with Binary Masking</h3>
<ul>
<li><strong>Authors: </strong>Mehmet Y. Turali, Mehmet E. Lorasdagi, Ali T. Koc, Suleyman S. Kozat</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11250">https://arxiv.org/abs/2401.11250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11250">https://arxiv.org/pdf/2401.11250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11250]] AFS-BM: Enhancing Model Performance through Adaptive Feature Selection  with Binary Masking(https://arxiv.org/abs/2401.11250)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>We study the problem of feature selection in general machine learning (ML) context, which is one of the most critical subjects in the field. Although, there exist many feature selection methods, however, these methods face challenges such as scalability, managing high-dimensional data, dealing with correlated features, adapting to variable feature importance, and integrating domain knowledge. To this end, we introduce the ``Adaptive Feature Selection with Binary Masking" (AFS-BM) which remedies these problems. AFS-BM achieves this by joint optimization for simultaneous feature selection and model training. In particular, we do the joint optimization and binary masking to continuously adapt the set of features and model parameters during the training process. This approach leads to significant improvements in model accuracy and a reduction in computational requirements. We provide an extensive set of experiments where we compare AFS-BM with the established feature selection methods using well-known datasets from real-life competitions. Our results show that AFS-BM makes significant improvement in terms of accuracy and requires significantly less computational complexity. This is due to AFS-BM's ability to dynamically adjust to the changing importance of features during the training process, which an important contribution to the field. We openly share our code for the replicability of our results and to facilitate further research.</li>
<li><strong>摘要：</strong>我们研究一般机器学习（ML）背景下的特征选择问题，这是该领域最关键的主题之一。尽管存在许多特征选择方法，但是这些方法面临着可扩展性、管理高维数据、处理相关特征、适应可变特征重要性和整合领域知识等挑战。为此，我们引入了“带有二进制掩码的自适应特征选择”（AFS-BM）来解决这些问题。AFS-BM 通过同时进行特征选择和模型训练的联合优化来实现这一点。特别是，我们进行了联合优化和二进制掩码，以在训练过程中不断调整特征集和模型参数。这种方法可以显着提高模型精度并减少计算要求。我们提供了一组广泛的实验，将 AFS-BM 与已建立的模型进行比较使用来自现实生活竞赛的知名数据集的特征选择方法。我们的结果表明，AFS-BM 在准确性方面取得了显着提高，并且需要的计算复杂性显着降低。这是由于 AFS-BM 能够动态调整以适应不断变化的重要性培训过程中的功能，这是对该领域的重要贡献。我们公开分享我们的代码，以实现结果的可复制性并促进进一步的研究。</li>
</ul>

<h3>Title: Word-Level ASR Quality Estimation for Efficient Corpus Sampling and  Post-Editing through Analyzing Attentions of a Reference-Free Metric</h3>
<ul>
<li><strong>Authors: </strong>Golara Javadi, Kamer Ali Yuksel, Yunsu Kim, Thiago Castro Ferreira, Mohamed Al-Badrashiny</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11268">https://arxiv.org/abs/2401.11268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11268">https://arxiv.org/pdf/2401.11268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11268]] Word-Level ASR Quality Estimation for Efficient Corpus Sampling and  Post-Editing through Analyzing Attentions of a Reference-Free Metric(https://arxiv.org/abs/2401.11268)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>In the realm of automatic speech recognition (ASR), the quest for models that not only perform with high accuracy but also offer transparency in their decision-making processes is crucial. The potential of quality estimation (QE) metrics is introduced and evaluated as a novel tool to enhance explainable artificial intelligence (XAI) in ASR systems. Through experiments and analyses, the capabilities of the NoRefER (No Reference Error Rate) metric are explored in identifying word-level errors to aid post-editors in refining ASR hypotheses. The investigation also extends to the utility of NoRefER in the corpus-building process, demonstrating its effectiveness in augmenting datasets with insightful annotations. The diagnostic aspects of NoRefER are examined, revealing its ability to provide valuable insights into model behaviors and decision patterns. This has proven beneficial for prioritizing hypotheses in post-editing workflows and fine-tuning ASR models. The findings suggest that NoRefER is not merely a tool for error detection but also a comprehensive framework for enhancing ASR systems' transparency, efficiency, and effectiveness. To ensure the reproducibility of the results, all source codes of this study are made publicly available.</li>
<li><strong>摘要：</strong>在自动语音识别 (ASR) 领域，寻求不仅能够高精度执行而且能够在决策过程中提供透明度的模型至关重要。质量估计 (QE) 指标的潜力被介绍和评估，作为一种增强 ASR 系统中可解释人工智能 (XAI) 的新工具。通过实验和分析，探索了 NoRefER（无参考错误率）指标在识别单词级错误方面的能力，以帮助译后编辑完善 ASR 假设。该调查还扩展到 NoRefER 在语料库构建过程中的实用性，证明了其通过富有洞察力的注释来增强数据集的有效性。对 NoRefER 的诊断方面进行了检查，揭示了其为模型行为和决策模式提供有价值的见解的能力。事实证明，这对于在后期编辑工作流程中确定假设的优先级和微调 ASR 模型是有益的。研究结果表明，NoRefER 不仅仅是一个错误检测工具，而且还是一个增强 ASR 系统透明度、效率和有效性的综合框架。为了确保结果的可重复性，本研究的所有源代码均公开可用。</li>
</ul>

<h3>Title: Long-Term Fair Decision Making through Deep Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yaowei Hu, Yongkai Wu, Lu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11288">https://arxiv.org/abs/2401.11288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11288">https://arxiv.org/pdf/2401.11288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11288]] Long-Term Fair Decision Making through Deep Generative Models(https://arxiv.org/abs/2401.11288)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper studies long-term fair machine learning which aims to mitigate group disparity over the long term in sequential decision-making systems. To define long-term fairness, we leverage the temporal causal graph and use the 1-Wasserstein distance between the interventional distributions of different demographic groups at a sufficiently large time step as the quantitative metric. Then, we propose a three-phase learning framework where the decision model is trained on high-fidelity data generated by a deep generative model. We formulate the optimization problem as a performative risk minimization and adopt the repeated gradient descent algorithm for learning. The empirical evaluation shows the efficacy of the proposed method using both synthetic and semi-synthetic datasets.</li>
<li><strong>摘要：</strong>本文研究了长期公平机器学习，旨在长期减轻顺序决策系统中的群体差异。为了定义长期公平性，我们利用时间因果图，并使用足够大的时间步长下不同人口群体的干预分布之间的 1-Wasserstein 距离作为定量指标。然后，我们提出了一个三阶段学习框架，其中决策模型根据深度生成模型生成的高保真数据进行训练。我们将优化问题表述为执行风险最小化，并采用重复梯度下降算法进行学习。实证评估显示了所提出的方法使用合成和半合成数据集的有效性。</li>
</ul>

<h3>Title: PRILoRA: Pruned and Rank-Increasing Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Nadav Benedek, Lior Wolf</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11316">https://arxiv.org/abs/2401.11316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11316">https://arxiv.org/pdf/2401.11316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11316]] PRILoRA: Pruned and Rank-Increasing Low-Rank Adaptation(https://arxiv.org/abs/2401.11316)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora, rag</a></li>
<li><strong>Abstract: </strong>With the proliferation of large pre-trained language models (PLMs), fine-tuning all model parameters becomes increasingly inefficient, particularly when dealing with numerous downstream tasks that entail substantial training and storage costs. Several approaches aimed at achieving parameter-efficient fine-tuning (PEFT) have been proposed. Among them, Low-Rank Adaptation (LoRA) stands out as an archetypal method, incorporating trainable rank decomposition matrices into each target module. Nevertheless, LoRA does not consider the varying importance of each layer. To address these challenges, we introduce PRILoRA, which linearly allocates a different rank for each layer, in an increasing manner, and performs pruning throughout the training process, considering both the temporary magnitude of weights and the accumulated statistics of the input to any given layer. We validate the effectiveness of PRILoRA through extensive experiments on eight GLUE benchmarks, setting a new state of the art.</li>
<li><strong>摘要：</strong>随着大型预训练语言模型 (PLM) 的激增，微调所有模型参数变得越来越低效，特别是在处理需要大量训练和存储成本的大量下游任务时。已经提出了几种旨在实现参数高效微调（PEFT）的方法。其中，低秩适应（LoRA）作为一种原型方法脱颖而出，它将可训练的秩分解矩阵合并到每个目标模块中。尽管如此，LoRA 并没有考虑每一层的不同重要性。为了应对这些挑战，我们引入了 PRILoRA，它以递增的方式为每一层线性分配不同的排名，并在整个训练过程中执行剪枝，同时考虑权重的临时大小和任何给定层输入的累积统计数据。我们通过八个 GLUE 基准的广泛实验验证了 PRILoRA 的有效性，设定了新的技术水平。</li>
</ul>

<h3>Title: Analyzing Task-Encoding Tokens in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Bai, Heyan Huang, Cesare Spinoso-Di Piano, Marc-Antoine Rondeau, Sanxing Chen, Yang Gao, Jackie Chi Kit Cheung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11323">https://arxiv.org/abs/2401.11323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11323">https://arxiv.org/pdf/2401.11323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11323]] Analyzing Task-Encoding Tokens in Large Language Models(https://arxiv.org/abs/2401.11323)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) has become an effective solution for few-shot learning in natural language processing. Past work has found that, during this process, representations of the last prompt token are utilized to store task reasoning procedures, thereby explaining the working mechanism of in-context learning. In this paper, we seek to locate and analyze other task-encoding tokens whose representations store task reasoning procedures. Supported by experiments that ablate the representations of different token types, we find that template and stopword tokens are the most prone to be task-encoding tokens. In addition, we demonstrate experimentally that lexical cues, repetition, and text formats are the main distinguishing characteristics of these tokens. Our work provides additional insights into how large language models (LLMs) leverage task reasoning procedures in ICL and suggests that future work may involve using task-encoding tokens to improve the computational efficiency of LLMs at inference time and their ability to handle long sequences.</li>
<li><strong>摘要：</strong>上下文学习（ICL）已成为自然语言处理中小样本学习的有效解决方案。过去的工作发现，在此过程中，最后一个提示标记的表示被用来存储任务推理过程，从而解释了上下文学习的工作机制。在本文中，我们寻求定位和分析其他任务编码标记，其表示形式存储任务推理过程。在消除不同标记类型表示的实验的支持下，我们发现模板和停用词标记最有可能成为任务编码标记。此外，我们通过实验证明词汇提示、重复和文本格式是这些标记的主要区别特征。我们的工作提供了关于大型语言模型 (LLM) 如何利用 ICL 中的任务推理过程的更多见解，并表明未来的工作可能涉及使用任务编码标记来提高 LLM 在推理时的计算效率及其处理长序列的能力。</li>
</ul>

<h3>Title: Detecting Hidden Triggers: Mapping Non-Markov Reward Functions to Markov</h3>
<ul>
<li><strong>Authors: </strong>Gregory Hyde, Eugene Santos Jr</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11325">https://arxiv.org/abs/2401.11325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11325">https://arxiv.org/pdf/2401.11325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11325]] Detecting Hidden Triggers: Mapping Non-Markov Reward Functions to Markov(https://arxiv.org/abs/2401.11325)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Many Reinforcement Learning algorithms assume a Markov reward function to guarantee optimality. However, not all reward functions are known to be Markov. In this paper, we propose a framework for mapping non-Markov reward functions into equivalent Markov ones by learning a Reward Machine - a specialized reward automaton. Unlike the general practice of learning Reward Machines, we do not require a set of high-level propositional symbols from which to learn. Rather, we learn \emph{hidden triggers} directly from data that encode them. We demonstrate the importance of learning Reward Machines versus their Deterministic Finite-State Automata counterparts, for this task, given their ability to model reward dependencies in a single automaton. We formalize this distinction in our learning objective. Our mapping process is constructed as an Integer Linear Programming problem. We prove that our mappings provide consistent expectations for the underlying process. We empirically validate our approach by learning black-box non-Markov Reward functions in the Officeworld Domain. Additionally, we demonstrate the effectiveness of learning dependencies between rewards in a new domain, Breakfastworld.</li>
<li><strong>摘要：</strong>许多强化学习算法都采用马尔可夫奖励函数来保证最优性。然而，并非所有奖励函数都是马尔可夫函数。在本文中，我们提出了一个框架，通过学习奖励机（一种专门的奖励自动机）将非马尔可夫奖励函数映射到等效的马尔可夫奖励函数。与学习奖励机器的一般实践不同，我们不需要一组高级命题符号来学习。相反，我们直接从编码隐藏触发器的数据中学习\emph{隐藏触发器}。我们证明了学习奖励机相对于确定性有限状态自动机的重要性，因为它们能够在单个自动机中对奖励依赖关系进行建模。我们将这种区别正式纳入我们的学习目标。我们的映射过程被构造为整数线性规划问题。我们证明我们的映射为底层流程提供了一致的期望。我们通过学习 Officeworld 域中的黑盒非马尔可夫奖励函数来实证验证我们的方法。此外，我们还展示了在新领域“早餐世界”中学习奖励之间依赖关系的有效性。</li>
</ul>

<h3>Title: ProLex: A Benchmark for Language Proficiency-oriented Lexical  Substitution</h3>
<ul>
<li><strong>Authors: </strong>Xuanming Zhang, Zixun Chen, Zhou Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11356">https://arxiv.org/abs/2401.11356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11356">https://arxiv.org/pdf/2401.11356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11356]] ProLex: A Benchmark for Language Proficiency-oriented Lexical  Substitution(https://arxiv.org/abs/2401.11356)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat, rag</a></li>
<li><strong>Abstract: </strong>Lexical Substitution discovers appropriate substitutes for a given target word in a context sentence. However, the task fails to consider substitutes that are of equal or higher proficiency than the target, an aspect that could be beneficial for language learners looking to improve their writing. To bridge this gap, we propose a new task, language proficiency-oriented lexical substitution. We also introduce ProLex, a novel benchmark designed to assess systems' ability to generate not only appropriate substitutes but also substitutes that demonstrate better language proficiency. Besides the benchmark, we propose models that can automatically perform the new task. We show that our best model, a Llama2-13B model fine-tuned with task-specific synthetic data, outperforms ChatGPT by an average of 3.2% in F-score and achieves comparable results with GPT-4 on ProLex.</li>
<li><strong>摘要：</strong>词汇替换发现上下文句子中给定目标词的适当替换。然而，该任务未能考虑与目标水平相同或更高的替代品，这对于希望提高写作能力的语言学习者来说可能是有益的。为了弥补这一差距，我们提出了一项新任务，即以语言能力为导向的词汇替换。我们还介绍了 ProLex，这是一种新颖的基准测试，旨在评估系统不仅生成适当替代项的能力，而且还评估系统生成表现出更好语言能力的替代项的能力。除了基准之外，我们还提出了可以自动执行新任务的模型。我们证明，我们的最佳模型（使用特定于任务的合成数据进行微调的 Llama2-13B 模型）在 F 分数方面平均优于 ChatGPT 3.2%，并且在 ProLex 上实现了与 GPT-4 相当的结果。</li>
</ul>

<h3>Title: PepHarmony: A Multi-View Contrastive Learning Framework for Integrated  Sequence and Structure-Based Peptide Encoding</h3>
<ul>
<li><strong>Authors: </strong>Ruochi Zhang, Haoran Wu, Chang Liu, Huaping Li, Yuqian Wu, Kewei Li, Yifan Wang, Yifan Deng, Jiahui Chen, Fengfeng Zhou, Xin Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11360">https://arxiv.org/abs/2401.11360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11360">https://arxiv.org/pdf/2401.11360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11360]] PepHarmony: A Multi-View Contrastive Learning Framework for Integrated  Sequence and Structure-Based Peptide Encoding(https://arxiv.org/abs/2401.11360)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora, code</a></li>
<li><strong>Abstract: </strong>Recent advances in protein language models have catalyzed significant progress in peptide sequence representation. Despite extensive exploration in this field, pre-trained models tailored for peptide-specific needs remain largely unaddressed due to the difficulty in capturing the complex and sometimes unstable structures of peptides. This study introduces a novel multi-view contrastive learning framework PepHarmony for the sequence-based peptide encoding task. PepHarmony innovatively combines both sequence- and structure-level information into a sequence-level encoding module through contrastive learning. We carefully select datasets from the Protein Data Bank (PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences and structures. The experimental data highlights PepHarmony's exceptional capability in capturing the intricate relationship between peptide sequences and structures compared with the baseline and fine-tuned models. The robustness of our model is confirmed through extensive ablation studies, which emphasize the crucial roles of contrastive loss and strategic data sorting in enhancing predictive performance. The proposed PepHarmony framework serves as a notable contribution to peptide representations, and offers valuable insights for future applications in peptide drug discovery and peptide engineering. We have made all the source code utilized in this study publicly accessible via GitHub at https://github.com/zhangruochi/PepHarmony or this http URL</li>
<li><strong>摘要：</strong>蛋白质语言模型的最新进展促进了肽序列表示的重大进展。尽管在这一领域进行了广泛的探索，但由于难以捕获复杂且有时不稳定的肽结构，针对肽特定需求定制的预训练模型在很大程度上仍未得到解决。本研究引入了一种新颖的多视图对比学习框架 PepHharmony，用于基于序列的肽编码任务。 PepHharmony 通过对比学习创新地将序列级和结构级信息组合成序列级编码模块。我们从蛋白质数据库 (PDB) 和 AlphaFold 数据库中精心选择数据集，以涵盖广泛的肽序列和结构。与基线和微调模型相比，实验数据凸显了 PepHharmony 在捕捉肽序列和结构之间复杂关系方面的卓越能力。我们模型的稳健性通过广泛的消融研究得到了证实，这些研究强调了对比损失和战略数据排序在增强预测性能方面的关键作用。所提出的 PepHharmony 框架对肽表示做出了显着贡献，并为肽药物发现和肽工程的未来应用提供了宝贵的见解。我们已将本研究中使用的所有源代码通过 GitHub https://github.com/zhangruochi/PepHarmony 或此 http URL 公开访问</li>
</ul>

<h3>Title: Confidence Preservation Property in Knowledge Distillation Abstractions</h3>
<ul>
<li><strong>Authors: </strong>Dmitry Vengertsev, Elena Sherman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11365">https://arxiv.org/abs/2401.11365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11365">https://arxiv.org/pdf/2401.11365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11365]] Confidence Preservation Property in Knowledge Distillation Abstractions(https://arxiv.org/abs/2401.11365)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Social media platforms prevent malicious activities by detecting harmful content of posts and comments. To that end, they employ large-scale deep neural network language models for sentiment analysis and content understanding. Some models, like BERT, are complex, and have numerous parameters, which makes them expensive to operate and maintain. To overcome these deficiencies, industry experts employ a knowledge distillation compression technique, where a distilled model is trained to reproduce the classification behavior of the original model. The distillation processes terminates when the distillation loss function reaches the stopping criteria. This function is mainly designed to ensure that the original and the distilled models exhibit alike classification behaviors. However, besides classification accuracy, there are additional properties of the original model that the distilled model should preserve to be considered as an appropriate abstraction. In this work, we explore whether distilled TinyBERT models preserve confidence values of the original BERT models, and investigate how this confidence preservation property could guide tuning hyperparameters of the distillation process.</li>
<li><strong>摘要：</strong>社交媒体平台通过检测帖子和评论的有害内容来防止恶意活动。为此，他们采用大规模深度神经网络语言模型进行情感分析和内容理解。有些模型（例如 BERT）很复杂，并且参数众多，这使得它们的运行和维护成本很高。为了克服这些缺陷，行业专家采用了知识蒸馏压缩技术，其中训练蒸馏模型以重现原始模型的分类行为。当蒸馏损失函数达到停止标准时，蒸馏过程终止。该函数主要是为了确保原始模型和蒸馏模型表现出相似的分类行为。然而，除了分类准确性之外，蒸馏模型还应保留原始模型的其他属性，以将其视为适当的抽象。在这项工作中，我们探讨了蒸馏后的 TinyBERT 模型是否保留了原始 BERT 模型的置信值，并研究了这种置信度保留特性如何指导调整蒸馏过程的超参数。</li>
</ul>

<h3>Title: Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing  Approach For Uncovering Edge Cases with Minimal Distribution Distortion</h3>
<ul>
<li><strong>Authors: </strong>Aly M. Kassem, Sherif Saad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11373">https://arxiv.org/abs/2401.11373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11373">https://arxiv.org/pdf/2401.11373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11373]] Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing  Approach For Uncovering Edge Cases with Minimal Distribution Distortion(https://arxiv.org/abs/2401.11373)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Adversarial attacks against NLP Deep Learning models are a significant concern. In particular, adversarial samples exploit the model's sensitivity to small input changes. While these changes appear insignificant on the semantics of the input sample, they result in significant decay in model performance. In this paper, we propose Targeted Paraphrasing via RL (TPRL), an approach to automatically learn a policy to generate challenging samples that most likely improve the model's performance. TPRL leverages FLAN T5, a language model, as a generator and employs a self learned policy using a proximal policy gradient to generate the adversarial examples automatically. TPRL's reward is based on the confusion induced in the classifier, preserving the original text meaning through a Mutual Implication score. We demonstrate and evaluate TPRL's effectiveness in discovering natural adversarial attacks and improving model performance through extensive experiments on four diverse NLP classification tasks via Automatic and Human evaluation. TPRL outperforms strong baselines, exhibits generalizability across classifiers and datasets, and combines the strengths of language modeling and reinforcement learning to generate diverse and influential adversarial examples.</li>
<li><strong>摘要：</strong>针对 NLP 深度学习模型的对抗性攻击是一个值得关注的问题。特别是，对抗性样本利用了模型对微小输入变化的敏感性。虽然这些变化在输入样本的语义上显得微不足道，但它们会导致模型性能显着下降。在本文中，我们提出了通过 RL 进行目标释义（TPRL），这是一种自动学习策略以生成最有可能提高模型性能的具有挑战性的样本的方法。 TPRL 利用 FLAN T5（一种语言模型）作为生成器，并采用使用近端策略梯度的自学习策略来自动生成对抗性示例。 TPRL 的奖励基于分类器中引起的混淆，通过相互蕴含分数保留原始文本含义。我们通过自动和人工评估对四种不同的 NLP 分类任务进行广泛的实验，展示和评估 TPRL 在发现自然对抗攻击和提高模型性能方面的有效性。 TPRL 的性能优于强大的基线，表现出跨分类器和数据集的通用性，并结合了语言建模和强化学习的优势来生成多样化且有影响力的对抗性示例。</li>
</ul>

<h3>Title: Language Models as Hierarchy Encoders</h3>
<ul>
<li><strong>Authors: </strong>Yuan He, Zhangdie Yuan, Jiaoyan Chen, Ian Horrocks</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11374">https://arxiv.org/abs/2401.11374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11374">https://arxiv.org/pdf/2401.11374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11374]] Language Models as Hierarchy Encoders(https://arxiv.org/abs/2401.11374)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, rag</a></li>
<li><strong>Abstract: </strong>Interpreting hierarchical structures latent in language is a key limitation of current language models (LMs). While previous research has implicitly leveraged these hierarchies to enhance LMs, approaches for their explicit encoding are yet to be explored. To address this, we introduce a novel approach to re-train transformer encoder-based LMs as Hierarchy Transformer encoders (HiTs), harnessing the expansive nature of hyperbolic space. Our method situates the output embedding space of pre-trained LMs within a Poincar\'e ball with a curvature that adapts to the embedding dimension, followed by re-training on hyperbolic cluster and centripetal losses. These losses are designed to effectively cluster related entities (input as texts) and organise them hierarchically. We evaluate HiTs against pre-trained and fine-tuned LMs, focusing on their capabilities in simulating transitive inference, predicting subsumptions, and transferring knowledge across hierarchies. The results demonstrate that HiTs consistently outperform both pre-trained and fine-tuned LMs in these tasks, underscoring the effectiveness and transferability of our re-trained hierarchy encoders.</li>
<li><strong>摘要：</strong>解释语言中潜在的层次结构是当前语言模型（LM）的一个关键限制。虽然之前的研究已经隐式地利用这些层次结构来增强语言模型，但其显式编码的方法仍有待探索。为了解决这个问题，我们引入了一种新方法，利用双曲空间的扩展性质，将基于变压器编码器的 LM 重新训练为层次变压器编码器 (HiT)。我们的方法将预训练的 LM 的输出嵌入空间放置在庞加莱球内，其曲率适应嵌入维度，然后对双曲簇和向心损失进行重新训练。这些损失旨在有效地聚类相关实体（作为文本输入）并按层次结构组织它们。我们根据预先训练和微调的 LM 来评估 HiT，重点关注它们在模拟传递推理、预测包含和跨层次结构转移知识方面的能力。结果表明，HiT 在这些任务中始终优于预训练和微调的 LM，强调了我们重新训练的层次编码器的有效性和可转移性。</li>
</ul>

<h3>Title: MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Mao Hong, Zhiyue Zhang, Yue Wu, Yanxun Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11380">https://arxiv.org/abs/2401.11380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11380">https://arxiv.org/pdf/2401.11380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11380]] MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning(https://arxiv.org/abs/2401.11380)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Model-based offline reinforcement learning methods (RL) have achieved state-of-the-art performance in many decision-making problems thanks to their sample efficiency and generalizability. Despite these advancements, existing model-based offline RL approaches either focus on theoretical studies without developing practical algorithms or rely on a restricted parametric policy space, thus not fully leveraging the advantages of an unrestricted policy space inherent to model-based methods. To address this limitation, we develop MoMA, a model-based mirror ascent algorithm with general function approximations under partial coverage of offline data. MoMA distinguishes itself from existing literature by employing an unrestricted policy class. In each iteration, MoMA conservatively estimates the value function by a minimization procedure within a confidence set of transition models in the policy evaluation step, then updates the policy with general function approximations instead of commonly-used parametric policy classes in the policy improvement step. Under some mild assumptions, we establish theoretical guarantees of MoMA by proving an upper bound on the suboptimality of the returned policy. We also provide a practically implementable, approximate version of the algorithm. The effectiveness of MoMA is demonstrated via numerical studies.</li>
<li><strong>摘要：</strong>基于模型的离线强化学习方法（RL）凭借其样本效率和泛化性，在许多决策问题上取得了最先进的性能。尽管取得了这些进步，现有的基于模型的离线强化学习方法要么专注于理论研究而不开发实用算法，要么依赖于受限的参数策略空间，因此没有充分利用基于模型的方法固有的不受限制的策略空间的优势。为了解决这个限制，我们开发了 MoMA，一种基于模型的镜像上升算法，在部分覆盖离线数据的情况下具有通用函数逼近。现代艺术博物馆通过采用不受限制的政策类别来区别于现有文献。在每次迭代中，MoMA 在政策评估步骤中通过转换模型置信集中的最小化过程保守地估计价值函数，然后在政策改进步骤中使用通用函数近似而不是常用的参数政策类更新政策。在一些温和的假设下，我们通过证明返回策略次优性的上限来建立 MoMA 的理论保证。我们还提供了该算法的实际可实现的近似版本。现代艺术博物馆的有效性通过数值研究得到证明。</li>
</ul>

<h3>Title: Using Large Language Model for End-to-End Chinese ASR and NER</h3>
<ul>
<li><strong>Authors: </strong>Yuang Li, Jiawei Yu, Yanqing Zhao, Min Zhang, Mengxin Ren, Xiaofeng Zhao, Xiaosong Qiao, Chang Su, Miaomiao Ma, Hao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11382">https://arxiv.org/abs/2401.11382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11382">https://arxiv.org/pdf/2401.11382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11382]] Using Large Language Model for End-to-End Chinese ASR and NER(https://arxiv.org/abs/2401.11382)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, code, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Mapping speech tokens to the same feature space as text tokens has become the paradigm for the integration of speech modality into decoder-only large language models (LLMs). An alternative approach is to use an encoder-decoder architecture that incorporates speech features through cross-attention. This approach, however, has received less attention in the literature. In this work, we connect the Whisper encoder with ChatGLM3 and provide in-depth comparisons of these two approaches using Chinese automatic speech recognition (ASR) and name entity recognition (NER) tasks. We evaluate them not only by conventional metrics like the F1 score but also by a novel fine-grained taxonomy of ASR-NER errors. Our experiments reveal that encoder-decoder architecture outperforms decoder-only architecture with a short context, while decoder-only architecture benefits from a long context as it fully exploits all layers of the LLM. By using LLM, we significantly reduced the entity omission errors and improved the entity ASR accuracy compared to the Conformer baseline. Additionally, we obtained a state-of-the-art (SOTA) F1 score of 0.805 on the AISHELL-NER test set by using chain-of-thought (CoT) NER which first infers long-form ASR transcriptions and then predicts NER labels.</li>
<li><strong>摘要：</strong>将语音标记映射到与文本标记相同的特征空间已成为将语音模态集成到仅解码器的大型语言模型 (LLM) 中的范例。另一种方法是使用编码器-解码器架构，通过交叉注意力合并语音特征。然而，这种方法在文献中受到的关注较少。在这项工作中，我们将 Whisper 编码器与 ChatGLM3 连接起来，并使用中文自动语音识别 (ASR) 和名称实体识别 (NER) 任务对这两种方法进行深入比较。我们不仅通过 F1 分数等传统指标来评估它们，还通过一种新颖的 ASR-NER 错误细粒度分类法来评估它们。我们的实验表明，编码器-解码器架构在短上下文中优于仅解码器架构，而仅解码器架构则受益于长上下文，因为它充分利用了 LLM 的所有层。通过使用 LLM，与 Conformer 基线相比，我们显着减少了实体遗漏错误并提高了实体 ASR 准确性。此外，我们通过使用思想链 (CoT) NER 在 AISHELL-NER 测试集上获得了最先进的 (SOTA) F1 分数 0.805，该 NER 首先推断长格式 ASR 转录，然后预测 NER 标签。</li>
</ul>

<h3>Title: MedLM: Exploring Language Models for Medical Question Answering Systems</h3>
<ul>
<li><strong>Authors: </strong>Niraj Yagnik, Jay Jhaveri, Vivek Sharma, Gabriel Pila, Asma Ben, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11389">https://arxiv.org/abs/2401.11389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11389">https://arxiv.org/pdf/2401.11389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11389]] MedLM: Exploring Language Models for Medical Question Answering Systems(https://arxiv.org/abs/2401.11389)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the face of rapidly expanding online medical literature, automated systems for aggregating and summarizing information are becoming increasingly crucial for healthcare professionals and patients. Large Language Models (LLMs), with their advanced generative capabilities, have shown promise in various NLP tasks, and their potential in the healthcare domain, particularly for Closed-Book Generative QnA, is significant. However, the performance of these models in domain-specific tasks such as medical Q&A remains largely unexplored. This study aims to fill this gap by comparing the performance of general and medical-specific distilled LMs for medical Q&A. We aim to evaluate the effectiveness of fine-tuning domain-specific LMs and compare the performance of different families of Language Models. The study will address critical questions about these models' reliability, comparative performance, and effectiveness in the context of medical Q&A. The findings will provide valuable insights into the suitability of different LMs for specific applications in the medical domain.</li>
<li><strong>摘要：</strong>面对快速增长的在线医学文献，用于聚合和总结信息的自动化系统对于医疗保健专业人员和患者变得越来越重要。大型语言模型 (LLM) 凭借其先进的生成能力，在各种 NLP 任务中表现出了良好的前景，并且它们在医疗保健领域的潜力，特别是闭卷生成 QnA 的潜力是巨大的。然而，这些模型在医疗问答等特定领域任务中的性能在很大程度上仍未得到探索。本研究旨在通过比较一般和医疗特定的蒸馏 LM 在医学问答中的性能来填补这一空白。我们的目标是评估微调特定领域语言模型的有效性，并比较不同语言模型系列的性能。该研究将解决有关这些模型在医学问答背景下的可靠性、比较性能和有效性的关键问题。研究结果将为了解不同 LM 对医学领域特定应用的适用性提供有价值的见解。</li>
</ul>

<h3>Title: Causal Generative Explainers using Counterfactual Inference: A Case  Study on the Morpho-MNIST Dataset</h3>
<ul>
<li><strong>Authors: </strong>Will Taylor-Melanson, Zahra Sadeghi, Stan Matwin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11394">https://arxiv.org/abs/2401.11394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11394">https://arxiv.org/pdf/2401.11394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11394]] Causal Generative Explainers using Counterfactual Inference: A Case  Study on the Morpho-MNIST Dataset(https://arxiv.org/abs/2401.11394)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this paper, we propose leveraging causal generative learning as an interpretable tool for explaining image classifiers. Specifically, we present a generative counterfactual inference approach to study the influence of visual features (i.e., pixels) as well as causal factors through generative learning. To this end, we first uncover the most influential pixels on a classifier's decision by varying the value of a causal attribute via counterfactual inference and computing both Shapely and contrastive explanations for counterfactual images with these different attribute values. We then establish a Monte-Carlo mechanism using the generator of a causal generative model in order to adapt Shapley explainers to produce feature importances for the human-interpretable attributes of a causal dataset in the case where a classifier has been trained exclusively on the images of the dataset. Finally, we present optimization methods for creating counterfactual explanations of classifiers by means of counterfactual inference, proposing straightforward approaches for both differentiable and arbitrary classifiers. We exploit the Morpho-MNIST causal dataset as a case study for exploring our proposed methods for generating counterfacutl explantions. We employ visual explanation methods from OmnixAI open source toolkit to compare them with our proposed methods. By employing quantitative metrics to measure the interpretability of counterfactual explanations, we find that our proposed methods of counterfactual explanation offer more interpretable explanations compared to those generated from OmnixAI. This finding suggests that our methods are well-suited for generating highly interpretable counterfactual explanations on causal datasets.</li>
<li><strong>摘要：</strong>在本文中，我们建议利用因果生成学习作为解释图像分类器的可解释工具。具体来说，我们提出了一种生成反事实推理方法，通过生成学习来研究视觉特征（即像素）以及因果因素的影响。为此，我们首先通过反事实推理改变因果属性的值，并计算具有这些不同属性值的反事实图像的形状和对比解释，从而发现对分类器决策最有影响力的像素。然后，我们使用因果生成模型的生成器建立蒙特卡罗机制，以便在分类器专门针对以下图像进行训练的情况下，使 Shapley 解释器能够为因果数据集的人类可解释属性生成特征重要性。数据集。最后，我们提出了通过反事实推理来创建分类器的反事实解释的优化方法，为可微分类器和任意分类器提出了简单的方法。我们利用 Morpho-MNIST 因果数据集作为案例研究，探索我们提出的生成反事实外植的方法。我们采用 OmnixAI 开源工具包中的视觉解释方法来将它们与我们提出的方法进行比较。通过采用定量指标来衡量反事实解释的可解释性，我们发现与 OmnixAI 生成的方法相比，我们提出的反事实解释方法提供了更多可解释的解释。这一发现表明我们的方法非常适合对因果数据集生成高度可解释的反事实解释。</li>
</ul>

<h3>Title: Visual Imitation Learning with Calibrated Contrastive Representation</h3>
<ul>
<li><strong>Authors: </strong>Yunke Wang, Linwei Tao, Bo Du, Yutian Lin, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11396">https://arxiv.org/abs/2401.11396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11396">https://arxiv.org/pdf/2401.11396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11396]] Visual Imitation Learning with Calibrated Contrastive Representation(https://arxiv.org/abs/2401.11396)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, agent</a></li>
<li><strong>Abstract: </strong>Adversarial Imitation Learning (AIL) allows the agent to reproduce expert behavior with low-dimensional states and actions. However, challenges arise in handling visual states due to their less distinguishable representation compared to low-dimensional proprioceptive features. While existing methods resort to adopt complex network architectures or separate the process of learning representation and decision-making, they overlook valuable intra-agent information within demonstrations. To address this problem, this paper proposes a simple and effective solution by incorporating calibrated contrastive representative learning into visual AIL framework. Specifically, we present an image encoder in visual AIL, utilizing a combination of unsupervised and supervised contrastive learning to extract valuable features from visual states. Based on the fact that the improved agent often produces demonstrations of varying quality, we propose to calibrate the contrastive loss by treating each agent demonstrations as a mixed sample. The incorporation of contrastive learning can be jointly optimized with the AIL framework, without modifying the architecture or incurring significant computational costs. Experimental results on DMControl Suite demonstrate our proposed method is sample efficient and can outperform other compared methods from different aspects.</li>
<li><strong>摘要：</strong>对抗性模仿学习（AIL）允许代理通过低维状态和动作重现专家行为。然而，由于与低维本体感受特征相比，视觉状态的表示不太明显，因此在处理视觉状态时出现了挑战。虽然现有方法采用复杂的网络架构或将学习表示和决策的过程分开，但它们忽略了演示中有价值的代理内信息。为了解决这个问题，本文提出了一种简单有效的解决方案，将校准对比代表性学习纳入视觉AIL框架中。具体来说，我们在视觉 AIL 中提出了一种图像编码器，利用无监督和监督对比学习的组合从视觉状态中提取有价值的特征。基于改进后的代理经常产生不同质量的演示的事实，我们建议通过将每个代理演示视为混合样本来校准对比损失。对比学习的结合可以与 AIL 框架联合优化，而无需修改架构或产生大量计算成本。 DMControl Suite 上的实验结果表明，我们提出的方法具有样本效率，并且可以从不同方面优于其他比较方法。</li>
</ul>

<h3>Title: Enabling clustering algorithms to detect clusters of varying densities  through scale-invariant data preprocessing</h3>
<ul>
<li><strong>Authors: </strong>Sunil Aryal, Jonathan R. Wells, Arbind Agrahari Baniya, KC Santosh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11402">https://arxiv.org/abs/2401.11402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11402">https://arxiv.org/pdf/2401.11402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11402]] Enabling clustering algorithms to detect clusters of varying densities  through scale-invariant data preprocessing(https://arxiv.org/abs/2401.11402)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this paper, we show that preprocessing data using a variant of rank transformation called 'Average Rank over an Ensemble of Sub-samples (ARES)' makes clustering algorithms robust to data representation and enable them to detect varying density clusters. Our empirical results, obtained using three most widely used clustering algorithms-namely KMeans, DBSCAN, and DP (Density Peak)-across a wide range of real-world datasets, show that clustering after ARES transformation produces better and more consistent results.</li>
<li><strong>摘要：</strong>在本文中，我们表明，使用称为“子样本集合的平均排名 (ARES)”的排名变换变体来预处理数据，使得聚类算法对数据表示具有鲁棒性，并使它们能够检测不同密度的聚类。我们的实证结果是使用三种最广泛使用的聚类算法（即 KMeans、DBSCAN 和 DP（密度峰值））在各种实际数据集中获得的，结果表明 ARES 转换后的聚类会产生更好、更一致的结果。</li>
</ul>

<h3>Title: MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks  via Text Prompts</h3>
<ul>
<li><strong>Authors: </strong>Haoqiang Guo, Sendong Zhao, Haochun Wang, Yanrui Du, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11403">https://arxiv.org/abs/2401.11403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11403">https://arxiv.org/pdf/2401.11403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11403]] MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks  via Text Prompts(https://arxiv.org/abs/2401.11403)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code, agent</a></li>
<li><strong>Abstract: </strong>Deep learning is now widely used in drug discovery, providing significant acceleration and cost reduction. As the most fundamental building block, molecular representation is essential for predicting molecular properties to enable various downstream applications. Most existing methods attempt to incorporate more information to learn better representations. However, not all features are equally important for a specific task. Ignoring this would potentially compromise the training efficiency and predictive accuracy. To address this issue, we propose a novel approach, which treats language models as an agent and molecular pretraining models as a knowledge base. The agent accentuates task-relevant features in the molecular representation by understanding the natural language description of the task, just as a tailor customizes clothes for clients. Thus, we call this approach MolTailor. Evaluations demonstrate MolTailor's superior performance over baselines, validating the efficacy of enhancing relevance for molecular representation learning. This illustrates the potential of language model guided optimization to better exploit and unleash the capabilities of existing powerful molecular representation methods. Our codes and appendix are available at https://github.com/SCIR-HI/MolTailor.</li>
<li><strong>摘要：</strong>深度学习现在广泛应用于药物发现，提供显着的加速和成本降低。作为最基本的构建模块，分子表示对于预测分子特性以实现各种下游应用至关重要。大多数现有方法试图结合更多信息来学习更好的表示。然而，并非所有功能对于特定任务都同样重要。忽略这一点可能会损害训练效率和预测准确性。为了解决这个问题，我们提出了一种新方法，将语言模型视为代理，将分子预训练模型视为知识库。代理通过理解任务的自然语言描述来强调分子表示中与任务相关的特征，就像裁缝为客户定制衣服一样。因此，我们将这种方法称为 MolTailor。评估表明 MolTailor 的性能优于基线，验证了增强分子表征学习相关性的功效。这说明了语言模型引导优化的潜力，可以更好地利用和释放现有强大分子表示方法的功能。我们的代码和附录可在 https://github.com/SCIR-HI/MolTailor 上获取。</li>
</ul>

<h3>Title: Open the Black Box: Step-based Policy Updates for Temporally-Correlated  Episodic Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ge Li, Hongyi Zhou, Dominik Roth, Serge Thilges, Fabian Otto, Rudolf Lioutikov, Gerhard Neumann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11437">https://arxiv.org/abs/2401.11437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11437">https://arxiv.org/pdf/2401.11437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11437]] Open the Black Box: Step-based Policy Updates for Temporally-Correlated  Episodic Reinforcement Learning(https://arxiv.org/abs/2401.11437)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic RL (ERL) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque \emph{black boxes}. In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL.</li>
<li><strong>摘要：</strong>当前强化学习（RL）的进展主要集中在学习基于步骤的策略，这些策略为每个感知状态生成行动。虽然这些方法有效地利用了环境交互中的步骤信息，但它们经常忽略动作之间的时间相关性，导致探索效率低下和轨迹不平滑，这在实际硬件上实现起来具有挑战性。情景强化学习 (ERL) 试图通过探索捕获动作相关性的参数空间来克服这些挑战。然而，这些方法通常会损害数据效率，因为它们将轨迹视为不透明的\emph{黑匣子}。在这项工作中，我们介绍了一种新颖的 ERL 算法，即时间相关情景 RL (TCE)，该算法有效地利用情景策略更新中的步骤信息，打开了现有 ERL 方法中的“黑匣子”，同时保留了参数空间中的平滑一致的探索。 TCE 协同结合了基于步骤的 RL 和情景 RL 的优点，实现了与最新 ERL 方法相当的性能，同时保持了类似于最先进 (SoTA) 基于步骤的 RL 的数据效率。</li>
</ul>

<h3>Title: Sequential Model for Predicting Patient Adherence in Subcutaneous  Immunotherapy for Allergic Rhinitis</h3>
<ul>
<li><strong>Authors: </strong>Li Yin, Xiong Yu, Fan Wenxin, Wang Kai, Yu Qingqing, Si Liping, van der Smagt Patrick, Tang Jun, Chen Nutan</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11447">https://arxiv.org/abs/2401.11447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11447">https://arxiv.org/pdf/2401.11447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11447]] Sequential Model for Predicting Patient Adherence in Subcutaneous  Immunotherapy for Allergic Rhinitis(https://arxiv.org/abs/2401.11447)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage novel machine learning models to precisely predict the risk of non-adherence of patients and related systematic symptom scores, to provide a novel approach in the management of long-term AIT. Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence prediction capabilities. Results: Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from $60\,\%$ to $72\%$, and for LSTM models, it is $66\,\%$ to $84\,\%$, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and $2.22$, while for LSTM models it is between $1.09$ and $1.77$. Notably, these RMSEs are significantly lower than the random prediction error of $4.55$. Conclusion: We creatively apply sequential models in the long-term management of SCIT with promising accuracy in the prediction of SCIT nonadherence in Allergic Rhinitis (AR) patients. While LSTM outperforms SLAC in adherence prediction, SLAC excels in score prediction for patients undergoing SCIT for AR. The state-action-based SLAC adds flexibility, presenting a novel and effective approach for managing long-term AIT.</li>
<li><strong>摘要：</strong>目的：皮下免疫疗法（SCIT）是过敏性鼻炎的长期病因治疗方法。如何增强患者的依从性，以最大限度地发挥变应原免疫治疗（AIT）的益处，在AIT的管理中起着至关重要的作用。本研究旨在利用新颖的机器学习模型来精确预测患者不依从的风险和相关的系统症状评分，为长期 AIT 的管理提供一种新方法。方法：该研究开发并分析了两种模型：顺序潜在演员评论家 (SLAC) 和长短期记忆 (LSTM)，并根据评分和依从性预测能力对其进行评估。结果：排除第一个时间步的有偏差样本，SLAC 模型的预测依从精度为 $60\,\%$ 到 $72\%$，对于 LSTM 模型，为 $66\,\%$ 到 $84\ ,\%$，根据时间步长变化。 SLAC 模型的均方根误差 (RMSE) 范围在 0.93 美元到 2.22 美元之间，而 LSTM 模型的均方根误差 (RMSE) 范围在 1.09 美元到 1.77 美元之间。值得注意的是，这些 RMSE 显着低于 4.55 美元的随机预测误差。结论：我们创造性地将序贯模型应用于 SCIT 的长期管理，在预测过敏性鼻炎 (AR) 患者 SCIT 不依从性方面具有良好的准确性。虽然 LSTM 在依从性预测方面优于 SLAC，但 SLAC 在针对 AR 接受 SCIT 的患者的评分预测方面表现出色。基于状态-操作的 SLAC 增加了灵活性，为管理长期 AIT 提供了一种新颖且有效的方法。</li>
</ul>

<h3>Title: Linear Alignment: A Closed-form Solution for Aligning Human Preferences  without Tuning and Feedback</h3>
<ul>
<li><strong>Authors: </strong>Songyang Gao, Qiming Ge, Wei Shen, Shihan Dou, Junjie Ye, Xiao Wang, Rui Zheng, Yicheng Zou, Zhi Chen, Hang Yan, Qi Zhang, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11458">https://arxiv.org/abs/2401.11458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11458">https://arxiv.org/pdf/2401.11458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11458]] Linear Alignment: A Closed-form Solution for Aligning Human Preferences  without Tuning and Feedback(https://arxiv.org/abs/2401.11458)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>The success of AI assistants based on Language Models (LLMs) hinges on Reinforcement Learning from Human Feedback (RLHF) to comprehend and align with user intentions. However, traditional alignment algorithms, such as PPO, are hampered by complex annotation and training requirements. This reliance limits the applicability of RLHF and hinders the development of professional assistants tailored to diverse human preferences. In this work, we introduce \textit{Linear Alignment}, a novel algorithm that aligns language models with human preferences in one single inference step, eliminating the reliance on data annotation and model training. Linear alignment incorporates a new parameterization for policy optimization under divergence constraints, which enables the extraction of optimal policy in a closed-form manner and facilitates the direct estimation of the aligned response. Extensive experiments on both general and personalized preference datasets demonstrate that linear alignment significantly enhances the performance and efficiency of LLM alignment across diverse scenarios. Our code and dataset will be published on \url{https://github.com/Wizardcoast/Linear_Alignment.git}.</li>
<li><strong>摘要：</strong>基于语言模型 (LLM) 的人工智能助手的成功取决于人类反馈的强化学习 (RLHF) 来理解和符合用户意图。然而，传统的比对算法（例如 PPO）受到复杂注释和训练要求的阻碍。这种依赖限制了 RLHF 的适用性，并阻碍了针对不同人类偏好的专业助理的开发。在这项工作中，我们引入了 \textit{Linear Alignment}，这是一种新颖的算法，可以在一个推理步骤中将语言模型与人类偏好对齐，从而消除对数据注释和模型训练的依赖。线性对齐结合了发散约束下策略优化的新参数化，这使得能够以封闭形式的方式提取最优策略，并有利于直接估计对齐响应。对通用和个性化偏好数据集的广泛实验表明，线性对齐显着提高了跨不同场景的 LLM 对齐的性能和效率。我们的代码和数据集将发布在 \url{https://github.com/Wizardcoast/Linear_Alignment.git} 上。</li>
</ul>

<h3>Title: Over-Reasoning and Redundant Calculation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cheng-Han Chiang, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11467">https://arxiv.org/abs/2401.11467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11467">https://arxiv.org/pdf/2401.11467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11467]] Over-Reasoning and Redundant Calculation of Large Language Models(https://arxiv.org/abs/2401.11467)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can solve problems step-by-step. While this chain-of-thought (CoT) reasoning boosts LLMs' performance, it is unclear if LLMs \textit{know} when to use CoT and whether those CoT are always necessary to answer the question. This paper shows that LLMs tend to generate redundant calculations and reasoning on a manually constructed math QA dataset, GSM8K-Zero. GSM8K-Zero is constructed such that the questions can be answered without any calculations, but LLMs, including Llama-2 models and Claude-2, tend to generate lengthy and unnecessary calculations to answer the questions. We also conduct experiments to explain why LLMs generate redundant calculations and reasonings. GSM8K-Zero is publicly available at https://github.com/d223302/Over-Reasoning-of-LLMs and https://huggingface.co/datasets/dcml0714/GSM8K-Zero.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）可以逐步解决问题。虽然这种思想链 (CoT) 推理提高了法学硕士的表现，但尚不清楚法学硕士\textit{知道}何时使用 CoT 以及这些 CoT 是否始终需要回答问题。本文表明，法学硕士倾向于在手动构建的数学 QA 数据集 GSM8K-Zero 上生成冗余计算和推理。 GSM8K-Zero 的构造使得无需任何计算即可回答问题，但法学硕士（包括 Llama-2 模型和 Claude-2）往往会生成冗长且不必要的计算来回答问题。我们还进行了实验来解释为什么法学硕士会产生冗余的计算和推理。 GSM8K-Zero 可在 https://github.com/d223302/Over-Reasoning-of-LLMs 和 https://huggingface.co/datasets/dcml0714/GSM8K-Zero 上公开获取。</li>
</ul>

<h3>Title: With Greater Text Comes Greater Necessity: Inference-Time Training Helps  Long Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Y. Wang, D. Ma, D. Cai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11504">https://arxiv.org/abs/2401.11504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11504">https://arxiv.org/pdf/2401.11504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11504]] With Greater Text Comes Greater Necessity: Inference-Time Training Helps  Long Text Generation(https://arxiv.org/abs/2401.11504)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context, lora</a></li>
<li><strong>Abstract: </strong>Long text generation, such as novel writing or discourse-level translation with extremely long contexts, presents significant challenges to current language models. Existing methods mainly focus on extending the model's context window through strategies like length extrapolation. However, these approaches demand substantial hardware resources during the training and/or inference phases. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the KV cache to store all context information, Temp-Lora embeds this information directly into the model's parameters. In the process of long text generation, we use a temporary Lora module, progressively trained with text generated previously. This approach not only efficiently preserves contextual knowledge but also prevents any permanent alteration to the model's parameters given that the module is discarded post-generation. Extensive experiments on the PG19 language modeling benchmark and the GuoFeng discourse-level translation benchmark validate the effectiveness of Temp-Lora. Our results show that: 1) Temp-Lora substantially enhances generation quality for long texts, as indicated by a 13.2% decrease in perplexity on a subset of PG19, and a 29.6% decrease in perplexity along with a 53.2% increase in BLEU score on GuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text generation methods, and 3) Temp-Lora can greatly reduce computational costs by shortening the context window. While ensuring a slight improvement in generation quality (a decrease of 3.8% in PPL), it enables a reduction of 70.5% in the FLOPs required for inference and a 51.5% decrease in latency.</li>
<li><strong>摘要：</strong>长文本生成，例如具有极长上下文的小说写作或话语级翻译，对当前的语言模型提出了重大挑战。现有方法主要侧重于通过长度外推等策略来扩展模型的上下文窗口。然而，这些方法在训练和/或推理阶段需要大量的硬件资源。我们提出的方法 Temp-Lora 引入了另一种概念。 Temp-Lora 不依赖 KV 缓存来存储所有上下文信息，而是将此信息直接嵌入到模型的参数中。在长文本生成的过程中，我们使用一个临时的 Lora 模块，用之前生成的文本逐步进行训练。这种方法不仅有效地保留了上下文知识，而且还防止了模型参数在生成后被丢弃的情况下发生任何永久性更改。在PG19语言建模基准和国风话语级翻译基准上进行的大量实验验证了Temp-Lora的有效性。我们的结果表明：1) Temp-Lora 显着提高了长文本的生成质量，PG19 子集的困惑度降低了 13.2%，困惑度降低了 29.6%，同时 BLEU 得分提高了 53.2%。 GuoFeng，2）Temp-Lora 兼容并增强了大多数现有的长文本生成方法，3）Temp-Lora 可以通过缩短上下文窗口来大大降低计算成本。在保证生成质量小幅提升（PPL降低3.8%）的同时，推理所需的FLOPs降低了70.5%，延迟降低了51.5%。</li>
</ul>

<h3>Title: CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray  Report Labeling</h3>
<ul>
<li><strong>Authors: </strong>Jawook Gu, Han-Cheol Cho, Jiho Kim, Kihyun You, Eun Kyoung Hong, Byungseok Roh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11505">https://arxiv.org/abs/2401.11505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11505">https://arxiv.org/pdf/2401.11505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11505]] CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray  Report Labeling(https://arxiv.org/abs/2401.11505)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, code</a></li>
<li><strong>Abstract: </strong>Free-text radiology reports present a rich data source for various medical tasks, but effectively labeling these texts remains challenging. Traditional rule-based labeling methods fall short of capturing the nuances of diverse free-text patterns. Moreover, models using expert-annotated data are limited by data scarcity and pre-defined classes, impacting their performance, flexibility and scalability. To address these issues, our study offers three main contributions: 1) We demonstrate the potential of GPT as an adept labeler using carefully designed prompts. 2) Utilizing only the data labeled by GPT, we trained a BERT-based labeler, CheX-GPT, which operates faster and more efficiently than its GPT counterpart. 3) To benchmark labeler performance, we introduced a publicly available expert-annotated test set, MIMIC-500, comprising 500 cases from the MIMIC validation set. Our findings demonstrate that CheX-GPT not only excels in labeling accuracy over existing models, but also showcases superior efficiency, flexibility, and scalability, supported by our introduction of the MIMIC-500 dataset for robust benchmarking. Code and models are available at https://github.com/kakaobrain/CheXGPT.</li>
<li><strong>摘要：</strong>自由文本放射学报告为各种医疗任务提供了丰富的数据源，但有效标记这些文本仍然具有挑战性。传统的基于规则的标记方法无法捕捉各种自由文本模式的细微差别。此外，使用专家注释数据的模型受到数据稀缺和预定义类的限制，影响其性能、灵活性和可扩展性。为了解决这些问题，我们的研究提供了三个主要贡献：1）我们使用精心设计的提示展示了 GPT 作为熟练标记器的潜力。 2）仅利用 GPT 标记的数据，我们训练了一个基于 BERT 的标记器 CheX-GPT，它比 GPT 对应的标记器运行得更快、更高效。 3) 为了对贴标机性能进行基准测试，我们引入了一个公开的专家注释测试集 MIMIC-500，其中包含 MIMIC 验证集中的 500 个案例。我们的研究结果表明，CheX-GPT 不仅在标记准确性方面优于现有模型，而且在我们引入用于稳健基准测试的 MIMIC-500 数据集的支持下，还展示了卓越的效率、灵活性和可扩展性。代码和模型可在 https://github.com/kakaobrain/CheXGPT 获取。</li>
</ul>

<h3>Title: Information-Theoretic State Variable Selection for Reinforcement  Learning</h3>
<ul>
<li><strong>Authors: </strong>Charles Westphal, Stephen Hailes, Mirco Musolesi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11512">https://arxiv.org/abs/2401.11512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11512">https://arxiv.org/pdf/2401.11512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11512]] Information-Theoretic State Variable Selection for Reinforcement  Learning(https://arxiv.org/abs/2401.11512)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Identifying the most suitable variables to represent the state is a fundamental challenge in Reinforcement Learning (RL). These variables must efficiently capture the information necessary for making optimal decisions. In order to address this problem, in this paper, we introduce the Transfer Entropy Redundancy Criterion (TERC), an information-theoretic criterion, which determines if there is \textit{entropy transferred} from state variables to actions during training. We define an algorithm based on TERC that provably excludes variables from the state that have no effect on the final performance of the agent, resulting in more sample efficient learning. Experimental results show that this speed-up is present across three different algorithm classes (represented by tabular Q-learning, Actor-Critic, and Proximal Policy Optimization (PPO)) in a variety of environments. Furthermore, to highlight the differences between the proposed methodology and the current state-of-the-art feature selection approaches, we present a series of controlled experiments on synthetic data, before generalizing to real-world decision-making tasks. We also introduce a representation of the problem that compactly captures the transfer of information from state variables to actions as Bayesian networks.</li>
<li><strong>摘要：</strong>确定最合适的变量来表示状态是强化学习（RL）中的一个基本挑战。这些变量必须有效地捕获做出最佳决策所需的信息。为了解决这个问题，在本文中，我们引入了传递熵冗余准则（TERC），这是一种信息论准则，它确定训练过程中是否存在从状态变量到动作的\textit{熵转移}。我们定义了一种基于 TERC 的算法，该算法可证明从状态中排除对代理最终性能没有影响的变量，从而实现更高效的样本学习。实验结果表明，这种加速存在于各种环境中的三种不同算法类别（以表格 Q 学习、Actor-Critic 和近端策略优化 (PPO) 为代表）中。此外，为了强调所提出的方法与当前最先进的特征选择方法之间的差异，我们在推广到现实世界的决策任务之前，对合成数据进行了一系列受控实验。我们还引入了问题的表示，它紧凑地捕获了从状态变量到动作的信息传输作为贝叶斯网络。</li>
</ul>

<h3>Title: Distributed Multi-Task Learning for Stochastic Bandits with Context  Distribution and Stage-wise Constraints</h3>
<ul>
<li><strong>Authors: </strong>Jiabin Lin, Shana Moothedath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11563">https://arxiv.org/abs/2401.11563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11563">https://arxiv.org/pdf/2401.11563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11563]] Distributed Multi-Task Learning for Stochastic Bandits with Context  Distribution and Stage-wise Constraints(https://arxiv.org/abs/2401.11563)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We present the problem of conservative distributed multi-task learning in stochastic linear contextual bandits with heterogeneous agents. This extends conservative linear bandits to a distributed setting where M agents tackle different but related tasks while adhering to stage-wise performance constraints. The exact context is unknown, and only a context distribution is available to the agents as in many practical applications that involve a prediction mechanism to infer context, such as stock market prediction and weather forecast. We propose a distributed upper confidence bound (UCB) algorithm, DiSC-UCB. Our algorithm constructs a pruned action set during each round to ensure the constraints are met. Additionally, it includes synchronized sharing of estimates among agents via a central server using well-structured synchronization steps. We prove the regret and communication bounds on the algorithm. We extend the problem to a setting where the agents are unaware of the baseline reward. For this setting, we provide a modified algorithm, DiSC-UCB2, and we show that the modified algorithm achieves the same regret and communication bounds. We empirically validated the performance of our algorithm on synthetic data and real-world Movielens-100K data.</li>
<li><strong>摘要：</strong>我们提出了具有异构代理的随机线性上下文强盗的保守分布式多任务学习问题。这将保守的线性老虎机扩展到分布式环境，其中 M 个代理处理不同但相关的任务，同时遵守阶段性性能约束。确切的上下文是未知的，并且代理只能使用上下文分布，就像在许多涉及预测机制来推断上下文的实际应用中一样，例如股票市场预测和天气预报。我们提出了一种分布式置信上限（UCB）算法，DiSC-UCB。我们的算法在每一轮中构造一个修剪后的动作集，以确保满足约束。此外，它还包括使用结构良好的同步步骤通过中央服务器在代理之间同步共享估计。我们证明了算法的遗憾和通信界限。我们将问题扩展到代理不知道基线奖励的环境。对于此设置，我们提供了一种修改后的算法 DiSC-UCB2，并且我们表明修改后的算法实现了相同的遗憾和通信界限。我们根据经验验证了我们的算法在合成数据和现实世界 Movielens-100K 数据上的性能。</li>
</ul>

<h3>Title: Thompson Sampling for Stochastic Bandits with Noisy Contexts: An  Information-Theoretic Regret Analysis</h3>
<ul>
<li><strong>Authors: </strong>Sharu Theresa Jose, Shana Moothedath</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11565">https://arxiv.org/abs/2401.11565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11565">https://arxiv.org/pdf/2401.11565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11565]] Thompson Sampling for Stochastic Bandits with Noisy Contexts: An  Information-Theoretic Regret Analysis(https://arxiv.org/abs/2401.11565)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We explore a stochastic contextual linear bandit problem where the agent observes a noisy, corrupted version of the true context through a noise channel with an unknown noise parameter. Our objective is to design an action policy that can approximate" that of an oracle, which has access to the reward model, the channel parameter, and the predictive distribution of the true context from the observed noisy context. In a Bayesian framework, we introduce a Thompson sampling algorithm for Gaussian bandits with Gaussian context noise. Adopting an information-theoretic analysis, we demonstrate the Bayesian regret of our algorithm concerning the oracle's action policy. We also extend this problem to a scenario where the agent observes the true context with some delay after receiving the reward and show that delayed true contexts lead to lower Bayesian regret. Finally, we empirically demonstrate the performance of the proposed algorithms against baselines.</li>
<li><strong>摘要：</strong>我们探索了一个随机上下文线性强盗问题，其中代理通过具有未知噪声参数的噪声通道观察到真实上下文的噪声、损坏版本。我们的目标是设计一个可以近似“预言机”的行动策略，它可以访问奖励模型、通道参数以及来自观察到的噪声上下文的真实上下文的预测分布。在贝叶斯框架中，我们引入具有高斯上下文噪声的高斯强盗的汤普森采样算法。采用信息论分析，我们证明了我们的算法关于预言机行动策略的贝叶斯遗憾。我​​们还将这个问题扩展到代理用一些东西观察真实上下文的场景。收到奖励后的延迟，并表明延迟的真实上下文会导致较低的贝叶斯后悔。最后，我们凭经验证明所提出的算法相对于基线的性能。</li>
</ul>

<h3>Title: Robust Evaluation Measures for Evaluating Social Biases in Masked  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11601">https://arxiv.org/abs/2401.11601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11601">https://arxiv.org/pdf/2401.11601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11601]] Robust Evaluation Measures for Evaluating Social Biases in Masked  Language Models(https://arxiv.org/abs/2401.11601)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Many evaluation measures are used to evaluate social biases in masked language models (MLMs). However, we find that these previously proposed evaluation measures are lacking robustness in scenarios with limited datasets. This is because these measures are obtained by comparing the pseudo-log-likelihood (PLL) scores of the stereotypical and anti-stereotypical samples using an indicator function. The disadvantage is the limited mining of the PLL score sets without capturing its distributional information. In this paper, we represent a PLL score set as a Gaussian distribution and use Kullback Leibler (KL) divergence and Jensen Shannon (JS) divergence to construct evaluation measures for the distributions of stereotypical and anti-stereotypical PLL scores. Experimental results on the publicly available datasets StereoSet (SS) and CrowS-Pairs (CP) show that our proposed measures are significantly more robust and interpretable than those proposed previously.</li>
<li><strong>摘要：</strong>许多评估措施用于评估掩码语言模型（MLM）中的社会偏见。然而，我们发现这些先前提出的评估方法在数据集有限的场景中缺乏鲁棒性。这是因为这些度量是通过使用指示函数比较刻板印象和反刻板印象样本的伪对数似然 (PLL) 分数而获得的。缺点是在不捕获其分布信息的情况下对 PLL 分数集的挖掘有限。在本文中，我们将 PLL 分数集表示为高斯分布，并使用 Kullback Leibler (KL) 散度和 Jensen Shannon (JS) 散度来构建刻板和反刻板 PLL 分数分布的评估措施。在公开数据集 StereoSet (SS) 和 CrowS-Pairs (CP) 上的实验结果表明，我们提出的措施比之前提出的措施更加稳健和可解释。</li>
</ul>

<h3>Title: Graph Edits for Counterfactual Explanations: A Unified GNN Approach</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Chaidos, Angeliki Dimitriou, Maria Lymperaiou, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11609">https://arxiv.org/abs/2401.11609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11609">https://arxiv.org/pdf/2401.11609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11609]] Graph Edits for Counterfactual Explanations: A Unified GNN Approach(https://arxiv.org/abs/2401.11609)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Counterfactuals have been established as a popular explainability technique which leverages a set of minimal edits to alter the prediction of a classifier. When considering conceptual counterfactuals, the edits requested should correspond to salient concepts present in the input data. At the same time, conceptual distances are defined by knowledge graphs, ensuring the optimality of conceptual edits. In this work, we extend previous endeavors on conceptual counterfactuals by introducing \textit{graph edits as counterfactual explanations}: should we represent input data as graphs, which is the shortest graph edit path that results in an alternative classification label as provided by a black-box classifier?</li>
<li><strong>摘要：</strong>反事实已被确立为一种流行的可解释性技术，它利用一组最小的编辑来改变分类器的预测。在考虑概念性反事实时，请求的编辑应与输入数据中存在的显着概念相对应。同时，通过知识图来定义概念距离，保证概念编辑的最优性。在这项工作中，我们通过引入 \textit{图形编辑作为反事实解释}来扩展之前在概念性反事实方面的努力：我们是否应该将输入数据表示为图形，这是最短的图形编辑路径，可产生由黑色提供的替代分类标签-box分类器？</li>
</ul>

<h3>Title: In-context Learning with Retrieved Demonstrations for Language Models: A  Survey</h3>
<ul>
<li><strong>Authors: </strong>an Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11624">https://arxiv.org/abs/2401.11624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11624">https://arxiv.org/pdf/2401.11624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11624]] In-context Learning with Retrieved Demonstrations for Language Models: A  Survey(https://arxiv.org/abs/2401.11624)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training procedures, and inference algorithms.</li>
<li><strong>摘要：</strong>语言模型，特别是预训练的大型语言模型，已经展示了作为少镜头上下文学习器（ICL）的非凡能力，只需在输入上下文中进行几次演示即可适应新任务。然而，模型执行 ICL 的能力对小样本演示的选择很敏感。最近的一项发展是检索针对每个输入查询定制的演示，而不是使用一组固定的演示。演示检索的实现相对简单，利用现有的数据库和检索系统。这不仅提高了学习过程的效率和可扩展性，而且还被证明可以减少手动示例选择中固有的偏差。鉴于 ICL 令人鼓舞的结果和不断增长的研究以及检索到的演示，我们对该领域的研究进行了广泛的回顾。在本次调查中，我们讨论并比较了检索模型、检索训练过程和推理算法的不同设计选择。</li>
</ul>

<h3>Title: Freely Long-Thinking Transformer (FraiLT)</h3>
<ul>
<li><strong>Authors: </strong>Akbay Tabak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11626">https://arxiv.org/abs/2401.11626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11626">https://arxiv.org/pdf/2401.11626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11626]] Freely Long-Thinking Transformer (FraiLT)(https://arxiv.org/abs/2401.11626)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Freely Long-Thinking Transformer (FraiLT) is an improved transformer model designed to enhance processing capabilities without scaling up size. It utilizes a recursive approach, iterating over a subset of layers multiple times, and introduces iteration encodings to maintain awareness across these cycles. Iteration encoding allows FraiLT to achieve the interpretive depth of larger models in a compact form. When evaluated on a synthetic story dataset, FraiLT outperformed larger models, showcasing its ability to deliver high-quality performance while reducing memory demands. This model represents a step forward towards more efficient and accessible language models.</li>
<li><strong>摘要：</strong>Freely Long-Thinking Transformer (FraiLT) 是一种改进的变压器模型，旨在增强处理能力而不扩大尺寸。它采用递归方法，多次迭代层的子集，并引入迭代编码来保持跨这些周期的感知。迭代编码使 FraiLT 能够以紧凑的形式实现较大模型的解释深度。在合成故事数据集上进行评估时，FraiLT 的性能优于较大的模型，展示了其在降低内存需求的同时提供高质量性能的能力。该模型代表着向更高效、更易于访问的语言模型迈出了一步。</li>
</ul>

<h3>Title: Reframing Offline Reinforcement Learning as a Regression Problem</h3>
<ul>
<li><strong>Authors: </strong>Prajwal Koirala, Cody Fleming</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11630">https://arxiv.org/abs/2401.11630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11630">https://arxiv.org/pdf/2401.11630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11630]] Reframing Offline Reinforcement Learning as a Regression Problem(https://arxiv.org/abs/2401.11630)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>The study proposes the reformulation of offline reinforcement learning as a regression problem that can be solved with decision trees. Aiming to predict actions based on input states, return-to-go (RTG), and timestep information, we observe that with gradient-boosted trees, the agent training and inference are very fast, the former taking less than a minute. Despite the simplification inherent in this reformulated problem, our agent demonstrates performance that is at least on par with established methods. This assertion is validated by testing it across standard datasets associated with D4RL Gym-MuJoCo tasks. We further discuss the agent's ability to generalize by testing it on two extreme cases, how it learns to model the return distributions effectively even with highly skewed expert datasets, and how it exhibits robust performance in scenarios with sparse/delayed rewards.</li>
<li><strong>摘要：</strong>该研究提出将离线强化学习重新表述为可以用决策树解决的回归问题。为了根据输入状态、返回路径 (RTG) 和时间步信息来预测动作，我们观察到，使用梯度增强树，代理训练和推理速度非常快，前者只需要不到一分钟。尽管这个重新表述的问题固有的简化，但我们的智能体表现出的性能至少与既定方法相当。通过在与 D4RL Gym-MuJoCo 任务相关的标准数据集上进行测试来验证此断言。我们通过在两种极端情况下测试代理来进一步讨论代理的泛化能力，即使在专家数据集高度倾斜的情况下，它如何学习有效地对回报分布进行建模，以及它如何在奖励稀疏/延迟的场景中表现出稳健的性能。</li>
</ul>

<h3>Title: Revolutionizing Finance with LLMs: An Overview of Applications and  Insights</h3>
<ul>
<li><strong>Authors: </strong>Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, Ninghao Liu, Tianming Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11641">https://arxiv.org/abs/2401.11641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11641">https://arxiv.org/pdf/2401.11641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11641]] Revolutionizing Finance with LLMs: An Overview of Applications and  Insights(https://arxiv.org/abs/2401.11641)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, rag</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models (LLMs) like ChatGPT have seen considerable advancements and have been applied in diverse fields. Built on the Transformer architecture, these models are trained on extensive datasets, enabling them to understand and generate human language effectively. In the financial domain, the deployment of LLMs is gaining momentum. These models are being utilized for automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. Leveraging their natural language processing capabilities, LLMs can distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction. In this study, we provide a comprehensive overview of the emerging integration of LLMs into various financial tasks. Additionally, we conducted holistic tests on multiple financial tasks through the combination of natural language instructions. Our findings show that GPT-4 effectively follow prompt instructions across various financial tasks. This survey and evaluation of LLMs in the financial domain aim to deepen the understanding of LLMs' current role in finance for both financial practitioners and LLM researchers, identify new research and application prospects, and highlight how these technologies can be leveraged to solve practical challenges in the finance industry.</li>
<li><strong>摘要：</strong>近年来，像 ChatGPT 这样的大型语言模型（LLM）取得了长足的进步，并已应用于各个领域。这些模型建立在 Transformer 架构之上，在广泛的数据集上进行训练，使它们能够有效地理解和生成人类语言。在金融领域，法学硕士的部署势头强劲。这些模型用于自动生成财务报告、预测市场趋势、分析投资者情绪以及提供个性化的财务建议。法学硕士利用其自然语言处理能力，可以从大量财务数据中提取关键见解，帮助机构做出明智的投资选择，并提高运营效率和客户满意度。在这项研究中，我们全面概述了法学硕士与各种财务任务的新兴整合。此外，我们还结合自然语言指令对多项财务任务进行了整体测试。我们的研究结果表明，GPT-4 在各种财务任务中有效地遵循即时指令。此次对金融领域法学硕士的调查和评估旨在加深金融从业者和法学硕士研究人员对法学硕士当前在金融领域的作用的理解，确定新的研究和应用前景，并强调如何利用这些技术来解决金融领域的实际挑战。金融业。</li>
</ul>

<h3>Title: LW-FedSSL: Resource-efficient Layer-wise Federated Self-supervised  Learning</h3>
<ul>
<li><strong>Authors: </strong>Ye Lin Tun, Chu Myaet Thwal, Le Quang Huy, Minh N. H. Nguyen, Choong Seon Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11647">https://arxiv.org/abs/2401.11647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11647">https://arxiv.org/pdf/2401.11647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11647]] LW-FedSSL: Resource-efficient Layer-wise Federated Self-supervised  Learning(https://arxiv.org/abs/2401.11647)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Many recent studies integrate federated learning (FL) with self-supervised learning (SSL) to take advantage of raw training data distributed across edge devices. However, edge devices often struggle with high computation and communication costs imposed by SSL and FL algorithms. To tackle this hindrance, we propose LW-FedSSL, a layer-wise federated self-supervised learning approach that allows edge devices to incrementally train one layer of the model at a time. LW-FedSSL comprises server-side calibration and representation alignment mechanisms to maintain comparable performance with end-to-end FedSSL while significantly lowering clients' resource requirements. The server-side calibration mechanism takes advantage of the resource-rich server in an FL environment to assist in global model training. Meanwhile, the representation alignment mechanism encourages closeness between representations of FL local models and those of the global model. Our experiments show that LW-FedSSL has a $3.3 \times$ lower memory requirement and a $3.2 \times$ cheaper communication cost than its end-to-end counterpart. We also explore a progressive training strategy called Prog-FedSSL that outperforms end-to-end training with a similar memory requirement and a $1.8 \times$ cheaper communication cost.</li>
<li><strong>摘要：</strong>最近的许多研究将联邦学习 (FL) 与自监督学习 (SSL) 相结合，以利用分布在边缘设备上的原始训练数据。然而，边缘设备常常面临 SSL 和 FL 算法带来的高计算和通信成本。为了解决这一障碍，我们提出了 LW-FedSSL，这是一种分层联合自监督学习方法，允许边缘设备一次增量训练模型的一层。 LW-FedSSL 包括服务器端校准和表示对齐机制，以保持与端到端 FedSSL 相当的性能，同时显着降低客户端的资源需求。服务器端标定机制利用FL环境中资源丰富的服务器来辅助全局模型训练。同时，表示对齐机制鼓励 FL 局部模型的表示与全局模型的表示之间的接近。我们的实验表明，与端到端对应方案相比，LW-FedSSL 的内存需求降低了 3.3 美元，通信成本降低了 3.2 美元。我们还探索了一种名为 Prog-FedSSL 的渐进式训练策略，该策略在内存要求相似的情况下优于端到端训练，并且通信成本便宜 1.8 美元。</li>
</ul>

<h3>Title: Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal  Contrastive EHR Modelling with Hierarchical Regularisation</h3>
<ul>
<li><strong>Authors: </strong>Heejoon Koo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11648">https://arxiv.org/abs/2401.11648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11648">https://arxiv.org/pdf/2401.11648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11648]] Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal  Contrastive EHR Modelling with Hierarchical Regularisation(https://arxiv.org/abs/2401.11648)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.</li>
<li><strong>摘要：</strong>使用电子健康记录 (EHR) 预测下次就诊诊断是医疗保健领域的一项重要任务，对于为医疗保健提供者和患者制定积极的未来计划至关重要。尽管如此，许多先前的研究并没有充分解决 EHR 数据固有的异构和层次特征，不可避免地导致性能不佳。为此，我们提出了 NECHO，一种新颖的以医疗代码为中心的多模式对比 EHR 学习框架，具有分层正则化功能。首先，我们使用定制的网络设计和一对双峰对比损失来整合包括医疗代码、人口统计和临床记录在内的多方面信息，所有这些信息都围绕医疗代码表示。我们还使用医学本体中的父母级别信息来规范特定于模态的编码器，以学习 EHR 数据的层次结构。 MIMIC-III 数据的一系列实验证明了我们方法的有效性。</li>
</ul>

<h3>Title: Zero-Space Cost Fault Tolerance for Transformer-based Language Models on  ReRAM</h3>
<ul>
<li><strong>Authors: </strong>Bingbing Li, Geng Yuan, Zigeng Wang, Shaoyi Huang, Hongwu Peng, Payman Behnam, Wujie Wen, Hang Liu, Caiwen Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11664">https://arxiv.org/abs/2401.11664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11664">https://arxiv.org/pdf/2401.11664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11664]] Zero-Space Cost Fault Tolerance for Transformer-based Language Models on  ReRAM(https://arxiv.org/abs/2401.11664)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Resistive Random Access Memory (ReRAM) has emerged as a promising platform for deep neural networks (DNNs) due to its support for parallel in-situ matrix-vector multiplication. However, hardware failures, such as stuck-at-fault defects, can result in significant prediction errors during model inference. While additional crossbars can be used to address these failures, they come with storage overhead and are not efficient in terms of space, energy, and cost. In this paper, we propose a fault protection mechanism that incurs zero space cost. Our approach includes: 1) differentiable structure pruning of rows and columns to reduce model redundancy, 2) weight duplication and voting for robust output, and 3) embedding duplicated most significant bits (MSBs) into the model weight. We evaluate our method on nine tasks of the GLUE benchmark with the BERT model, and experimental results prove its effectiveness.</li>
<li><strong>摘要：</strong>电阻式随机存取存储器 (ReRAM) 由于支持并行原位矩阵向量乘法，已成为深度神经网络 (DNN) 的一个有前景的平台。然而，硬件故障（例如固定故障缺陷）可能会在模型推理过程中导致严重的预测错误。虽然可以使用额外的交叉开关来解决这些故障，但它们会带来存储开销，并且在空间、能源和成本方面效率不高。在本文中，我们提出了一种零空间成本的故障保护机制。我们的方法包括：1）行和列的可微结构修剪以减少模型冗余，2）权重复制和投票以获得稳健的输出，3）将重复的最高有效位（MSB）嵌入到模型权重中。我们使用 BERT 模型在 GLUE 基准的九个任务上评估我们的方法，实验结果证明了其有效性。</li>
</ul>

<h3>Title: P2DT: Mitigating Forgetting in task-incremental Learning with  progressive prompt Decision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Wang, Xiaoyang Qu, Jing Xiao, Bokui Chen, Jianzong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11666">https://arxiv.org/abs/2401.11666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11666">https://arxiv.org/pdf/2401.11666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11666]] P2DT: Mitigating Forgetting in task-incremental Learning with  progressive prompt Decision Transformer(https://arxiv.org/abs/2401.11666)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, rag, agent</a></li>
<li><strong>Abstract: </strong>Catastrophic forgetting poses a substantial challenge for managing intelligent agents controlled by a large model, causing performance degradation when these agents face new tasks. In our work, we propose a novel solution - the Progressive Prompt Decision Transformer (P2DT). This method enhances a transformer-based model by dynamically appending decision tokens during new task training, thus fostering task-specific policies. Our approach mitigates forgetting in continual and offline reinforcement learning scenarios. Moreover, P2DT leverages trajectories collected via traditional reinforcement learning from all tasks and generates new task-specific tokens during training, thereby retaining knowledge from previous studies. Preliminary results demonstrate that our model effectively alleviates catastrophic forgetting and scales well with increasing task environments.</li>
<li><strong>摘要：</strong>灾难性遗忘给管理由大型模型控制的智能代理带来了巨大的挑战，当这些代理面临新任务时，会导致性能下降。在我们的工作中，我们提出了一种新颖的解决方案 - 渐进式提示决策转换器（P2DT）。该方法通过在新任务训练期间动态附加决策标记来增强基于变压器的模型，从而促进特定于任务的策略。我们的方法可以减轻持续和离线强化学习场景中的遗忘。此外，P2DT 利用传统强化学习从所有任务中收集的轨迹，并在训练期间生成新的特定于任务的标记，从而保留以前研究中的知识。初步结果表明，我们的模型有效地减轻了灾难性遗忘，并且随着任务环境的增加而很好地扩展。</li>
</ul>

<h3>Title: INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free  Class-incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Wang, Xiaoyang Qu, Jing Xiao, Bokui Chen, Jianzong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11667">https://arxiv.org/abs/2401.11667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11667">https://arxiv.org/pdf/2401.11667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11667]] INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free  Class-incremental Learning(https://arxiv.org/abs/2401.11667)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, code</a></li>
<li><strong>Abstract: </strong>This paper introduces INCPrompt, an innovative continual learning solution that effectively addresses catastrophic forgetting. INCPrompt's key innovation lies in its use of adaptive key-learner and task-aware prompts that capture task-relevant information. This unique combination encapsulates general knowledge across tasks and encodes task-specific knowledge. Our comprehensive evaluation across multiple continual learning benchmarks demonstrates INCPrompt's superiority over existing algorithms, showing its effectiveness in mitigating catastrophic forgetting while maintaining high performance. These results highlight the significant impact of task-aware incremental prompting on continual learning performance.</li>
<li><strong>摘要：</strong>本文介绍了 INCPrompt，这是一种创新的持续学习解决方案，可有效解决灾难性遗忘问题。 INCPrompt 的关键创新在于它使用自适应关键学习者和任务感知提示来捕获任务相关信息。这种独特的组合封装了跨任务的一般知识并编码了特定于任务的知识。我们对多个持续学习基准的综合评估证明了 INCPrompt 相对于现有算法的优越性，显示了其在保持高性能的同时减轻灾难性遗忘的有效性。这些结果凸显了任务感知增量提示对持续学习绩效的显着影响。</li>
</ul>

<h3>Title: Keep Decoding Parallel with Effective Knowledge Distillation from  Language Models to End-to-end Speech Recognisers</h3>
<ul>
<li><strong>Authors: </strong>Michael Hentschel, Yuta Nishikawa, Tatsuya Komatsu, Yusuke Fujita</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11700">https://arxiv.org/abs/2401.11700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11700">https://arxiv.org/pdf/2401.11700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11700]] Keep Decoding Parallel with Effective Knowledge Distillation from  Language Models to End-to-end Speech Recognisers(https://arxiv.org/abs/2401.11700)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>This study presents a novel approach for knowledge distillation (KD) from a BERT teacher model to an automatic speech recognition (ASR) model using intermediate layers. To distil the teacher's knowledge, we use an attention decoder that learns from BERT's token probabilities. Our method shows that language model (LM) information can be more effectively distilled into an ASR model using both the intermediate layers and the final layer. By using the intermediate layers as distillation target, we can more effectively distil LM knowledge into the lower network layers. Using our method, we achieve better recognition accuracy than with shallow fusion of an external LM, allowing us to maintain fast parallel decoding. Experiments on the LibriSpeech dataset demonstrate the effectiveness of our approach in enhancing greedy decoding with connectionist temporal classification (CTC).</li>
<li><strong>摘要：</strong>这项研究提出了一种从 BERT 教师模型到使用中间层的自动语音识别（ASR）模型的知识蒸馏（KD）的新方法。为了提炼教师的知识，我们使用了一个从 BERT 的 token 概率中学习的注意力解码器。我们的方法表明，使用中间层和最终层可以更有效地将语言模型 (LM) 信息提取到 ASR 模型中。通过使用中间层作为蒸馏目标，我们可以更有效地将LM知识蒸馏到较低的网络层。使用我们的方法，我们比外部 LM 的浅层融合实现了更好的识别精度，使我们能够保持快速并行解码。 LibriSpeech 数据集上的实验证明了我们的方法在通过联结时间分类 (CTC) 增强贪婪解码方面的有效性。</li>
</ul>

<h3>Title: Graph Condensation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Gao, Junliang Yu, Wei Jiang, Tong Chen, Wentao Zhang, Hongzhi Yin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11720">https://arxiv.org/abs/2401.11720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11720">https://arxiv.org/pdf/2401.11720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11720]] Graph Condensation: A Survey(https://arxiv.org/abs/2401.11720)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The burgeoning volume of graph data poses significant challenges in storage, transmission, and particularly the training of graph neural networks (GNNs). To address these challenges, graph condensation (GC) has emerged as an innovative solution. GC focuses on synthesizing a compact yet highly representative graph, on which GNNs can achieve performance comparable to trained on the large original graph. The notable efficacy of GC and its broad prospects have garnered significant attention and spurred extensive research. This survey paper provides an up-to-date and systematic overview of GC, organizing existing research into four categories aligned with critical GC evaluation criteria: effectiveness, generalization, fairness, and efficiency. To facilitate an in-depth and comprehensive understanding of GC, we examine various methods under each category and thoroughly discuss two essential components within GC: optimization strategies and condensed graph generation. Additionally, we introduce the applications of GC in a variety of fields, and highlight the present challenges and novel insights in GC, promoting advancements in future research.</li>
<li><strong>摘要：</strong>图数据量的迅速增长给存储、传输，特别是图神经网络（GNN）的训练带来了巨大的挑战。为了应对这些挑战，图压缩（GC）作为一种创新解决方案应运而生。 GC 专注于合成一个紧凑但具有高度代表性的图，在该图上 GNN 可以实现与在大型原始图上训练的性能相当的性能。 GC显着的功效及其广阔的前景引起了人们的广泛关注和广泛的研究。本调查论文提供了 GC 的最新且系统的概述，将现有研究分为与关键 GC 评估标准一致的四个类别：有效性、泛化性、公平性和效率。为了促进对 GC 的深入和全面的理解，我们研究了每个类别下的各种方法，并深入讨论了 GC 中的两个基本组成部分：优化策略和压缩图生成。此外，我们介绍了气相色谱在各个领域的应用，并强调了气相色谱目前的挑战和新颖的见解，促进了未来研究的进步。</li>
</ul>

<h3>Title: Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language  Conversion for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yile Wang, Sijie Cheng, Zixin Sun, Peng Li, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11725">https://arxiv.org/abs/2401.11725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11725">https://arxiv.org/pdf/2401.11725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11725]] Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language  Conversion for Language Models(https://arxiv.org/abs/2401.11725)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, code, chat, rag</a></li>
<li><strong>Abstract: </strong>Symbols (or more broadly, non-natural language textual representations) such as numerical sequences, molecular formulas, and table delimiters widely exist, playing important roles in various tasks such as abstract reasoning, chemical property prediction, and table question answering. Despite the impressive natural language comprehension capabilities of large language models (LLMs), their reasoning abilities for symbols remain inadequate, which could attributed to the difference between symbol representations and general natural languages. We propose symbol-to-language (S2L), a tuning-free method that enables large language models to solve symbol-related problems with information expressed in natural language. Specifically, S2L first converts the symbols involved to language-based representations, which can be implemented by prompting LLMs or leveraging external tools, then these language-based representations are integrated into the original problem via direct substitution or concatenation, serving as useful input information for LLMs. We evaluate the S2L method using both API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight symbol-related tasks, ranging from symbol-only abstract reasoning to sentiment analysis in social media. Experimental results show that S2L consistently leads to superior performance. For example, by employing S2L for GPT-4, there can be average significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and Dyck language, respectively. Codes and data are available at https://github.com/THUNLP-MT/symbol2language.</li>
<li><strong>摘要：</strong>数字序列、分子式和表格分隔符等符号（或更广泛地非自然语言文本表示）广泛存在，在抽象推理、化学性质预测和表格问答等各种任务中发挥着重要作用。尽管大型语言模型（LLM）的自然语言理解能力令人印象深刻，但它们对符号的推理能力仍然不足，这可能归因于符号表示与一般自然语言之间的差异。我们提出了符号到语言（S2L），这是一种免调整方法，使大型语言模型能够利用自然语言表达的信息来解决与符号相关的问题。具体来说，S2L首先将涉及的符号转换为基于语言的表示，这可以通过提示LLM或利用外部工具来实现，然后这些基于语言的表示通过直接替换或串联的方式集成到原始问题中，作为有用的输入信息法学硕士。我们使用基于 API（GPT-4、ChatGPT）和开源 (OpenChat) 模型在八个与符号相关的任务中评估 S2L 方法，范围从仅符号抽象推理到社交媒体中的情感分析。实验结果表明，S2L 始终能够带来卓越的性能。例如，通过对 GPT-4 采用 S2L，1D-ARC 和 Dyck 语言的子任务平均可分别显着提高 +21.9% 和 +9.5%。代码和数据可在 https://github.com/THUNLP-MT/symbol2language 获取。</li>
</ul>

<h3>Title: Attention on Personalized Clinical Decision Support System: Federated  Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Chu Myaet Thwal, Kyi Thar, Ye Lin Tun, Choong Seon Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11736">https://arxiv.org/abs/2401.11736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11736">https://arxiv.org/pdf/2401.11736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11736]] Attention on Personalized Clinical Decision Support System: Federated  Learning Approach(https://arxiv.org/abs/2401.11736)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Health management has become a primary problem as new kinds of diseases and complex symptoms are introduced to a rapidly growing modern society. Building a better and smarter healthcare infrastructure is one of the ultimate goals of a smart city. To the best of our knowledge, neural network models are already employed to assist healthcare professionals in achieving this goal. Typically, training a neural network requires a rich amount of data but heterogeneous and vulnerable properties of clinical data introduce a challenge for the traditional centralized network. Moreover, adding new inputs to a medical database requires re-training an existing model from scratch. To tackle these challenges, we proposed a deep learning-based clinical decision support system trained and managed under a federated learning paradigm. We focused on a novel strategy to guarantee the safety of patient privacy and overcome the risk of cyberattacks while enabling large-scale clinical data mining. As a result, we can leverage rich clinical data for training each local neural network without the need for exchanging the confidential data of patients. Moreover, we implemented the proposed scheme as a sequence-to-sequence model architecture integrating the attention mechanism. Thus, our objective is to provide a personalized clinical decision support system with evolvable characteristics that can deliver accurate solutions and assist healthcare professionals in medical diagnosing.</li>
<li><strong>摘要：</strong>快速发展的现代社会，新的疾病种类和复杂的症状不断出现，健康管理已成为首要问题。建设更好、更智能的医疗基础设施是智慧城市的最终目标之一。据我们所知，神经网络模型已经被用来帮助医疗保健专业人员实现这一目标。通常，训练神经网络需要大量数据，但临床数据的异构性和脆弱性给传统的集中式网络带来了挑战。此外，向医学数据库添加新输入需要从头开始重新训练现有模型。为了应对这些挑战，我们提出了一种基于深度学习的临床决策支持系统，在联邦学习范式下进行训练和管理。我们专注于一种新颖的策略，以保证患者隐私的安全并克服网络攻击的风险，同时实现大规模临床数据挖掘。因此，我们可以利用丰富的临床数据来训练每个局部神经网络，而无需交换患者的机密数据。此外，我们将所提出的方案实现为集成注意力机制的序列到序列模型架构。因此，我们的目标是提供具有可进化特征的个性化临床决策支持系统，可以提供准确的解决方案并协助医疗保健专业人员进行医疗诊断。</li>
</ul>

<h3>Title: FedGTA: Topology-aware Averaging for Federated Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Xunkai Li, Zhengyu Wu, Wentao Zhang, Yinlin Zhu, Rong-Hua Li, Guoren Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11755">https://arxiv.org/abs/2401.11755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11755">https://arxiv.org/pdf/2401.11755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11755]] FedGTA: Topology-aware Averaging for Federated Graph Learning(https://arxiv.org/abs/2401.11755)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Federated Graph Learning (FGL) is a distributed machine learning paradigm that enables collaborative training on large-scale subgraphs across multiple local systems. Existing FGL studies fall into two categories: (i) FGL Optimization, which improves multi-client training in existing machine learning models; (ii) FGL Model, which enhances performance with complex local models and multi-client interactions. However, most FGL optimization strategies are designed specifically for the computer vision domain and ignore graph structure, presenting dissatisfied performance and slow convergence. Meanwhile, complex local model architectures in FGL Models studies lack scalability for handling large-scale subgraphs and have deployment limitations. To address these issues, we propose Federated Graph Topology-aware Aggregation (FedGTA), a personalized optimization strategy that optimizes through topology-aware local smoothing confidence and mixed neighbor features. During experiments, we deploy FedGTA in 12 multi-scale real-world datasets with the Louvain and Metis split. This allows us to evaluate the performance and robustness of FedGTA across a range of scenarios. Extensive experiments demonstrate that FedGTA achieves state-of-the-art performance while exhibiting high scalability and efficiency. The experiment includes ogbn-papers100M, the most representative large-scale graph database so that we can verify the applicability of our method to large-scale graph learning. To the best of our knowledge, our study is the first to bridge large-scale graph learning with FGL using this optimization strategy, contributing to the development of efficient and scalable FGL methods.</li>
<li><strong>摘要：</strong>联合图学习（FGL）是一种分布式机器学习范例，可以跨多个本地系统对大规模子图进行协作训练。现有的 FGL 研究分为两类：（i）FGL 优化，改进现有机器学习模型中的多客户端训练； (ii) FGL 模型，通过复杂的本地模型和多客户端交互来提高性能。然而，大多数FGL优化策略都是专门针对计算机视觉领域而设计的，忽略了图结构，表现出性能不佳且收敛缓慢。同时，FGL 模型研究中复杂的局部模型架构缺乏处理大规模子图的可扩展性，并且存在部署限制。为了解决这些问题，我们提出了联邦图拓扑感知聚合（FedGTA），这是一种个性化优化策略，通过拓扑感知局部平滑置信度和混合邻居特征进行优化。在实验过程中，我们将 FedGTA 部署在 12 个具有 Louvain 和 Metis 分割的多尺度现实世界数据集中。这使我们能够在一系列场景中评估 FedGTA 的性能和稳健性。大量实验表明，FedGTA 实现了最先进的性能，同时表现出高可扩展性和效率。实验包括最具代表性的大规模图数据库ogbn-papers100M，以便验证我们的方法对大规模图学习的适用性。据我们所知，我们的研究是第一个使用这种优化策略将大规模图学习与 FGL 联系起来的研究，有助于开发高效且可扩展的 FGL 方法。</li>
</ul>

<h3>Title: Towards Effective and General Graph Unlearning via Mutual Evolution</h3>
<ul>
<li><strong>Authors: </strong>Xunkai Li, Yulin Zhao, Zhengyu Wu, Wentao Zhang, Rong-Hua Li, Guoren Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11760">https://arxiv.org/abs/2401.11760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11760">https://arxiv.org/pdf/2401.11760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11760]] Towards Effective and General Graph Unlearning via Mutual Evolution(https://arxiv.org/abs/2401.11760)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of AI applications, the growing needs for data privacy and model robustness have highlighted the importance of machine unlearning, especially in thriving graph-based scenarios. However, most existing graph unlearning strategies primarily rely on well-designed architectures or manual process, rendering them less user-friendly and posing challenges in terms of deployment efficiency. Furthermore, striking a balance between unlearning performance and framework generalization is also a pivotal concern. To address the above issues, we propose \underline{\textbf{M}}utual \underline{\textbf{E}}volution \underline{\textbf{G}}raph \underline{\textbf{U}}nlearning (MEGU), a new mutual evolution paradigm that simultaneously evolves the predictive and unlearning capacities of graph unlearning. By incorporating aforementioned two components, MEGU ensures complementary optimization in a unified training framework that aligns with the prediction and unlearning requirements. Extensive experiments on 9 graph benchmark datasets demonstrate the superior performance of MEGU in addressing unlearning requirements at the feature, node, and edge levels. Specifically, MEGU achieves average performance improvements of 2.7\%, 2.5\%, and 3.2\% across these three levels of unlearning tasks when compared to state-of-the-art baselines. Furthermore, MEGU exhibits satisfactory training efficiency, reducing time and space overhead by an average of 159.8x and 9.6x, respectively, in comparison to retraining GNN from scratch.</li>
<li><strong>摘要：</strong>随着人工智能应用的快速发展，对数据隐私和模型鲁棒性的需求不断增长，凸显了机器学习的重要性，尤其是在蓬勃发展的基于图的场景中。然而，大多数现有的图遗忘策略主要依赖于精心设计的架构或手动流程，这使得它们不太用户友好，并在部署效率方面提出了挑战。此外，在遗忘性能和框架泛化之间取得平衡也是一个关键问题。为了解决上述问题，我们提出 \underline{\textbf{M}}utual \underline{\textbf{E}}volution \underline{\textbf{G}}raph \underline{\textbf{U}}nlearning (MEGU ），一种新的相互进化范式，它同时进化了图遗忘的预测和遗忘能力。通过整合上述两个组件，MEGU 确保在符合预测和遗忘要求的统一训练框架中进行互补优化。对 9 个图基准数据集的广泛实验证明了 MEGU 在解决特征、节点和边缘级别的忘却需求方面的卓越性能。具体来说，与最先进的基线相比，MEGU 在这三个级别的遗忘任务中实现了 2.7\%、2.5\% 和 3.2\% 的平均性能提升。此外，MEGU 表现出令人满意的训练效率，与从头开始重新训练 GNN 相比，时间和空间开销平均分别减少了 159.8 倍和 9.6 倍。</li>
</ul>

<h3>Title: Generalization and Informativeness of Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Matteo Zecchin, Sangwoo Park, Osvaldo Simeone, Fredrik Hellström</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11810">https://arxiv.org/abs/2401.11810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11810">https://arxiv.org/pdf/2401.11810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11810]] Generalization and Informativeness of Conformal Prediction(https://arxiv.org/abs/2401.11810)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The safe integration of machine learning modules in decision-making processes hinges on their ability to quantify uncertainty. A popular technique to achieve this goal is conformal prediction (CP), which transforms an arbitrary base predictor into a set predictor with coverage guarantees. While CP certifies the predicted set to contain the target quantity with a user-defined tolerance, it does not provide control over the average size of the predicted sets, i.e., over the informativeness of the prediction. In this work, a theoretical connection is established between the generalization properties of the base predictor and the informativeness of the resulting CP prediction sets. To this end, an upper bound is derived on the expected size of the CP set predictor that builds on generalization error bounds for the base predictor. The derived upper bound provides insights into the dependence of the average size of the CP set predictor on the amount of calibration data, the target reliability, and the generalization performance of the base predictor. The theoretical insights are validated using simple numerical regression and classification tasks.</li>
<li><strong>摘要：</strong>机器学习模块在决策过程中的安全集成取决于它们量化不确定性的能力。实现这一目标的一种流行技术是保形预测（CP），它将任意基本预测器转换为具有覆盖保证的集合预测器。虽然 CP 证明预测集包含具有用户定义容差的目标数量，但它无法控制预测集的平均大小，即预测的信息量。在这项工作中，在基本预测器的泛化属性和所得 CP 预测集的信息量之间建立了理论联系。为此，根据 CP 集预测器的预期大小导出上限，该上限建立在基本预测器的泛化误差界限上。导出的上限提供了对 CP 集预测器的平均大小对校准数据量、目标可靠性和基本预测器的泛化性能的依赖性的深入了解。使用简单的数值回归和分类任务验证了理论见解。</li>
</ul>

<h3>Title: Hallucination is Inevitable: An Innate Limitation of Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Xu, Sanjay Jain, Mohan Kankanhalli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11817">https://arxiv.org/abs/2401.11817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11817">https://arxiv.org/pdf/2401.11817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11817]] Hallucination is Inevitable: An Innate Limitation of Large Language  Models(https://arxiv.org/abs/2401.11817)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.</li>
<li><strong>摘要：</strong>幻觉已被广泛认为是大型语言模型（LLM）的一个重大缺陷。有许多作品试图减少幻觉的程度。到目前为止，这些努力大多是经验性的，无法回答是否可以完全消除的根本问题。在本文中，我们将问题形式化，并表明在法学硕士中消除幻觉是不可能的。具体来说，我们定义了一个正式的世界，其中幻觉被定义为可计算的 LLM 和可计算的真实函数之间的不一致。通过利用学习理论的结果，我们表明法学硕士无法学习所有可计算函数，因此总是会产生幻觉。由于正式世界是现实世界的一部分，而现实世界要复杂得多，对于现实世界的法学硕士来说，幻觉也是不可避免的。此外，对于受可证明时间复杂性限制的现实世界法学硕士，我们描述了容易产生幻觉的任务并凭经验验证了我们的主张。最后，利用正式的世界框架，我们讨论了现有幻觉缓解剂的可能机制和功效，以及对法学硕士安全部署的实际影响。</li>
</ul>

<h3>Title: SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in  Chinese</h3>
<ul>
<li><strong>Authors: </strong>Liang Xu, Hang Xue, Lei Zhu, Kangkang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11819">https://arxiv.org/abs/2401.11819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11819">https://arxiv.org/pdf/2401.11819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11819]] SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in  Chinese(https://arxiv.org/abs/2401.11819)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>We introduce SuperCLUE-Math6(SC-Math6), a new benchmark dataset to evaluate the mathematical reasoning abilities of Chinese language models. SC-Math6 is designed as an upgraded Chinese version of the GSM8K dataset with enhanced difficulty, diversity, and application scope. It consists of over 2000 mathematical word problems requiring multi-step reasoning and providing natural language solutions. We propose an innovative scheme to quantify the reasoning capability of large models based on performance over problems with different reasoning steps. Experiments on 12 representative Chinese models demonstrate a clear stratification of reasoning levels, with top models like GPT-4 showing superior performance. SC-Math6 fills the gap in Chinese mathematical reasoning benchmarks and provides a comprehensive testbed to advance the intelligence of Chinese language models.</li>
<li><strong>摘要：</strong>我们引入了 SuperCLUE-Math6（SC-Math6），这是一个新的基准数据集，用于评估中文模型的数学推理能力。 SC-Math6被设计为GSM8K数据集的升级中文版，增强了难度、多样性和应用范围。它包含 2000 多个需要多步推理并提供自然语言解决方案的数学应用题。我们提出了一种创新方案，根据不同推理步骤问题的性能来量化大型模型的推理能力。对 12 个具有代表性的中国模型进行的实验表明，推理水平存在明显的分层，其中 GPT-4 等顶级模型表现出优越的性能。 SC-Math6填补了中文数学推理基准的空白，并为提升中文语言模型的智能性提供了全面的测试平台。</li>
</ul>

<h3>Title: AI for social science and social science of AI: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, Xianpei Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11839">https://arxiv.org/abs/2401.11839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11839">https://arxiv.org/pdf/2401.11839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11839]] AI for social science and social science of AI: A Survey(https://arxiv.org/abs/2401.11839)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in artificial intelligence, particularly with the emergence of large language models (LLMs), have sparked a rethinking of artificial general intelligence possibilities. The increasing human-like capabilities of AI are also attracting attention in social science research, leading to various studies exploring the combination of these two fields. In this survey, we systematically categorize previous explorations in the combination of AI and social science into two directions that share common technical approaches but differ in their research objectives. The first direction is focused on AI for social science, where AI is utilized as a powerful tool to enhance various stages of social science research. While the second direction is the social science of AI, which examines AI agents as social entities with their human-like cognitive and linguistic capabilities. By conducting a thorough review, particularly on the substantial progress facilitated by recent advancements in large language models, this paper introduces a fresh perspective to reassess the relationship between AI and social science, provides a cohesive framework that allows researchers to understand the distinctions and connections between AI for social science and social science of AI, and also summarized state-of-art experiment simulation platforms to facilitate research in these two directions. We believe that as AI technology continues to advance and intelligent agents find increasing applications in our daily lives, the significance of the combination of AI and social science will become even more prominent.</li>
<li><strong>摘要：</strong>人工智能的最新进展，特别是大型语言模型（LLM）的出现，引发了人们对通用人工智能可能性的重新思考。人工智能日益增强的类人能力也引起了社会科学研究的关注，引发了各种研究探索这两个领域的结合。在本次调查中，我们将以往人工智能与社会科学结合的探索系统地分为两个技术方法相同但研究目标不同的方向。第一个方向侧重于社会科学领域的人工智能，人工智能被用作加强社会科学研究各个阶段的强大工具。第二个方向是人工智能的社会科学，它将人工智能代理视为具有类人认知和语言能力的社会实体。通过进行彻底的回顾，特别是对大型语言模型的最新进展所促成的实质性进展，本文引入了重新评估人工智能和社会科学之间关系的新视角，提供了一个有凝聚力的框架，使研究人员能够理解两者之间的区别和联系。人工智能社会科学和人工智能社会科学，并总结了最先进的实验模拟平台，以促进这两个方向的研究。我们相信，随着人工智能技术的不断进步，智能体在我们日常生活中的应用越来越广泛，人工智能与社会科学结合的意义将更加凸显。</li>
</ul>

<h3>Title: Learning to Approximate Adaptive Kernel Convolution on Graphs</h3>
<ul>
<li><strong>Authors: </strong>Jaeyoon Sim, Sooyeon Jeon, InJun Choi, Guorong Wu, Won Hwa Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11840">https://arxiv.org/abs/2401.11840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11840">https://arxiv.org/pdf/2401.11840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11840]] Learning to Approximate Adaptive Kernel Convolution on Graphs(https://arxiv.org/abs/2401.11840)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Various Graph Neural Networks (GNNs) have been successful in analyzing data in non-Euclidean spaces, however, they have limitations such as oversmoothing, i.e., information becomes excessively averaged as the number of hidden layers increases. The issue stems from the intrinsic formulation of conventional graph convolution where the nodal features are aggregated from a direct neighborhood per layer across the entire nodes in the graph. As setting different number of hidden layers per node is infeasible, recent works leverage a diffusion kernel to redefine the graph structure and incorporate information from farther nodes. Unfortunately, such approaches suffer from heavy diagonalization of a graph Laplacian or learning a large transform matrix. In this regards, we propose a diffusion learning framework, where the range of feature aggregation is controlled by the scale of a diffusion kernel. For efficient computation, we derive closed-form derivatives of approximations of the graph convolution with respect to the scale, so that node-wise range can be adaptively learned. With a downstream classifier, the entire framework is made trainable in an end-to-end manner. Our model is tested on various standard datasets for node-wise classification for the state-of-the-art performance, and it is also validated on a real-world brain network data for graph classifications to demonstrate its practicality for Alzheimer classification.</li>
<li><strong>摘要：</strong>各种图神经网络（GNN）已经成功地分析了非欧几里得空间中的数据，但是它们存在诸如过度平滑之类的局限性，即随着隐藏层数量的增加，信息变得过度平均。该问题源于传统图卷积的内在公式，其中节点特征是从图中整个节点的每层直接邻域聚合的。由于为每个节点设置不同数量的隐藏层是不可行的，最近的工作利用扩散内核来重新定义图结构并合并来自更远节点的信息。不幸的是，此类方法受到图拉普拉斯算子的严重对角化或学习大变换矩阵的影响。在这方面，我们提出了一种扩散学习框架，其中特征聚合的范围由扩散内核的规模控制。为了高效计算，我们推导了图卷积相对于尺度的近似的封闭形式导数，以便可以自适应地学习节点范围。通过下游分类器，整个框架可以以端到端的方式进行训练。我们的模型在各种标准数据集上进行了节点级分类测试，以实现最先进的性能，并且还在真实世界的大脑网络数据上进行了图形分类验证，以证明其对阿尔茨海默病分类的实用性。</li>
</ul>

<h3>Title: ExtruOnt: An ontology for describing a type of manufacturing machine for  Industry 4.0 systems</h3>
<ul>
<li><strong>Authors: </strong>Víctor Julio Ramírez-Durán, Idoia Berges, Arantza Illarramendi</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11848">https://arxiv.org/abs/2401.11848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11848">https://arxiv.org/pdf/2401.11848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11848]] ExtruOnt: An ontology for describing a type of manufacturing machine for  Industry 4.0 systems(https://arxiv.org/abs/2401.11848)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Semantically rich descriptions of manufacturing machines, offered in a machine-interpretable code, can provide interesting benefits in Industry 4.0 scenarios. However, the lack of that type of descriptions is evident. In this paper we present the development effort made to build an ontology, called ExtruOnt, for describing a type of manufacturing machine, more precisely, a type that performs an extrusion process (extruder). Although the scope of the ontology is restricted to a concrete domain, it could be used as a model for the development of other ontologies for describing manufacturing machines in Industry 4.0 scenarios. The terms of the ExtruOnt ontology provide different types of information related with an extruder, which are reflected in distinct modules that constitute the ontology. Thus, it contains classes and properties for expressing descriptions about components of an extruder, spatial connections, features, and 3D representations of those components, and finally the sensors used to capture indicators about the performance of this type of machine. The ontology development process has been carried out in close collaboration with domain experts.</li>
<li><strong>摘要：</strong>以机器可解释的代码提供的制造机器的语义丰富的描述可以在工业 4.0 场景中提供有趣的好处。然而，显然缺乏这种类型的描述。在本文中，我们介绍了构建一个名为 ExtruOnt 的本体的开发工作，用于描述一种制造机器，更准确地说，是一种执行挤压过程的类型（挤压机）。尽管本体的范围仅限于具体领域，但它可以用作开发其他本体的模型，用于描述工业 4.0 场景中的制造机器。 ExtruOnt本体的术语提供了与挤出机相关的不同类型的信息，这些信息反映在构成本体的不同模块中。因此，它包含用于表达有关挤出机组件、空间连接、特征和这些组件的 3D 表示的描述的类和属性，以及最后用于捕获有关此类机器性能指标的传感器。本体开发过程是与领域专家密切合作进行的。</li>
</ul>

<h3>Title: The Right Model for the Job: An Evaluation of Legal Multi-Label  Classification Baselines</h3>
<ul>
<li><strong>Authors: </strong>Martina Forster, Claudia Schulz, Prudhvi Nokku, Melicaalsadat Mirsafian, Jaykumar Kasundra, Stavroula Skylaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11852">https://arxiv.org/abs/2401.11852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11852">https://arxiv.org/pdf/2401.11852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11852]] The Right Model for the Job: An Evaluation of Legal Multi-Label  Classification Baselines(https://arxiv.org/abs/2401.11852)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Multi-Label Classification (MLC) is a common task in the legal domain, where more than one label may be assigned to a legal document. A wide range of methods can be applied, ranging from traditional ML approaches to the latest Transformer-based architectures. In this work, we perform an evaluation of different MLC methods using two public legal datasets, POSTURE50K and EURLEX57K. By varying the amount of training data and the number of labels, we explore the comparative advantage offered by different approaches in relation to the dataset properties. Our findings highlight DistilRoBERTa and LegalBERT as performing consistently well in legal MLC with reasonable computational demands. T5 also demonstrates comparable performance while offering advantages as a generative model in the presence of changing label sets. Finally, we show that the CrossEncoder exhibits potential for notable macro-F1 score improvements, albeit with increased computational costs.</li>
<li><strong>摘要：</strong>多标签分类（MLC）是法律领域中的一项常见任务，其中可能会为一份法律文档分配多个标签。可以应用多种方法，从传统的 ML 方法到最新的基于 Transformer 的架构。在这项工作中，我们使用两个公共法律数据集 POSTURE50K 和 EURLEX57K 对不同的 MLC 方法进行评估。通过改变训练数据量和标签数量，我们探索了不同方法在数据集属性方面提供的比较优势。我们的研究结果强调 DistilRoBERTa 和 LegalBERT 在具有合理计算需求的合法 MLC 中始终表现良好。 T5 还展示了可比较的性能，同时在标签集不断变化的情况下提供作为生成模型的优势。最后，我们表明 CrossEncoder 表现出显着提高宏 F1 分数的潜力，尽管计算成本有所增加。</li>
</ul>

<h3>Title: Improving Small Language Models' Mathematical Reasoning via Mix Thoughts  Distillation</h3>
<ul>
<li><strong>Authors: </strong>Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11864">https://arxiv.org/abs/2401.11864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11864">https://arxiv.org/pdf/2401.11864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11864]] Improving Small Language Models' Mathematical Reasoning via Mix Thoughts  Distillation(https://arxiv.org/abs/2401.11864)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Mix Thoughts Distillation (MTD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while MTD enables these models to achieve state-of-the-art reasoning performance.</li>
<li><strong>摘要：</strong>这项工作通过在不影响性能的情况下将高级大型语言模型 (LLM) 的数学推理能力压缩为数十亿参数的小语言模型 (SLM) 来解决民主化高级大型语言模型 (LLM) 的挑战。我们引入了思想方程蒸馏（EoTD），这是一种新技术，它将推理过程封装到基于方程的表示中，以构建用于微调 SLM 的 EoTD 数据集。此外，我们提出了混合思维蒸馏（MTD）框架来增强 SLM 的推理性能。这涉及创建具有多个思维过程的推理数据集并使用它进行微调。我们的实验结果表明，EoTD 显着提高了 SLM 的推理能力，而 MTD 使这些模型能够实现最先进的推理性能。</li>
</ul>

<h3>Title: Toward Semantic Interoperability of Electronic Health Records</h3>
<ul>
<li><strong>Authors: </strong>Idoia Berges, Jesús Bermúdez, Arantza Illarramendi</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11865">https://arxiv.org/abs/2401.11865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11865">https://arxiv.org/pdf/2401.11865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11865]] Toward Semantic Interoperability of Electronic Health Records(https://arxiv.org/abs/2401.11865)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Although the goal of achieving semantic interoperability of electronic health records (EHRs) is pursued by many researchers, it has not been accomplished yet. In this paper, we present a proposal that smoothes out the way toward the achievement of that goal. In particular, our study focuses on medical diagnoses statements. In summary, the main contributions of our ontology-based proposal are the following: first, it includes a canonical ontology whose EHR-related terms focus on semantic aspects. As a result, their descriptions are independent of languages and technology aspects used in different organizations to represent EHRs. Moreover, those terms are related to their corresponding codes in well-known medical terminologies. Second, it deals with modules that allow obtaining rich ontological representations of EHR information managed by proprietary models of health information systems. The features of one specific module are shown as reference. Third, it considers the necessary mapping axioms between ontological terms enhanced with so-called path mappings. This feature smoothes out structural differences between heterogeneous EHR representations, allowing proper alignment of information.</li>
<li><strong>摘要：</strong>尽管实现电子健康记录（EHR）语义互操作性的目标是许多研究人员追求的目标，但尚未实现。在本文中，我们提出了一项提案，为实现该目标铺平道路。我们的研究特别关注医疗诊断陈述。总之，我们基于本体的提案的主要贡献如下：首先，它包括一个规范本体，其 EHR 相关术语侧重于语义方面。因此，它们的描述独立于不同组织中用于表示 EHR 的语言和技术方面。此外，这些术语与众所周知的医学术语中相应的代码相关。其次，它涉及的模块允许获取由健康信息系统专有模型管理的 EHR 信息的丰富本体表示。显示某一特定模块的功能以供参考。第三，它考虑了通过所谓的路径映射增强的本体术语之间必要的映射公理。此功能消除了异构 EHR 表示之间的结构差异，从而实现信息的正确对齐。</li>
</ul>

<h3>Title: PsySafe: A Comprehensive Framework for Psychological-based Attack,  Defense, and Evaluation of Multi-agent System Safety</h3>
<ul>
<li><strong>Authors: </strong>Zaibin Zhang, Yongting Zhang, Lijun Li, Hongzhi Gao, Lijun Wang, Huchuan Lu, Feng Zhao, Yu Qiao, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11880">https://arxiv.org/abs/2401.11880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11880">https://arxiv.org/pdf/2401.11880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11880]] PsySafe: A Comprehensive Framework for Psychological-based Attack,  Defense, and Evaluation of Multi-agent System Safety(https://arxiv.org/abs/2401.11880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, agent</a></li>
<li><strong>Abstract: </strong>Multi-agent systems, augmented with Large Language Models (LLMs), demonstrate significant capabilities for collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. From the perspective of agent psychology, we discover that the dark psychological states of agents can lead to severe safety issues. To address these issues, we propose a comprehensive framework grounded in agent psychology. In our framework, we focus on three aspects: identifying how dark personality traits in agents might lead to risky behaviors, designing defense strategies to mitigate these risks, and evaluating the safety of multi-agent systems from both psychological and behavioral perspectives. Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents' propensity for self-reflection when engaging in dangerous behavior, and the correlation between agents' psychological assessments and their dangerous behaviors. We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems. We will make our data and code publicly accessible at https:/github.com/AI4Good24/PsySafe.</li>
<li><strong>摘要：</strong>通过大型语言模型 (LLM) 增强的多智能体系统展示了集体智能的重要功能。然而，出于恶意目的而滥用这些情报可能会带来重大风险。迄今为止，对与多智能体系统相关的安全问题的全面研究仍然有限。从代理人心理学的角度来看，我们发现代理人的黑暗心理状态会导致严重的安全问题。为了解决这些问题，我们提出了一个基于主体心理学的综合框架。在我们的框架中，我们关注三个方面：识别智能体的黑暗人格特征如何导致危险行为，设计防御策略来减轻这些风险，以及从心理和行为角度评估多智能体系统的安全性。我们的实验揭示了一些有趣的现象，例如代理人之间的集体危险行为、代理人在从事危险行为时的自我反思倾向，以及代理人的心理评估与其危险行为之间的相关性。我们预计我们的框架和观察将为进一步研究多智能体系统的安全性提供有价值的见解。我们将在 https://github.com/AI4Good24/PsySafe 公开我们的数据和代码。</li>
</ul>

<h3>Title: Blinded by Generated Contexts: How Language Models Merge Generated and  Retrieved Contexts for Open-Domain QA?</h3>
<ul>
<li><strong>Authors: </strong>Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11911">https://arxiv.org/abs/2401.11911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11911">https://arxiv.org/pdf/2401.11911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11911]] Blinded by Generated Contexts: How Language Models Merge Generated and  Retrieved Contexts for Open-Domain QA?(https://arxiv.org/abs/2401.11911)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how well LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a task specifically designed to identify whether the answers, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To support this task, we develop a methodology to construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs towards generated contexts, as evidenced across state-of-the-art open (Llama2-7b/13b) and closed (GPT 3.5/4) systems. We further identify two key factors contributing to this bias: i) Contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) The segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs. Our analysis enhances the understanding of how LLMs merge diverse contexts, offering valuable insights for advancing current augmentation methods for LLMs.</li>
<li><strong>摘要：</strong>虽然辅助信息已成为增强大型语言模型 (LLM) 的关键，但对于 LLM 如何很好地合并这些上下文（具体生成和检索），人们知之甚少。为了研究这一点，我们制定了一项专门设计的任务，用于识别从生成的上下文和检索到的上下文的集成中得出的答案是否归因于生成的上下文或检索到的上下文。为了支持这项任务，我们开发了一种方法来构建具有冲突上下文的数据集，其中每个问题都与生成的和检索的上下文配对，但其中只有一个包含正确的答案。我们的实验揭示了法学硕士对生成上下文的显着偏见，这一点在最先进的开放 (Llama2-7b/13b) 和封闭 (GPT 3.5/4) 系统中得到了证明。我们进一步确定了造成这种偏见的两个关键因素：i）法学硕士生成的背景通常与问题表现出更大的相似性，增加了他们被选择的可能性； ii) 检索上下文中使用的分割过程破坏了它们的完整性，从而阻碍了它们在法学硕士中的充分利用。我们的分析增强了对法学硕士如何融合不同背景的理解，为推进法学硕士当前的增强方法提供了宝贵的见解。</li>
</ul>

<h3>Title: The Bigger the Better? Rethinking the Effective Model Scale in Long-term  Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Jinliang Deng, Xuan Song, Ivor W. Tsang, Hui Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11929">https://arxiv.org/abs/2401.11929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11929">https://arxiv.org/pdf/2401.11929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11929]] The Bigger the Better? Rethinking the Effective Model Scale in Long-term  Time Series Forecasting(https://arxiv.org/abs/2401.11929)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, distinguished by its focus on extensive input sequences, in contrast to the constrained lengths typical of traditional approaches. While longer sequences inherently convey richer information, potentially enhancing predictive precision, prevailing techniques often respond by escalating model complexity. These intricate models can inflate into millions of parameters, incorporating parameter-intensive elements like positional encodings, feed-forward networks and self-attention mechanisms. This complexity, however, leads to prohibitive model scale, particularly given the time series data's semantic simplicity. Motivated by the pursuit of parsimony, our research employs conditional correlation and auto-correlation as investigative tools, revealing significant redundancies within the input data. Leveraging these insights, we introduce the HDformer, a lightweight Transformer variant enhanced with hierarchical decomposition. This novel architecture not only inverts the prevailing trend toward model expansion but also accomplishes precise forecasting with drastically fewer computations and parameters. Remarkably, HDformer outperforms existing state-of-the-art LTSF models, while requiring over 99\% fewer parameters. Through this work, we advocate a paradigm shift in LTSF, emphasizing the importance to tailor the model to the inherent dynamics of time series data-a timely reminder that in the realm of LTSF, bigger is not invariably better.</li>
<li><strong>摘要：</strong>长期时间序列预测 (LTSF) 代表了时间序列分析的一个关键前沿，其特点是关注广泛的输入序列，而不是传统方法典型的受限长度。虽然较长的序列本质上传达了更丰富的信息，有可能提高预测精度，但流行的技术通常会通过增加模型复杂性来应对。这些复杂的模型可以膨胀为数百万个参数，并结合位置编码、前馈网络和自注意力机制等参数密集型元素。然而，这种复杂性导致模型规模过高，特别是考虑到时间序列数据的语义简单性。出于追求简约的动机，我们的研究采用条件相关和自相关作为研究工具，揭示了输入数据中的显着冗余。利用这些见解，我们引入了 HDformer，这是一种通过分层分解增强的轻量级 Transformer 变体。这种新颖的架构不仅扭转了模型扩展的普遍趋势，而且还以极少的计算和参数实现了精确的预测。值得注意的是，HDformer 的性能优于现有最先进的 LTSF 模型，同时需要的参数减少了 99% 以上。通过这项工作，我们提倡 LTSF 的范式转变，强调根据时间序列数据的内在动态定制模型的重要性 - 及时提醒我们，在 LTSF 领域，越大并不总是越好。</li>
</ul>

<h3>Title: Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Liu, Zhi Han, Yandong Tang, Xi-Le Zhao, Yao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11940">https://arxiv.org/abs/2401.11940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11940">https://arxiv.org/pdf/2401.11940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11940]] Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent(https://arxiv.org/abs/2401.11940)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it is worth noting that our method does not require the precise estimation of the tensor tubal-rank. Even in cases where the tubal-rank is slightly overestimated, our approach continues to demonstrate robust performance. A series of experiments have been carried out to demonstrate that, as compared to other popular ones, our approach exhibits superior performance in multiple scenarios, in terms of the faster computational speed and the smaller convergence error.</li>
<li><strong>摘要：</strong>本文考虑从少量损坏的线性测量中恢复具有底层低管秩结构的张量的问题。解决此类问题的传统方法需要计算张量奇异值分解（t-SVD），这是一个计算密集型过程，使得它们对于处理大规模张量是不切实际的。为了解决这一挑战，我们提出了一种基于类似于 Burer-Monteiro (BM) 方法的分解过程的高效且有效的低管秩张量恢复方法。准确地说，我们的基本方法是将一个大张量分解为两个较小的因子张量，然后通过分解梯度下降（FGD）来解决问题。该策略消除了 t-SVD 计算的需要，从而降低了计算成本和存储要求。我们提供严格的理论分析，以确保 FGD 在无噪声和噪声情况下的收敛。此外，值得注意的是，我们的方法不需要精确估计张量 tubeal-rank。即使在输卵管等级被稍微高估的情况下，我们的方法仍然表现出稳健的性能。一系列的实验表明，与其他流行的方法相比，我们的方法在多种场景下表现出优越的性能，计算速度更快，收敛误差更小。</li>
</ul>

<h3>Title: Benchmarking Large Multimodal Models against Common Corruptions</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Zhang, Tianyu Pang, Chao Du, Yi Ren, Bo Li, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11943">https://arxiv.org/abs/2401.11943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11943">https://arxiv.org/pdf/2401.11943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11943]] Benchmarking Large Multimodal Models against Common Corruptions(https://arxiv.org/abs/2401.11943)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>This technical report aims to fill a deficiency in the assessment of large multimodal models (LMMs) by specifically examining the self-consistency of their outputs when subjected to common corruptions. We investigate the cross-modal interactions between text, image, and speech, encompassing four essential generation tasks: text-to-image, image-to-text, text-to-speech, and speech-to-text. We create a comprehensive benchmark, named MMCBench, that covers more than 100 popular LMMs (totally over 150 model checkpoints). A thorough evaluation under common corruptions is critical for practical deployment and facilitates a better understanding of the reliability of cutting-edge LMMs. The benchmarking code is available at https://github.com/sail-sg/MMCBench</li>
<li><strong>摘要：</strong>本技术报告旨在通过专门检查大型多模态模型（LMM）在遭受常见腐败时输出的自洽性来填补评估的缺陷。我们研究文本、图像和语音之间的跨模式交互，包括四个基本生成任务：文本到图像、图像到文本、文本到语音和语音到文本。我们创建了一个名为 MMCBench 的综合基准测试，涵盖 100 多个流行的 LMM（总共超过 150 个模型检查点）。对常见损坏进行彻底评估对于实际部署至关重要，并且有助于更好地了解尖端 LMM 的可靠性。基准测试代码可在 https://github.com/sail-sg/MMCBench 获取</li>
</ul>

<h3>Title: CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding  Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11944">https://arxiv.org/abs/2401.11944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11944">https://arxiv.org/pdf/2401.11944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11944]] CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding  Benchmark(https://arxiv.org/abs/2401.11944)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU. CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. CMMMU focuses on complex perception and reasoning with domain-specific knowledge in the Chinese context. We evaluate 11 open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%, indicating a large space for improvement. CMMMU will boost the community to build the next-generation LMMs towards expert artificial intelligence and promote the democratization of LMMs by providing diverse language contexts.</li>
<li><strong>摘要：</strong>随着大型多模态模型 (LMM) 的功能不断进步，评估 LMM 性能的需求也日益增长。此外，在汉语等非英语环境中评估 LMM 的高级知识和推理能力存在更大的差距。我们推出了 CMMMU，这是一种新的中国大规模多学科多模态理解基准，旨在评估 LMM 在中国背景下需要大学水平学科知识和深思熟虑推理的任务。 CMMMU受到MMMU的启发并严格遵循MMMU的注释和分析模式。 CMMMU 包括从大学考试、测验和教科书中手动收集的 12k 多模态问题，涵盖六个核心学科：艺术与设计、商业、科学、健康与医学、人文与社会科学以及技术与工程，与其姊妹篇 MMMU 一样。这些问题涵盖 30 个主题，包含 39 种高度异构的图像类型，例如图表、图表、地图、表格、乐谱和化学结构。 CMMMU 专注于在中国背景下利用特定领域知识进行复杂的感知和推理。我们评估了 11 个开源 LLM 和一个专有的 GPT-4V(ision)。即使GPT-4V也只能达到42%的准确率，这表明还有很大的改进空间。 CMMMU 将推动社区构建面向专家人工智能的下一代 LMM，并通过提供多样化的语言环境来促进 LMM 的民主化。</li>
</ul>

<h3>Title: Cross-Validation Conformal Risk Control</h3>
<ul>
<li><strong>Authors: </strong>Kfir M. Cohen, Sangwoo Park, Osvaldo Simeone, Shlomo Shamai (Shitz)</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11974">https://arxiv.org/abs/2401.11974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11974">https://arxiv.org/pdf/2401.11974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11974]] Cross-Validation Conformal Risk Control(https://arxiv.org/abs/2401.11974)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Conformal risk control (CRC) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees. Generalizing conformal prediction (CP), with CRC, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate. The original CRC requires the available data set to be split between training and validation data sets. This can be problematic when data availability is limited, resulting in inefficient set predictors. In this paper, a novel CRC method is introduced that is based on cross-validation, rather than on validation as the original CRC. The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions. CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor. Furthermore, numerical experiments show that CV-CRC can reduce the average set size with respect to CRC when the available data are limited.</li>
<li><strong>摘要：</strong>保形风险控制 (CRC) 是最近提出的一项技术，它将事后应用于传统点预测器以提供校准保证。通过 CRC 推广保形预测 (CP)，可以确保从点预测器中提取的一组预测器的校准，以控制风险函数，例如错误覆盖概率或假阴性率。原始 CRC 要求将可用数据集分为训练数据集和验证数据集。当数据可用性有限时，这可能会出现问题，导致集合预测器效率低下。本文介绍了一种新的 CRC 方法，该方法基于交叉验证，而不是像原始 CRC 那样基于验证。所提出的交叉验证 CRC (CV-CRC) 将 jackknife-minmax 的版本从 CP 扩展到 CRC，从而可以控制更广泛的风险函数。 CV-CRC 被证明可以为设定预测器的平均风险提供理论保证。此外，数值实验表明，当可用数据有限时，CV-CRC 可以相对于 CRC 减小平均集大小。</li>
</ul>

<h3>Title: Expert-Driven Monitoring of Operational ML Models</h3>
<ul>
<li><strong>Authors: </strong>Joran Leest, Claudia Raibulet, Ilias Gerostathopoulos, Patricia Lago</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.11993">https://arxiv.org/abs/2401.11993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.11993">https://arxiv.org/pdf/2401.11993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.11993]] Expert-Driven Monitoring of Operational ML Models(https://arxiv.org/abs/2401.11993)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We propose Expert Monitoring, an approach that leverages domain expertise to enhance the detection and mitigation of concept drift in machine learning (ML) models. Our approach supports practitioners by consolidating domain expertise related to concept drift-inducing events, making this expertise accessible to on-call personnel, and enabling automatic adaptability with expert oversight.</li>
<li><strong>摘要：</strong>我们提出专家监控，这是一种利用领域专业知识来增强机器学习 (ML) 模型中概念漂移的检测和缓解的方法。我们的方法通过整合与概念漂移引发事件相关的领域专业知识，使待命人员能够获取这些专业知识，并在专家监督下实现自动适应来为从业者提供支持。</li>
</ul>

<h3>Title: Integrating Statistical Significance and Discriminative Power in Pattern  Discovery</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Alexandre, Rafael S. Costa, Rui Henriques</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12000">https://arxiv.org/abs/2401.12000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12000">https://arxiv.org/pdf/2401.12000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12000]] Integrating Statistical Significance and Discriminative Power in Pattern  Discovery(https://arxiv.org/abs/2401.12000)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Pattern discovery plays a central role in both descriptive and predictive tasks across multiple domains. Actionable patterns must meet rigorous statistical significance criteria and, in the presence of target variables, further uphold discriminative power. Our work addresses the underexplored area of guiding pattern discovery by integrating statistical significance and discriminative power criteria into state-of-the-art algorithms while preserving pattern quality. We also address how pattern quality thresholds, imposed by some algorithms, can be rectified to accommodate these additional criteria. To test the proposed methodology, we select the triclustering task as the guiding pattern discovery case and extend well-known greedy and multi-objective optimization triclustering algorithms, $\delta$-Trimax and TriGen, that use various pattern quality criteria, such as Mean Squared Residual (MSR), Least Squared Lines (LSL), and Multi Slope Measure (MSL). Results from three case studies show the role of the proposed methodology in discovering patterns with pronounced improvements of discriminative power and statistical significance without quality deterioration, highlighting its importance in supervisedly guiding the search. Although the proposed methodology is motivated over multivariate time series data, it can be straightforwardly extended to pattern discovery tasks involving multivariate, N-way (N>3), transactional, and sequential data structures. Availability: The code is freely available at https://github.com/JupitersMight/MOF_Triclustering under the MIT license.</li>
<li><strong>摘要：</strong>模式发现在跨多个领域的描述性和预测性任务中发挥着核心作用。可操作的模式必须满足严格的统计显着性标准，并且在存在目标变量的情况下，进一步维护区分力。我们的工作通过将统计显着性和判别力标准集成到最先进的算法中，同时保持模式质量，解决了指导模式发现的尚未探索的领域。我们还解决了如何纠正某些算法强加的模式质量阈值以适应这些附加标准。为了测试所提出的方法，我们选择三聚类任务作为指导模式发现案例，并扩展了著名的贪婪和多目标优化三聚类算法 $\delta$-Trimax 和 TriGen，它们使用各种模式质量标准，例如平均值残差平方 (MSR)、最小二乘线 (LSL) 和多斜率测量 (MSL)。三个案例研究的结果显示了所提出的方法在发现模式方面的作用，这些模式具有判别力和统计显着性的显着提高，而质量没有下降，强调了其在监督指导搜索方面的重要性。尽管所提出的方法是基于多变量时间序列数据，但它可以直接扩展到涉及多变量、N 路 (N>3)、事务性和顺序数据结构的模式发现任务。可用性：该代码可在 MIT 许可下从 https://github.com/JupitersMight/MOF_Triclustering 免费获取。</li>
</ul>

<h3>Title: HgbNet: predicting hemoglobin level/anemia degree from EHR data</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Zhi, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, Miguel Rodrigues</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12002">https://arxiv.org/abs/2401.12002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12002">https://arxiv.org/pdf/2401.12002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12002]] HgbNet: predicting hemoglobin level/anemia degree from EHR data(https://arxiv.org/abs/2401.12002)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Anemia is a prevalent medical condition that typically requires invasive blood tests for diagnosis and monitoring. Electronic health records (EHRs) have emerged as valuable data sources for numerous medical studies. EHR-based hemoglobin level/anemia degree prediction is non-invasive and rapid but still faces some challenges due to the fact that EHR data is typically an irregular multivariate time series containing a significant number of missing values and irregular time intervals. To address these issues, we introduce HgbNet, a machine learning-based prediction model that emulates clinicians' decision-making processes for hemoglobin level/anemia degree prediction. The model incorporates a NanDense layer with a missing indicator to handle missing values and employs attention mechanisms to account for both local irregularity and global irregularity. We evaluate the proposed method using two real-world datasets across two use cases. In our first use case, we predict hemoglobin level/anemia degree at moment T+1 by utilizing records from moments prior to T+1. In our second use case, we integrate all historical records with additional selected test results at moment T+1 to predict hemoglobin level/anemia degree at the same moment, T+1. HgbNet outperforms the best baseline results across all datasets and use cases. These findings demonstrate the feasibility of estimating hemoglobin levels and anemia degree from EHR data, positioning HgbNet as an effective non-invasive anemia diagnosis solution that could potentially enhance the quality of life for millions of affected individuals worldwide. To our knowledge, HgbNet is the first machine learning model leveraging EHR data for hemoglobin level/anemia degree prediction.</li>
<li><strong>摘要：</strong>贫血是一种普遍的疾病，通常需要侵入性血液检查来诊断和监测。电子健康记录 (EHR) 已成为众多医学研究的宝贵数据源。基于 EHR 的血红蛋白水平/贫血程度预测是无创且快速的，但由于 EHR 数据通常是不规则的多元时间序列，包含大量缺失值和不规则的时间间隔，因此仍然面临一些挑战。为了解决这些问题，我们引入了 HgbNet，这是一种基于机器学习的预测模型，可模拟临床医生的血红蛋白水平/贫血程度预测决策过程。该模型结合了带有缺失指标的 NanDense 层来处理缺失值，并采用注意力机制来解释局部不规则性和全局不规则性。我们使用跨两个用例的两个真实数据集来评估所提出的方法。在我们的第一个用例中，我们通过利用 T+1 之前时刻的记录来预测 T+1 时刻的血红蛋白水平/贫血程度。在我们的第二个用例中，我们将所有历史记录与 T+1 时刻的其他选定测试结果相结合，以预测同一时刻 T+1 的血红蛋白水平/贫血程度。 HgbNet 在所有数据集和用例中均优于最佳基线结果。这些发现证明了根据 EHR 数据估计血红蛋白水平和贫血程度的可行性，将 HgbNet 定位为一种有效的非侵入性贫血诊断解决方案，有可能提高全球数百万受影响个体的生活质量。据我们所知，HgbNet 是第一个利用 EHR 数据进行血红蛋白水平/贫血程度预测的机器学习模型。</li>
</ul>

<h3>Title: ALMs: Authorial Language Models for Authorship Attribution</h3>
<ul>
<li><strong>Authors: </strong>Weihang Huang, Akira Murakami, Jack Grieve</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12005">https://arxiv.org/abs/2401.12005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12005">https://arxiv.org/pdf/2401.12005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12005]] ALMs: Authorial Language Models for Authorship Attribution(https://arxiv.org/abs/2401.12005)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce an authorship attribution method called Authorial Language Models (ALMs) that involves identifying the most likely author of a questioned document based on the perplexity of the questioned document calculated for a set of causal language models fine-tuned on the writings of a set of candidate author. We benchmarked ALMs against state-of-art-systems using the CCAT50 dataset and the Blogs50 datasets. We find that ALMs achieves a macro-average accuracy score of 83.6% on Blogs50, outperforming all other methods, and 74.9% on CCAT50, matching the performance of the best method. To assess the performance of ALMs on shorter texts, we also conducted text ablation testing. We found that to reach a macro-average accuracy of 70%, ALMs needs 40 tokens on Blogs50 and 400 tokens on CCAT50, while to reach 60% ALMs requires 20 tokens on Blogs50 and 70 tokens on CCAT50.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了一种称为作者语言模型（ALM）的作者归属方法，该方法涉及根据受质疑文档的困惑度来识别受质疑文档最有可能的作者，该困惑度是针对一组针对著作进行微调的因果语言模型计算的一组候选作者。我们使用 CCAT50 数据集和 Blogs50 数据集对 ALM 与最先进的系统进行了基准测试。我们发现 ALM 在 Blogs50 上的宏观平均准确度得分为 83.6%，优于所有其他方法；在 CCAT50 上的宏观平均准确度得分为 74.9%，与最佳方法的性能相当。为了评估 ALM 在较短文本上的性能，我们还进行了文本消融测试。我们发现，要达到 70% 的宏观平均准确率，ALM 在 Blogs50 上需要 40 个令牌，在 CCAT50 上需要 400 个令牌，而要达到 60% 的 ALM 在 Blogs50 上需要 20 个令牌，在 CCAT50 上需要 70 个令牌。</li>
</ul>

<h3>Title: TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for  Lazy Clients</h3>
<ul>
<li><strong>Authors: </strong>Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12012">https://arxiv.org/abs/2401.12012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12012">https://arxiv.org/pdf/2401.12012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12012]] TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for  Lazy Clients(https://arxiv.org/abs/2401.12012)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and can significantly accelerate convergence for federated classification task, especially when clients are "lazy" and train their models solely for few epochs for next global aggregation. TurboSVM-FL extensively utilizes support vector machine to conduct selective aggregation and max-margin spread-out regularization on class embeddings. We evaluate TurboSVM-FL on multiple datasets including FEMNIST, CelebA, and Shakespeare using user-independent validation with non-iid data distribution. Our results show that TurboSVM-FL can significantly outperform existing popular algorithms on convergence rate and reduce communication rounds while delivering better test metrics including accuracy, F1 score, and MCC.</li>
<li><strong>摘要：</strong>联邦学习是一种分布式协作机器学习范式，近年来发展势头强劲。在联邦学习中，中央服务器定期与客户端协调模型，并聚合客户端在本地训练的模型，而无需访问本地数据。尽管具有潜力，联邦学习的实施仍然遇到一些挑战，主要是由于数据异构性导致的收敛缓慢。在跨设备联合学习场景中，缓慢的收敛变得尤其成问题，其中客户端可能受到计算能力和存储空间的强烈限制，因此抵消了在客户端引起额外计算或内存成本的方法，例如辅助目标项和更大的训练迭代可能不切实际。在本文中，我们提出了一种新颖的联邦聚合策略 TurboSVM-FL，它不会给客户端带来额外的计算负担，并且可以显着加速联邦分类任务的收敛，特别是当客户端“懒惰”并且仅训练其模型的时候下一次全球聚合的纪元。 TurboSVM-FL广泛利用支持向量机对类嵌入进行选择性聚合和最大边距展开正则化。我们使用非独立同分布数据分布的用户独立验证在多个数据集（包括 FEMNIST、CelebA 和 Shakespeare）上评估 TurboSVM-FL。我们的结果表明，TurboSVM-FL 在收敛速度方面可以显着优于现有的流行算法，并减少通信次数，同时提供更好的测试指标，包括准确性、F1 分数和 MCC。</li>
</ul>

<h3>Title: Momentum-SAM: Sharpness Aware Minimization without Computational  Overhead</h3>
<ul>
<li><strong>Authors: </strong>Marlon Becker, Frederick Altrock, Benjamin Risse</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12033">https://arxiv.org/abs/2401.12033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12033">https://arxiv.org/pdf/2401.12033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12033]] Momentum-SAM: Sharpness Aware Minimization without Computational  Overhead(https://arxiv.org/abs/2401.12033)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>The recently proposed optimization algorithm for deep neural networks Sharpness Aware Minimization (SAM) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss. While significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making SAM unfeasible in case of limited computationally capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose Momentum-SAM (MSAM), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over SGD or Adam. We evaluate MSAM in detail and reveal insights on separable mechanisms of NAG, SAM and MSAM regarding training optimization and generalization. Code is available at https://github.com/MarlonBecker/MSAM.</li>
<li><strong>摘要：</strong>最近提出的深度神经网络锐度感知最小化（SAM）优化算法建议在梯度计算之前通过梯度上升步骤扰动参数，以引导优化进入平坦损失的参数空间区域。虽然可以证明显着的泛化改进，从而减少过拟合，但由于额外需要梯度计算，计算成本加倍，使得 SAM 在计算能力有限的情况下不可行。受 Nesterov 加速梯度 (NAG) 的启发，我们提出了 Momentum-SAM (MSAM)，它会扰动累积动量向量方向上的参数，以实现低锐度，而无需比 SGD 或 Adam 显着的计算开销或内存需求。我们详细评估了 MSAM，并揭示了 NAG、SAM 和 MSAM 在训练优化和泛化方面的可分离机制的见解。代码可在 https://github.com/MarlonBecker/MSAM 获取。</li>
</ul>

<h3>Title: Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated  Text</h3>
<ul>
<li><strong>Authors: </strong>Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12070">https://arxiv.org/abs/2401.12070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12070">https://arxiv.org/pdf/2401.12070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12070]] Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated  Text(https://arxiv.org/abs/2401.12070)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data.</li>
<li><strong>摘要：</strong>检测现代大型语言模型生成的文本被认为很困难，因为法学硕士和人类都可以表现出各种复杂的行为。然而，我们发现基于对比两种密切相关的语言模型的分数在区分人类生成的文本和机器生成的文本方面非常准确。基于这种机制，我们提出了一种新颖的 LLM 检测器，只需要使用一对预先训练的 LLM 进行简单的计算。该方法称为双筒望远镜，无需任何训练数据即可实现最先进的精度。它能够识别来自一系列现代法学硕士的机器文本，而无需任何特定于模型的修改。我们在多种文本源和不同情况下对双筒望远镜进行了全面评估。尽管没有接受任何 ChatGPT 数据的训练，但 Binoptics 在各种文档类型中都能以 0.01% 的误报率检测到 90% 以上的 ChatGPT（和其他法学硕士）生成的样本。</li>
</ul>

<h3>Title: Temporal Blind Spots in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jonas Wallat, Adam Jatowt, Avishek Anand</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12078">https://arxiv.org/abs/2401.12078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12078">https://arxiv.org/pdf/2401.12078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12078]] Temporal Blind Spots in Large Language Models(https://arxiv.org/abs/2401.12078)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently gained significant attention due to their unparalleled ability to perform various natural language processing tasks. These models, benefiting from their advanced natural language understanding capabilities, have demonstrated impressive zero-shot performance. However, the pre-training data utilized in LLMs is often confined to a specific corpus, resulting in inherent freshness and temporal scope limitations. Consequently, this raises concerns regarding the effectiveness of LLMs for tasks involving temporal intents. In this study, we aim to investigate the underlying limitations of general-purpose LLMs when deployed for tasks that require a temporal understanding. We pay particular attention to handling factual temporal knowledge through three popular temporal QA datasets. Specifically, we observe low performance on detailed questions about the past and, surprisingly, for rather new information. In manual and automatic testing, we find multiple temporal errors and characterize the conditions under which QA performance deteriorates. Our analysis contributes to understanding LLM limitations and offers valuable insights into developing future models that can better cater to the demands of temporally-oriented tasks. The code is available\footnote{https://github.com/jwallat/temporalblindspots}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 最近因其执行各种自然语言处理任务的无与伦比的能力而受到广泛关注。这些模型受益于其先进的自然语言理解能力，展示了令人印象深刻的零样本性能。然而，法学硕士中使用的预训练数据通常仅限于特定的语料库，导致固有的新鲜度和时间范围的限制。因此，这引起了人们对法学硕士对于涉及时间意图的任务的有效性的担忧。在这项研究中，我们的目标是调查通用法学硕士在部署用于需要时间理解的任务时的潜在局限性。我们特别关注通过三个流行的时态 QA 数据集来处理事实时态知识。具体来说，我们观察到在有关过去的详细问题上表现不佳，令人惊讶的是，对于相当新的信息。在手动和自动测试中，我们发现多个时间错误并描述了 QA 性能恶化的条件。我们的分析有助于理解法学硕士的局限性，并为开发未来模型提供宝贵的见解，这些模型可以更好地满足面向时间的任务的需求。代码可用\footnote{https://github.com/jwallat/temporalblindspots}。</li>
</ul>

<h3>Title: West-of-N: Synthetic Preference Generation for Improved Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12086">https://arxiv.org/abs/2401.12086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12086">https://arxiv.org/pdf/2401.12086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12086]] West-of-N: Synthetic Preference Generation for Improved Reward Modeling(https://arxiv.org/abs/2401.12086)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The success of reinforcement learning from human feedback (RLHF) in language model alignment is strongly dependent on the quality of the underlying reward model. In this paper, we present a novel approach to improve reward model quality by generating synthetic preference data, thereby augmenting the training dataset with on-policy, high-quality preference pairs. Motivated by the promising results of Best-of-N sampling strategies in language model training, we extend their application to reward model training. This results in a self-training strategy to generate preference pairs by selecting the best and worst candidates in a pool of responses to a given query. Empirically, we find that this approach improves the performance of any reward model, with an effect comparable to the addition of a similar quantity of human preference data. This work opens up new avenues of research for improving RLHF for language model alignment, by offering synthetic preference generation as a solution to reward modeling challenges.</li>
<li><strong>摘要：</strong>语言模型对齐中基于人类反馈的强化学习 (RLHF) 的成功很大程度上取决于底层奖励模型的质量。在本文中，我们提出了一种通过生成合成偏好数据来提高奖励模型质量的新方法，从而用符合策略的高质量偏好对来扩充训练数据集。受到 Best-of-N 采样策略在语言模型训练中取得的有希望的结果的激励，我们将其应用扩展到奖励模型训练。这导致了一种自我训练策略，通过在给定查询的响应池中选择最佳和最差的候选者来生成偏好对。根据经验，我们发现这种方法可以提高任何奖励模型的性能，其效果与添加相似数量的人类偏好数据相当。这项工作通过提供综合偏好生成作为奖励建模挑战的解决方案，为改进语言模型对齐的 RLHF 开辟了新的研究途径。</li>
</ul>

<h3>Title: Revisiting Demonstration Selection Strategies in In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Keqin Peng, Liang Ding, Yancheng Yuan, Xuebo Liu, Min Zhang, Yuanxin Ouyang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12087">https://arxiv.org/abs/2401.12087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12087">https://arxiv.org/pdf/2401.12087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12087]] Revisiting Demonstration Selection Strategies in In-Context Learning(https://arxiv.org/abs/2401.12087)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model. However, the performance of ICL varies significantly with the choice of demonstrations, and it is still unclear why this happens or what factors will influence its choice. In this work, we first revisit the factors contributing to this variance from both data and model aspects, and find that the choice of demonstration is both data- and model-dependent. We further proposed a data- and model-dependent demonstration selection method, \textbf{TopK + ConE}, based on the assumption that \textit{the performance of a demonstration positively correlates with its contribution to the model's understanding of the test samples}, resulting in a simple and effective recipe for ICL. Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales. Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods. Code will be released.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已显示出使用上下文学习 (ICL) 执行各种任务的令人印象深刻的能力，其中使用一些示例来描述模型的任务。然而，ICL 的性能随着演示的选择而显着变化，目前还不清楚为什么会发生这种情况或哪些因素会影响其选择。在这项工作中，我们首先从数据和模型方面重新审视造成这种差异的因素，并发现演示的选择既依赖于数据又依赖于模型。我们进一步提出了一种依赖于数据和模型的演示选择方法，\textbf{TopK + ConE}，基于\textit{演示的性能与其对模型理解测试样本的贡献正相关}的假设，从而形成简单有效的 ICL 配方。根据经验，我们的方法在不同模型规模的语言理解和生成任务方面都取得了一致的改进。进一步的分析证实，除了不同情况下的通用性和稳定性之外，我们的方法还为先前方法的有效性提供了统一的解释。代码将被发布。</li>
</ul>

<h3>Title: An Empirical Analysis of In-context Learning Abilities of LLMs for MT</h3>
<ul>
<li><strong>Authors: </strong>Pranjal A. Chitale, Jay Gala, Varun Gumma, Mitesh M. Khapra, Raj Dabre</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12097">https://arxiv.org/abs/2401.12097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12097">https://arxiv.org/pdf/2401.12097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12097]] An Empirical Analysis of In-context Learning Abilities of LLMs for MT(https://arxiv.org/abs/2401.12097)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) has consistently demonstrated superior performance over zero-shot performance in large language models (LLMs). However, the understanding of the dynamics of ICL and the aspects that influence downstream performance remains limited, especially for natural language generation (NLG) tasks. This work aims to address this gap by investigating the ICL capabilities of LLMs and studying the impact of different aspects of the in-context demonstrations for the task of machine translation (MT). Our preliminary investigations aim to discern whether in-context learning (ICL) is predominantly influenced by demonstrations or instructions by applying diverse perturbations to in-context demonstrations while preserving the task instruction. We observe varying behavior to perturbed examples across different model families, notably with BLOOM-7B derivatives being severely influenced by noise, whereas Llama 2 derivatives not only exhibit robustness but also tend to show enhancements over the clean baseline when subject to perturbed demonstrations. This suggests that the robustness of ICL may be governed by several factors, including the type of noise, perturbation direction (source or target), the extent of pretraining of the specific model, and fine-tuning for downstream tasks if applicable. Further investigation is warranted to develop a comprehensive understanding of these factors in future research.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 在大型语言模型 (LLM) 中始终表现出优于零样本性能的性能。然而，对 ICL 动态以及影响下游性能的方面的理解仍然有限，特别是对于自然语言生成 (NLG) 任务。这项工作旨在通过调查法学硕士的 ICL 能力并研究机器翻译 (MT) 任务的上下文演示的不同方面的影响来解决这一差距。我们的初步调查旨在通过对情境演示应用不同的扰动，同时保留任务指令，来辨别情境学习（ICL）是否主要受到演示或指令的影响。我们观察到不同模型系列中扰动示例的不同行为，特别是 BLOOM-7B 导数受到噪声的严重影响，而 Llama 2 导数不仅表现出鲁棒性，而且在受到扰动演示时往往会表现出相对于干净基线的增强。这表明 ICL 的鲁棒性可能受多个因素影响，包括噪声类型、扰动方向（源或目标）、特定模型的预训练程度以及下游任务的微调（如果适用）。需要进一步调查，以便在未来的研究中全面了解这些因素。</li>
</ul>

<h3>Title: On-Time Delivery in Crowdshipping Systems: An Agent-Based Approach Using  Streaming Data</h3>
<ul>
<li><strong>Authors: </strong>Jeremias Dötterl, Ralf Bruns, Jürgen Dunkel, Sascha Ossowski</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12108">https://arxiv.org/abs/2401.12108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12108">https://arxiv.org/pdf/2401.12108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12108]] On-Time Delivery in Crowdshipping Systems: An Agent-Based Approach Using  Streaming Data(https://arxiv.org/abs/2401.12108)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag, agent</a></li>
<li><strong>Abstract: </strong>In parcel delivery, the "last mile" from the parcel hub to the customer is costly, especially for time-sensitive delivery tasks that have to be completed within hours after arrival. Recently, crowdshipping has attracted increased attention as a new alternative to traditional delivery modes. In crowdshipping, private citizens ("the crowd") perform short detours in their daily lives to contribute to parcel delivery in exchange for small incentives. However, achieving desirable crowd behavior is challenging as the crowd is highly dynamic and consists of autonomous, self-interested individuals. Leveraging crowdshipping for time-sensitive deliveries remains an open challenge. In this paper, we present an agent-based approach to on-time parcel delivery with crowds. Our system performs data stream processing on the couriers' smartphone sensor data to predict delivery delays. Whenever a delay is predicted, the system attempts to forge an agreement for transferring the parcel from the current deliverer to a more promising courier nearby. Our experiments show that through accurate delay predictions and purposeful task transfers many delays can be prevented that would occur without our approach.</li>
<li><strong>摘要：</strong>在包裹递送中，从包裹枢纽到客户的“最后一英里”成本高昂，特别是对于必须在到达后数小时内完成的时间敏感的递送任务。近年来，众包作为传统配送模式的新替代方案越来越受到关注。在众包中，私人公民（“人群”）在日常生活中绕道一小段路，为包裹递送做出贡献，以换取小额奖励。然而，实现理想的群体行为具有挑战性，因为群体是高度动态的，并且由自主、自利的个人组成。利用众包进行时间敏感的交付仍然是一个公开的挑战。在本文中，我们提出了一种基于代理的方法，可以在人群中按时递送包裹。我们的系统对快递员的智能手机传感器数据进行数据流处理，以预测递送延迟。每当预测到延误时，系统都会尝试达成协议，将包裹从当前的投递者转移到附近更有前途的投递者。我们的实验表明，通过准确的延迟预测和有目的的任务转移，可以防止在没有我们的方法的情况下发生的许多延迟。</li>
</ul>

<h3>Title: The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, Jay Pujara</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12117">https://arxiv.org/abs/2401.12117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12117">https://arxiv.org/pdf/2401.12117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12117]] The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large  Language Models(https://arxiv.org/abs/2401.12117)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments expose the difficulty of solving such problems while showcasing the immense gap between open-source and closed-source models. We also reveal critical shortcomings with individual visual and textual modules, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experiment with various methods, such as Chain-of-Thought prompting, resulting in a significant (up to 100%) boost in performance.</li>
<li><strong>摘要：</strong>虽然大语言模型 (LLM) 仍在被新领域采用并用于新颖的应用程序，但我们正在经历新一代基础模型的涌入，即多模态大语言模型 (MLLM)。这些模型整合了语言和视觉信息，为在两种模式的交叉点展示更复杂的推理能力开辟了新的可能性。然而，尽管 MLLM 具有革命性的前景，但我们对其推理能力的了解仍然有限。在这项研究中，我们使用 Raven 渐进矩阵的变体来评估开源和闭源 MLLM 的非语言抽象推理能力。我们的实验暴露了解决此类问题的难度，同时展示了开源和闭源模型之间的巨大差距。我们还揭示了各个视觉和文本模块的关键缺陷，使模型受到低性能上限的影响。最后，为了提高 MLLM 的性能，我们尝试了各种方法，例如思想链提示，从而显着提高了性能（高达 100%）。</li>
</ul>

<h3>Title: Anisotropy Is Inherent to Self-Attention in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Nathan Godey, Éric de la Clergerie, Benoît Sagot</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12143">https://arxiv.org/abs/2401.12143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12143">https://arxiv.org/pdf/2401.12143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12143]] Anisotropy Is Inherent to Self-Attention in Transformers(https://arxiv.org/abs/2401.12143)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations suggest that anisotropy is actually inherent to Transformers-based models.</li>
<li><strong>摘要：</strong>表征退化问题是基于 Transformers 的自监督学习方法中广泛观察到的现象。在 NLP 中，它采用各向异性的形式，这是隐藏表示的奇异属性，这使得它们在角距离（余弦相似性）方面意外地彼此接近。最近的一些工作倾向于表明各向异性是优化令牌长尾分布的交叉熵损失的结果。我们在本文中表明，各向异性也可以在具有特定目标的语言模型中凭经验观察到，这些目标不应直接遭受相同的后果。我们还表明，各向异性问题扩展到接受其他模式训练的变形金刚。我们的观察表明，各向异性实际上是基于 Transformers 的模型所固有的。</li>
</ul>

<h3>Title: In-Context Learning for Extreme Multi-Label Classification</h3>
<ul>
<li><strong>Authors: </strong>Karel D'Oosterlinck, Omar Khattab, François Remy, Thomas Demeester, Chris Develder, Christopher Potts</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12178">https://arxiv.org/abs/2401.12178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12178">https://arxiv.org/pdf/2401.12178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12178]] In-Context Learning for Extreme Multi-Label Classification(https://arxiv.org/abs/2401.12178)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code</a></li>
<li><strong>Abstract: </strong>Multi-label classification problems with thousands of classes are hard to solve with in-context learning alone, as language models (LMs) might lack prior knowledge about the precise classes or how to assign them, and it is generally infeasible to demonstrate every class in a prompt. We propose a general program, $\texttt{Infer--Retrieve--Rank}$, that defines multi-step interactions between LMs and retrievers to efficiently tackle such problems. We implement this program using the $\texttt{DSPy}$ programming model, which specifies in-context systems in a declarative manner, and use $\texttt{DSPy}$ optimizers to tune it towards specific datasets by bootstrapping only tens of few-shot examples. Our primary extreme classification program, optimized separately for each task, attains state-of-the-art results across three benchmarks (HOUSE, TECH, TECHWOLF). We apply the same program to a benchmark with vastly different characteristics and attain competitive performance as well (BioDEX). Unlike prior work, our proposed solution requires no finetuning, is easily applicable to new tasks, alleviates prompt engineering, and requires only tens of labeled examples. Our code is public at https://github.com/KarelDO/xmc.dspy.</li>
<li><strong>摘要：</strong>具有数千个类别的多标签分类问题很难仅通过上下文学习来解决，因为语言模型（LM）可能缺乏关于精确类别或如何分配它们的先验知识，并且通常不可行一个提示。我们提出了一个通用程序 $\texttt{Infer--Retrieve--Rank}$，它定义了 LM 和检索器之间的多步骤交互，以有效地解决此类问题。我们使用 $\texttt{DSPy}$ 编程模型来实现该程序，该模型以声明方式指定上下文系统，并使用 $\texttt{DSPy}$ 优化器通过仅引导数十个数据集来将其调整为特定的数据集拍摄示例。我们的主要极限分类程序针对每项任务分别进行了优化，在三个基准测试（HOUSE、TECH、TECHWOLF）中获得了最先进的结果。我们将相同的程序应用于具有截然不同特征的基准，并获得了具有竞争力的性能（BioDEX）。与之前的工作不同，我们提出的解决方案不需要微调，可以轻松应用于新任务，减轻即时工程的负担，并且只需要数十个标记示例。我们的代码在 https://github.com/KarelDO/xmc.dspy 上公开。</li>
</ul>

<h3>Title: Universal Neurons in GPT2 Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei Kheirkhah, Qinyi Sun, Will Hathaway, Neel Nanda, Dimitris Bertsimas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12181">https://arxiv.org/abs/2401.12181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12181">https://arxiv.org/pdf/2401.12181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12181]] Universal Neurons in GPT2 Language Models(https://arxiv.org/abs/2401.12181)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>A basic question within the emerging field of mechanistic interpretability is the degree to which neural networks learn the same underlying mechanisms. In other words, are neural mechanisms universal across different models? In this work, we study the universality of individual neurons across GPT2 models trained from different initial random seeds, motivated by the hypothesis that universal neurons are likely to be interpretable. In particular, we compute pairwise correlations of neuron activations over 100 million tokens for every neuron pair across five different seeds and find that 1-5\% of neurons are universal, that is, pairs of neurons which consistently activate on the same inputs. We then study these universal neurons in detail, finding that they usually have clear interpretations and taxonomize them into a small number of neuron families. We conclude by studying patterns in neuron weights to establish several universal functional roles of neurons in simple circuits: deactivating attention heads, changing the entropy of the next token distribution, and predicting the next token to (not) be within a particular set.</li>
<li><strong>摘要：</strong>机械可解释性这一新兴领域的一个基本问题是神经网络学习相同底层机制的程度。换句话说，神经机制在不同模型中是否通用？在这项工作中，我们研究了从不同初始随机种子训练的 GPT2 模型中单个神经元的普遍性，其动机是假设通用神经元可能是可解释的。特别是，我们计算了五个不同种子中每个神经元对超过 1 亿个令牌的神经元激活的成对相关性，发现 1-5% 的神经元是通用的，即在相同输入上一致激活的神经元对。然后，我们详细研究这些通用神经元，发现它们通常有明确的解释，并将它们分类为少数神经元家族。最后，我们通过研究神经元权重模式来建立神经元在简单电路中的几种通用功能角色：停用注意力头、改变下一个令牌分布的熵以及预测下一个令牌是否在特定集合内。</li>
</ul>

<h3>Title: WARM: On the Benefits of Weight Averaged Reward Models</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12187">https://arxiv.org/abs/2401.12187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12187">https://arxiv.org/pdf/2401.12187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12187]] WARM: On the Benefits of Weight Averaged Reward Models(https://arxiv.org/abs/2401.12187)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM.</li>
<li><strong>摘要：</strong>通过强化学习 (RLHF) 将大型语言模型 (LLM) 与人类偏好保持一致可能会导致奖励黑客行为，其中 LLM 利用奖励模型 (RM) 中的失败来实现看似高额的奖励，但没有达到根本目标。在设计 RM 来减轻奖励黑客攻击时，我们发现了两个主要挑战：强化学习过程中的分配变化和人类偏好的不一致。作为解决方案，我们提出了权重平均奖励模型（WARM），首先微调多个 RM，然后在权重空间中对它们进行平均。该策略遵循以下观察结果：在共享相同的预训练时，微调权重保持线性模式连接。通过平均权重，与传统的预测集成相比，WARM 提高了效率，同时提高了分布变化下的可靠性以及对偏好不一致的鲁棒性。我们使用 best-of-N 和 RL 方法对摘要任务进行的实验表明，WARM 提高了 LLM 预测的整体质量和一致性；例如，使用 WARM 微调的策略 RL 相对于使用单个 RM 微调的策略 RL 的获胜率为 79.4%。</li>
</ul>

<h3>Title: Text Embedding Inversion Attacks on Multilingual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiyi Chen, Heather Lent, Johannes Bjerva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12192">https://arxiv.org/abs/2401.12192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12192">https://arxiv.org/pdf/2401.12192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12192]] Text Embedding Inversion Attacks on Multilingual Language Models(https://arxiv.org/abs/2401.12192)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Representing textual information as real-numbered embeddings has become the norm in NLP. Moreover, with the rise of public interest in large language models (LLMs), Embeddings as a Service (EaaS) has rapidly gained traction as a business model. This is not without outstanding security risks, as previous research has demonstrated that sensitive data can be reconstructed from embeddings, even without knowledge of the underlying model that generated them. However, such work is limited by its sole focus on English, leaving all other languages vulnerable to attacks by malicious actors. %As many international and multilingual companies leverage EaaS, there is an urgent need for research into multilingual LLM security. To this end, this work investigates LLM security from the perspective of multilingual embedding inversion. Concretely, we define the problem of black-box multilingual and cross-lingual inversion attacks, with special attention to a cross-domain scenario. Our findings reveal that multilingual models are potentially more vulnerable to inversion attacks than their monolingual counterparts. This stems from the reduced data requirements for achieving comparable inversion performance in settings where the underlying language is not known a-priori. To our knowledge, this work is the first to delve into multilinguality within the context of inversion attacks, and our findings highlight the need for further investigation and enhanced defenses in the area of NLP Security.</li>
<li><strong>摘要：</strong>将文本信息表示为实数嵌入已成为 NLP 的规范。此外，随着公众对大型语言模型 (LLM) 兴趣的增加，嵌入即服务 (EaaS) 作为一种商业模式迅速受到关注。这并非没有突出的安全风险，因为之前的研究已经证明，即使不了解生成敏感数据的底层模型，也可以从嵌入中重建敏感数据。然而，此类工作因其仅关注英语而受到限制，使得所有其他语言都容易受到恶意行为者的攻击。由于许多国际和多语言公司利用 EaaS，迫切需要研究多语言 LLM 安全性。为此，本工作从多语言嵌入反转的角度研究LLM安全性。具体来说，我们定义了黑盒多语言和跨语言反转攻击问题，特别关注跨域场景。我们的研究结果表明，多语言模型可能比单语言模型更容易受到反转攻击。这是因为在底层语言事先未知的情况下，实现可比较的反演性能所需的数据量减少了。据我们所知，这项工作是第一个在反转攻击的背景下深入研究多语言性的工作，我们的发现强调了在 NLP 安全领域进一步调查和加强防御的必要性。</li>
</ul>

<h3>Title: APT: Adaptive Pruning and Tuning Pretrained Language Models for  Efficient Training and Inference</h3>
<ul>
<li><strong>Authors: </strong>Bowen Zhao, Hannaneh Hajishirzi, Qingqing Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12200">https://arxiv.org/abs/2401.12200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12200">https://arxiv.org/pdf/2401.12200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12200]] APT: Adaptive Pruning and Tuning Pretrained Language Models for  Efficient Training and Inference(https://arxiv.org/abs/2401.12200)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models' performance with 70% parameters remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces large LMs memory training footprint by up to 70%.</li>
<li><strong>摘要：</strong>众所周知，大型语言模型 (LM) 的微调和推理成本高昂。对预训练 LM 进行参数高效微调会通过更新少量 LM 参数来减少训练内存，但不会提高推理效率。结构化剪枝通过删除一致的参数块来提高 LM 推理效率，但通常会增加训练内存和时间。为了提高训练和推理效率，我们引入了 APT，它可以自适应地修剪和调整 LM 的参数。在微调的早期阶段，APT动态添加显着的调整参数以实现快速准确的收敛，同时丢弃不重要的参数以提高效率。与基线相比，我们的实验表明，在修剪 RoBERTa 和 T5 模型（保留 40% 参数）时，APT 保持高达 98% 的任务性能，同时保持 86.4% LLaMA 模型的性能（保留 70% 参数）。此外，APT 将 LM 微调速度提高了 8 倍，并将大型 LM 训练内存占用减少了 70%。</li>
</ul>

<h3>Title: Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization</h3>
<ul>
<li><strong>Authors: </strong>Animesh Basak Chowdhury, Marco Romanelli, Benjamin Tan, Ramesh Karri, Siddharth Garg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12205">https://arxiv.org/abs/2401.12205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12205">https://arxiv.org/pdf/2401.12205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12205]] Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization(https://arxiv.org/abs/2401.12205)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, agent</a></li>
<li><strong>Abstract: </strong>Logic synthesis, a pivotal stage in chip design, entails optimizing chip specifications encoded in hardware description languages like Verilog into highly efficient implementations using Boolean logic gates. The process involves a sequential application of logic minimization heuristics (``synthesis recipe"), with their arrangement significantly impacting crucial metrics such as area and delay. Addressing the challenge posed by the broad spectrum of design complexities - from variations of past designs (e.g., adders and multipliers) to entirely novel configurations (e.g., innovative processor instructions) - requires a nuanced `synthesis recipe` guided by human expertise and intuition. This study conducts a thorough examination of learning and search techniques for logic synthesis, unearthing a surprising revelation: pre-trained agents, when confronted with entirely novel designs, may veer off course, detrimentally affecting the search trajectory. We present ABC-RL, a meticulously tuned $\alpha$ parameter that adeptly adjusts recommendations from pre-trained agents during the search process. Computed based on similarity scores through nearest neighbor retrieval from the training dataset, ABC-RL yields superior synthesis recipes tailored for a wide array of hardware designs. Our findings showcase substantial enhancements in the Quality-of-result (QoR) of synthesized circuits, boasting improvements of up to 24.8% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an impressive up to 9x reduction in runtime (iso-QoR) when compared to current state-of-the-art methodologies.</li>
<li><strong>摘要：</strong>逻辑综合是芯片设计的关键阶段，需要将用 Verilog 等硬件描述语言编码的芯片规格优化为使用布尔逻辑门的高效实现。该过程涉及逻辑最小化启发法（“综合配方”）的顺序应用，其排列显着影响关键指标，例如面积和延迟。解决广泛的设计复杂性带来的挑战 - 来自过去设计的变化（例如， 、加法器和乘法器）到全新的配置（例如，创新的处理器指令）——需要由人类专业知识和直觉指导的细致入微的“综合配方”。这项研究对逻辑综合的学习和搜索技术进行了彻底的检查，发现了一个令人惊讶的启示：预先训练的智能体，当面对全新的设计时，可能会偏离路线，对搜索轨迹产生不利影响。我们提出了 ABC-RL，一个精心调整的 $\alpha$ 参数，可以在搜索过程中熟练地调整来自预先训练的智能体的建议ABC-RL 是根据训练数据集中的最近邻检索的相似性分数进行计算的，可产生针对各种硬件设计量身定制的卓越合成方案。我们的研究结果展示了合成电路的结果质量 (QoR) 的显着增强，与最先进的技术相比，提高了高达 24.8%。此外，与当前最先进的方法相比，ABC-RL 的运行时间 (iso-QoR) 显着缩短了 9 倍。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
