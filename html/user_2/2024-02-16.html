<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-16</h1>
<h3>Title: Mathematical Explanations</h3>
<ul>
<li><strong>Authors: </strong>Joseph Y. Halpern</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09413">https://arxiv.org/abs/2402.09413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09413">https://arxiv.org/pdf/2402.09413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09413]] Mathematical Explanations(https://arxiv.org/abs/2402.09413)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>A definition of what counts as an explanation of mathematical statement, and when one explanation is better than another, is given. Since all mathematical facts must be true in all causal models, and hence known by an agent, mathematical facts cannot be part of an explanation (under the standard notion of explanation). This problem is solved using impossible possible worlds.</li>
<li><strong>摘要：</strong>给出了什么才算是数学陈述的解释的定义，以及何时一种解释比另一种解释更好。由于所有数学事实在所有因果模型中都必须是真实的，因此被代理所知，因此数学事实不能成为解释的一部分（在解释的标准概念下）。这个问题是用不可能的可能世界来解决的。</li>
</ul>

<h3>Title: Fourier Circuits in Neural Networks: Unlocking the Potential of Large  Language Models in Mathematical Reasoning and Modular Arithmetic</h3>
<ul>
<li><strong>Authors: </strong>Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09469">https://arxiv.org/abs/2402.09469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09469">https://arxiv.org/pdf/2402.09469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09469]] Fourier Circuits in Neural Networks: Unlocking the Potential of Large  Language Models in Mathematical Reasoning and Modular Arithmetic(https://arxiv.org/abs/2402.09469)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the evolving landscape of machine learning, a pivotal challenge lies in deciphering the internal representations harnessed by neural networks and Transformers. Building on recent progress toward comprehending how networks execute distinct target functions, our study embarks on an exploration of the underlying reasons behind networks adopting specific computational strategies. We direct our focus to the complex algebraic learning task of modular addition involving $k$ inputs. Our research presents a thorough analytical characterization of the features learned by stylized one-hidden layer neural networks and one-layer Transformers in addressing this task. A cornerstone of our theoretical framework is the elucidation of how the principle of margin maximization shapes the features adopted by one-hidden layer neural networks. Let $p$ denote the modulus, $D_p$ denote the dataset of modular arithmetic with $k$ inputs and $m$ denote the network width. We demonstrate that a neuron count of $ m \geq 2^{2k-2} \cdot (p-1) $, these networks attain a maximum $ L_{2,k+1} $-margin on the dataset $ D_p $. Furthermore, we establish that each hidden-layer neuron aligns with a specific Fourier spectrum, integral to solving modular addition problems. By correlating our findings with the empirical observations of similar studies, we contribute to a deeper comprehension of the intrinsic computational mechanisms of neural networks. Furthermore, we observe similar computational mechanisms in the attention matrix of the Transformer. This research stands as a significant stride in unraveling their operation complexities, particularly in the realm of complex algebraic tasks.</li>
<li><strong>摘要：</strong>在不断发展的机器学习领域，一个关键的挑战在于破译神经网络和 Transformer 所利用的内部表示。基于最近在理解网络如何执行不同目标功能方面取得的进展，我们的研究开始探索网络采用特定计算策略背后的根本原因。我们将重点放在涉及 $k$ 输入的模加法的复杂代数学习任务上。我们的研究对程式化单隐藏层神经网络和单层 Transformer 在解决这一任务时所学到的特征进行了全面的分析表征。我们理论框架的基石是阐明边缘最大化原理如何塑造单隐藏层神经网络所采用的特征。令$p$表示模数，$D_p$表示具有$k$输入的模算术数据集，$m$表示网络宽度。我们证明神经元数量为 $ m \geq 2^{2k-2} \cdot (p-1) $，这些网络在数据集 $ D_p $ 上达到最大 $ L_{2,k+1} $-margin 。此外，我们确定每个隐藏层神经元都与特定的傅里叶谱对齐，这是解决模加法问题的组成部分。通过将我们的发现与类似研究的实证观察联系起来，我们有助于更深入地理解神经网络的内在计算机制。此外，我们在 Transformer 的注意力矩阵中观察到类似的计算机制。这项研究在解决其运算复杂性方面迈出了重大一步，特别是在复杂代数任务领域。</li>
</ul>

<h3>Title: Rationality Report Cards: Assessing the Economic Rationality of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Narun Raman, Taylor Lundy, Samuel Amouyal, Yoav Levine, Kevin Leyton-Brown, Moshe Tennenholtz</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09552">https://arxiv.org/abs/2402.09552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09552">https://arxiv.org/pdf/2402.09552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09552]] Rationality Report Cards: Assessing the Economic Rationality of Large  Language Models(https://arxiv.org/abs/2402.09552)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>There is increasing interest in using LLMs as decision-making "agents." Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions -- and more broadly, determining whether an LLM agent is reliable enough to be trusted -- requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained "elements" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a "rationality report card." Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models' ability to exhibit rational behavior.</li>
<li><strong>摘要：</strong>人们越来越有兴趣使用法学硕士作为决策“代理人”。这样做包括许多自由度：应该使用哪种模型；应该如何提示；是否应该要求其进行反省、进行链式推理等？解决这些问题——更广泛地说，确定法学硕士代理人是否足够可靠值得信任——需要一种评估此类代理人经济合理性的方法。在本文中，我们提供了一个。我们首先调查有关理性决策的经济文献，对主体应表现出的大量细粒度“元素”以及它们之间的依赖关系进行分类。然后，我们提出一个基准分布，对法学硕士在这些要素上的表现进行定量评分，并结合用户提供的评分标准，生成“理性成绩单”。最后，我们描述了 14 个不同 LLM 的大规模实证实验的结果，描述了当前的技术水平以及不同模型大小对模型表现出理性行为的能力的影响。</li>
</ul>

<h3>Title: Bidirectional Generative Pre-training for Improving Time Series  Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Song, Qincheng Lu, He Zhu, Yue Li</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09558">https://arxiv.org/abs/2402.09558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09558">https://arxiv.org/pdf/2402.09558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09558]] Bidirectional Generative Pre-training for Improving Time Series  Representation Learning(https://arxiv.org/abs/2402.09558)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-series sequences, even more so after fine-tuning on the task.</li>
<li><strong>摘要：</strong>学习判别任务的时间序列表示一直是一个长期存在的挑战。当前的预训练方法仅限于单向下一个令牌预测或随机屏蔽令牌预测。我们提出了一种称为双向及时生成预训练变压器（BiTimelyGPT）的新颖架构，它通过交替变压器层中的下一个令牌和上一个令牌预测来对时间序列数据进行预训练。此预训练任务保留了时间序列的原始分布和数据形状。此外，全秩前向和后向注意力矩阵表现出更具表现力的表示能力。使用生物信号数据，BiTimelyGPT 在预测神经功能、疾病诊断和生理体征方面表现出卓越的性能。通过可视化注意力热图，我们观察到预训练的 BiTimelyGPT 可以从时间序列中识别出有区别的片段，在对任务进行微调后更是如此。</li>
</ul>

<h3>Title: Large Language Model-Based Interpretable Machine Learning Control in  Building Energy Systems</h3>
<ul>
<li><strong>Authors: </strong>Liang Zhang, Zhelun Chen</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09584">https://arxiv.org/abs/2402.09584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09584">https://arxiv.org/pdf/2402.09584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09584]] Large Language Model-Based Interpretable Machine Learning Control in  Building Energy Systems(https://arxiv.org/abs/2402.09584)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The potential of Machine Learning Control (MLC) in HVAC systems is hindered by its opaque nature and inference mechanisms, which is challenging for users and modelers to fully comprehend, ultimately leading to a lack of trust in MLC-based decision-making. To address this challenge, this paper investigates and explores Interpretable Machine Learning (IML), a branch of Machine Learning (ML) that enhances transparency and understanding of models and their inferences, to improve the credibility of MLC and its industrial application in HVAC systems. Specifically, we developed an innovative framework that combines the principles of Shapley values and the in-context learning feature of Large Language Models (LLMs). While the Shapley values are instrumental in dissecting the contributions of various features in ML models, LLM provides an in-depth understanding of rule-based parts in MLC; combining them, LLM further packages these insights into a coherent, human-understandable narrative. The paper presents a case study to demonstrate the feasibility of the developed IML framework for model predictive control-based precooling under demand response events in a virtual testbed. The results indicate that the developed framework generates and explains the control signals in accordance with the rule-based rationale.</li>
<li><strong>摘要：</strong>机器学习控制 (MLC) 在 HVAC 系统中的潜力因其不透明的性质和推理机制而受到阻碍，这对用户和建模者来说很难完全理解，最终导致对基于 MLC 的决策缺乏信任。为了应对这一挑战，本文研究和探索了可解释机器学习 (IML)，它是机器学习 (ML) 的一个分支，可增强模型及其推论的透明度和理解，以提高 MLC 及其在 HVAC 系统中的工业应用的可信度。具体来说，我们开发了一个创新框架，结合了 Shapley 值的原则和大型语言模型 (LLM) 的上下文学习功能。虽然 Shapley 值有助于剖析 ML 模型中各种特征的贡献，但 LLM 提供了对 MLC 中基于规则的部分的深入理解；将它们结合起来，法学硕士进一步将这些见解打包成一个连贯的、人类可以理解的叙述。本文提出了一个案例研究，证明了所开发的 IML 框架在虚拟测试台中需求响应事件下基于模型预测控制的预冷的可行性。结果表明，所开发的框架根据基于规则的原理生成并解释控制信号。</li>
</ul>

<h3>Title: Emerging Opportunities of Using Large Language Language Models for  Translation Between Drug Molecules and Indications</h3>
<ul>
<li><strong>Authors: </strong>David Oniani, Jordan Hilsman, Chengxi Zang, Junmei Wang, Lianjin Cai, Jan Zawala, Yanshan Wang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09588">https://arxiv.org/abs/2402.09588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09588">https://arxiv.org/pdf/2402.09588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09588]] Emerging Opportunities of Using Large Language Language Models for  Translation Between Drug Molecules and Indications(https://arxiv.org/abs/2402.09588)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test existing LLMs on this new task. Specifically, we consider nine variations of the T5 LLM and evaluate them on two public datasets obtained from ChEMBL and DrugBank. Our experiments show the early results of using LLMs for this task and provide a perspective on the state-of-the-art. We also emphasize the current limitations and discuss future work that has the potential to improve the performance on this task. The creation of molecules from indications, or vice versa, will allow for more efficient targeting of diseases and significantly reduce the cost of drug discovery, with the potential to revolutionize the field of drug discovery in the era of generative AI.</li>
<li><strong>摘要：</strong>药物分子是改变生物体精神或身体状态的物质。每种批准的药物都有一个适应症，这是指该药物用于治疗特定疾病的治疗用途。虽然大语言模型（LLM）是一种生成人工智能（AI）技术，最近已经证明了在分子及其文本描述之间翻译的有效性，但在其在促进药物分子和适应症之间的翻译方面的应用仍然存在研究空白，反之亦然，这可以极大地有利于药物发现过程。根据给定适应症生成药物的能力将有助于发现针对特定疾病或目标的药物，并最终为患者提供更好的治疗。在本文中，我们首先提出一个新任务，即药物分子和相应适应症之间的翻译，然后在这个新任务上测试现有的法学硕士。具体来说，我们考虑了 T5 LLM 的九种变体，并在从 ChEMBL 和 DrugBank 获得的两个公共数据集上对其进行评估。我们的实验展示了使用法学硕士完成这项任务的早期结果，并提供了对最新技术的看法。我们还强调当前的局限性，并讨论未来有可能提高这项任务性能的工作。根据适应症创建分子，反之亦然，将能够更有效地靶向疾病，并显着降低药物发现的成本，有可能在生成人工智能时代彻底改变药物发现领域。</li>
</ul>

<h3>Title: LogicPrpBank: A Corpus for Logical Implication and Equivalence</h3>
<ul>
<li><strong>Authors: </strong>Zhexiong Liu, Jing Zhang, Jiaying Lu, Wenjing Ma, Joyce C Ho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09609">https://arxiv.org/abs/2402.09609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09609">https://arxiv.org/pdf/2402.09609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09609]] LogicPrpBank: A Corpus for Logical Implication and Equivalence(https://arxiv.org/abs/2402.09609)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Logic reasoning has been critically needed in problem-solving and decision-making. Although Language Models (LMs) have demonstrated capabilities of handling multiple reasoning tasks (e.g., commonsense reasoning), their ability to reason complex mathematical problems, specifically propositional logic, remains largely underexplored. This lack of exploration can be attributed to the limited availability of annotated corpora. Here, we present a well-labeled propositional logic corpus, LogicPrpBank, containing 7093 Propositional Logic Statements (PLSs) across six mathematical subjects, to study a brand-new task of reasoning logical implication and equivalence. We benchmark LogicPrpBank with widely-used LMs to show that our corpus offers a useful resource for this challenging task and there is ample room for model improvement.</li>
<li><strong>摘要：</strong>逻辑推理在解决问题和决策中至关重要。尽管语言模型（LM）已经证明了处理多种推理任务（例如常识推理）的能力，但它们推理复杂数学问题（特别是命题逻辑）的能力仍然很大程度上未被充分开发。缺乏探索可归因于注释语料库的可用性有限。在这里，我们提出了一个标记良好的命题逻辑语料库，LogicPrpBank，包含跨越六个数学学科的 7093 个命题逻辑语句（PLS），用于研究逻辑蕴涵和等价推理的全新任务。我们使用广泛使用的语言模型对 LogicPrpBank 进行基准测试，以表明我们的语料库为这项具有挑战性的任务提供了有用的资源，并且模型还有足够的改进空间。</li>
</ul>

<h3>Title: Probabilistic Reasoning in Generative Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09614">https://arxiv.org/abs/2402.09614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09614">https://arxiv.org/pdf/2402.09614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09614]] Probabilistic Reasoning in Generative Large Language Models(https://arxiv.org/abs/2402.09614)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper considers the challenges that Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values. This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning. To deal with this problem, we first introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs. We then leverage this new dataset to thoroughly illustrate the specific limitations of LLMs for tasks involving probabilistic reasoning and present several strategies that map the problem to different formal representations, including Python code, probabilistic inference algorithms, and probabilistic logical programming. We conclude by providing an evaluation of our methods on BLInD and on an adaptation of a causal reasoning question-answering dataset, which further shows their practical effectiveness.</li>
<li><strong>摘要：</strong>本文考虑了大型语言模型 (LLM) 在对文本进行推理时所面临的挑战，其中包含涉及通过概率值明确量化的不确定性的信息。这种类型的推理与从日常对话到医疗决策等各种环境相关。尽管法学硕士的数学推理能力有所提高，但在概率推理方面他们仍然表现出很大的困难。为了解决这个问题，我们首先引入贝叶斯语言推理数据集（BLInD），这是一个专门为测试法学硕士概率推理能力而设计的新数据集。然后，我们利用这个新数据集彻底说明法学硕士对于涉及概率推理的任务的具体限制，并提出几种将问题映射到不同形式表示的策略，包括Python代码、概率推理算法和概率逻辑编程。最后，我们对我们的 BLInD 方法和因果推理问答数据集的改编进行了评估，这进一步表明了它们的实际有效性。</li>
</ul>

<h3>Title: API Pack: A Massive Multilingual Dataset for API Call Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09615">https://arxiv.org/abs/2402.09615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09615">https://arxiv.org/pdf/2402.09615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09615]] API Pack: A Massive Multilingual Dataset for API Call Generation(https://arxiv.org/abs/2402.09615)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.</li>
<li><strong>摘要：</strong>我们推出了 API Pack，这是一个多语言数据集，具有超过一百万个指令 API 调用对，旨在提高大型语言模型的 API 调用生成功能。通过实验，我们证明了 API Pack 在增强此专门任务的模型方面的功效，同时保持了一般编码的整体熟练程度。仅在 20,000 个 Python 实例上对 CodeLlama-13B 进行微调，在生成未见过的 API 调用方面，准确率分别比 GPT-3.5 和 GPT-4 高 10% 和 5% 以上。扩展到 10 万个示例可以提高对训练期间未见过的新 API 的泛化能力。此外，无需每种语言的大量数据即可实现跨语言 API 调用生成。数据集、微调模型和整体代码库可在 https://github.com/anonymous_url 上公开获取。</li>
</ul>

<h3>Title: LLM-Enhanced User-Item Interactions: Leveraging Edge Information for  Optimized Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Xinyuan Wang, Liang Wu, Liangjie Hong, Hao Liu, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09617">https://arxiv.org/abs/2402.09617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09617">https://arxiv.org/pdf/2402.09617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09617]] LLM-Enhanced User-Item Interactions: Leveraging Edge Information for  Optimized Recommendations(https://arxiv.org/abs/2402.09617)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The extraordinary performance of large language models has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains. However, the potential of these models in mining relationships from graph data remains under-explored. Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining. Yet, current cutting-edge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks. A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis. We focus on how to utilize existing LLMs for mining and understanding relationships in graph data, applying these techniques to recommendation tasks. We propose an innovative framework that combines the strong contextual representation capabilities of LLMs with the relationship extraction and analysis functions of GNNs for mining relationships in graph data. Specifically, we design a new prompt construction framework that integrates relational information of graph data into natural language expressions, aiding LLMs in more intuitively grasping the connectivity information within graph data. Additionally, we introduce graph relationship understanding and analysis functions into LLMs to enhance their focus on connectivity information in graph data. Our evaluation on real-world datasets demonstrates the framework's ability to understand connectivity information in graph data.</li>
<li><strong>摘要：</strong>大型语言模型的非凡表现不仅重塑了自然语言处理领域的研究格局，而且还展示了其在各个领域的非凡应用潜力。然而，这些模型在从图数据中挖掘关系方面的潜力仍未得到充分探索。图神经网络作为近年来的热门研究领域，关于关系挖掘的研究众多。然而，当前图神经网络的前沿研究尚未与大型语言模型有效集成，导致图关系挖掘任务的效率和能力有限。主要挑战是法学硕士无法深入利用图中的边缘信息，这对于理解复杂的节点关系至关重要。这一差距限制了法学硕士从图结构中提取有意义的见解的潜力，限制了它们在更复杂的基于图的分析中的适用性。我们专注于如何利用现有的法学硕士来挖掘和理解图数据中的关系，并将这些技术应用于推荐任务。我们提出了一种创新框架，将 LLM 强大的上下文表示能力与 GNN 的关系提取和分析功能相结合，用于挖掘图数据中的关系。具体来说，我们设计了一个新的提示构建框架，将图数据的关系信息集成到自然语言表达中，帮助法学硕士更直观地掌握图数据中的连接信息。此外，我们将图关系理解和分析功能引入法学硕士，以增强他们对图数据中连接信息的关注。我们对现实世界数据集的评估证明了该框架理解图形数据中的连接信息的能力。</li>
</ul>

<h3>Title: MiMiC: Minimally Modified Counterfactuals in the Representation Space</h3>
<ul>
<li><strong>Authors: </strong>Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, Ponnurangam Kumaraguru</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09631">https://arxiv.org/abs/2402.09631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09631">https://arxiv.org/pdf/2402.09631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09631]] MiMiC: Minimally Modified Counterfactuals in the Representation Space(https://arxiv.org/abs/2402.09631)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity. We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that enables controlled generation. We demonstrate the effectiveness of the proposed approaches in mitigating bias in multiclass classification and in reducing the generation of toxic language, outperforming strong baselines.</li>
<li><strong>摘要：</strong>语言模型经常表现出不良行为，例如性别偏见或有毒语言。事实证明，对表征空间的干预可以通过改变 LM 行为来有效缓解此类问题。我们首先表明，两种著名的干预技术，线性擦除和转向向量，不能实现高度控制并且表达能力有限。然后，我们提出了一种新颖的干预方法，用于在表示空间中生成表达性反事实，旨在使源类（例如“有毒”）的表示与目标类（例如“无毒”）的表示相似。该方法概括了先前的线性干预技术，在高斯假设下利用封闭式解决方案解决推土机问题，并为表示空间的几何组织提供了理论保证。我们进一步建立在这项技术的基础上，并推导出一种非线性干预，以实现受控发电。我们证明了所提出的方法在减轻多类分类中的偏差和减少有毒语言的产生方面的有效性，优于强基线。</li>
</ul>

<h3>Title: Multi-Fidelity Methods for Optimization: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Ke Li, Fan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09638">https://arxiv.org/abs/2402.09638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09638">https://arxiv.org/pdf/2402.09638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09638]] Multi-Fidelity Methods for Optimization: A Survey(https://arxiv.org/abs/2402.09638)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Real-world black-box optimization often involves time-consuming or costly experiments and simulations. Multi-fidelity optimization (MFO) stands out as a cost-effective strategy that balances high-fidelity accuracy with computational efficiency through a hierarchical fidelity approach. This survey presents a systematic exploration of MFO, underpinned by a novel text mining framework based on a pre-trained language model. We delve deep into the foundational principles and methodologies of MFO, focusing on three core components -- multi-fidelity surrogate models, fidelity management strategies, and optimization techniques. Additionally, this survey highlights the diverse applications of MFO across several key domains, including machine learning, engineering design optimization, and scientific discovery, showcasing the adaptability and effectiveness of MFO in tackling complex computational challenges. Furthermore, we also envision several emerging challenges and prospects in the MFO landscape, spanning scalability, the composition of lower fidelities, and the integration of human-in-the-loop approaches at the algorithmic level. We also address critical issues related to benchmarking and the advancement of open science within the MFO community. Overall, this survey aims to catalyze further research and foster collaborations in MFO, setting the stage for future innovations and breakthroughs in the field.</li>
<li><strong>摘要：</strong>现实世界的黑盒优化通常涉及耗时或昂贵的实验和模拟。多重保真度优化 (MFO) 是一种经济高效的策略，通过分层保真度方法平衡高保真度精度与计算效率。本次调查以基于预训练语言模型的新颖文本挖掘框架为基础，对 MFO 进行了系统探索。我们深入研究 MFO 的基本原理和方法，重点关注三个核心组成部分——多保真代理模型、保真管理策略和优化技术。此外，本次调查还重点介绍了 MFO 在机器学习、工程设计优化和科学发现等多个关键领域的多样化应用，展示了 MFO 在应对复杂计算挑战方面的适应性和有效性。此外，我们还预见了 MFO 领域的一些新挑战和前景，包括可扩展性、低保真度的组成以及算法层面的人机交互方法的集成。我们还解决与 MFO 社区内的基准测试和开放科学进步相关的关键问题。总体而言，本次调查旨在促进 MFO 领域的进一步研究并促进合作，为该领域未来的创新和突破奠定基础。</li>
</ul>

<h3>Title: Answer is All You Need: Instruction-following Text Embedding via  Answering the Question</h3>
<ul>
<li><strong>Authors: </strong>Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09642">https://arxiv.org/abs/2402.09642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09642">https://arxiv.org/pdf/2402.09642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09642]] Answer is All You Need: Instruction-following Text Embedding via  Answering the Question(https://arxiv.org/abs/2402.09642)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This work aims to build a text embedder that can capture characteristics of texts specified by user instructions. Despite its tremendous potential to deploy user-oriented embeddings, none of previous approaches provides a concrete solution for it. This paper offers a new viewpoint, which treats the instruction as a question about the input text and encodes the expected answers to obtain the representation accordingly. Intuitively, texts with the same (implicit) semantics would share similar answers following the instruction, thus leading to more similar embeddings. Specifically, we propose InBedder that instantiates this embed-via-answering idea by only fine-tuning language models on abstractive question answering tasks. InBedder demonstrates significantly improved instruction-following capabilities according to our proposed instruction awareness tests and instruction robustness tests, when applied to both large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based LMs (e.g., roberta-large). Additionally, our qualitative analysis of clustering outcomes, achieved by applying different instructions to the same corpus, demonstrates a high degree of interpretability.</li>
<li><strong>摘要：</strong>这项工作旨在构建一个文本嵌入器，可以捕获用户指令指定的文本特征。尽管它在部署面向用户的嵌入方面具有巨大的潜力，但以前的方法都没有为其提供具体的解决方案。本文提供了一种新的观点，它将指令视为关于输入文本的问题，并对预期答案进行编码以获得相应的表示。直观上，具有相同（隐式）语义的文本将在指令后共享相似的答案，从而导致更相似的嵌入。具体来说，我们建议 InBedder 通过仅在抽象问答任务上微调语言模型来实例化这种通过回答嵌入的想法。根据我们提出的指令意识测试和指令稳健性测试，当应用于大型语言模型 (LLM)（例如 llama-2-7b）和较小的基于编码器的 LM（例如 roberta-）时，InBedder 展示了显着改进的指令跟踪能力。大的）。此外，我们通过对同一语料库应用不同的指令来实现对聚类结果的定性分析，证明了高度的可解释性。</li>
</ul>

<h3>Title: GPT-4's assessment of its performance in a USMLE-based case study</h3>
<ul>
<li><strong>Authors: </strong>Uttam Dhakal, Aniket Kumar Singh, Suman Devkota, Yogesh Sapkota, Bishal Lamichhane, Suprinsa Paudyal, Chandra Dhakal</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09654">https://arxiv.org/abs/2402.09654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09654">https://arxiv.org/pdf/2402.09654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09654]] GPT-4's assessment of its performance in a USMLE-based case study(https://arxiv.org/abs/2402.09654)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study investigates GPT-4's assessment of its performance in healthcare applications. A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it. Understanding the performance of LLM is paramount in exploring its utility in sensitive areas like healthcare. This study contributes to the ongoing discourse on the reliability of AI, particularly of LLMs like GPT-4, within healthcare, offering insights into how feedback mechanisms might be optimized to enhance AI-assisted medical education and decision support.</li>
<li><strong>摘要：</strong>本研究调查了 GPT-4 对其在医疗保健应用中的性能的评估。使用一种简单的提示技术来提示法学硕士从美国医学执照考试（USMLE）问卷中提取的问题，并要求其在提出问题之前和提出问题之后评估其置信度得分。问卷分为两组：有反馈的问题（WF）和问题后无反馈的问题（NF）。该模型被要求在每个问题之前和之后提供绝对和相对置信度分数。使用统计工具对实验结果进行分析，以研究 WF 和 NF 组的置信度变异性。此外，还进行了序贯分析以观察 WF 组和 NF 组的表现差异。结果表明，反馈会影响相对信心，但不会持续增加或减少它。了解法学硕士的表现对于探索其在医疗保健等敏感领域的效用至关重要。这项研究为医疗保健领域人工智能（尤其是 GPT-4 等法学硕士）可靠性的持续讨论做出了贡献，提供了有关如何优化反馈机制以增强人工智能辅助医疗教育和决策支持的见解。</li>
</ul>

<h3>Title: The Butterfly Effect of Model Editing: Few Edits Can Trigger Large  Language Models Collapse</h3>
<ul>
<li><strong>Authors: </strong>Wanli Yang, Fei Sun, Xinyu Ma, Xun Liu, Dawei Yin, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09656">https://arxiv.org/abs/2402.09656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09656">https://arxiv.org/pdf/2402.09656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09656]] The Butterfly Effect of Model Editing: Few Edits Can Trigger Large  Language Models Collapse(https://arxiv.org/abs/2402.09656)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse after only few edits. To facilitate further research, we have utilized ChatGPT to develop a new dataset, HardCF, based on those hard cases. This dataset aims to establish the foundation for pioneering research in reliable model editing and the mechanisms underlying editing-induced model collapse. We hope this work can draw the community's attention to the potential risks inherent in model editing practices.</li>
<li><strong>摘要：</strong>尽管模型编辑在修改大型语言模型 (LLM) 知识方面显示出了希望，但它对 LLM 固有能力的影响常常被忽视。在这项工作中，我们揭示了一个关键现象：即使是单个编辑也可能触发模型崩溃，表现为各种基准任务中的性能显着下降。然而，每次编辑后对法学硕士进行基准测试虽然是防止此类崩溃所必需的，但实际上非常耗时且耗费资源。为了缓解这一问题，我们建议使用困惑度作为替代指标，并通过大量实验进行验证，证明其与下游任务绩效的强相关性。我们进一步对顺序编辑（现实世界场景的实际设置）进行了深入研究，涉及各种编辑方法和法学硕士，重点关注我们之前的单一编辑研究中的困难案例。结果表明，几乎所有检查过的编辑方法仅经过几次编辑后就会导致模型崩溃。为了促进进一步的研究，我们利用 ChatGPT 基于这些困难案例开发了一个新的数据集 HardCF。该数据集旨在为可靠模型编辑和编辑引起的模型崩溃的机制奠定基础。我们希望这项工作能够引起社区对模型编辑实践中固有的潜在风险的关注。</li>
</ul>

<h3>Title: How to Train Data-Efficient LLMs</h3>
<ul>
<li><strong>Authors: </strong>Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed H. Chi, James Caverlee, Julian McAuley, Derek Zhiyuan Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09668">https://arxiv.org/abs/2402.09668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09668">https://arxiv.org/pdf/2402.09668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09668]] How to Train Data-Efficient LLMs(https://arxiv.org/abs/2402.09668)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The training of large language models (LLMs) is expensive. In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample. In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover the performance of the full data, while models trained on Ask-LLM data consistently outperform full-data training -- even when we reject 90% of the original dataset, while converging up to 70% faster.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的训练非常昂贵。在本文中，我们研究了预训练法学硕士的数据有效方法，即旨在优化模型质量和训练资源/数据消耗的帕累托前沿的技术。我们试图了解与基于（i）计算昂贵的数据质量估计的数据选择例程相关的权衡，以及（ii）特征空间中覆盖范围和基于多样性的措施的最大化。我们的第一项技术 Ask-LLM 利用指令调整的 LLM 的零样本推理功能来直接评估训练示例的质量。为了实现覆盖范围，我们提出了密度采样，它对数据分布进行建模以选择不同的样本。在我们对 19 个采样器的比较中，涉及数百个评估任务和预训练运行，我们发现 Ask-LLM 和 Density 是各自类别中的最佳方法。覆盖采样可以恢复完整数据的性能，而在 Ask-LLM 数据上训练的模型始终优于完整数据训练——即使我们拒绝 90% 的原始数据集，同时收敛速度提高了 70%。</li>
</ul>

<h3>Title: PAL: Proxy-Guided Black-Box Attack on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chawin Sitawarin, Norman Mu, David Wagner, Alexandre Araujo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09674">https://arxiv.org/abs/2402.09674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09674">https://arxiv.org/pdf/2402.09674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09674]] PAL: Proxy-Guided Black-Box Attack on Large Language Models(https://arxiv.org/abs/2402.09674)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs (PAL), the first optimization-based attack on LLMs in a black-box query-only setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs. Our attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on Llama-2-7B, compared to 4% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple baseline for query-based attacks. We believe the techniques proposed in this work will enable more comprehensive safety testing of LLMs and, in the long term, the development of better security guardrails. The code can be found at https://github.com/chawins/pal.</li>
<li><strong>摘要：</strong>近几个月来，大型语言模型 (LLM) 的受欢迎程度激增，但它们已被证明具有在被操纵时生成有害内容的能力。虽然安全微调等技术旨在最大限度地减少有害使用，但最近的研究表明，法学硕士仍然容易受到引起毒性反应的攻击。在这项工作中，我们介绍了对 LLM 的代理引导攻击 (PAL)，这是在黑盒仅查询设置中针对 LLM 的第一个基于优化的攻击。特别是，它依赖于代理模型来指导优化以及为现实世界的 LLM API 设计的复杂损失。我们的攻击在 GPT-3.5-Turbo 上实现了 84% 的攻击成功率 (ASR)，在 Llama-2-7B 上实现了 48% 的攻击成功率 (ASR)，而当前技术水平为 4%。我们还提出了 GCG++，这是对 GCG 攻击的改进，在白盒 Llama-2-7B 上达到 94% ASR，以及 LLM 随机搜索攻击 (RAL)，这是基于查询的攻击的强大但简单的基线。我们相信这项工作中提出的技术将使法学硕士能够进行更全面的安全测试，并从长远来看，开发更好的安全护栏。代码可以在 https://github.com/chawins/pal 找到。</li>
</ul>

<h3>Title: An Analysis of Langauge Frequency and Error Correction for Esperanto</h3>
<ul>
<li><strong>Authors: </strong>Junhong Liang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09696">https://arxiv.org/abs/2402.09696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09696">https://arxiv.org/pdf/2402.09696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09696]] An Analysis of Langauge Frequency and Error Correction for Esperanto(https://arxiv.org/abs/2402.09696)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Current Grammar Error Correction (GEC) initiatives tend to focus on major languages, with less attention given to low-resource languages like Esperanto. In this article, we begin to bridge this gap by first conducting a comprehensive frequency analysis using the Eo-GP dataset, created explicitly for this purpose. We then introduce the Eo-GEC dataset, derived from authentic user cases and annotated with fine-grained linguistic details for error identification. Leveraging GPT-3.5 and GPT-4, our experiments show that GPT-4 outperforms GPT-3.5 in both automated and human evaluations, highlighting its efficacy in addressing Esperanto's grammatical peculiarities and illustrating the potential of advanced language models to enhance GEC strategies for less commonly studied languages.</li>
<li><strong>摘要：</strong>当前的语法错误纠正 (GEC) 计划往往侧重于主要语言，而较少关注世界语等资源匮乏的语言。在本文中，我们首先使用专门为此目的创建的 Eo-GP 数据集进行全面的频率分析，以此来弥补这一差距。然后，我们介绍 Eo-GEC 数据集，该数据集源自真实的用户案例，并用细粒度的语言细节进行注释以进行错误识别。利用 GPT-3.5 和 GPT-4，我们的实验表明，GPT-4 在自动评估和人工评估中均优于 GPT-3.5，突出了其在解决世界语语法特性方面的功效，并说明了高级语言模型在增强不太常见的 GEC 策略方面的潜力。学习过语言。</li>
</ul>

<h3>Title: Improving Non-autoregressive Machine Translation with Error Exposure and  Consistency Regularization</h3>
<ul>
<li><strong>Authors: </strong>Xinran Chen, Sufeng Duan, Gongshen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09725">https://arxiv.org/abs/2402.09725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09725">https://arxiv.org/pdf/2402.09725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09725]] Improving Non-autoregressive Machine Translation with Error Exposure and  Consistency Regularization(https://arxiv.org/abs/2402.09725)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens. However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference. The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores compared to the base models, respectively, and our CMLMC-EECR achieves the best performance with a comparable translation quality with the Transformer. The experiments results demonstrate the effectiveness of our method.</li>
<li><strong>摘要：</strong>作为 IR-NAT（基于迭代细化的 NAT）框架之一，条件屏蔽语言模型（CMLM）采用屏蔽预测范式来重新预测屏蔽的低置信度令牌。然而，CMLM 受到训练和推理之间数据分布差异的影响，其中观察到的标记在两种情况下生成不同。在本文中，我们通过错误暴露和一致性正则化（EECR）的训练方法来解决这个问题。我们在训练期间基于模型预测构建混合序列，并提出在不完善的观察条件下对屏蔽标记进行优化。我们还设计了一种一致性学习方法来约束不同观察情况下屏蔽标记的数据分布，以缩小训练和推理之间的差距。与基础模型相比，在五个翻译基准上的实验分别获得了 0.68 和 0.40 BLEU 分数的平均改进，并且我们的 CMLMC-EECR 在与 Transformer 相当的翻译质量下实现了最佳性能。实验结果证明了我们方法的有效性。</li>
</ul>

<h3>Title: A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts</h3>
<ul>
<li><strong>Authors: </strong>Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, Ian Fischer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09727">https://arxiv.org/abs/2402.09727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09727">https://arxiv.org/pdf/2402.09727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09727]] A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts(https://arxiv.org/abs/2402.09727)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, prompt, agent</a></li>
<li><strong>Abstract: </strong>Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3-20x.</li>
<li><strong>摘要：</strong>当前的大型语言模型（LLM）不仅限于某些最大上下文长度，而且无法稳健地消耗长输入。为了解决这些限制，我们提出了 ReadAgent，这是一种 LLM 代理系统，在我们的实验中将有效上下文长度增加了 20 倍。受人类交互阅读长文档方式的启发，我们将 ReadAgent 实现为一个简单的提示系统，该系统使用法学硕士的高级语言功能来 (1) 决定在记忆片段中一起存储哪些内容，(2) 将这些记忆片段压缩为短片段（3）如果 ReadAgent 需要提醒自己相关细节来完成任务，则采取行动查找原文中的段落。我们使用检索方法、原始长上下文和要点记忆来根据基线评估 ReadAgent。这些评估是针对三个长文档阅读理解任务进行的：QuALITY、NarrativeQA 和 QMSum。 ReadAgent 在所有三项任务上均优于基线，同时将有效上下文窗口扩展了 3-20 倍。</li>
</ul>

<h3>Title: Federated Prompt-based Decision Transformer for Customized VR Services  in Mobile Edge Computing System</h3>
<ul>
<li><strong>Authors: </strong>Tailin Zhou, Jiadong Yu, Jun Zhang, Danny H.K. Tsang</a></li>
<li><strong>Subjects: </strong>cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09729">https://arxiv.org/abs/2402.09729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09729">https://arxiv.org/pdf/2402.09729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09729]] Federated Prompt-based Decision Transformer for Customized VR Services  in Mobile Edge Computing System(https://arxiv.org/abs/2402.09729)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>This paper investigates resource allocation to provide heterogeneous users with customized virtual reality (VR) services in a mobile edge computing (MEC) system. We first introduce a quality of experience (QoE) metric to measure user experience, which considers the MEC system's latency, user attention levels, and preferred resolutions. Then, a QoE maximization problem is formulated for resource allocation to ensure the highest possible user experience,which is cast as a reinforcement learning problem, aiming to learn a generalized policy applicable across diverse user environments for all MEC servers. To learn the generalized policy, we propose a framework that employs federated learning (FL) and prompt-based sequence modeling to pre-train a common decision model across MEC servers, which is named FedPromptDT. Using FL solves the problem of insufficient local MEC data while protecting user privacy during offline training. The design of prompts integrating user-environment cues and user-preferred allocation improves the model's adaptability to various user environments during online execution.</li>
<li><strong>摘要：</strong>本文研究了移动边缘计算（MEC）系统中的资源分配，以便为异构用户提供定制的虚拟现实（VR）服务。我们首先引入体验质量 (QoE) 指标来衡量用户体验，该指标考虑了 MEC 系统的延迟、用户注意力级别和首选分辨率。然后，制定 QoE 最大化问题来进行资源分配，以确保尽可能高的用户体验，这被视为强化学习问题，旨在学习适用于所有 MEC 服务器的跨不同用户环境的通用策略。为了学习广义策略，我们提出了一个框架，该框架采用联邦学习（FL）和基于提示的序列建模来预训练跨 MEC 服务器的通用决策模型，该模型名为 FedPromptDT。使用FL解决了本地MEC数据不足的问题，同时保护了离线训练时用户的隐私。结合用户环境提示和用户偏好分配的提示设计提高了模型在线执行时对各种用户环境的适应性。</li>
</ul>

<h3>Title: Do LLMs Know about Hallucination? An Empirical Investigation of LLM's  Hidden States</h3>
<ul>
<li><strong>Authors: </strong>Hanyu Duan, Yi Yang, Kar Yan Tam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09733">https://arxiv.org/abs/2402.09733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09733">https://arxiv.org/pdf/2402.09733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09733]] Do LLMs Know about Hallucination? An Empirical Investigation of LLM's  Hidden States(https://arxiv.org/abs/2402.09733)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates. To do this, we introduce an experimental framework which allows examining LLM's hidden states in different hallucination situations. Building upon this framework, we conduct a series of experiments with language models in the LLaMA family (Touvron et al., 2023). Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one. We then apply various model interpretation techniques to help understand and explain the findings better. Moreover, informed by the empirical observations, we show great potential of using the guidance derived from LLM's hidden representation space to mitigate hallucination. We believe this work provides insights into how LLMs produce hallucinated answers and how to make them occur less often.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）可以编造不真实的答案，这被称为幻觉。这项研究旨在了解法学硕士是否、如何以及在多大程度上意识到幻觉。更具体地说，我们检查法学硕士在正确回答问题时和出现幻觉时在其隐藏状态下的反应是否以及如何不同。为此，我们引入了一个实验框架，允许检查法学硕士在不同幻觉情况下的隐藏状态。在此框架的基础上，我们对 LLaMA 家族中的语言模型进行了一系列实验（Touvron 等人，2023）。我们的实证研究结果表明，法学硕士在处理真实的回应与捏造的回应时的反应不同。然后，我们应用各种模型解释技术来帮助更好地理解和解释研究结果。此外，根据经验观察，我们显示出利用法学硕士隐藏表示空间的指导来减轻幻觉的巨大潜力。我们相信这项工作提供了关于法学硕士如何产生幻觉答案以及如何减少它们出现频率的见解。</li>
</ul>

<h3>Title: Agents Need Not Know Their Purpose</h3>
<ul>
<li><strong>Authors: </strong>Paulo Garcia</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09734">https://arxiv.org/abs/2402.09734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09734">https://arxiv.org/pdf/2402.09734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09734]] Agents Need Not Know Their Purpose(https://arxiv.org/abs/2402.09734)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Ensuring artificial intelligence behaves in such a way that is aligned with human values is commonly referred to as the alignment challenge. Prior work has shown that rational agents, behaving in such a way that maximizes a utility function, will inevitably behave in such a way that is not aligned with human values, especially as their level of intelligence goes up. Prior work has also shown that there is no "one true utility function"; solutions must include a more holistic approach to alignment. This paper describes oblivious agents: agents that are architected in such a way that their effective utility function is an aggregation of a known and hidden sub-functions. The hidden component, to be maximized, is internally implemented as a black box, preventing the agent from examining it. The known component, to be minimized, is knowledge of the hidden sub-function. Architectural constraints further influence how agent actions can evolve its internal environment model. We show that an oblivious agent, behaving rationally, constructs an internal approximation of designers' intentions (i.e., infers alignment), and, as a consequence of its architecture and effective utility function, behaves in such a way that maximizes alignment; i.e., maximizing the approximated intention function. We show that, paradoxically, it does this for whatever utility function is used as the hidden component and, in contrast with extant techniques, chances of alignment actually improve as agent intelligence grows.</li>
<li><strong>摘要：</strong>确保人工智能的行为方式与人类价值观一致通常被称为一致性挑战。先前的研究表明，理性主体在以最大化效用函数的方式行事时，将不可避免地以与人类价值观不符的方式行事，特别是当他们的智力水平提高时。先前的工作还表明，不存在“一个真正的效用函数”；解决方案必须包括更全面的协调方法。本文描述了不经意的代理：代理的架构方式使得它们的有效效用函数是已知和隐藏子函数的聚合。要最大化的隐藏组件在内部实现为黑匣子，以防止代理检查它。要最小化的已知分量是隐藏子函数的知识。架构约束进一步影响代理行为如何发展其内部环境模型。我们表明，一个不经意的代理人，行为理性，构建了设计者意图的内部近似（即推断对齐），并且由于其架构和有效的效用函数，其行为方式最大化对齐；即最大化近似意图函数。我们证明，矛盾的是，无论使用什么效用函数作为隐藏组件，它都会这样做，并且与现有技术相比，随着智能体智能的增长，对齐的机会实际上会增加。</li>
</ul>

<h3>Title: QuRating: Selecting High-Quality Data for Training Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09739">https://arxiv.org/abs/2402.09739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09739">https://arxiv.org/pdf/2402.09739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09739]] QuRating: Selecting High-Quality Data for Training Language Models(https://arxiv.org/abs/2402.09739)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity, as selecting only the highest-rated documents leads to poor results. When we sample using quality ratings as logits over documents, our models achieve lower perplexity and stronger in-context learning performance than baselines. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.</li>
<li><strong>摘要：</strong>选择高质量的预训练数据对于创建强大的语言模型非常重要，但现有方法依赖于简单的启发式方法。我们引入了 QuRating，一种选择预训练数据的方法，可以捕获人类直观感知的文本的抽象品质。在本文中，我们研究了四种品质——写作风格、所需的专业知识、事实和琐事以及教育价值。我们发现法学硕士能够辨别这些品质，并观察到他们更擅长对文本进行成对判断，而不是直接评估文本的质量。我们训练 QuRater 模型来从成对判断中学习标量评分，并用它来注释 260B 训练语料库，其中包含四个标准中每一个标准的质量评分。在我们的实验中，我们根据不同的质量评分选择 30B 个 token，并在所选数据上训练 1.3B 参数的语言模型。我们发现平衡质量和多样性非常重要，因为仅选择评分最高的文档会导致结果不佳。当我们使用质量评级作为文档的逻辑进行采样时，我们的模型比基线实现了更低的困惑度和更强的上下文学习性能。除了数据选择之外，我们还使用质量评级来构建培训课程，在不更改培训数据集的情况下提高性能。我们广泛分析质量评级并讨论其特征、偏见和更广泛的影响。</li>
</ul>

<h3>Title: AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern  Doctors for Clinical Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09742">https://arxiv.org/abs/2402.09742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09742">https://arxiv.org/pdf/2402.09742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09742]] AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern  Doctors for Clinical Diagnosis(https://arxiv.org/abs/2402.09742)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The incorporation of Large Language Models (LLMs) in healthcare marks a significant advancement. However, the application has predominantly been limited to discriminative and question-answering tasks, which does not fully leverage their interactive potential. To address this limitation, our paper presents AI Hospital, a framework designed to build a real-time interactive diagnosis environment. To simulate the procedure, we collect high-quality medical records to create patient, examiner, and medical director agents. AI Hospital is then utilized for the interactive evaluation and collaboration of LLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark where various LLMs serve as intern doctors for interactive diagnosis. Subsequently, to improve diagnostic accuracy, we introduce a collaborative mechanism that involves iterative discussions and a dispute resolution process under the supervision of the medical director. In our experiments, we validate the reliability of AI Hospital. The results not only explore the feasibility of apply LLMs in clinical consultation but also confirm the effectiveness of the dispute resolution focused collaboration method.</li>
<li><strong>摘要：</strong>将大型语言模型 (LLM) 纳入医疗保健领域标志着一项重大进步。然而，该应用程序主要局限于判别性和问答任务，没有充分发挥其交互潜力。为了解决这个限制，我们的论文提出了 AI Hospital，一个旨在构建实时交互式诊断环境的框架。为了模拟该过程，我们收集高质量的医疗记录来创建患者、检查员和医疗主任代理。然后利用人工智能医院对法学硕士进行交互式评估和协作。最初，我们创建了一个多视图医学评估（MVME）基准，其中各种法学硕士作为实习医生进行交互式诊断。随后，为了提高诊断准确性，我们引入了一种协作机制，该机制涉及在医疗主任的监督下进行迭代讨论和争议解决流程。在我们的实验中，我们验证了人工智能医院的可靠性。结果不仅探讨了法学硕士应用于临床会诊的可行性，而且证实了以争议解决为中心的协作方法的有效性。</li>
</ul>

<h3>Title: Model Compression and Efficient Inference for Large Language Models: A  Survey</h3>
<ul>
<li><strong>Authors: </strong>Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, Xiaofei He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09748">https://arxiv.org/abs/2402.09748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09748">https://arxiv.org/pdf/2402.09748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09748]] Model Compression and Efficient Inference for Large Language Models: A  Survey(https://arxiv.org/abs/2402.09748)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large models, such as quantization and pruning, start to explore tuning-free algorithms. (2) Large models emphasize versatility and generalization rather than performance on a single task. Hence, many algorithms, such as knowledge distillation, focus on how to preserving their versatility and generalization after compression. Since these two characteristics were not very pronounced in early large models, we further distinguish large language models into medium models and ``real'' large models. Additionally, we also provide an introduction to some mature frameworks for efficient inference of large models, which can support basic compression or acceleration algorithms, greatly facilitating model deployment for users.</li>
<li><strong>摘要：</strong>基于 Transformer 的大型语言模型取得了巨大的成功。然而，推理过程中产生的大量内存和计算成本使得在资源受限的设备上部署大型模型变得具有挑战性。在本文中，我们从算法的角度研究大型语言模型的压缩和高效推理方法。从分类上看，与较小的模型类似，大型语言模型的压缩和加速算法仍然可以分为量化、剪枝、蒸馏、紧凑架构设计、动态网络等。然而，与较小的模型相比，大型语言模型有两个突出的特征：（1）大多数压缩算法在压缩后需要对模型进行微调甚至重新训练。大型模型最值得注意的方面是与模型微调或训练相关的成本非常高。因此，许多针对大型模型的算法，例如量化和剪枝，开始探索免调优算法。 (2) 大型模型强调多功能性和泛化性，而不是单一任务的性能。因此，许多算法，例如知识蒸馏，重点关注如何在压缩后保留其通用性和泛化性。由于这两个特征在早期的大型模型中并不是很明显，因此我们进一步将大型语言模型分为中型模型和“真正的”大型模型。此外，我们还介绍了一些成熟的大型模型高效推理框架，可以支持基本的压缩或加速算法，极大地方便了用户的模型部署。</li>
</ul>

<h3>Title: Efficient Language Adaptive Pre-training: Extending State-of-the-Art  Large Language Models for Polish</h3>
<ul>
<li><strong>Authors: </strong>Szymon Ruciński</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09759">https://arxiv.org/abs/2402.09759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09759">https://arxiv.org/pdf/2402.09759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09759]] Efficient Language Adaptive Pre-training: Extending State-of-the-Art  Large Language Models for Polish(https://arxiv.org/abs/2402.09759)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study explores the potential of fine-tuning foundational English Large Language Models (LLMs) for generating Polish text. The first step involves Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB, consisting of 276 million Polish tokens. The LAPT is followed by additional fine-tuning aimed at solving nine KLEJ challenges. Our trained model Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02 among decoder-based Polish models but also closely rivals the performance of the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9 tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn Polish. The LAPT was completed in less than five days using a consumer GPU, highlighting the method's efficiency. The proficiency of the model in Polish was significantly enhanced, demonstrating the viability of this approach for adding new languages to existing LLMs by training just 1.2% of its parameters. To contribute to the community's collaborative progress, the model has been released as open-source.</li>
<li><strong>摘要：</strong>本研究探讨了微调基础英语大语言模型 (LLM) 用于生成波兰语文本的潜力。第一步涉及在 3.11 GB 的高质量数据集（包含 2.76 亿个波兰语标记）上进行语言自适应预训练 (LAPT)。 LAPT 之后进行了额外的微调，旨在解决九项 KLEJ 挑战。我们训练的模型 Curie-7B-v1 不仅生成基于解码器的波兰语模型中困惑度最低的 3.02 的波兰语文本，而且还可以与最佳波兰语编码器-解码器模型的性能相媲美，其中 8 个模型的差距小于 2% 9 项任务。 Curie-7B-v1 使用大约 2-3% 的典型数据集大小来学习波兰语。 LAPT 使用消费级 GPU 在不到五天内完成，凸显了该方法的效率。该模型的波兰语熟练程度显着提高，证明了这种方法通过仅训练 1.2% 的参数即可向现有法学硕士添加新语言的可行性。为了促进社区的协作进步，该模型已作为开源发布。</li>
</ul>

<h3>Title: Grounding Language Model with Chunking-Free In-Context Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09760">https://arxiv.org/abs/2402.09760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09760">https://arxiv.org/pdf/2402.09760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09760]] Grounding Language Model with Chunking-Free In-Context Retrieval(https://arxiv.org/abs/2402.09760)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval. CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained. Our evaluations of CFIC on a range of open QA datasets demonstrate its superiority in retrieving relevant and accurate evidence, offering a significant improvement over traditional methods. By doing away with the need for document chunking, CFIC presents a more streamlined, effective, and efficient retrieval solution, making it a valuable advancement in the field of RAG systems.</li>
<li><strong>摘要：</strong>本文提出了一种新颖的无分块上下文内 (CFIC) 检索方法，专为检索增强生成 (RAG) 系统量身定制。由于处理冗长文档和过滤掉不相关内容的挑战，传统的 RAG 系统经常难以使用精确的证据文本来做出基础响应。常用的解决方案，例如文档分块和调整语言模型来处理较长的上下文，都有其局限性。这些方法要么破坏文本的语义连贯性，要么无法有效解决证据检索中的噪声和不准确问题。 CFIC 通过绕过传统的分块过程来解决这些挑战。它利用文档的编码隐藏状态进行上下文检索，采用自动解码来准确识别用户查询所需的特定证据文本，从而消除了分块的需要。 CFIC 通过结合两种解码策略进一步增强，即约束句子前缀解码和跳过解码。这些策略不仅提高了检索过程的效率，而且确保了生成的基础文本证据的保真度。我们对一系列开放 QA 数据集对 CFIC 的评估证明了其在检索相关和准确证据方面的优越性，比传统方法有了显着改进。通过消除文档分块的需要，CFIC 提供了一种更加简化、有效和高效的检索解决方案，使其成为 RAG 系统领域的宝贵进步。</li>
</ul>

<h3>Title: Aligning Crowd Feedback via Distributional Preference Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09764">https://arxiv.org/abs/2402.09764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09764">https://arxiv.org/pdf/2402.09764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09764]] Aligning Crowd Feedback via Distributional Preference Reward Modeling(https://arxiv.org/abs/2402.09764)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately. In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with a diverse set of human preferences. To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM policy to generate responses favoured by the population. Our experiments show that DPRM significantly enhances the alignment of LLMs with population preference, yielding more accurate, unbiased, and contextually appropriate responses.</li>
<li><strong>摘要：</strong>深度强化学习广泛用于使大型语言模型 (LLM) 与人类偏好保持一致。然而，传统的奖励模型主要依赖于一组选定的个人提供的人类注释。这种依赖性可能会无意中导致模型偏向于反映这些注释者的倾向，从而无法充分代表更广泛人群的期望。在本文中，我们介绍了分布偏好奖励模型（DPRM），这是一个简单而有效的框架，可以将大型语言模型与不同的人类偏好相结合。为此，我们通过贝塔分布来表征偏好，它可以动态适应偏好趋势的波动。最重要的是，我们设计了一个基于最优传输的损失来校准 DPRM 以与偏好分布保持一致。最后，预期奖励被用来微调法学硕士政策，以产生人们喜欢的反应。我们的实验表明，DPRM 显着增强了法学硕士与人群偏好的一致性，产生更准确、公正且适合情境的响应。</li>
</ul>

<h3>Title: NutePrune: Efficient Progressive Pruning with Numerous Teachers for  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shengrui Li, Xueting Han, Jing Bai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09773">https://arxiv.org/abs/2402.09773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09773">https://arxiv.org/pdf/2402.09773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09773]] NutePrune: Efficient Progressive Pruning with Numerous Teachers for  Large Language Models(https://arxiv.org/abs/2402.09773)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach allows us to leverage numerous teachers with varying capacities to progressively guide the pruned model, enhancing overall performance. Extensive experiments across various tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot experiments, NutePrune retains 97.17% of the performance of the original model at 20% sparsity and 95.07% at 25% sparsity.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的规模相当大，带来了显着的部署挑战，特别是在资源受限的硬件上。结构化剪枝提供了一种压缩 LLM 的有效方法，从而降低存储成本并提高推理速度，从而实现更高效的利用。在这项工作中，我们研究数据高效和资源高效的结构修剪方法，以获得更小但仍然强大的模型。知识蒸馏非常适合剪枝，因为完整的模型可以成为剪枝学生的优秀老师。然而，由于内存限制，在法学硕士的背景下，这变得具有挑战性。为了解决这个问题，我们提出了一种有效的渐进式多教师剪枝方法（NutePrune）。 NutePrune 通过仅加载一个完整模型并将其与各种掩模和 LoRA 模块集成来减轻过多的内存成本，使其能够在教师和学生角色之间无缝切换。这种方法使我们能够利用众多具有不同能力的教师来逐步指导修剪后的模型，从而提高整体性能。跨各种任务的大量实验证明了 NutePrune 的有效性。在 LLaMA-7B 零样本实验中，NutePrune 在 20% 稀疏度下保留了原始模型 97.17% 的性能，在 25% 稀疏度下保留了 95.07% 的性能。</li>
</ul>

<h3>Title: EFUF: Efficient Fine-grained Unlearning Framework for Mitigating  Hallucinations in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, Xinyu Dai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09801">https://arxiv.org/abs/2402.09801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09801">https://arxiv.org/pdf/2402.09801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09801]] EFUF: Efficient Fine-grained Unlearning Framework for Mitigating  Hallucinations in Multimodal Large Language Models(https://arxiv.org/abs/2402.09801)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）在过去几年中引起了越来越多的关注，但它们仍然可能生成包含相应图像中不存在的物体的描述，这种现象称为物体幻觉。为了消除幻觉，现有方法手动注释有幻觉和没有幻觉的配对响应，然后采用各种对齐算法来提高图像和文本之间的对齐能力。然而，它们不仅在微调阶段需要大量的计算资源，而且还需要昂贵的人工注释来构建比对算法所需的配对数据。为了解决这些问题，我们借鉴了遗忘的思想，提出了一种高效的细粒度遗忘框架（EFUF），它可以消除幻觉，而不需要配对数据。大量的实验表明，我们的方法始终如一地减少幻觉，同时以适度的计算开销保持生成质量。我们的代码和数据集将公开。</li>
</ul>

<h3>Title: Knowledge of Pretrained Language Models on Surface Information of Tokens</h3>
<ul>
<li><strong>Authors: </strong>Tatsuya Hiraoka, Naoaki Okazaki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09808">https://arxiv.org/abs/2402.09808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09808">https://arxiv.org/pdf/2402.09808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09808]] Knowledge of Pretrained Language Models on Surface Information of Tokens(https://arxiv.org/abs/2402.09808)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Do pretrained language models have knowledge regarding the surface information of tokens? We examined the surface information stored in word or subword embeddings acquired by pretrained language models from the perspectives of token length, substrings, and token constitution. Additionally, we evaluated the ability of models to generate knowledge regarding token surfaces. We focused on 12 pretrained language models that were mainly trained on English and Japanese corpora. Experimental results demonstrate that pretrained language models have knowledge regarding token length and substrings but not token constitution. Additionally, the results imply that there is a bottleneck on the decoder side in terms of effectively utilizing acquired knowledge.</li>
<li><strong>摘要：</strong>预训练的语言模型是否了解令牌的表面信息？我们从标记长度、子串和标记构成的角度检查了预训练语言模型获取的单词或子词嵌入中存储的表面信息。此外，我们评估了模型生成有关令牌表面的知识的能力。我们重点研究了 12 种预训练语言模型，这些模型主要在英语和日语语料库上进行训练。实验结果表明，预训练的语言模型具有有关标记长度和子字符串的知识，但不具有标记构成的知识。此外，结果表明解码器端在有效利用所获得的知识方面存在瓶颈。</li>
</ul>

<h3>Title: All in One and One for All: A Simple yet Effective Method towards  Cross-domain Graph Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09834">https://arxiv.org/abs/2402.09834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09834">https://arxiv.org/pdf/2402.09834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09834]] All in One and One for All: A Simple yet Effective Method towards  Cross-domain Graph Pretraining(https://arxiv.org/abs/2402.09834)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了计算机视觉 (CV) 和自然语言处理 (NLP) 领域。法学硕士最显着的进步之一是，单一模型是在跨越多个领域的庞大且多样化的数据集上进行训练的——我们将这种范式称为“一体化”。这种方法使法学硕士具有超强的泛化能力，有助于对各种数据分布的全面理解。利用这些能力，单一的法学硕士在多个领域展现出卓越的多功能性——我们称之为“一应俱全”的范例。然而，将这一想法应用到图领域仍然是一个艰巨的挑战，跨域预训练通常会导致负迁移。这个问题在小样本学习场景中尤其重要，因为训练数据的缺乏需要结合外部知识源。为了应对这一挑战，我们提出了一种称为 PrEtraining Graph Coordinators (GCOPE) 的新方法，该方法利用不同图数据集的潜在共性来增强小样本学习。我们的新颖方法涉及一个统一框架，该框架在预训练阶段合并不同的图形数据集，以提取有意义的知识并将其转移到目标任务。跨多个图形数据集的广泛实验证明了我们方法的卓越功效。通过成功利用多个图数据集的协同潜力进行预训练，我们的工作为图基础模型领域做出了开创性贡献。</li>
</ul>

<h3>Title: Beyond Imitation: Generating Human Mobility from Context-aware Reasoning  with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Shao, Fengli Xu, Bingbing Fan, Jingtao Ding, Yuan Yuan, Meng Wang, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09836">https://arxiv.org/abs/2402.09836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09836">https://arxiv.org/pdf/2402.09836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09836]] Beyond Imitation: Generating Human Mobility from Context-aware Reasoning  with Large Language Models(https://arxiv.org/abs/2402.09836)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control. However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models. Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions. They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness. Inspired by the emergent reasoning ability in LLMs, we propose a radical perspective shift that reformulates mobility generation as a commonsense reasoning problem. In this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour. Specifically, we design a context-aware chain-of-thoughts prompting technique to align LLMs with context-aware mobility behaviour by few-shot in-context learning. Besides, MobiGeaR employ a divide-and-coordinate mechanism to exploit the synergistic effect between LLM reasoning and mechanistic gravity model. It leverages the step-by-step LLM reasoning to recursively generate a temporal template of activity intentions, which are then mapped to physical locations with a mechanistic gravity model. Experiments on two real-world datasets show MobiGeaR achieves state-of-the-art performance across all metrics, and substantially reduces the size of training samples at the same time. Besides, MobiGeaR also significantly improves the semantic-awareness of mobility generation by improving the intention accuracy by 62.23% and the generated mobility data is proven effective in boosting the performance of downstream applications. The implementation of our approach is available in the paper.</li>
<li><strong>摘要：</strong>人员流动行为与交通拥堵、疫情控制等各种重要社会问题密切相关。然而，收集移动数据可能非常昂贵，并且涉及严重的隐私问题，迫切需要高质量的生成移动模型。之前的工作重点是从训练样本中学习行为分布，并通过对学习的分布进行采样来生成新的移动数据。他们无法有效地捕捉驱动移动行为的连贯意图，导致样本效率和语义意识低下。受法学硕士新兴推理能力的启发，我们提出了一种彻底的视角转变，将流动性生成重新表述为常识性推理问题。在本文中，我们设计了一种新颖的移动生成推理（MobiGeaR）框架，该框架促使法学硕士递归地生成移动行为。具体来说，我们设计了一种情境感知的思想链提示技术，通过少量的情境学习，使法学硕士与情境感知的移动行为保持一致。此外，MobiGeaR采用分而协调机制来利用LLM推理和机械引力模型之间的协同效应。它利用逐步的法学硕士推理来递归地生成活动意图的时间模板，然后使用机械重力模型将其映射到物理位置。对两个真实世界数据集的实验表明，MobiGeaR 在所有指标上都实现了最先进的性能，同时大大减少了训练样本的大小。此外，MobiGeaR 还显着提高了移动生成的语义感知，将意图准确性提高了 62.23%，并且生成的移动数据被证明可以有效提升下游应用程序的性能。我们的方法的实现可以在本文中找到。</li>
</ul>

<h3>Title: Performative Reinforcement Learning in Gradually Shifting Environments</h3>
<ul>
<li><strong>Authors: </strong>Ben Rank, Stelios Triantafyllou, Debmalya Mandal, Goran Radanovic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09838">https://arxiv.org/abs/2402.09838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09838">https://arxiv.org/pdf/2402.09838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09838]] Performative Reinforcement Learning in Gradually Shifting Environments(https://arxiv.org/abs/2402.09838)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>When Reinforcement Learning (RL) agents are deployed in practice, they might impact their environment and change its dynamics. Ongoing research attempts to formally model this phenomenon and to analyze learning algorithms in these models. To this end, we propose a framework where the current environment depends on the deployed policy as well as its previous dynamics. This is a generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy. We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR). We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment. Unlike previous approaches, MDRR combines samples from multiple deployments in its training. This makes MDRR particularly suitable for scenarios where the environment's response strongly depends on its previous dynamics, which are common in practice. We experimentally compare the algorithms using a simulation-based testbed and our results show that MDRR converges significantly faster than previous approaches.</li>
<li><strong>摘要：</strong>当强化学习（RL）代理在实践中部署时，它们可能会影响环境并改变其动态。正在进行的研究试图对这种现象进行正式建模并分析这些模型中的学习算法。为此，我们提出了一个框架，其中当前环境取决于已部署的政策及其先前的动态。这是表演强化学习 (PRL) 的推广 [Mandal et al., 2023]。与 PRL 不同，我们的框架允许对环境逐渐适应已部署策略的场景进行建模。我们将表演预测文献中的两种算法应用到我们的设置中，并提出了一种称为混合延迟重复再训练（MDRR）的新颖算法。我们提供这些算法收敛的条件，并使用三个指标对它们进行比较：再训练次数、近似保证和每次部署的样本数。与以前的方法不同，MDRR 在训练中结合了多个部署的样本。这使得 MDRR 特别适合环境响应强烈依赖于其先前动态的场景，这在实践中很常见。我们使用基于模拟的测试台对算法进行实验比较，结果表明 MDRR 的收敛速度明显快于以前的方法。</li>
</ul>

<h3>Title: LAPDoc: Layout-Aware Prompting for Documents</h3>
<ul>
<li><strong>Authors: </strong>Marcel Lamott, Yves-Noel Weweler, Adrian Ulges, Faisal Shafait, Dirk Krechel, Darko Obradovic</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09841">https://arxiv.org/abs/2402.09841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09841">https://arxiv.org/pdf/2402.09841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09841]] LAPDoc: Layout-Aware Prompting for Documents(https://arxiv.org/abs/2402.09841)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks. Opposed to that there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout. This involves a separate fine-tuning step for which additional training data is required. At present, no document transformers with comparable generalization to LLMs are available That raises the question which type of model is to be preferred for document understanding tasks. In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment. We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information. In our experiments we investigate the effects on the commercial ChatGPT model and the open-source LLM Solar. We demonstrate that using our approach both LLMs show improved performance on various standard document benchmarks. In addition, we study the impact of noisy OCR and layout errors, as well as the limitations of LLMs when it comes to utilizing document layout. Our results indicate that layout enrichment can improve the performance of purely text-based LLMs for document understanding by up to 15% compared to just using plain document text. In conclusion, this approach should be considered for the best model choice between text-based LLM or multi-modal document transformers.</li>
<li><strong>摘要：</strong>使用大量纯文本数据训练大型语言模型 (LLM) 的最新进展导致跨许多领域和任务（包括特定于文档的任务）的强泛化。与此相反，有一种趋势是训练专为文档理解而定制的多模式变压器架构，这些架构专门设计用于将文本输入与相应的文档布局融合。这涉及一个单独的微调步骤，需要额外的训练数据。目前，还没有具有与法学硕士相当的泛化能力的文档转换器，这就提出了一个问题：文档理解任务应该首选哪种类型的模型。在本文中，我们研究了通过使用布局丰富将纯基于文本的法学硕士用于特定于文档的任务的可能性。我们探索直接修改和基于规则的方法，用布局信息丰富纯文本的 LLM 提示。在我们的实验中，我们研究了对商业 ChatGPT 模型和开源 LLM Solar 的影响。我们证明，使用我们的方法，两位法学硕士在各种标准文档基准测试中都表现出了改进的性能。此外，我们还研究了嘈杂的 OCR 和布局错误的影响，以及法学硕士在利用文档布局时的局限性。我们的结果表明，与仅使用纯文档文本相比，布局丰富可以将纯基于文本的法学硕士在文档理解方面的性能提高高达 15%。总之，应该考虑使用这种方法来选择基于文本的 LLM 或多模式文档转换器之间的最佳模型。</li>
</ul>

<h3>Title: Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent</h3>
<ul>
<li><strong>Authors: </strong>Quentin Gallouédec, Edward Beeching, Clément Romac, Emmanuel Dellandréa</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09844">https://arxiv.org/abs/2402.09844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09844">https://arxiv.org/pdf/2402.09844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09844]] Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent(https://arxiv.org/abs/2402.09844)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. In this paper, we present Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-sourced (see https://huggingface.co/jat-project/jat), including a pioneering general-purpose dataset.</li>
<li><strong>摘要：</strong>寻找可以跨多个领域无缝运行的通用模型仍然是机器学习研究的一个关键目标。强化学习 (RL) 中的主流方法通常将模型限制为单模态框架内的单个任务，这一限制与通用、多领域模型的更广泛愿景形成鲜明对比。在本文中，我们提出了 Jack of All Trades (JAT)，这是一种基于变压器的模型，其独特的设计针对处理顺序决策任务和多模态数据类型进行了优化。 JAT 模型通过在不同的 RL 基准上实现强大的性能，以及在计算机视觉 (CV) 和自然语言处理 (NLP) 任务上取得可喜的结果，展示了其强大的功能和多功能性，所有这些都使用一组权重。 JAT 模型标志着朝着更通用、跨领域的 AI 模型设计迈出了重要一步，值得注意的是，它是同类模型中第一个完全开源的模型（参见 https://huggingface.co/jat-project/jat ），包括一个开创性的通用数据集。</li>
</ul>

<h3>Title: Camouflage is all you need: Evaluating and Enhancing Language Model  Robustness Against Camouflage Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Álvaro Huertas-García, Alejandro Martín, Javier Huertas-Tato, David Camacho</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09874">https://arxiv.org/abs/2402.09874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09874">https://arxiv.org/pdf/2402.09874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09874]] Camouflage is all you need: Evaluating and Enhancing Language Model  Robustness Against Camouflage Adversarial Attacks(https://arxiv.org/abs/2402.09874)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Adversarial attacks represent a substantial challenge in Natural Language Processing (NLP). This study undertakes a systematic exploration of this challenge in two distinct phases: vulnerability evaluation and resilience enhancement of Transformer-based models under adversarial attacks. In the evaluation phase, we assess the susceptibility of three Transformer configurations, encoder-decoder, encoder-only, and decoder-only setups, to adversarial attacks of escalating complexity across datasets containing offensive language and misinformation. Encoder-only models manifest a 14% and 21% performance drop in offensive language detection and misinformation detection tasks, respectively. Decoder-only models register a 16% decrease in both tasks, while encoder-decoder models exhibit a maximum performance drop of 14% and 26% in the respective tasks. The resilience-enhancement phase employs adversarial training, integrating pre-camouflaged and dynamically altered data. This approach effectively reduces the performance drop in encoder-only models to an average of 5% in offensive language detection and 2% in misinformation detection tasks. Decoder-only models, occasionally exceeding original performance, limit the performance drop to 7% and 2% in the respective tasks. Although not surpassing the original performance, Encoder-decoder models can reduce the drop to an average of 6% and 2% respectively. Results suggest a trade-off between performance and robustness, with some models maintaining similar performance while gaining robustness. Our study and adversarial training techniques have been incorporated into an open-source tool for generating camouflaged datasets. However, methodology effectiveness depends on the specific camouflage technique and data encountered, emphasizing the need for continued exploration.</li>
<li><strong>摘要：</strong>对抗性攻击是自然语言处理（NLP）领域的重大挑战。本研究分两个不同阶段对这一挑战进行了系统探索：脆弱性评估和对抗性攻击下基于 Transformer 的模型的弹性增强。在评估阶段，我们评估了三种 Transformer 配置（编码器-解码器、仅编码器和仅解码器设置）对包含攻击性语言和错误信息的数据集的复杂性不断升级的对抗性攻击的敏感性。仅编码器模型在攻击性语言检测和错误信息检测任务中的性能分别下降了 14% 和 21%。仅解码器模型在这两项任务中的性能下降了 16%，而编码器-解码器模型在相应任务中的最大性能下降了 14% 和 26%。弹性增强阶段采用对抗性训练，整合预先伪装和动态更改的数据。这种方法有效地将仅编码器模型的攻击性语言检测任务的性能下降平均降低到 5%，错误信息检测任务的性能下降平均降低到 2%。仅解码器模型偶尔会超过原始性能，将各自任务中的性能下降限制在 7% 和 2%。虽然没有超越原来的性能，但Encoder-decoder模型可以将下降幅度分别降低到平均6%和2%。结果表明，性能和鲁棒性之间存在权衡，一些模型在保持相似性能的同时获得了鲁棒性。我们的研究和对抗训练技术已被纳入用于生成伪装数据集的开源工具中。然而，方法的有效性取决于具体的伪装技术和遇到的数据，强调需要继续探索。</li>
</ul>

<h3>Title: On Computing Plans with Uniform Action Costs</h3>
<ul>
<li><strong>Authors: </strong>Alberto Pozanco, Daniel Borrajo, Manuela Veloso</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09877">https://arxiv.org/abs/2402.09877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09877">https://arxiv.org/pdf/2402.09877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09877]] On Computing Plans with Uniform Action Costs(https://arxiv.org/abs/2402.09877)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In many real-world planning applications, agents might be interested in finding plans whose actions have costs that are as uniform as possible. Such plans provide agents with a sense of stability and predictability, which are key features when humans are the agents executing plans suggested by planning tools. This paper adapts three uniformity metrics to automated planning, and introduce planning-based compilations that allow to lexicographically optimize sum of action costs and action costs uniformity. Experimental results both in well-known and novel planning benchmarks show that the reformulated tasks can be effectively solved in practice to generate uniform plans.</li>
<li><strong>摘要：</strong>在许多现实世界的规划应用程序中，代理可能有兴趣找到其行动成本尽可能统一的计划。此类计划为智能体提供了稳定性和可预测性，这是当人类作为智能体执行规划工具建议的计划时的关键特征。本文将三个均匀性指标应用于自动规划，并引入基于规划的编译，允许按字典顺序优化行动成本和行动成本均匀性的总和。众所周知的和新颖的规划基准中的实验结果表明，重新制定的任务可以在实践中有效地解决，以生成统一的计划。</li>
</ul>

<h3>Title: Inadequacies of Large Language Model Benchmarks in the Era of Generative  Artificial Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, Malka N. Halgamuge</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09880">https://arxiv.org/abs/2402.09880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09880">https://arxiv.org/pdf/2402.09880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09880]] Inadequacies of Large Language Model Benchmarks in the Era of Generative  Artificial Intelligence(https://arxiv.org/abs/2402.09880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral profiling to accurately capture LLMs' complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems' integration into society.</li>
<li><strong>摘要：</strong>具有新兴功能的大型语言模型（LLM）的迅速普及激发了公众对评估和比较不同LLM的好奇心，导致许多研究人员提出了他们的LLM基准。注意到这些基准的初步不足，我们开始了一项研究，以功能和安全为支柱，通过人员、流程和技术的视角，使用我们新颖的统一评估框架，严格评估 23 个最先进的 LLM 基准。我们的研究发现了重大局限性，包括偏见、衡量真实推理的困难、适应性、实施不一致、即时工程复杂性、评估者多样性以及在一项综合评估中忽视文化和意识形态规范。我们的讨论强调，鉴于人工智能（AI）的进步，迫切需要标准化方法、监管确定性和道德准则，包括倡导从静态基准演变为动态行为分析，以准确捕捉法学硕士的复杂行为和潜在风险。我们的研究强调了法学硕士评估方法范式转变的必要性，强调了合作努力制定普遍接受的基准和加强人工智能系统融入社会的重要性。</li>
</ul>

<h3>Title: Generative Representational Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09906">https://arxiv.org/abs/2402.09906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09906">https://arxiv.org/pdf/2402.09906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09906]] Generative Representational Instruction Tuning(https://arxiv.org/abs/2402.09906)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.</li>
<li><strong>摘要：</strong>所有基于文本的语言问题都可以简化为生成或嵌入。当前的模型仅在其中之一上表现良好。我们引入了生成表征指令调优（GRIT），通过指令来训练大型语言模型来处理生成任务和嵌入任务。与其他开放模型相比，我们生成的 GritLM 7B 在大规模文本嵌入基准 (MTEB) 上树立了新的技术水平，并在一系列生成任务上优于同等规模的所有模型。通过进一步扩展，GritLM 8x7B 的性能优于我们尝试过的所有开放生成语言模型，同时仍然是最好的嵌入模型之一。值得注意的是，我们发现 GRIT 仅匹配生成数据或嵌入数据的训练，因此我们可以在不损失性能的情况下统一两者。除其他好处外，通过 GRIT 进行统一，不再需要单独的检索和生成模型，可以将长文档的检索增强生成 (RAG) 速度提高 60% 以上。模型、代码等可在 https://github.com/ContextualAI/gritlm 免费获取。</li>
</ul>

<h3>Title: DE-COP: Detecting Copyrighted Content in Language Models Training Data</h3>
<ul>
<li><strong>Authors: </strong>André V. Duarte, Xuandong Zhao, Arlindo L. Oliveira, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09910">https://arxiv.org/abs/2402.09910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09910">https://arxiv.org/pdf/2402.09910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09910]] DE-COP: Detecting Copyrighted Content in Language Models Training Data(https://arxiv.org/abs/2402.09910)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give $\approx$ 4% accuracy. Our code and datasets are available at https://github.com/avduarte333/DE-COP_Method</li>
<li><strong>摘要：</strong>考虑到训练数据通常是不公开的，我们如何检测语言模型的训练过程中是否使用了受版权保护的内容？我们的动机是，语言模型可能会从其训练文本中识别逐字摘录。我们提出了 DE-COP，一种确定培训中是否包含受版权保护的内容的方法。 DE-COP 的核心方法是通过多项选择题来探究法学硕士，其选项包括逐字文本及其释义。我们构建了 BookTection，这是一个基准，包含模型训练截止之前和之后出版的 165 本书的摘录以及它们的释义。我们的实验表明，DE-COP 在具有可用 logits 的模型上的检测性能 (AUC) 超过了先前最佳方法 9.6%。此外，DE-COP 在完全黑盒模型上检测可疑书籍时也达到了 72% 的平均准确度，而之前的方法的准确度约为 4%。我们的代码和数据集可在 https://github.com/avduarte333/DE-COP_Method 获取</li>
</ul>

<h3>Title: Enhancing Large Language Models with Pseudo- and Multisource- Knowledge  Graphs for Open-ended Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09911">https://arxiv.org/abs/2402.09911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09911">https://arxiv.org/pdf/2402.09911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09911]] Enhancing Large Language Models with Pseudo- and Multisource- Knowledge  Graphs for Open-ended Question Answering(https://arxiv.org/abs/2402.09911)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ended questions. For precise questions, we observe a minimum accuracy improvement of 7.5. Moreover, there is also demonstration that this framework exhibits generalizability across different KG sources. In summary, our results pave the way for enhancing LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the context of open-ended questions.</li>
<li><strong>摘要：</strong>减轻大型语言模型（LLM）的幻觉并增强它们是一项至关重要的任务。尽管一些现有方法采用模型自我增强技术，但它们无法有效解决未知的事实幻觉。使用知识图（KG）增强方法无法同时解决不同知识图源之间的泛化和开放式答案问题的增强。为了解决这些限制，提出了一个结合伪图生成和原子知识验证的框架。在开放式问答环境中使用 KG 来增强 LLM 是通过利用伪图生成来实现的。原子知识验证利用原子级的知识查询和验证来实现不同KG源下的泛化性。与基线相比，这种方法在开放式问题的 ROUGE-L 分数上至少提高了 11.5。对于精确问题，我们观察到准确度至少提高了 7.5。此外，还有证据表明该框架在不同的 KG 来源中表现出通用性。总之，我们的结果为通过合并伪知识图谱和多源知识图谱来增强法学硕士铺平了道路，特别是在开放式问题的背景下。</li>
</ul>

<h3>Title: BUSTER: a "BUSiness Transaction Entity Recognition" dataset</h3>
<ul>
<li><strong>Authors: </strong>Andrea Zugarini, Andrew Zamai, Marco Ernandes, Leonardo Rigutini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09916">https://arxiv.org/abs/2402.09916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09916">https://arxiv.org/pdf/2402.09916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09916]] BUSTER: a "BUSiness Transaction Entity Recognition" dataset(https://arxiv.org/abs/2402.09916)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. One of the reasons resides in the displacement between popular benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health. To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists of 3779 manually annotated documents on financial transactions. We establish several baselines exploiting both general-purpose and domain-specific language models. The best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to BUSTER.</li>
<li><strong>摘要：</strong>尽管自然语言处理在过去几年中取得了重大突破，但将这些进步转化为现实世界的业务案例可能具有挑战性。原因之一在于流行基准与实际数据之间的偏差。缺乏监管、类别不平衡、嘈杂的数据和冗长的文档往往会影响金融、法律和健康等垂直领域的实际问题。为了支持面向行业的研究，我们推出了 BUSTER，一个商业交易实体识别数据集。该数据集包含 3779 个有关金融交易的手动注释文档。我们利用通用和特定领域的语言模型建立了几个基线。性能最佳的模型还用于自动注释 6196 个文档，我们将其作为 BUSTER 的附加银语料库发布。</li>
</ul>

<h3>Title: Paying Attention to Deflections: Mining Pragmatic Nuances for  Whataboutism Detection in Online Discourse</h3>
<ul>
<li><strong>Authors: </strong>Khiem Phi, Noushin Salek Faramarzi, Chenlu Wang, Ritwik Banerjee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09934">https://arxiv.org/abs/2402.09934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09934">https://arxiv.org/pdf/2402.09934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09934]] Paying Attention to Deflections: Mining Pragmatic Nuances for  Whataboutism Detection in Online Discourse(https://arxiv.org/abs/2402.09934)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.</li>
<li><strong>摘要：</strong>Whataboutism 是扰乱叙事和散布不信任的有效工具，但在定量 NLP 研究中仍未得到充分探索。此外，过去的工作并没有区分其作为错误信息和宣传策略的用途与作为实用和语义框架工具的用途。我们引入了来自 Twitter 和 YouTube 的新数据集，揭示了Whataboutism、宣传和“tu quoque 谬论”之间的重叠和区别。此外，借鉴语言语义学的最新成果，我们将“what about”词汇结构与“whataboutism”区分开来。我们的实验揭示了其精确检测的独特挑战，促使引入一种使用注意力权重进行负样本挖掘​​的新方法。我们报告说，与 Twitter 和 YouTube 收藏中之前最先进的方法相比，分别有了 4% 和 10% 的显着改进。</li>
</ul>

<h3>Title: Generative AI in the Construction Industry: A State-of-the-art Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Tarek Zayed</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.HC, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09939">https://arxiv.org/abs/2402.09939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09939">https://arxiv.org/pdf/2402.09939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09939]] Generative AI in the Construction Industry: A State-of-the-art Analysis(https://arxiv.org/abs/2402.09939)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their own data, comprising steps such as data collection, dataset curation, training custom large language model (LLM), model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a generative model for querying contract documents. The results show that retrieval augmented generation (RAG) improves the baseline LLM by 5.2, 9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study provides academics and construction professionals with a comprehensive analysis and practical framework to guide the adoption of generative AI techniques to enhance productivity, quality, safety, and sustainability across the construction industry.</li>
<li><strong>摘要：</strong>建筑业是全球经济的重要部门，但在设计、规划、采购、检查和维护等各个流程中面临着许多生产力挑战。生成人工智能 (AI) 可以根据某些输入或先验知识创建新颖且真实的数据或内容，例如文本、图像、视频或代码，提供创新和颠覆性的解决方案来应对这些挑战。然而，有关建筑行业生成式人工智能的现状、机遇和挑战的文献还存在空白。本研究旨在通过对建筑中的生成人工智能进行最先进的分析来填补这一空白，其三个目标是：（1）审查和分类建筑行业现有和新兴的生成人工智能机遇和挑战； (2) 为建筑公司使用自己的数据构建定制的生成式人工智能解决方案提出一个框架，包括数据收集、数据集管理、训练定制大语言模型（LLM）、模型评估和部署等步骤； (3) 通过开发用于查询合同文档的生成模型的案例研究来演示该框架。结果表明，检索增强生成 (RAG) 在质量、相关性和可重复性方面将基线 LLM 提高了 5.2%、9.4% 和 4.8%。这项研究为学者和建筑专业人士提供了全面的分析和实践框架，以指导采用生成式人工智能技术，以提高整个建筑行业的生产力、质量、安全和可持续性。</li>
</ul>

<h3>Title: Explaining Probabilistic Models with Distributional Values</h3>
<ul>
<li><strong>Authors: </strong>Luca Franceschi, Michele Donini, Cédric Archambeau, Matthias Seeger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09947">https://arxiv.org/abs/2402.09947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09947">https://arxiv.org/pdf/2402.09947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09947]] Explaining Probabilistic Models with Distributional Values(https://arxiv.org/abs/2402.09947)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A large branch of explainable machine learning is grounded in cooperative game theory. However, research indicates that game-theoretic explanations may mislead or be hard to interpret. We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class). This paper addresses such gap for probabilistic models by generalising cooperative games and value operators. We introduce the distributional values, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs. We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models.</li>
<li><strong>摘要：</strong>可解释的机器学习的一个重要分支是以合作博弈论为基础的。然而，研究表明博弈论的解释可能会产生误导或难以解释。我们认为，人们希望解释的内容（例如分类器的输出）与 SHAP 等当前方法所解释的内容（例如类的标量概率）之间经常存在严重的不匹配。本文通过推广合作博弈和价值算子来解决概率模型的这种差距。我们引入分布值、跟踪模型输出变化的随机变量（例如预测类别的翻转），并导出高斯、伯努利和分类收益博弈的分析表达式。我们进一步建立了几个特征属性，并表明我们的框架通过视觉和语言模型的案例研究提供了细粒度和富有洞察力的解释。</li>
</ul>

<h3>Title: Multi-Word Tokenization for Sequence Compression</h3>
<ul>
<li><strong>Authors: </strong>Leonidas Gee, Leonardo Rigutini, Marco Ernandes, Andrea Zugarini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09949">https://arxiv.org/abs/2402.09949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09949">https://arxiv.org/pdf/2402.09949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09949]] Multi-Word Tokenization for Sequence Compression(https://arxiv.org/abs/2402.09949)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length and budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.</li>
<li><strong>摘要：</strong>事实证明，大型语言模型在建模各种任务方面非常成功。然而，这需要高昂的计算成本，阻碍了更广泛的工业应用。在此 pa005 per 中，我们介绍了 MWT：一种多字分词器，它通过将频繁的多字表达式表示为单个标记来超越单词边界。 MWT 产生更紧凑、更高效的标记化，从而带来两个好处：(1) 在给定固定序列长度和预算的情况下，由于输入数据的覆盖范围更大，因此性能得到提高； (2) 由于能够减少序列长度且性能下降可以忽略不计，推理速度更快、更轻。我们的结果表明，MWT 在较短的序列长度上更加稳健，因此可以通过早期序列截断实现大幅加速。</li>
</ul>

<h3>Title: Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of  In-Context Learning for Persona-based Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiashu Pu, Yajing Wan, Yuru Zhang, Jing Chen, Ling Cheng, Qian Shao, Yongzhu Chang, Tangjie Lv, Rongsheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09954">https://arxiv.org/abs/2402.09954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09954">https://arxiv.org/pdf/2402.09954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09954]] Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of  In-Context Learning for Persona-based Dialogue Generation(https://arxiv.org/abs/2402.09954)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demos, increasing the number of demos still improves dialogue performance, proving that LLMs can learn from corrupted dialogue demos. Previous explanations of the ICL mechanism, such as $n$-gram induction head, cannot fully account for this phenomenon.</li>
<li><strong>摘要：</strong>以往的上下文学习（ICL）研究主要集中在分类、机器翻译、text2table等任务上，而关于ICL是否能够改善类人对话生成的研究却很少。我们的工作通过系统地研究大型语言模型（LLM）在基于角色的对话生成中的 ICL 功能，对高质量的真实人类中文对话数据集进行广泛的实验来填补这一空白。根据实验结果，我们得出三个结论：1）调整提示指令是提高发电质量最直接、有效、经济的方法； 2）随机检索演示（demos）取得了最好的结果，可能是由于多样性和有效信息量更大；与直觉相反，检索具有与查询相同的上下文的演示效果最差； 3）即使我们破坏了演示中的多轮关联和单轮语义，增加演示的数量仍然可以提高对话性能，证明LLM可以从损坏的对话演示中学习。先前对 ICL 机制的解释，例如 $n$-gram 感应头，无法完全解释这一现象。</li>
</ul>

<h3>Title: Case Study: Testing Model Capabilities in Some Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Min Zhang, Sato Takumi, Jack Zhang, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09967">https://arxiv.org/abs/2402.09967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09967">https://arxiv.org/pdf/2402.09967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09967]] Case Study: Testing Model Capabilities in Some Reasoning Tasks(https://arxiv.org/abs/2402.09967)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in generating personalized content and facilitating interactive dialogues, showcasing their remarkable aptitude for a myriad of applications. However, their capabilities in reasoning and providing explainable outputs, especially within the context of reasoning abilities, remain areas for improvement. In this study, we delve into the reasoning abilities of LLMs, highlighting the current challenges and limitations that hinder their effectiveness in complex reasoning scenarios.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 擅长生成个性化内容和促进交互式对话，展示了它们在各种应用程序中的卓越能力。然而，他们的推理和提供可解释输出的能力，特别是在推理能力方面，仍然有待改进。在这项研究中，我们深入研究了法学硕士的推理能力，强调了当前阻碍其在复杂推理场景中有效性的挑战和限制。</li>
</ul>

<h3>Title: Fast Vocabulary Transfer for Language Model Compression</h3>
<ul>
<li><strong>Authors: </strong>Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, Paolo Torroni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09977">https://arxiv.org/abs/2402.09977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09977">https://arxiv.org/pdf/2402.09977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09977]] Fast Vocabulary Transfer for Language Model Compression(https://arxiv.org/abs/2402.09977)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.</li>
<li><strong>摘要：</strong>现实世界的业务应用程序需要在语言模型性能和大小之间进行权衡。我们提出了一种依赖词汇迁移的模型压缩新方法。我们在各种垂直领域和下游任务上评估该方法。我们的结果表明，词汇迁移可以有效地与其他压缩技术结合使用，从而显着减少模型大小和推理时间，同时略微影响性能。</li>
</ul>

<h3>Title: Symmetry-Breaking Augmentations for Ad Hoc Teamwork</h3>
<ul>
<li><strong>Authors: </strong>Ravi Hammond, Dustin Craggs, Mingyu Guo, Jakob Foerster, Ian Reid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09984">https://arxiv.org/abs/2402.09984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09984">https://arxiv.org/pdf/2402.09984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09984]] Symmetry-Breaking Augmentations for Ad Hoc Teamwork(https://arxiv.org/abs/2402.09984)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In many collaborative settings, artificial intelligence (AI) agents must be able to adapt to new teammates that use unknown or previously unobserved strategies. While often simple for humans, this can be challenging for AI agents. For example, if an AI agent learns to drive alongside others (a training set) that only drive on one side of the road, it may struggle to adapt this experience to coordinate with drivers on the opposite side, even if their behaviours are simply flipped along the left-right symmetry. To address this we introduce symmetry-breaking augmentations (SBA), which increases diversity in the behaviour of training teammates by applying a symmetry-flipping operation. By learning a best-response to the augmented set of teammates, our agent is exposed to a wider range of behavioural conventions, improving performance when deployed with novel teammates. We demonstrate this experimentally in two settings, and show that our approach improves upon previous ad hoc teamwork results in the challenging card game Hanabi. We also propose a general metric for estimating symmetry-dependency amongst a given set of policies.</li>
<li><strong>摘要：</strong>在许多协作环境中，人工智能 (AI) 代理必须能够适应使用未知或以前未观察到的策略的新队友。虽然这对于人类来说通常很简单，但对于人工智能代理来说可能具有挑战性。例如，如果人工智能代理学习与仅在道路一侧驾驶的其他人（训练集）一起驾驶，它可能很难适应这种体验以与另一侧的驾驶员协调，即使他们的行为只是翻转沿左右对称。为了解决这个问题，我们引入了对称破坏增强（SBA），它通过应用对称翻转操作来增加训练队友行为的多样性。通过学习对增强的队友集的最佳响应，我们的代理可以接触到更广泛的行为惯例，从而在与新队友一起部署时提高性能。我们在两种设置中通过实验证明了这一点，并表明我们的方法改进了之前在具有挑战性的纸牌游戏 Hanabi 中的临时团队合作结果。我们还提出了一个用于估计给定策略集之间的对称依赖性的通用指标。</li>
</ul>

<h3>Title: Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning  under Distribution Shifts</h3>
<ul>
<li><strong>Authors: </strong>Tobias Enders, James Harrison, Maximilian Schiffer</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09992">https://arxiv.org/abs/2402.09992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09992">https://arxiv.org/pdf/2402.09992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09992]] Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning  under Distribution Shifts(https://arxiv.org/abs/2402.09992)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We study the robustness of deep reinforcement learning algorithms against distribution shifts within contextual multi-stage stochastic combinatorial optimization problems from the operations research domain. In this context, risk-sensitive algorithms promise to learn robust policies. While this field is of general interest to the reinforcement learning community, most studies up-to-date focus on theoretical results rather than real-world performance. With this work, we aim to bridge this gap by formally deriving a novel risk-sensitive deep reinforcement learning algorithm while providing numerical evidence for its efficacy. Specifically, we introduce discrete Soft Actor-Critic for the entropic risk measure by deriving a version of the Bellman equation for the respective Q-values. We establish a corresponding policy improvement result and infer a practical algorithm. We introduce an environment that represents typical contextual multi-stage stochastic combinatorial optimization problems and perform numerical experiments to empirically validate our algorithm's robustness against realistic distribution shifts, without compromising performance on the training distribution. We show that our algorithm is superior to risk-neutral Soft Actor-Critic as well as to two benchmark approaches for robust deep reinforcement learning. Thereby, we provide the first structured analysis on the robustness of reinforcement learning under distribution shifts in the realm of contextual multi-stage stochastic combinatorial optimization problems.</li>
<li><strong>摘要：</strong>我们研究了运筹学领域的深度强化学习算法针对上下文多阶段随机组合优化问题中的分布变化的鲁棒性。在这种情况下，风险敏感算法有望学习稳健的策略。虽然强化学习社区普遍对该领域感兴趣，但最新的大多数研究都侧重于理论结果而不是现实世界的表现。通过这项工作，我们的目标是通过正式推导一种新颖的风险敏感深度强化学习算法来弥补这一差距，同时为其功效提供数值证据。具体来说，我们通过推导各个 Q 值的贝尔曼方程版本，引入离散 Soft Actor-Critic 来进行熵风险度量。我们建立了相应的策略改进结果并推断出实用的算法。我们引入了一个代表典型上下文多阶段随机组合优化问题的环境，并进行数值实验，以实证方式验证我们的算法针对实际分布变化的鲁棒性，而不会影响训练分布的性能。我们证明，我们的算法优于风险中性的 Soft Actor-Critic 以及两种稳健深度强化学习的基准方法。因此，我们首次对上下文多阶段随机组合优化问题领域中分布变化下强化学习的鲁棒性进行了结构化分析。</li>
</ul>

<h3>Title: LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed  Tasks in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, Fei Wu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09997">https://arxiv.org/abs/2402.09997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09997">https://arxiv.org/pdf/2402.09997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09997]] LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed  Tasks in the Wild(https://arxiv.org/abs/2402.09997)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing efficient batch inference to accommodate heterogeneous requests. Experimental results indicate that LoraRetriever consistently outperforms the baselines, highlighting its practical effectiveness and versatility.</li>
<li><strong>摘要：</strong>低秩适应 (LoRA) 为微调大型语言模型 (LLM) 提供了有效而高效的解决方案。 LoRA 的模块化和即插即用特性使得能够集成不同领域特定的 LoRA，从而增强法学硕士的能力。先前关于利用多个 LoRA 的研究要么侧重于特定的孤立下游任务，要么修复训练期间 LoRA 的选择。然而，在现实场景中，LLM 会收到涵盖不同任务的不同提示，并且候选 LoRA 池通常会动态更新。为了弥补这一差距，我们提出了 LoraRetriever，这是一个检索然后组合框架，可以根据输入提示自适应地检索和组合多个 LoRA。 LoraRetriever 包含三个主要组件：首先，识别和检索与给定输入相关的 LoRA；其次，制定有效整合检索到的LoRA的策略；第三，开发高效的批量推理来适应异构请求。实验结果表明，LoraRetriever 始终优于基线，凸显了其实际有效性和多功能性。</li>
</ul>

<h3>Title: Self-Augmented In-Context Learning for Unsupervised Word Translation</h3>
<ul>
<li><strong>Authors: </strong>Yaoyiran Li, Anna Korhonen, Ivan Vulić</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10024">https://arxiv.org/abs/2402.10024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10024">https://arxiv.org/pdf/2402.10024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10024]] Self-Augmented In-Context Learning for Unsupervised Word Translation(https://arxiv.org/abs/2402.10024)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised BLI performance, we also conduct comprehensive analyses on SAIL and discuss its limitations.</li>
<li><strong>摘要：</strong>最近的工作表明，虽然大型语言模型（LLM）在少数镜头设置中表现出强大的单词翻译或双语词典归纳（BLI）能力，但它们仍然无法在无监督场景中与“传统”基于映射的方法相媲美。没有可用的种子翻译对，特别是对于资源较低的语言。为了应对法学硕士的这一挑战，我们提出了用于无监督 BLI 的自我增强上下文学习（SAIL）：从零样本提示开始，SAIL 迭代地引入一组用于上下文学习的高置信度单词翻译对（ICL ）从法学硕士，然后以 ICL 方式重新应用于同一个法学硕士。我们的方法显示，在跨越多种语言对的两个已建立的 BLI 基准上，LLM 的零样本提示取得了显着的进步，并且全面优于基于映射的基准。除了实现最先进的无监督 BLI 性能外，我们还对 SAIL 进行全面分析并讨论其局限性。</li>
</ul>

<h3>Title: RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization  Method for Alignment of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, Prathap Ramachandra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10038">https://arxiv.org/abs/2402.10038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10038">https://arxiv.org/pdf/2402.10038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10038]] RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization  Method for Alignment of Large Language Models(https://arxiv.org/abs/2402.10038)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.</li>
<li><strong>摘要：</strong>基于人类反馈的强化学习 (RLHF) 已被广泛用于使大型语言模型与用户意图保持一致。然而，基于 RLHF 的近端策略优化 (PPO) 偶尔会不稳定，需要大量的超参数微调，并且在对齐过程中最大化估计奖励的计算成本很高。最近，提出了直接偏好优化（DPO）来解决这些挑战。然而，DPO 依赖于人类注释者和替代 LLM 生成的对比响应，而不是政策模型，限制了 RLHF 的有效性。在本文中，我们通过系统地结合拒绝采样 (RS) 和 DPO 来解决这两个挑战。我们提出的方法 RS-DPO 始于监督微调策略模型 (SFT) 的开发。每个提示的一组不同的 k 个响应是直接从 SFT 模型中采样的。 RS-DPO 根据奖励分布识别对比样本对。最后，我们将 DPO 与对比样本一起应用，以使模型符合人类偏好。我们的实验表明，我们提出的方法可以有效地在有限的资源环境下微调 LLM，从而提高与用户意图的一致性。此外，它的性能优于现有方法，包括 RS、PPO 和 DPO。</li>
</ul>

<h3>Title: SwissNYF: Tool Grounded LLM Agents for Black Box Setting</h3>
<ul>
<li><strong>Authors: </strong>Somnath Sendhil Kumar, Dhruv Jain, Eshaan Agarwal, Raunak Pandey</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10051">https://arxiv.org/abs/2402.10051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10051">https://arxiv.org/pdf/2402.10051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10051]] SwissNYF: Tool Grounded LLM Agents for Black Box Setting(https://arxiv.org/abs/2402.10051)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have demonstrated enhanced capabilities in function-calling, these advancements primarily rely on accessing the functions' responses. This methodology is practical for simpler APIs but faces scalability issues with irreversible APIs that significantly impact the system, such as a database deletion API. Similarly, processes requiring extensive time for each API call and those necessitating forward planning, like automated action pipelines, present complex challenges. Furthermore, scenarios often arise where a generalized approach is needed because algorithms lack direct access to the specific implementations of these functions or secrets to use them. Traditional tool planning methods are inadequate in these cases, compelling the need to operate within black-box environments. Unlike their performance in tool manipulation, LLMs excel in black-box tasks, such as program synthesis. Therefore, we harness the program synthesis capabilities of LLMs to strategize tool usage in black-box settings, ensuring solutions are verified prior to implementation. We introduce TOPGUN, an ingeniously crafted approach leveraging program synthesis for black box tool planning. Accompanied by SwissNYF, a comprehensive suite that integrates black-box algorithms for planning and verification tasks, addressing the aforementioned challenges and enhancing the versatility and effectiveness of LLMs in complex API interactions. The public code for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 已经展示了函数调用方面的增强功能，但这些进步主要依赖于访问函数的响应。这种方法对于更简单的 API 来说是实用的，但面临着不可逆 API 的可扩展性问题，这些问题会对系统产生重大影响，例如数据库删除 API。同样，每次 API 调用都需要大量时间的流程以及需要前瞻性规划的流程（例如自动化操作管道）也带来了复杂的挑战。此外，经常出现需要通用方法的场景，因为算法无法直接访问这些函数的特定实现或使用它们的秘密。传统的工具规划方法在这些情况下是不够的，迫使需要在黑盒环境中操作。与工具操作方面的表现不同，法学硕士在黑盒任务（例如程序综合）方面表现出色。因此，我们利用法学硕士的程序综合能力来制定黑盒设置中的工具使用策略，确保解决方案在实施之前得到验证。我们介绍 TOPGUN，这是一种巧妙设计的方法，利用程序综合进行黑盒工具规划。伴随着 SwissNYF，这是一个集成了用于规划和验证任务的黑盒算法的综合套件，解决了上述挑战并增强了法学硕士在复杂 API 交互中的多功能性和有效性。 SwissNYF 的公共代码位于 https://github.com/iclr-dummy-user/SwissNYF。</li>
</ul>

<h3>Title: Unmemorization in Large Language Models via Self-Distillation and  Deliberate Imagination</h3>
<ul>
<li><strong>Authors: </strong>Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vulić</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10052">https://arxiv.org/abs/2402.10052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10052">https://arxiv.org/pdf/2402.10052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10052]] Unmemorization in Large Language Models via Self-Distillation and  Deliberate Imagination(https://arxiv.org/abs/2402.10052)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While displaying impressive generation capabilities across many tasks, Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data. This raises an essential question: how should we prevent such undesired behavior of LLMs while maintaining their strong generation and natural language understanding (NLU) capabilities? In this work, we introduce a novel approach termed deliberate imagination in the context of LLM unlearning. Instead of trying to forget memorized data, we employ a self-distillation framework, guiding LLMs to deliberately imagine alternative scenarios. As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks. Our results demonstrate the usefulness of this approach across different models and sizes, and also with parameter-efficient fine-tuning, offering a novel pathway to addressing the challenges with private and sensitive data in LLM applications.</li>
<li><strong>摘要：</strong>虽然在许多任务中显示出令人印象深刻的生成能力，但大型语言模型 (LLM) 仍然面临隐私侵犯和敏感数据意外泄露等关键问题。这就提出了一个基本问题：我们应该如何防止法学硕士的这种不良行为，同时保持其强大的生成和自然语言理解（NLU）能力？在这项工作中，我们在法学硕士遗忘的背景下引入了一种称为刻意想象的新颖方法。我们没有试图忘记记忆的数据，而是采用了自我蒸馏框架，引导法学硕士有意识地想象替代场景。正如广泛的实验所证明的那样，所提出的方法不仅有效地忘记了目标文本，而且还保留了法学硕士在开放式生成任务以及 NLU 任务中的能力。我们的结果证明了这种方法在不同模型和规模上的实用性，并且通过参数高效的微调，为解决法学硕士应用程序中私人和敏感数据的挑战提供了一种新颖的途径。</li>
</ul>

<h3>Title: Towards Safer Large Language Models through Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10058">https://arxiv.org/abs/2402.10058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10058">https://arxiv.org/pdf/2402.10058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10058]] Towards Safer Large Language Models through Machine Unlearning(https://arxiv.org/abs/2402.10058)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的快速发展已经证明了它们在各个领域的巨大潜力，这归因于它们广泛的预训练知识和卓越的通用性。然而，法学硕士在遇到有问题的提示时经常会遇到生成有害内容的挑战。为了解决这个问题，现有的工作试图实施基于梯度上升的方法来防止法学硕士产生有害的输出。虽然这些方法可能有效，但它们经常影响模型响应正常提示的效用。为了解决这一差距，我们引入了选择性知识否定遗忘（SKU），这是一种针对法学硕士的新型遗忘框架，旨在消除有害知识，同时保留正常提示的实用性。具体来说，SKU由两个阶段组成：有害知识获取阶段和知识否定阶段。第一阶段旨在识别和获取模型中的有害知识，而第二阶段致力于消除这些知识。 SKU 有选择地隔离和删除模型参数中的有害知识，确保模型的性能在正常提示下保持稳健。我们在各种 LLM 架构上进行的实验表明，SKU 在删除有害信息和保留实用性之间找到了良好的平衡点。</li>
</ul>

<h3>Title: Both Matter: Enhancing the Emotional Intelligence of Large Language  Models without Compromising the General Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Weixiang Zhao, Zhuojun Li, Shilong Wang, Yang Wang, Yulin Hu, Yanyan Zhao, Chen Wei, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10073">https://arxiv.org/abs/2402.10073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10073">https://arxiv.org/pdf/2402.10073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10073]] Both Matter: Enhancing the Emotional Intelligence of Large Language  Models without Compromising the General Intelligence(https://arxiv.org/abs/2402.10073)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \underline{\textbf{Mo}}dular \underline{\textbf{E}}motional \underline{\textbf{I}}ntelligence enhancement method (\textbf{MoEI}), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs without compromise their GI. Extensive experiments on two representative LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness of MoEI to improving EI while maintain GI.</li>
<li><strong>摘要：</strong>情绪智能（EI）由情绪感知、情绪认知和情绪表达组成，对于当前基于大语言模型（LLM）的会话通用人工智能助理来说，在改善用户交互体验方面发挥着关键作用。之前的工作主要集中在通过对 EI 相关的分类或回归任务进行简单微调来提高他们的情绪感知能力。然而，这会导致 EI 的不完全增强和一般智力（GI）的灾难性遗忘。为此，我们首先引入 \textsc{EiBench}，它是文本到文本形成中 EI 相关任务的大规模集合，其任务指令涵盖了 EI 的所有三个方面，为全面的 EI 奠定了坚实的基础。法学硕士的 EI 增强。然后是一种新颖的 \underline{\textbf{Mo}}dular \underline{\textbf{E}}motional \underline{\textbf{I}}智能增强方法（\textbf{MoEI}），由模块化参数扩展和帧内组成-互调制，旨在全面增强LLM的EI而不损害其GI。对两个代表性的基于 LLM 的助手 Flan-T5 和 LLaMA-2-Chat 进行的广泛实验证明了 MoEI 在提高 EI 的同时保持 GI 的有效性。</li>
</ul>

<h3>Title: QUICK: Quantization-aware Interleaving and Conflict-free Kernel for  efficient LLM inference</h3>
<ul>
<li><strong>Authors: </strong>Taesu Kim, Jongho Lee, Daehyun Ahn, Sarang Kim, Jiwoong Choi, Minkyu Kim, Hyungjun Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10076">https://arxiv.org/abs/2402.10076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10076">https://arxiv.org/pdf/2402.10076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10076]] QUICK: Quantization-aware Interleaving and Conflict-free Kernel for  efficient LLM inference(https://arxiv.org/abs/2402.10076)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce QUICK, a group of novel optimized CUDA kernels for the efficient inference of quantized Large Language Models (LLMs). QUICK addresses the shared memory bank-conflict problem of state-of-the-art mixed precision matrix multiplication kernels. Our method interleaves the quantized weight matrices of LLMs offline to skip the shared memory write-back after the dequantization. We demonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger batches and up to 1.94x throughput gain on representative LLM models on various NVIDIA GPU devices.</li>
<li><strong>摘要：</strong>我们引入了 QUICK，这是一组新颖的优化 CUDA 内核，用于量化大型语言模型 (LLM) 的高效推理。 QUICK 解决了最先进的混合精度矩阵乘法内核的共享内存库冲突问题。我们的方法离线交错 LLM 的量化权重矩阵，以跳过去量化后的共享内存写回。我们展示了在较大批量上比现有 AutoAWQ 内核高达 1.91 倍的加速，以及在各种 NVIDIA GPU 设备上的代表性 LLM 模型上高达 1.94 倍的吞吐量增益。</li>
</ul>

<h3>Title: Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots  in Ophthalmology and LLM-based evaluation using GPT-4</h3>
<ul>
<li><strong>Authors: </strong>Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10083">https://arxiv.org/abs/2402.10083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10083">https://arxiv.org/pdf/2402.10083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10083]] Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots  in Ophthalmology and LLM-based evaluation using GPT-4(https://arxiv.org/abs/2402.10083)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Purpose: To assess the alignment of GPT-4-based evaluation to human clinician experts, for the evaluation of responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology questions and paired answers were created by ophthalmologists to represent commonly asked patient questions, divided into fine-tuning (368; 92%), and testing (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset, additional 8 glaucoma QnA pairs were included. 200 responses to the testing dataset were generated by 5 fine-tuned LLMs for evaluation. A customized clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on clinical accuracy, relevance, patient safety, and ease of understanding. GPT-4 evaluation was then compared against ranking by 5 clinicians for clinical alignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest (87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%), LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4 evaluation demonstrated significant agreement with human clinician rankings, with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80 respectively; while correlation based on Cohen Kappa was more modest at 0.50. Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical inaccuracies in the LLM-generated responses, which were appropriately identified by the GPT-4 evaluation. Conclusion: The notable clinical alignment of GPT-4 evaluation highlighted its potential to streamline the clinical evaluation of LLM chatbot responses to healthcare-related queries. By complementing the existing clinician-dependent manual grading, this efficient and automated evaluation could assist the validation of future developments in LLM applications for healthcare.</li>
<li><strong>摘要：</strong>目的：评估基于 GPT-4 的评估与人类临床专家的一致性，以评估对由微调的 LLM 聊天机器人生成的眼科相关患者查询的响应。方法：由眼科医生创建 400 个眼科问题和配对答案来代表患者常见问题，分为微调（368 个；92%）和测试（40 个；8%）。我们找到了 5 个不同的 LLM，包括 LLAMA2-7b、LLAMA2-7b-Chat、LLAMA2-13b 和 LLAMA2-13b-Chat。对于测试数据集，还包括另外 8 个青光眼 QnA 对。 5 个经过微调的法学硕士生成了 200 个对测试数据集的响应以进行评估。基于临床准确性、相关性、患者安全性和易于理解性，使用定制的临床评估标准来指导 GPT-4 评估。然后将 GPT-4 评估与 5 名临床医生的排名进行比较，以实现临床一致性。结果：在所有微调的 LLM 中，GPT-3.5 得分最高（87.1%），其次是 LLAMA2-13b（80.9%）、LLAMA2-13b-chat（75.5%）、LLAMA2-7b-Chat（70%）和LLAMA2-7b (68.8%) 基于 GPT-4 评估。 GPT-4 评估显示与人类临床医生排名显着一致，Spearman 和 Kendall Tau 相关系数分别为 0.90 和 0.80；而基于 Cohen Kappa 的相关性则较为温和，为 0.50。值得注意的是，定性分析和青光眼子分析揭示了 LLM 生成的反应中的临床不准确性，这些错误通过 GPT-4 评估得到了适当的识别。结论：GPT-4 评估的显着临床一致性凸显了其简化 LLM 聊天机器人对医疗保健相关查询响应的临床评估的潜力。通过补充现有的依赖临床医生的手动评分，这种高效、自动化的评估可以帮助验证医疗保健法学硕士应用的未来发展。</li>
</ul>

<h3>Title: GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on  Geometry Problem-Solving</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Zhang, Zhongzhi Li, Mingliang Zhang, Fei Yin, Chenglin Liu, Yashar Moshfeghi</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10104">https://arxiv.org/abs/2402.10104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10104">https://arxiv.org/pdf/2402.10104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10104]] GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on  Geometry Problem-Solving(https://arxiv.org/abs/2402.10104)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\% accuracy rate on the main subset but only a 6.00\% accuracy on the challenging subset. This highlights the critical need for testing models against datasets on which they have not been pre-trained. Additionally, our findings indicate that GPT-series models perform more effectively on problems they have rephrased, suggesting a promising method for enhancing model capabilities.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）和多模态模型（MM）的最新进展证明了它们在解决问题方面的卓越能力。然而，他们解决几何数学问题的能力尚未得到彻底评估，这需要对文本和视觉信息的综合理解。为了弥补这一差距，我们引入了 GeoEval 基准测试，这是一个综合集合，其中包括 2000 个问题的主要子集、一个专注于后向推理的 750 个问题子集、2000 个问题的增强子集以及 300 个问题的硬子集。该基准有助于更深入地研究法学硕士和硕士在解决几何数学问题方面的表现。我们对这些不同子集的十个法学硕士和硕士的评估表明，WizardMath 模型表现出色，在主要子集上达到了 55.67\% 的准确率，但在具有挑战性的子集上仅达到 6.00\% 的准确率。这凸显了针对未经预先训练的数据集测试模型的迫切需要。此外，我们的研究结果表明，GPT 系列模型在解决它们重新表述的问题上表现得更有效，这提出了一种增强模型能力的有前途的方法。</li>
</ul>

<h3>Title: Quantized Embedding Vectors for Controllable Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cheng Kang, Xinye Chen, Yong Hu, Daniel Novak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10107">https://arxiv.org/abs/2402.10107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10107">https://arxiv.org/pdf/2402.10107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10107]] Quantized Embedding Vectors for Controllable Diffusion Language Models(https://arxiv.org/abs/2402.10107)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Improving the controllability, portability, and inference speed of diffusion language models (DLMs) is a key challenge in natural language generation. While recent research has shown significant success in complex text generation with language models, the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models. To mitigate these issues, numerous well-established methods were proposed for neural network quantization. To further enhance their portability of independent deployment as well as improve their stability evaluated by language perplexity, we propose a novel approach called the Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via quantization. This leads to a gradient-based controller for the generation tasks, and more stable intermediate latent variables are obtained, which naturally brings in an accelerated convergence as well as better controllability. Additionally, the adaption fine-tuning method is employed to reduce tunable weights. Experimental results on five challenging fine-grained control tasks demonstrate that QE-CDLM compares favorably to existing methods in terms of quality and feasibility, achieving better perplexity and lightweight fine-tuning.</li>
<li><strong>摘要：</strong>提高扩散语言模型（DLM）的可控性、可移植性和推理速度是自然语言生成的关键挑战。尽管最近的研究在使用语言模型生成复杂文本方面取得了巨大成功，但对内存和计算能力的要求仍然很高，达不到预期，这自然会导致模型的可移植性低和不稳定。为了缓解这些问题，人们提出了许多行之有效的神经网络量化方法。为了进一步增强其独立部署的可移植性以及提高通过语言复杂性评估的稳定性，我们提出了一种称为量化嵌入可控扩散语言模型（QE-CDLM）的新方法。 QE-CDLM 基于最近成功的可控 DLM，通过量化重塑特定于任务的嵌入空间。这导致生成任务采用基于梯度的控制器，并获得更稳定的中间潜变量，这自然会带来加速收敛和更好的可控性。此外，采用自适应微调方法来减少可调权重。五种具有挑战性的细粒度控制任务的实验结果表明，QE-CDLM 在质量和可行性方面优于现有方法，实现了更好的困惑度和轻量级微调。</li>
</ul>

<h3>Title: Towards Reducing Diagnostic Errors with Interpretable Risk Prediction</h3>
<ul>
<li><strong>Authors: </strong>Denis Jered McInerney, William Dickinson, Lucy Flynn, Andrea Young, Geoffrey Young, Jan-Willem van de Meent, Byron C. Wallace</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10109">https://arxiv.org/abs/2402.10109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10109">https://arxiv.org/pdf/2402.10109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10109]] Towards Reducing Diagnostic Errors with Interpretable Risk Prediction(https://arxiv.org/abs/2402.10109)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.</li>
<li><strong>摘要：</strong>许多诊断错误的发生是因为临床医生无法轻松访问患者电子健康记录 (EHR) 中的相关信息。在这项工作中，我们提出了一种使用法学硕士来识别患者 EHR 数据中表明特定诊断风险增加或减少的证据的方法；我们的最终目标是增加证据的获取并减少诊断错误。特别是，我们提出了一种神经加性模型，可以在临床医生仍不确定的时间点进行基于个体化风险估计的证据支持的预测，旨在专门减轻诊断延迟和不完全差异引起的错误。为了训练这样的模型，有必要推断最终“真实”诊断的时间细粒度回顾性标签。我们与法学硕士一起这样做，以确保输入文本来自可以做出可靠诊断之前。我们使用法学硕士来检索初始证据池，然后根据模型学到的相关性来完善这组证据。我们通过模拟临床医生如何使用我们的方法在预定义的鉴别诊断列表之间做出决定，对我们的方法的有用性进行了深入评估。</li>
</ul>

<h3>Title: Selective Reflection-Tuning: Student-Selected Data Recycling for LLM  Instruction-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10110">https://arxiv.org/abs/2402.10110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10110">https://arxiv.org/pdf/2402.10110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10110]] Selective Reflection-Tuning: Student-Selected Data Recycling for LLM  Instruction-Tuning(https://arxiv.org/abs/2402.10110)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data. We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs. Our codes, models, and data will be released at https://github.com/tianyi-lab/Reflection_Tuning.</li>
<li><strong>摘要：</strong>指令调优对于大型语言模型 (LLM) 实现更好的指令跟踪和任务适应能力至关重要，但其成功在很大程度上依赖于训练数据质量。最近的许多方法都专注于提高数据质量，但常常忽视数据与正在微调的学生模型的兼容性。本文介绍了选择性反射调优，这是一种新颖的范式，它将教师法学硕士的反思和内省与学生法学硕士的数据选择能力相结合，以提高现有数据质量，以自动细化现有的指令调优数据。这种师生合作产生了高质量且与学生兼容的指令-响应对，从而实现了样本高效的指令调整和卓越性能的法学硕士。 Selective Reflection-Tuning 是一种数据增强和综合，通常可以提高 LLM 微调和自我改进，而无需收集全新数据。我们将我们的方法应用于 Alpaca 和 WizardLM 数据，并获得了更强大和顶级的 7B 和 13B LLM。我们的代码、模型和数据将在https://github.com/tianyi-lab/Reflection_Tuning发布。</li>
</ul>

<h3>Title: Zero-Shot Reasoning: Personalized Content Generation Without the Cold  Start Problem</h3>
<ul>
<li><strong>Authors: </strong>Davor Hafnar (1), Jure Demšar (1 and 2) ((1) Faculty of Computer and Information Science, University of Ljubljana (2) Department of Psychology, Faculty of Arts, University of Ljubljana)</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10133">https://arxiv.org/abs/2402.10133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10133">https://arxiv.org/pdf/2402.10133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10133]] Zero-Shot Reasoning: Personalized Content Generation Without the Cold  Start Problem(https://arxiv.org/abs/2402.10133)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Procedural content generation uses algorithmic techniques to create large amounts of new content for games at much lower production costs. In newer approaches, procedural content generation utilizes machine learning. However, these methods usually require expensive collection of large amounts of data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive. The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with large language models. Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it. Therefore, this paper presents a novel approach to achieving personalization by using large language models to propose levels based on the gameplay data continuously collected from individual players. We compared the levels generated using our approach with levels generated with more traditional procedural generation techniques. Our easily reproducible method has proven viable in a production setting and outperformed levels generated by traditional methods in the probability that a player will not quit the game mid-level.</li>
<li><strong>摘要：</strong>程序内容生成使用算法技术以低得多的生产成本为游戏创建大量新内容。在较新的方法中，程序内容生成利用机器学习。然而，这些方法通常需要昂贵的大量数据收集，以及相当复杂的学习模型的开发和训练，这可能非常耗时且昂贵。我们研究的核心是探索是否可以通过更实用和更通用的大型语言模型方法来降低使用个性化程序内容生成的障碍。将游戏内容与玩家偏好相匹配，对更喜欢游戏的玩家和开发者都有利，后者越来越依赖于玩家享受游戏才能将其货币化。因此，本文提出了一种实现个性化的新方法，即使用大型语言模型根据从个体玩家持续收集的游戏数据来建议关卡。我们将使用我们的方法生成的级别与使用更传统的程序生成技术生成的级别进行了比较。我们易于重现的方法已被证明在生产环境中是可行的，并且在玩家不会中途退出游戏的可能性方面优于传统方法生成的关卡。</li>
</ul>

<h3>Title: TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles</h3>
<ul>
<li><strong>Authors: </strong>Yinhong Liu, Yimai Fang, David Vandyke, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10137">https://arxiv.org/abs/2402.10137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10137">https://arxiv.org/pdf/2402.10137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10137]] TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles(https://arxiv.org/abs/2402.10137)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In light of recent advances in large language models~(LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios. However, the creation of high-quality annotated data for Task-Oriented Dialog~(TOD) is recognized to be slow and costly. To address these challenges, we introduce Task-Oriented Automatic Dialogs~(TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline. The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options. Two aspects of system response styles are considered, verbosity level and users' expression mirroring. We benchmark TOAD on two response generation tasks and the results show that modeling more verbose or responses without user expression mirroring is more challenging.</li>
<li><strong>摘要：</strong>鉴于大型语言模型（LLM）的最新进展，对下一代虚拟助手的期望包括增强跨不同使用场景的自然性和适应性。然而，为面向任务的对话（TOD）创建高质量的注释数据被认为是缓慢且昂贵的。为了应对这些挑战，我们引入了面向任务的自动对话（TOAD），这是一种新颖且可扩展的 TOD 数据集及其自动生成管道。 TOAD 数据集模拟真实的应用程序上下文交互，并提供各种系统响应样式选项。考虑系统响应风格的两个方面：详细程度和用户的表达镜像。我们在两个响应生成任务上对 TOAD 进行了基准测试，结果表明，在没有用户表情镜像的情况下对更详细或响应进行建模更具挑战性。</li>
</ul>

<h3>Title: ControlLM: Crafting Diverse Personalities for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Weng, Shizhu He, Kang Liu, Shengping Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10151">https://arxiv.org/abs/2402.10151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10151">https://arxiv.org/pdf/2402.10151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10151]] ControlLM: Crafting Diverse Personalities for Language Models(https://arxiv.org/abs/2402.10151)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As language models continue to scale in size and capability, they display an array of emerging behaviors, both beneficial and concerning. This heightens the need to control model behaviors. We hope to be able to control the personality traits of language models at the inference-time so as to have various character features, on top of which the requirements of different types of tasks can be met. Personality is a higher-level and more abstract behavioral representation for language models. We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference. This approach allows for the precise, real-time adjustment of model behavior. First, we demonstrate ControlLM's capacity to elicit diverse persona behaviors without any training, while precision control allows personality traits to closely match average human values. Subsequently, we showcase improved reasoning and question answering through selective amplification of beneficial attributes like conscientiousness and friendliness. We hope that this work will inspire research on controlling human-like behaviors of language models and provide insights for future research. Our code is publicly available at: https://github.com/wengsyx/ControlLM.</li>
<li><strong>摘要：</strong>随着语言模型的规模和能力不断扩大，它们表现出一系列新兴的行为，既有有益的，也有令人担忧的。这增加了控制模型行为的需要。我们希望能够在推理时控制语言模型的性格特征，使其具有多种性格特征，满足不同类型任务的要求。个性是语言模型的一种更高层次、更抽象的行为表示。我们引入了 ControlLM，它利用源自模型潜在空间中对比行为提示的差异激活模式来影响模型在推理时的个性特征。这种方法可以精确、实时地调整模型行为。首先，我们证明了 ControlLM 无需任何训练即可引发不同角色行为的能力，而精确控制则可以使人格特质与人类平均价值观紧密匹配。随后，我们通过选择性放大责任心和友善等有益属性来展示改进的推理和问题回答。我们希望这项工作能够启发控制语言模型的类人行为的研究，并为未来的研究提供见解。我们的代码公开于：https://github.com/wengsyx/ControlLM。</li>
</ul>

<h3>Title: Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study  for Diabetes Patients</h3>
<ul>
<li><strong>Authors: </strong>Mahyar Abbasian, Zhongqi Yang, Elahe Khatibi, Pengfei Zhang, Nitish Nagesh, Iman Azimi, Ramesh Jain, Amir M. Rahmani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10153">https://arxiv.org/abs/2402.10153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10153">https://arxiv.org/pdf/2402.10153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10153]] Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study  for Diabetes Patients(https://arxiv.org/abs/2402.10153)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabetes-related questions on daily meal choices and assessing the potential risks associated with the suggested diet. Our findings show that the proposed agent demonstrates superior performance in generating responses to manage essential nutrients.</li>
<li><strong>摘要：</strong>有效的糖尿病管理对于维持糖尿病患者的健康至关重要。大语言模型 (LLM) 为糖尿病管理开辟了新途径，提高了其疗效。然而，当前基于法学硕士的方法因其对一般来源的依赖以及缺乏与特定领域知识的整合而受到限制，导致响应不准确。在本文中，我们为糖尿病患者提出了一种充满知识、由法学硕士驱动的对话健康代理（CHA）。我们定制并利用开源 openCHA 框架，通过外部知识和分析能力增强我们的 CHA。这种整合涉及两个关键组成部分：1) 纳入美国糖尿病协会饮食指南和 Nutritionix 信息；2) 部署分析工具，实现营养摄入量计算并与指南进行比较。我们将提议的 CHA 与 GPT4 进行比较。我们的评估包括 100 个与糖尿病相关的每日膳食选择问题，并评估与建议饮食相关的潜在风险。我们的研究结果表明，所提出的药剂在产生管理必需营养素的反应方面表现出卓越的性能。</li>
</ul>

<h3>Title: Data Engineering for Scaling Language Models to 128K Context</h3>
<ul>
<li><strong>Authors: </strong>Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, Hao Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10171">https://arxiv.org/abs/2402.10171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10171">https://arxiv.org/pdf/2402.10171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10171]] Data Engineering for Scaling Language Models to 128K Context(https://arxiv.org/abs/2402.10171)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, long context</a></li>
<li><strong>Abstract: </strong>We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \textit{quantity} and \textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \textit{domain balance} and \textit{length upsampling}. Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.</li>
<li><strong>摘要：</strong>我们研究了将语言模型的上下文长度扩展到 128K 的持续预训练方法，重点是数据工程。我们假设长上下文建模，特别是 \textit{在任意输入位置利用信息的能力}，是一种大部分已经通过大规模预训练获得的能力，并且这种能力可以很容易地扩展到比通过对适当的数据混合进行轻量级连续预训练，可以在训练过程中看到（例如，4K 到 128K）。我们研究了连续预训练数据的 \textit{quantity} 和 \textit{quality}：（1）对于数量，我们表明 5 亿到 50 亿个 token 足以使模型能够在 128K 上下文中的任何位置检索信息; （2）对于质量，我们的结果同样强调\textit{域平衡}和\textit{长度上采样}。具体来说，我们发现在某些领域（例如书籍）上天真地对更长的数据进行上采样（现有工作的常见做法）会带来次优的性能，并且平衡的领域混合很重要。我们证明，在此类数据的 1B-5B 标记上对完整模型进行持续预训练是将语言模型的上下文长度扩展到 128K 的有效且经济实惠的策略。我们的配方优于强大的开源长上下文模型，并缩小了与 GPT-4 128K 等前沿模型的差距。</li>
</ul>

<h3>Title: OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ali AhmadiTeshnizi, Wenzhi Gao, Madeleine Udell</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10172">https://arxiv.org/abs/2402.10172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10172">https://arxiv.org/pdf/2402.10172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10172]] OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large  Language Models(https://arxiv.org/abs/2402.10172)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20\%$ and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30\%$.</li>
<li><strong>摘要：</strong>优化问题在从制造、分销到医疗保健的各个领域普遍存在。然而，大多数此类问题仍然是通过手工启发式解决，而不是通过最先进的求解器进行优化，因为制定和解决这些问题所需的专业知识限制了优化工具和技术的广泛采用。本文介绍了 OptiMUS，这是一种基于大型语言模型 (LLM) 的代理，旨在根据自然语言描述来制定和解决（混合整数）线性规划问题。 OptiMUS 可以开发数学模型，编写和调试求解器代码，评估生成的解决方案，并根据这些评估改进其模型和代码。 OptiMUS采用模块化结构来处理问题，使其能够处理长描述和复杂数据的问题，而无需长提示。实验表明，OptiMUS 在简单数据集上的性能优于现有最先进的方法超过 20\%$，在困难数据集（包括与本文一起发布的新数据集 NLP4LP，其特点是长期且复杂的问题）上优于现有最先进的方法超过 $20\%$ $30\%$。</li>
</ul>

<h3>Title: Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for  Positional Discourse Coherence</h3>
<ul>
<li><strong>Authors: </strong>Yinhong Liu, Yixuan Su, Ehsan Shareghi, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10175">https://arxiv.org/abs/2402.10175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10175">https://arxiv.org/pdf/2402.10175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10175]] Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for  Positional Discourse Coherence(https://arxiv.org/abs/2402.10175)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) have shown remarkable performance in aligning generated text with user intentions across various tasks. When it comes to long-form text generation, there has been a growing interest in generation from a discourse coherence perspective. However, existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence. The development of discourse-specific automatic evaluation methods for assessing the output of LLMs warrants greater focus and exploration. In this paper, we present a novel automatic metric designed to quantify the discourse divergence between two long-form articles. Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and GPT-4 coherence evaluation, outperforming existing evaluation methods.</li>
<li><strong>摘要：</strong>最近的大型语言模型（LLM）在跨各种任务将生成的文本与用户意图保持一致方面表现出了卓越的性能。当谈到长文本生成时，人们对从话语连贯性角度生成文本越来越感兴趣。然而，现有的词汇或语义指标（例如 BLEU、ROUGE、BertScore）无法有效捕获话语连贯性。用于评估法学硕士产出的特定于话语的自动评估方法的开发值得更多的关注和探索。在本文中，我们提出了一种新颖的自动度量，旨在量化两篇长篇文章之间的话语分歧。对来自代表性领域的三个数据集进行的广泛实验表明，我们的指标更符合人类偏好和 GPT-4 一致性评估，优于现有的评估方法。</li>
</ul>

<h3>Title: OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset</h3>
<ul>
<li><strong>Authors: </strong>Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10176">https://arxiv.org/abs/2402.10176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10176">https://arxiv.org/pdf/2402.10176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10176]] OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset(https://arxiv.org/abs/2402.10176)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.</li>
<li><strong>摘要：</strong>最近的工作表明，综合生成的数据集在训练大型语言模型（LLM）方面具有巨大潜力，特别是在获取目标技能方面。当前的大规模数学指令调整数据集，例如 MetaMathQA（Yu 等人，2024）和 MAmmoTH（Yue 等人，2024）是使用具有商业限制性许可的闭源 LLM 的输出构建的。限制在这些数据生成管道中使用开源法学硕士的一个关键原因是最好的闭源法学硕士（例如 GPT-4）和最好的开源法学硕士的数学技能之间存在巨大差距。基于开源 LLM 的最新进展、我们提出的提示新颖性和一些强力扩展，我们构建了 OpenMathInstruct-1，这是一个包含 180 万个问题解决方案对的数学指令调整数据集。该数据集是通过综合 GSM8K 和 MATH（两种流行的数学推理基准）的代码解释器解决方案而构建的，使用最近发布并获得许可的 Mixtral 模型。我们的最佳模型 OpenMath-CodeLlama-70B 在 OpenMathInstruct-1 的子集上进行训练，在 GSM8K 上获得了 84.6% 的分数，在 MATH 上获得了 50.7% 的分数，这与最好的 gpt 蒸馏模型具有竞争力。我们在商业许可下发布我们的代码、模型和 OpenMathInstruct-1 数据集。</li>
</ul>

<h3>Title: Large Scale Constrained Clustering With Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Benedikt Schesch, Marco Caserta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10177">https://arxiv.org/abs/2402.10177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10177">https://arxiv.org/pdf/2402.10177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10177]] Large Scale Constrained Clustering With Reinforcement Learning(https://arxiv.org/abs/2402.10177)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Given a network, allocating resources at clusters level, rather than at each node, enhances efficiency in resource allocation and usage. In this paper, we study the problem of finding fully connected disjoint clusters to minimize the intra-cluster distances and maximize the number of nodes assigned to the clusters, while also ensuring that no two nodes within a cluster exceed a threshold distance. While the problem can easily be formulated using a binary linear model, traditional combinatorial optimization solvers struggle when dealing with large-scale instances. We propose an approach to solve this constrained clustering problem via reinforcement learning. Our method involves training an agent to generate both feasible and (near) optimal solutions. The agent learns problem-specific heuristics, tailored to the instances encountered in this task. In the results section, we show that our algorithm finds near optimal solutions, even for large scale instances.</li>
<li><strong>摘要：</strong>给定一个网络，在集群级别而不是在每个节点分配资源可以提高资源分配和使用的效率。在本文中，我们研究了寻找完全连接的不相交簇的问题，以最小化簇内距离并最大化分配给簇的节点数量，同时确保簇内没有两个节点超过阈值距离。虽然可以使用二元线性模型轻松地表述该问题，但传统的组合优化求解器在处理大规模实例时会遇到困难。我们提出了一种通过强化学习来解决这个约束聚类问题的方法。我们的方法包括训练代理生成可行的和（接近）最优的解决方案。代理学习针对该任务中遇到的实例定制的特定问题启发法。在结果部分，我们表明我们的算法即使对于大规模实例也能找到接近最优的解决方案。</li>
</ul>

<h3>Title: TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and  Agent Generation</h3>
<ul>
<li><strong>Authors: </strong>Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10178">https://arxiv.org/abs/2402.10178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10178">https://arxiv.org/pdf/2402.10178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10178]] TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and  Agent Generation(https://arxiv.org/abs/2402.10178)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench is designed to assess agents' abilities in memory, planning, and tool usage across tasks of varying complexity. Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios.</li>
<li><strong>摘要：</strong>像 ChatGPT 这样的大型语言模型 (LLM) 的出现激发了基于 LLM 的代理的开发，这些代理能够解决复杂的现实世界任务。然而，由于方法上的限制，例如错误传播和有限的适应性，这些代理在任务执行过程中经常陷入困境。为了解决这个问题，我们提出了一种基于动态任务分解和代理生成（TDAG）的多代理框架。该框架动态地将复杂的任务分解为更小的子任务，并将每个子任务分配给专门生成的子代理，从而增强了对各种和不可预测的现实世界任务的适应性。同时，现有的基准通常缺乏评估复杂、多步骤任务中增量进度所需的粒度。为此，我们在旅行规划的背景下引入了 ItineraryBench，其特点是相互关联、逐渐复杂的任务和细粒度的评估系统。 ItineraryBench 旨在评估代理在不同复杂性任务中的记忆、规划和工具使用能力。我们的实验结果表明，TDAG 显着优于既定基线，展示了其在复杂任务场景中卓越的适应性和上下文感知能力。</li>
</ul>

<h3>Title: Rethinking Information Structures in RLHF: Reward Generalization from a  Graph Theory Perspective</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou, Han Yang, Josef Dai, Xuehai Pan, Yaodong Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.DM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10184">https://arxiv.org/abs/2402.10184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10184">https://arxiv.org/pdf/2402.10184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10184]] Rethinking Information Structures in RLHF: Reward Generalization from a  Graph Theory Perspective(https://arxiv.org/abs/2402.10184)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of our analysis is the superiority of the tree-based information structure in reward modeling, compared to chain-based baselines adopted by conventional RLHF methods. We derive that under highly complex contexts with limited data, the tree-based reward model (RM) induces up to $\Theta(\log n/\log\log n)$ times less variance than chain-based RM where $n$ is the dataset size. To validate our theoretical contribution, we demonstrate that on three different NLP tasks, the tree-based RM achieves 65% win rate on average against chain-based baselines. Looking forward, we hope our framework can serve as a step towards understanding goal misgeneralization.</li>
<li><strong>摘要：</strong>基于人类反馈的强化学习（RLHF）存在一个三难困境：高度多样化的上下文、低标记成本和可靠的对齐性能之间的不兼容。在这里，我们的目标是通过奖励建模期间数据集信息结构的设计来减轻这种不兼容性。具体来说，我们首先重新审视 RLHF 过程，并提出一个理论框架，将其描述为文本分布上的自动编码过程。我们的框架形式化了 RLHF 目标，即确保人类偏好和大语言模型 (LLM) 行为之间的分布一致性。在此框架的基础上，我们系统地研究了 RLHF 奖励建模阶段信息结构对性能的影响。为了进一步理解奖励建模阶段的奖励泛化，我们引入了一种基于随机图论的新方法，该方法对语义空间中的泛化进行建模。我们分析的一个关键见解是，与传统 RLHF 方法采用的基于链的基线相比，基于树的信息结构在奖励建模中具有优越性。我们得出结论，在数据有限的高度复杂的环境下，基于树的奖励模型 (RM) 产生的方差比基于链的 RM 低高达 $\Theta(\log n/\log\log n)$ 倍，其中 $n$是数据集大小。为了验证我们的理论贡献，我们证明了在三个不同的 NLP 任务中，基于树的 RM 相对于基于链的基线平均获得了 65% 的胜率。展望未来，我们希望我们的框架能够成为理解目标错误概括的一步。</li>
</ul>

<h3>Title: Uncertainty Decomposition and Quantification for In-Context Learning of  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10189">https://arxiv.org/abs/2402.10189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10189">https://arxiv.org/pdf/2402.10189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10189]] Uncertainty Decomposition and Quantification for In-Context Learning of  Large Language Models(https://arxiv.org/abs/2402.10189)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: \url{https://github.com/lingchen0331/UQ_ICL}.</li>
<li><strong>摘要：</strong>上下文学习已成为大型语言模型（LLM）的突破性能力，并通过在提示中提供一些与任务相关的演示而彻底改变了各个领域。然而，LLM回应的可信问题，例如幻觉，也得到了积极的讨论。现有的工作一直致力于量化法学硕士反应的不确定性，但它们往往忽视了法学硕士的复杂性和情境学习的独特性。在这项工作中，我们深入研究了与情境学习相关的法学硕士的预测不确定性，强调这种不确定性可能源于所提供的演示（任意不确定性）和与模型配置相关的模糊性（认知不确定性）。我们提出了一种新颖的公式和相应的估计方法来量化这两种类型的不确定性。所提出的方法提供了一种无监督的方式来以即插即用的方式理解上下文学习的预测。进行了大量的实验来证明分解的有效性。代码和数据可在：\url{https://github.com/lingchen0331/UQ_ICL} 获取。</li>
</ul>

<h3>Title: Multi-Excitation Projective Simulation with a Many-Body Physics Inspired  Inductive Bias</h3>
<ul>
<li><strong>Authors: </strong>Philip A. LeMaitre, Marius Krumm, Hans J. Briegel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DM, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10192">https://arxiv.org/abs/2402.10192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10192">https://arxiv.org/pdf/2402.10192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10192]] Multi-Excitation Projective Simulation with a Many-Body Physics Inspired  Inductive Bias(https://arxiv.org/abs/2402.10192)</code><input type="text"></li>
<li><strong>Keywords: </strong>chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them. While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for a dynamic hypergraph is put forward to describe the agent's training history along with applications to AI and hypergraph visualization. An inductive bias inspired by the remarkably successful few-body interaction models used in quantum many-body physics is formalized for our classical mePS framework and employed to tackle the exponential complexity associated with naive implementations of hypergraphs. We prove that our inductive bias reduces the complexity from exponential to polynomial, with the exponent representing the cutoff on how many particles can interact. We numerically apply our method to two toy environments and a more complex scenario modelling the diagnosis of a broken computer. These environments demonstrate the resource savings provided by an appropriate choice of inductive bias, as well as showcasing aspects of interpretability. A quantum model for mePS is also briefly outlined and some future directions for it are discussed.</li>
<li><strong>摘要：</strong>随着深度学习的惊人进步，依赖机器学习的应用程序越来越多地融入日常生活中。然而，大多数深度学习模型都具有不透明、类似神谕的性质，因此很难解释和理解它们的决策。这个问题导致了可解释人工智能（XAI）领域的发展。该领域的一种称为投影模拟 (PS) 的方法将思想链建模为粒子在图上的随机游走，图上的顶点附加了概念。虽然这种描述有多种好处，包括量化的可能性，但它不能自然地用于对同时结合多个概念的思想进行建模。为了克服这一限制，我们引入了多激励投影模拟（mePS），这种概括将思想链视为超图上多个粒子的随机游走。提出了动态超图的定义来描述代理的训练历史以及人工智能和超图可视化的应用。受量子多体物理中使用的非常成功的少体相互作用模型的启发，归纳偏差被形式化为我们的经典 mePS 框架，并用于解决与超图的简单实现相关的指数复杂性。我们证明，归纳偏差将复杂性从指数降低到多项式，其中指数代表可以相互作用的粒子数量的截止值。我们以数字方式将我们的方法应用于两个玩具环境和一个更复杂的场景，对损坏的计算机进行诊断建模。这些环境展示了通过适当选择归纳偏差可以节省资源，并展示可解释性的各个方面。还简要概述了 mePS 的量子模型，并讨论了它的一些未来方向。</li>
</ul>

<h3>Title: BitDelta: Your Fine-Tune May Only Be Worth One Bit</h3>
<ul>
<li><strong>Authors: </strong>James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10193">https://arxiv.org/abs/2402.10193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10193">https://arxiv.org/pdf/2402.10193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10193]] BitDelta: Your Fine-Tune May Only Be Worth One Bit(https://arxiv.org/abs/2402.10193)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常分两个阶段进行训练：对大型互联网规模数据集进行预训练，以及对下游任务进行微调。考虑到预训练的计算要求较高，可以直观地假设微调向模型添加的新信息较少，因此更可压缩。我们通过将微调模型的权重分解为其预训练的组件和附加增量来探索这一假设。我们引入了一种简单的方法 BitDelta，它成功地将这个增量量化为 1 位，而不会影响性能。这一有趣的发现不仅凸显了微调过程中添加的信息的潜在冗余，而且对微调模型的多租户服务和多租户存储具有重要意义。通过使用单个高精度基础模型以及多个 1 位增量，BitDelta 将 GPU 内存需求显着降低了 10 倍以上，这也可以转化为多租户设置中增强的生成延迟。我们通过 Llama-2 和 Mistral 模型系列以及高达 70B 参数的模型的实验来验证 BitDelta，在所有测试设置中展示了最小的性能下降。</li>
</ul>

<h3>Title: A Trembling House of Cards? Mapping Adversarial Attacks against Language  Agents</h3>
<ul>
<li><strong>Authors: </strong>Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10196">https://arxiv.org/abs/2402.10196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10196">https://arxiv.org/pdf/2402.10196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10196]] A Trembling House of Cards? Mapping Adversarial Attacks against Language  Agents(https://arxiv.org/abs/2402.10196)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Language agents powered by large language models (LLMs) have seen exploding development. Their capability of using language as a vehicle for thought and communication lends an incredible level of flexibility and versatility. People have quickly capitalized on this capability to connect LLMs to a wide range of external components and environments: databases, tools, the Internet, robotic embodiment, etc. Many believe an unprecedentedly powerful automation technology is emerging. However, new automation technologies come with new safety risks, especially for intricate systems like language agents. There is a surprisingly large gap between the speed and scale of their development and deployment and our understanding of their safety risks. Are we building a house of cards? In this position paper, we present the first systematic effort in mapping adversarial attacks against language agents. We first present a unified conceptual framework for agents with three major components: Perception, Brain, and Action. Under this framework, we present a comprehensive discussion and propose 12 potential attack scenarios against different components of an agent, covering different attack strategies (e.g., input manipulation, adversarial demonstrations, jailbreaking, backdoors). We also draw connections to successful attack strategies previously applied to LLMs. We emphasize the urgency to gain a thorough understanding of language agent risks before their widespread deployment.</li>
<li><strong>摘要：</strong>由大型语言模型 (LLM) 提供支持的语言代理取得了爆炸式的发展。他们使用语言作为思维和交流工具的能力提供了令人难以置信的灵活性和多功能性。人们很快就利用这种能力将法学硕士连接到各种外部组件和环境：数据库、工具、互联网、机器人实施例等。许多人相信一种前所未有的强大自动化技术正在出现。然而，新的自动化技术带来了新的安全风险，特别是对于语言代理等复杂系统。它们的开发和部署的速度和规模与我们对其安全风险的理解之间存在着惊人的巨大差距。我们正在建造一座纸牌屋吗？在这篇立场文件中，我们提出了在映射针对语言代理的对抗性攻击方面的第一个系统性工作。我们首先提出一个统一的智能体概念框架，包含三个主要组成部分：感知、大脑和行动。在此框架下，我们进行了全面的讨论，并针对代理的不同组件提出了 12 种潜在的攻击场景，涵盖了不同的攻击策略（例如输入操纵、对抗性演示、越狱、后门）。我们还与之前应用于法学硕士的成功攻击策略建立了联系。我们强调在广泛部署语言代理风险之前彻底了解其风险的紧迫性。</li>
</ul>

<h3>Title: Chain-of-Thought Reasoning Without Prompting</h3>
<ul>
<li><strong>Authors: </strong>Xuezhi Wang, Denny Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10200">https://arxiv.org/abs/2402.10200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10200">https://arxiv.org/pdf/2402.10200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10200]] Chain-of-Thought Reasoning Without Prompting(https://arxiv.org/abs/2402.10200)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding substantially outperforms the standard greedy decoding.</li>
<li><strong>摘要：</strong>在增强大语言模型（LLM）的推理能力方面，先前的研究主要集中在特定的提示技术上，例如少样本或零样本思维链（CoT）提示。这些方法虽然有效，但通常涉及手动密集型提示工程。我们的研究采用了一种新颖的方法，提出了这样的问题：法学硕士能否在没有提示的情况下有效推理？有趣的是，我们的研究结果表明，只需改变 \textit{decoding} 过程，就可以从预先训练的 LLM 中导出 CoT 推理路径。我们不是采用传统的贪婪解码，而是研究 top-$k$ 替代标记，发现 CoT 路径通常是这些序列中固有的。这种方法不仅绕过了提示的混杂因素，而且还允许我们评估法学硕士的 \textit{内在} 推理能力。此外，我们观察到解码路径中 CoT 的存在与模型解码答案的较高置信度相关。该置信度度量有效区分 CoT 和非 CoT 路径。对各种推理基准的广泛实证研究表明，所提出的 CoT 解码大大优于标准贪婪解码。</li>
</ul>

<h3>Title: Rewards-in-Context: Multi-objective Alignment of Foundation Models with  Dynamic Preference Adjustment</h3>
<ul>
<li><strong>Authors: </strong>Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, Jianshu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10207">https://arxiv.org/abs/2402.10207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10207">https://arxiv.org/pdf/2402.10207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10207]] Rewards-in-Context: Multi-objective Alignment of Foundation Models with  Dynamic Preference Adjustment(https://arxiv.org/abs/2402.10207)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around $10\%$ GPU hours compared with multi-objective RL baseline.</li>
<li><strong>摘要：</strong>我们考虑基础模型与人类偏好的多目标对齐问题，这是迈向有用且无害的人工智能系统的关键一步。然而，使用强化学习（RL）微调大型基础模型通常成本高昂且不稳定，并且人类偏好的多维性、异质性和冲突性质进一步使对齐过程变得复杂。在本文中，我们引入了上下文奖励（RiC），它在其提示上下文中调节基础模型对多个奖励的响应，并应用监督微调来进行调整。 RiC的显着特点是简单性和适应性，因为它只需要对单个基础模型进行监督微调，并支持在推理期间根据用户偏好进行动态调整。受抽象凸优化问题解析解的启发，我们的动态推理时间调整方法接近多目标的帕累托最优解。经验证据表明，与多目标 RL 基线相比，我们的方法在调整大型语言模型 (LLM) 和扩散模型以适应不同奖励方面的有效性，仅花费约 10\%$ 的 GPU 小时。</li>
</ul>

<h3>Title: Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10210">https://arxiv.org/abs/2402.10210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10210">https://arxiv.org/pdf/2402.10210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10210]] Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation(https://arxiv.org/abs/2402.10210)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images ("winner" and "loser" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.</li>
<li><strong>摘要：</strong>微调扩散模型仍然是生成人工智能（GenAI）领域尚未开发的前沿领域，特别是与微调大型语言模型（LLM）方面取得的显着进展相比。虽然稳定扩散 (SD) 和 SDXL 等尖端扩散模型依赖于监督微调，但在看到一定量的数据后，它们的性能不可避免地会趋于稳定。最近，强化学习（RL）已被用来利用人类偏好数据来微调扩散模型，但每个文本提示至少需要两张图像（“获胜者”和“失败者”图像）。在本文中，我们介绍了一种称为扩散模型自我调整微调（SPIN-Diffusion）的创新技术，其中扩散模型与其早期版本进行竞争，促进迭代的自我改进过程。我们的方法提供了传统监督微调和强化学习策略的替代方案，显着提高了模型性能和一致性。我们在 Pick-a-Pic 数据集上的实验表明，SPIN-Diffusion 从第一次迭代起就在人类偏好对齐和视觉吸引力方面优于现有的监督微调方法。到第二次迭代时，它在所有指标上都超过了基于 RLHF 的方法的性能，用更少的数据实现了这些结果。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
