<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-01</h1>
<h3>Title: ICE-SEARCH: A Language Model-Driven Feature Selection Approach</h3>
<ul>
<li><strong>Authors: </strong>Tianze (Tom)Yang, Tianyi (Tim)Yang, Shaoshan Liu, Fuyuan Lvu, Xue Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18609">https://arxiv.org/abs/2402.18609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18609">https://arxiv.org/pdf/2402.18609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18609]] ICE-SEARCH: A Language Model-Driven Feature Selection Approach(https://arxiv.org/abs/2402.18609)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate the efficacy of ICE-SEARCH in medical FS but also underscore the versatility, efficiency, and scalability of integrating LMs in FS tasks. The study emphasizes the critical role of incorporating domain-specific insights, illustrating ICE-SEARCH's robustness, generalizability, and swift convergence. This opens avenues for further research into comprehensive and intricate FS landscapes, marking a significant stride in the application of artificial intelligence in medical predictive analytics.</li>
<li><strong>摘要：</strong>这项研究推出了上下文进化搜索（ICE-SEARCH）方法，这是第一个将语言模型（LM）与特征选择（FS）任务的进化算法相融合的工作，并证明了其在医学预测分析（MPA）应用中的有效性。 ICE-SEARCH 在进化框架内利用 LM 固有的交叉和变异能力，通过模型全面的世界知识及其对各种角色的适应性显着提高 FS。我们对该方法的评估涵盖三个关键的 MPA 任务：中风、心血管疾病和糖尿病，其中 ICE-SEARCH 在确定医疗应用的基本特征方面优于传统的 FS 方法。 ICE-SEARCH 在中风预测和糖尿病预测方面实现了最先进 (SOTA) 的性能；决策随机 ICE-SEARCH 在心血管疾病预测方面排名 SOTA。我们的结果不仅证明了 ICE-SEARCH 在医疗 FS 中的功效，而且还强调了在 FS 任务中集成 LM 的多功能性、效率和可扩展性。该研究强调了整合特定领域见解的关键作用，说明了 ICE-SEARCH 的稳健性、普遍性和快速收敛性。这为进一步研究全面而复杂的金融服务领域开辟了道路，标志着人工智能在医学预测分析中的应用迈出了重大一步。</li>
</ul>

<h3>Title: Why Attention Graphs Are All We Need: Pioneering Hierarchical  Classification of Hematologic Cell Populations with LeukoGraph</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Nassajian Mojarrad, Lorenzo Bini, Thomas Matthes, Stéphane Marchand-Maillet</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.CB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18610">https://arxiv.org/abs/2402.18610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18610">https://arxiv.org/pdf/2402.18610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18610]] Why Attention Graphs Are All We Need: Pioneering Hierarchical  Classification of Hematologic Cell Populations with LeukoGraph(https://arxiv.org/abs/2402.18610)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In the complex landscape of hematologic samples such as peripheral blood or bone marrow, cell classification, delineating diverse populations into a hierarchical structure, presents profound challenges. This study presents LeukoGraph, a recently developed framework designed explicitly for this purpose employing graph attention networks (GATs) to navigate hierarchical classification (HC) complexities. Notably, LeukoGraph stands as a pioneering effort, marking the application of graph neural networks (GNNs) for hierarchical inference on graphs, accommodating up to one million nodes and millions of edges, all derived from flow cytometry data. LeukoGraph intricately addresses a classification paradigm where for example four different cell populations undergo flat categorization, while a fifth diverges into two distinct child branches, exemplifying the nuanced hierarchical structure inherent in complex datasets. The technique is more general than this example. A hallmark achievement of LeukoGraph is its F-score of 98%, significantly outclassing prevailing state-of-the-art methodologies. Crucially, LeukoGraph's prowess extends beyond theoretical innovation, showcasing remarkable precision in predicting both flat and hierarchical cell types across flow cytometry datasets from 30 distinct patients. This precision is further underscored by LeukoGraph's ability to maintain a correct label ratio, despite the inherent challenges posed by hierarchical classifications.</li>
<li><strong>摘要：</strong>在外周血或骨髓等复杂的血液样本中，细胞分类将不同的群体划分为层次结构，提出了深刻的挑战。这项研究提出了 LeukoGraph，这是一个最近开发的框架，专门为此目的而设计，采用图注意力网络（GAT）来导航层次分类（HC）复杂性。值得注意的是，LeukoGraph 是一项开创性的工作，标志着图神经网络 (GNN) 在图的层次推理中的应用，可容纳多达一百万个节点和数百万条边，所有这些都源自流式细胞术数据。 LeukoGraph 错综复杂地解决了一种分类范式，例如，四种不同的细胞群进行平面分类，而第五个细胞群则分为两个不同的子分支，举例说明了复杂数据集中固有的微妙层次结构。该技术比这个例子更通用。 LeukoGraph 的标志性成就是其 F 分数高达 98%，显着超过了流行的最先进方法。至关重要的是，LeukoGraph 的实力超越了理论创新，在预测来自 30 名不同患者的流式细胞术数据集中的扁平和分层细胞类型方面表现出了非凡的精度。尽管分层分类带来了固有的挑战，但 LeukoGraph 保持正确标签比率的能力进一步强调了这种精度。</li>
</ul>

<h3>Title: Large Language Models and Games: A Survey and Roadmap</h3>
<ul>
<li><strong>Authors: </strong>Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18659">https://arxiv.org/abs/2402.18659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18659">https://arxiv.org/pdf/2402.18659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18659]] Large Language Models and Games: A Survey and Roadmap(https://arxiv.org/abs/2402.18659)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 的研究以及随之而来的公众对该主题的参与呈爆炸式增长。虽然法学硕士最初是自然语言处理中的一个利基领域，但它在包括游戏在内的广泛应用和领域中表现出了巨大的潜力。本文调查了法学硕士在游戏中的各种应用的最新技术水平，并确定了法学硕士在游戏中可以扮演的不同角色。重要的是，我们讨论了未充分探索的领域和未来在游戏中使用法学硕士的有希望的方向，并协调了法学硕士在游戏领域的潜力和局限性。作为法学硕士和游戏交叉领域的第一份全面调查和路线图，我们希望本文能够成为这个令人兴奋的新领域突破性研究和创新的基础。</li>
</ul>

<h3>Title: FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability</h3>
<ul>
<li><strong>Authors: </strong>Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, Caiming Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18667">https://arxiv.org/abs/2402.18667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18667">https://arxiv.org/pdf/2402.18667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18667]] FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability(https://arxiv.org/abs/2402.18667)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo's role in guiding the selection of domain-specific AI agents. FoFo is released here at https://github.com/SalesforceAIResearch/FoFo.</li>
<li><strong>摘要：</strong>本文介绍了 FoFo，这是一个开创性的基准，用于评估大型语言模型 (LLM) 遵循复杂的、特定领域格式的能力，这是其作为人工智能代理应用的一项至关重要但尚未得到充分检验的能力。尽管法学硕士取得了进步，但现有的基准未能充分评估他们遵循格式的熟练程度。 FoFo 通过人工智能与人类协作方法开发的各种真实世界格式和指令填补了这一空白。我们对开源（例如，Llama 2、WizardLM）和闭源（例如，GPT-4、PALM2、Gemini）法学硕士的评估强调了三个关键发现：开源模型在格式遵守方面明显落后于闭源模型;法学硕士的格式遵循性能与其内容生成质量无关；法学硕士的格式熟练程度因不同领域而异。这些见解表明需要对格式遵循技能进行专门调整，并强调 FoFo 在指导选择特定领域 AI 代理方面的作用。 FoFo 发布于 https://github.com/SalesforceAIResearch/FoFo。</li>
</ul>

<h3>Title: Simple linear attention language models balance the recall-throughput  tradeoff</h3>
<ul>
<li><strong>Authors: </strong>Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher Ré</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18668">https://arxiv.org/abs/2402.18668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18668">https://arxiv.org/pdf/2402.18668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18668]] Simple linear attention language models balance the recall-throughput  tradeoff(https://arxiv.org/abs/2402.18668)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.</li>
<li><strong>摘要：</strong>最近的研究表明，基于注意力的语言模型在召回方面表现出色，能够根据之前在上下文中看到的标记来生成世代。然而，基于注意力的模型的效率在推理过程中因 KV 缓存的大量内存消耗而受到瓶颈。在这项工作中，我们探索是否可以在不影响召回率的情况下提高语言模型的效率（例如通过减少内存消耗）。通过将实验和理论应用于广泛的架构，我们确定了模型状态大小和召回能力之间的关键权衡。我们证明了注意力的有效替代方案（例如 H3、Mamba、RWKV）可以维持固定大小的循环状态，但在回忆方面却很困难。我们提出了一种结合线性和滑动窗口注意力的简单架构。通过改变 BASED 窗口大小和线性注意力特征维度，我们可以调整状态大小并遍历回忆-记忆权衡曲线的帕累托前沿，在一端恢复注意力的全部质量，在一端恢复注意力替代品的小状态大小其他。我们训练了高达 1.3b 个参数的语言模型，并表明 BASED 与困惑度方面最强的次二次模型（例如 Mamba）相匹配，并且在现实世界的回忆密集型任务中比它们高出 6.22 个准确度点。线性注意力的实现通常比优化的标准注意力实现效率低。为了使 BASED 具有竞争力，我们开发了 IO 感知算法，当使用 1.3b 参数模型生成 1024 个令牌时，该算法的语言生成吞吐量比 FlashAttention-2 高 24 倍。这项工作的代码位于：https://github.com/HazyResearch/based。</li>
</ul>

<h3>Title: Data Interpreter: An LLM Agent For Data Science</h3>
<ul>
<li><strong>Authors: </strong>Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, Chenglin Wu, Li Zhang, Min Yang, Xiawu Zheng</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18679">https://arxiv.org/abs/2402.18679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18679">https://arxiv.org/pdf/2402.18679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18679]] Data Interpreter: An LLM Agent For Data Science(https://arxiv.org/abs/2402.18679)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks. The solution will be released at https://github.com/geekan/MetaGPT.</li>
<li><strong>摘要：</strong>基于大型语言模型（LLM）的代理已经表现出显着的有效性。然而，在需要实时数据调整、由于各种任务之间复杂的依赖关系而需要优化专业知识以及识别逻辑错误以进行精确推理的能力的数据科学场景中，它们的性能可能会受到影响。在本研究中，我们介绍了数据解释器，这是一种旨在用代码解决问题的解决方案，强调三种关键技术来增强数据科学中的问题解决能力：1）具有分层图结构的动态规划，以实现实时数据适应性；2）工具集成动态地提高执行过程中的代码熟练程度，丰富所需的专业知识；3）反馈中的逻辑不一致识别，通过经验记录提高效率。我们在各种数据科学和实际任务中评估数据解释器。与开源基线相比，它表现出了卓越的性能，在机器学习任务方面表现出显着改进，从 0.86 提高到 0.95。此外，数学数据集增加了 26%，开放式任务显着提高了 112%。该解决方案将在 https://github.com/geekan/MetaGPT 发布。</li>
</ul>

<h3>Title: Learning to Compress Prompt in Natural Language Formats</h3>
<ul>
<li><strong>Authors: </strong>Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18700">https://arxiv.org/abs/2402.18700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18700">https://arxiv.org/pdf/2402.18700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18700]] Learning to Compress Prompt in Natural Language Formats(https://arxiv.org/abs/2402.18700)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framework compressing original prompts into NL formatted Capsule Prompt while maintaining the prompt utility and transferability. Specifically, to tackle the first challenge, the Nano-Capsulator is optimized by a reward function that interacts with the proposed semantics preserving loss. To address the second question, the Nano-Capsulator is optimized by a reward function featuring length constraints. Experimental results demonstrate that the Capsule Prompt can reduce 81.4% of the original length, decrease inference latency up to 4.5x, and save 80.1% of budget overheads while providing transferability across diverse LLMs and different datasets.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）擅长处理多种自然语言处理任务，但其能力受到长上下文、推理速度慢和计算结果成本高等性能较差的限制。部署具有精确且信息丰富的上下文的法学硕士可以帮助用户更有效、更经济地处理大规模数据集。现有的工作依赖于将长提示上下文压缩为软提示。然而，软提示压缩在不同 LLM 之间的可转移性方面遇到了限制，尤其是基于 API 的 LLM。为此，这项工作旨在以具有LLM可转移性的自然语言形式压缩冗长的提示。这带来了两个挑战：(i) 自然语言 (NL) 提示与反向传播不兼容，(ii) NL 提示在施加长度约束方面缺乏灵活性。在这项工作中，我们提出了一种自然语言提示封装（Nano-Capsulator）框架，将原始提示压缩为NL格式的胶囊提示，同时保持提示的实用性和可转移性。具体来说，为了解决第一个挑战，纳米胶囊通过奖励函数进行优化，该函数与所提出的保留损失的语义相互作用。为了解决第二个问题，纳米胶囊通过具有长度约束的奖励函数进行优化。实验结果表明，Capsule Prompt 可以减少原始长度的 81.4%，将推理延迟降低高达 4.5 倍，并节省 80.1% 的预算开销，同时提供跨不同法学硕士和不同数据集的可转移性。</li>
</ul>

<h3>Title: Commonsense Ontology Micropatterns</h3>
<ul>
<li><strong>Authors: </strong>Andrew Eells, Brandon Dave, Pascal Hitzler, Cogan Shimizu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18715">https://arxiv.org/abs/2402.18715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18715">https://arxiv.org/pdf/2402.18715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18715]] Commonsense Ontology Micropatterns(https://arxiv.org/abs/2402.18715)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The previously introduced Modular Ontology Modeling methodology (MOMo) attempts to mimic the human analogical process by using modular patterns to assemble more complex concepts. To support this, MOMo organizes organizes ontology design patterns into design libraries, which are programmatically queryable, to support accelerated ontology development, for both human and automated processes. However, a major bottleneck to large-scale deployment of MOMo is the (to-date) limited availability of ready-to-use ontology design patterns. At the same time, Large Language Models have quickly become a source of common knowledge and, in some cases, replacing search engines for questions. In this paper, we thus present a collection of 104 ontology design patterns representing often occurring nouns, curated from the common-sense knowledge available in LLMs, organized into a fully-annotated modular ontology design library ready for use with MOMo.</li>
<li><strong>摘要：</strong>前面介绍的模块化本体建模方法（MOMo）试图通过使用模块化模式来组装更复杂的概念来模仿人类的类比过程。为了支持这一点，MOMo 将本体设计模式组织到设计库中，这些设计库可以通过编程方式查询，以支持人工和自动化流程的加速本体开发。然而，大规模部署 MOMo 的一个主要瓶颈是（迄今为止）现成的本体设计模式的可用性有限。与此同时，大型语言模型已迅速成为常识的来源，并在某些情况下取代了问题的搜索引擎。因此，在本文中，我们提出了代表经常出现的名词的 104 个本体设计模式的集合，这些模式是根据法学硕士中可用的常识知识整理的，组织成一个完全注释的模块化本体设计库，可供 MOMo 使用。</li>
</ul>

<h3>Title: Priority Sampling of Large Language Models for Compilers</h3>
<ul>
<li><strong>Authors: </strong>Dejan Grubisic, Chris Cummins, Volker Seeker, Hugh Leather</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18734">https://arxiv.org/abs/2402.18734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18734">https://arxiv.org/pdf/2402.18734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18734]] Priority Sampling of Large Language Models for Compilers(https://arxiv.org/abs/2402.18734)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models show great potential in generating and optimizing code. Widely used sampling methods such as Nucleus Sampling increase the diversity of generation but often produce repeated samples for low temperatures and incoherent samples for high temperatures. Furthermore, the temperature coefficient has to be tuned for each task, limiting its usability. We present Priority Sampling, a simple and deterministic sampling technique that produces unique samples ordered by the model's confidence. Each new sample expands the unexpanded token with the highest probability in the augmented search tree. Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process. Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz. Moreover, it outperforms the autotuner used for the generation of labels for the training of the original model in just 30 samples.</li>
<li><strong>摘要：</strong>大型语言模型在生成和优化代码方面显示出巨大的潜力。广泛使用的采样方法（例如核采样）增加了生成的多样性，但通常会产生低温重复样本和高温不相干样本。此外，必须针对每个任务调整温度系数，限制了其可用性。我们提出了优先采样，这是一种简单且确定性的采样技术，可生成按模型置信度排序的独特样本。每个新样本都会在增强搜索树中以最高概率扩展未扩展的标记。此外，优先采样支持基于正则表达式的生成，提供可控且结构化的探索过程。对于任意数量的样本，优先采样都优于核采样，将原始模型的性能从 -Oz 的 2.87% 提高到 5%。此外，它的性能优于用于在仅 30 个样本中训练原始模型时生成标签的自动调谐器。</li>
</ul>

<h3>Title: On the Decision-Making Abilities in Role-Playing using Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Chenglei Shen, Guofu Xie, Xiao Zhang, Jun Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18807">https://arxiv.org/abs/2402.18807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18807">https://arxiv.org/pdf/2402.18807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18807]] On the Decision-Making Abilities in Role-Playing using Large Language  Models(https://arxiv.org/abs/2402.18807)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts. When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks. Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\&$exploitation trade-off ability, reasoning ability, and safety. Finally, we analyze the association between the performance of decision-making and the corresponding MBTI types through GPT-4. Extensive experiments demonstrate stable differences in the four aspects of decision-making abilities across distinct roles, signifying a robust correlation between decision-making abilities and the roles emulated by LLMs. These results underscore that LLMs can effectively impersonate varied roles while embodying their genuine sociological characteristics.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 现在越来越多地用于角色扮演任务，尤其是主要通过角色扮演提示来模仿特定领域的专家。在现实场景中进行交互时，角色的决策能力会显着影响其行为模式。在本文中，我们集中评估法学硕士在角色扮演后的决策能力，从而验证角色扮演的有效性。我们的目标是提供衡量标准和指导，以增强法学硕士在角色扮演任务中的决策能力。具体来说，我们首先使用 LLM 生成与代表人群细分的迈尔斯-布里格斯类型指标（缩写为 MBTI）的 16 种性格类型相对应的虚拟角色描述。然后，我们设计了具体的量化操作，从适应性、探索性、探索性权衡能力、推理能力和安全性四个方面来评估LLM角色扮演后的决策能力。最后，我们通过GPT-4分析决策绩效与相应MBTI类型之间的关联。大量实验表明，不同角色决策能力的四个方面存在稳定差异，这表明决策能力与法学硕士所模拟的角色之间存在强大的相关性。这些结果强调，法学硕士可以有效地扮演不同的角色，同时体现其真正的社会学特征。</li>
</ul>

<h3>Title: How do Large Language Models Handle Multilingualism?</h3>
<ul>
<li><strong>Authors: </strong>Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18815">https://arxiv.org/abs/2402.18815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18815">https://arxiv.org/pdf/2402.18815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18815]] How do Large Language Models Handle Multilingualism?(https://arxiv.org/abs/2402.18815)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specific Neuron Detection ($\texttt{PLND}$) method that effectively measures the significance of neurons when handling multilingual inputs. By comprehensive ablation analysis through deactivating neurons of different layers and structures, we verify the framework that we propose. Additionally, we demonstrate that we can utilize such a framework to effectively enhance the multilingual ability with much less training effort.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在多种语言上都表现出了卓越的性能。在这项工作中，我们深入研究了这样一个问题：法学硕士如何处理多语言？我们引入了一个描述法学硕士对多语言输入的处理的框架：在前几层中，法学硕士理解问题，将多语言输入转换为英语，以促进任务解决阶段。在中间层，法学硕士分别利用自注意力和前馈结构，通过用英语思考并结合多语言知识来获取事实内容来解决问题。在最后几层中，法学硕士生成与查询的原始语言一致的响应。此外，我们还研究了处理某种语言时语言特异性神经元的存在。为了检测输入语言激活的神经元，即使没有标签，我们创新地设计了一种并行语言特定神经元检测（$\texttt{PLND}$）方法，该方法可以有效测量处理多语言输入时神经元的重要性。通过停用不同层和结构的神经元进行综合消融分析，我们验证了我们提出的框架。此外，我们证明我们可以利用这样的框架以更少的训练量有效地增强多语言能力。</li>
</ul>

<h3>Title: Dual Operating Modes of In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Ziqian Lin, Kangwook Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18819">https://arxiv.org/abs/2402.18819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18819">https://arxiv.org/pdf/2402.18819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18819]] Dual Operating Modes of In-Context Learning(https://arxiv.org/abs/2402.18819)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution. With the closed-form expression, we obtain a quantitative understanding of the two operating modes of ICL. Furthermore, we shed light on an unexplained phenomenon observed in practice: under certain settings, the ICL risk initially increases and then decreases with more in-context examples. Our model offers a plausible explanation for this "early ascent" phenomenon: a limited number of in-context samples may lead to the retrieval of an incorrect skill, thereby increasing the risk, which will eventually diminish as task learning takes effect with more in-context samples. We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL, where in-context examples are assigned random labels. Lastly, we validate our findings and predictions via experiments involving Transformers and large language models.</li>
<li><strong>摘要：</strong>情境学习（ICL）表现出双重操作模式：任务学习，即从情境样本中获取新技能，以及任务检索，即定位和激活相关的预训练技能。最近的理论工作研究了各种数学模型来分析 ICL，但现有模型一次只能解释一种操作模式。我们引入了一种概率模型，可以同时解释 ICL 的双重操作模式。我们专注于线性函数的上下文学习，通过引入多个任务组和任务相关的输入分布来扩展现有的预训练数据模型。然后，我们分析最佳预训练模型在平方损失下的行为，即给定上下文示例的标签的 MMSE 估计器。将预训练任务分布视为先验，将上下文中的示例视为观察，我们推导了任务后验分布的封闭式表达式。通过封闭式表达式，我们对 ICL 的两种工作模式有了定量的了解。此外，我们揭示了在实践中观察到的一个无法解释的现象：在某些设置下，ICL 风险最初会随着更多上下文示例的增加而增加，然后降低。我们的模型为这种“早期上升”现象提供了一个合理的解释：有限数量的上下文样本可能会导致检索到不正确的技能，从而增加风险，随着任务学习在更多的上下文中生效，风险最终会降低。上下文样本。我们还从理论上分析了带有偏见标签的 ICL，例如零样本 ICL，其中上下文中的示例被分配了随机标签。最后，我们通过涉及 Transformer 和大型语言模型的实验验证了我们的发现和预测。</li>
</ul>

<h3>Title: When does word order matter and when doesn't it?</h3>
<ul>
<li><strong>Authors: </strong>Xuanda Chen, Timothy O'Donnell, Siva Reddy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18838">https://arxiv.org/abs/2402.18838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18838">https://arxiv.org/pdf/2402.18838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18838]] When does word order matter and when doesn't it?(https://arxiv.org/abs/2402.18838)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models (LMs) may appear insensitive to word order changes in natural language understanding (NLU) tasks. In this paper, we propose that linguistic redundancy can explain this phenomenon, whereby word order and other linguistic cues such as case markers provide overlapping and thus redundant information. Our hypothesis is that models exhibit insensitivity to word order when the order provides redundant information, and the degree of insensitivity varies across tasks. We quantify how informative word order is using mutual information (MI) between unscrambled and scrambled sentences. Our results show the effect that the less informative word order is, the more consistent the model's predictions are between unscrambled and scrambled sentences. We also find that the effect varies across tasks: for some tasks, like SST-2, LMs' prediction is almost always consistent with the original one even if the Pointwise-MI (PMI) changes, while for others, like RTE, the consistency is near random when the PMI gets lower, i.e., word order is really important.</li>
<li><strong>摘要：</strong>语言模型 (LM) 可能对自然语言理解 (NLU) 任务中的词序变化不敏感。在本文中，我们提出语言冗余可以解释这种现象，即词序和其他语言线索（例如格标记）提供了重叠的冗余信息。我们的假设是，当顺序提供冗余信息时，模型表现出对词序不敏感，并且不敏感的程度因任务而异。我们使用未打乱的句子和打乱的句子之间的互信息 (MI) 来量化词序的信息量。我们的结果表明，词序信息越少，模型的预测在未打乱的句子和打乱的句子之间越一致。我们还发现，不同任务的效果各不相同：对于某些任务，如 SST-2，即使 Pointwise-MI (PMI) 发生变化，LM 的预测也几乎总是与原始任务一致，而对于其他任务，如 RTE，一致性当 PMI 较低时，接近随机，即词序非常重要。</li>
</ul>

<h3>Title: Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient  Tuning</h3>
<ul>
<li><strong>Authors: </strong>Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, Wei Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18865">https://arxiv.org/abs/2402.18865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18865">https://arxiv.org/pdf/2402.18865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18865]] Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient  Tuning(https://arxiv.org/abs/2402.18865)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs continual learning scenario and find that it can strike a balance between plasticity and stability. Building upon these findings, we propose a simple yet effective method called Interpolation-based LoRA (I-LoRA), which constructs a dual-memory experience replay framework based on LoRA parameter interpolations. Extensive experiments and analysis on eight domain-specific CL benchmarks demonstrate that I-LoRA consistently show significant improvement over the previous state-of-the-art approaches with up to $11\%$ performance gains, providing a strong baseline and insights for future research on the large language model continual learning problem. Our code is available at \url{https://github.com/which47/LLMCL}.</li>
<li><strong>摘要：</strong>现有研究表明，大型语言模型（LLM）在语言理解和生成方面表现出卓越的性能。然而，当LLM在复杂多样的特定领域下游任务上不断进行微调时，历史任务的推理性能急剧下降，这被称为灾难性遗忘问题。需要在学习可塑性和记忆稳定性之间进行权衡。大量现有工作已经探索了诸如记忆重放、正则化和参数隔离等策略，但对于连续 LLM 微调场景中各种相邻最小值的几何连接知之甚少。在这项工作中，我们通过模式连接的镜头研究不同最小值的几何连接，这意味着不同的最小值可以通过低损耗谷连接。通过大量的实验，我们揭示了法学硕士持续学习场景中的模式连接现象，并发现它可以在可塑性和稳定性之间取得平衡。基于这些发现，我们提出了一种简单而有效的方法，称为基于插值的 LoRA (I-LoRA)，它构建了一个基于 LoRA 参数插值的双记忆体验回放框架。对八个特定领域 CL 基准的广泛实验和分析表明，I-LoRA 始终表现出比以前最先进的方法显着改进，性能提升高达 11\%$，为未来的研究提供了强大的基线和见解关于大语言模型持续学习问题。我们的代码可在 \url{https://github.com/which47/LLMCL} 获取。</li>
</ul>

<h3>Title: Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming</h3>
<ul>
<li><strong>Authors: </strong>Hany Hamed, Subin Kim, Dongyeong Kim, Jaesik Yoon, Sungjin Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18866">https://arxiv.org/abs/2402.18866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18866">https://arxiv.org/pdf/2402.18866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18866]] Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming(https://arxiv.org/abs/2402.18866)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question whether and how an agent can "dream better" in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way. In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks. The source code will be available at https://github.com/ahn-ml/drstrategy</li>
<li><strong>摘要：</strong>基于模型的强化学习（MBRL）一直是改善样本效率问题以及创建通才代理的主要方法。然而，在增强梦想本身的策略方面并没有做出太多努力。因此，代理人能否以及如何以更加结构化和战略性的方式“更好地梦想”是一个问题。在本文中，受认知科学观察表明人类在规划中使用空间分而治之策略的启发，我们提出了一种新的 MBRL 智能体，称为“策略博士”，它配备了一种新颖的梦想策略。所提出的智能体在梦中实现了一种类似分而治之的策略。这是通过学习一组潜在地标，然后利用它们来学习地标条件的高速公路策略来实现的。通过高速公路策略，智能体可以首先在梦中学习移动到地标，然后从那里以更有针对性的方式处理探索和成就任务。在实验中，我们表明所提出的模型在各种视觉复杂和部分可观察的导航任务中优于先前基于像素的 MBRL 方法。源代码可在 https://github.com/ahn-ml/drstrategy 获取</li>
</ul>

<h3>Title: Reducing Hallucinations in Entity Abstract Summarization with  Facts-Template Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Fangwei Zhu, Peiyi Wang, Zhifang Sui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18873">https://arxiv.org/abs/2402.18873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18873">https://arxiv.org/pdf/2402.18873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18873]] Reducing Hallucinations in Entity Abstract Summarization with  Facts-Template Decomposition(https://arxiv.org/abs/2402.18873)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Entity abstract summarization aims to generate a coherent description of a given entity based on a set of relevant Internet documents. Pretrained language models (PLMs) have achieved significant success in this task, but they may suffer from hallucinations, i.e. generating non-factual information about the entity. To address this issue, we decompose the summary into two components: Facts that represent the factual information about the given entity, which PLMs are prone to fabricate; and Template that comprises generic content with designated slots for facts, which PLMs can generate competently. Based on the facts-template decomposition, we propose SlotSum, an explainable framework for entity abstract summarization. SlotSum first creates the template and then predicts the fact for each template slot based on the input documents. Benefiting from our facts-template decomposition, SlotSum can easily locate errors and further rectify hallucinated predictions with external knowledge. We construct a new dataset WikiFactSum to evaluate the performance of SlotSum. Experimental results demonstrate that SlotSum could generate summaries that are significantly more factual with credible external knowledge.</li>
<li><strong>摘要：</strong>实体摘要摘要旨在基于一组相关的互联网文档生成给定实体的连贯描述。预训练语言模型（PLM）在这项任务中取得了巨大的成功，但它们可能会产生幻觉，即生成有关实体的非事实信息。为了解决这个问题，我们将摘要分解为两个部分： 表示有关给定实体的事实信息的事实，PLM 很容易伪造这些信息；包含通用内容和指定事实槽的模板，PLM 可以胜任生成这些内容。基于事实模板分解，我们提出了 SlotSum，一个可解释的实体抽象摘要框架。 SlotSum 首先创建模板，然后根据输入文档预测每个模板槽的事实。受益于我们的事实模板分解，SlotSum 可以轻松定位错误，并利用外部知识进一步纠正幻觉预测。我们构建了一个新的数据集 WikiFactSum 来评估 SlotSum 的性能。实验结果表明，SlotSum 可以通过可靠的外部知识生成更加真实的摘要。</li>
</ul>

<h3>Title: Updating Language Models with Unstructured Facts: Towards Practical  Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Xiaobao Wu, Liangming Pan, William Yang Wang, Anh Tuan Luu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18909">https://arxiv.org/abs/2402.18909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18909">https://arxiv.org/pdf/2402.18909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18909]] Updating Language Models with Unstructured Facts: Towards Practical  Knowledge Editing(https://arxiv.org/abs/2402.18909)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark. We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further show that this challenge persists even if we extract triplets as structured facts. Our analysis discloses key insights to motivate future research in UKE for more practical knowledge editing.</li>
<li><strong>摘要：</strong>知识编辑旨在将知识更新注入语言模型中，以保持其正确性和最新性。然而，其当前的评估策略明显不切实际：它们仅使用精心策划的结构化事实（具有主题、关系和对象的三元组）进行更新，而现实世界的知识更新通常出现在新闻文章等非结构化文本中。在本文中，我们提出了一个新的基准：非结构化知识编辑（UKE）。它直接使用非结构化文本作为知识更新（称为非结构化事实）来评估编辑性能。因此，UKE避免了结构化事实的费力构建，并实现了高效且响应迅速的知识编辑，成为更实用的基准。我们对新建的数据集进行了广泛的实验，并证明 UKE 对最先进的知识编辑方法提出了重大挑战，导致其关键性能下降。我们进一步表明，即使我们提取三元组作为结构化事实，这一挑战仍然存在。我们的分析揭示了推动 UKE 未来研究进行更实用的知识编辑的关键见解。</li>
</ul>

<h3>Title: AdaMergeX: Cross-Lingual Transfer with Large Language Models via  Adaptive Adapter Merging</h3>
<ul>
<li><strong>Authors: </strong>Yiran Zhao, Wenxuan Zhang, Huiming Wang, Kenji Kawaguchi, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18913">https://arxiv.org/abs/2402.18913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18913">https://arxiv.org/pdf/2402.18913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18913]] AdaMergeX: Cross-Lingual Transfer with Large Language Models via  Adaptive Adapter Merging(https://arxiv.org/abs/2402.18913)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by decoupling ''task ability'' and ''language ability'' by fine-tuning on the target task in the source language and another selected task in the target language, respectively. However, they fail to fully separate the task ability from the source language or the language ability from the chosen task. In this paper, we acknowledge the mutual reliance between task ability and language ability and direct our attention toward the gap between the target language and the source language on tasks. As the gap removes the impact of tasks, we assume that it remains consistent across tasks. Based on this assumption, we propose a new cross-lingual transfer method called $\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a reference task, we can determine that the divergence of adapters fine-tuned on the reference task in both languages follows the same distribution as the divergence of adapters fine-tuned on the target task in both languages. Hence, we can obtain target adapters by combining the other three adapters. Furthermore, we propose a structure-adaptive adapter merging method. Our empirical results demonstrate that our approach yields new and effective cross-lingual transfer, outperforming existing methods across all settings.</li>
<li><strong>摘要：</strong>作为对特定语言的目标任务直接微调的有效替代方案，跨语言迁移通过对目标进行微调，解耦“任务能力”和“语言能力”，解决了训练数据有限的挑战分别使用源语言的任务和目标语言的另一个选定任务。然而，他们未能将任务能力与源语言或语言能力与所选任务完全分开。在本文中，我们承认任务能力和语言能力之间的相互依赖，并将我们的注意力转向目标语言和源语言在任务上的差距。由于差距消除了任务的影响，我们假设它在任务之间保持一致。基于这个假设，我们提出了一种新的跨语言传输方法，称为 $\texttt{AdaMergeX}$，它利用自适应适配器合并。通过引入参考任务，我们可以确定两种语言中在参考任务上微调的适配器的分歧与两种语言中在目标任务上微调的适配器的分歧遵循相同的分布。因此，我们可以通过组合其他三个适配器来获得目标适配器。此外，我们提出了一种结构自适应适配器合并方法。我们的实证结果表明，我们的方法产生了新的有效的跨语言迁移，在所有设置中都优于现有方法。</li>
</ul>

<h3>Title: Stop Relying on No-Choice and Do not Repeat the Moves: Optimal,  Efficient and Practical Algorithms for Assortment Optimization</h3>
<ul>
<li><strong>Authors: </strong>Aadirupa Saha, Pierre Gaillard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18917">https://arxiv.org/abs/2402.18917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18917">https://arxiv.org/pdf/2402.18917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18917]] Stop Relying on No-Choice and Do not Repeat the Moves: Optimal,  Efficient and Practical Algorithms for Assortment Optimization(https://arxiv.org/abs/2402.18917)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, recommender systems, fine-tuning language models, amongst many. The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee. E.g., popularly used assortment selection algorithms often require the presence of a `strong reference' which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected -- all such requirements are quite unrealistic for practical applications. In this paper, we designed efficient algorithms for the problem of regret minimization in assortment selection with \emph{Plackett Luce} (PL) based user choices. We designed a novel concentration guarantee for estimating the score parameters of the PL model using `\emph{Pairwise Rank-Breaking}', which builds the foundation of our proposed algorithms. Moreover, our methods are practical, provably optimal, and devoid of the aforementioned limitations of the existing methods. Empirical evaluations corroborate our findings and outperform the existing baselines.</li>
<li><strong>摘要：</strong>我们通过偏好反馈解决主动在线分类优化问题，这是一个用于建模用户选择和子集效用最大化的框架。该框架可用于各种现实世界的应用程序，包括广告投放、在线零售、推荐系统、微调语言模型等。该问题虽然过去已经被研究过，但缺乏一种直观实用的解决方法，同时具有高效的算法和最优的后悔保证。例如，普遍使用的分类选择算法通常需要存在始终包含在选择集中的“强参考”，此外，它们还被设计为重复提供相同的分类，直到选择参考项为止——所有这些要求都非常高。对于实际应用来说不现实。在本文中，我们设计了有效的算法来解决基于 \emph{Plackett Luce} (PL) 的用户选择的品种选择中的遗憾最小化问题。我们设计了一种新颖的集中保证，用于使用“\emph{Pairwise Rank-Breaking}”估计 PL 模型的得分参数，这为我们提出的算法奠定了基础。此外，我们的方法是实用的，被证明是最佳的，并且没有现有方法的上述限制。实证评估证实了我们的发现并优于现有的基线。</li>
</ul>

<h3>Title: PopALM: Popularity-Aligned Language Models for Social Media Trendy  Response Prediction</h3>
<ul>
<li><strong>Authors: </strong>Erxin Yu, Jing Li, Chunpu Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.18950">https://arxiv.org/abs/2402.18950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.18950">https://arxiv.org/pdf/2402.18950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.18950]] PopALM: Popularity-Aligned Language Models for Social Media Trendy  Response Prediction(https://arxiv.org/abs/2402.18950)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Social media platforms are daily exhibiting millions of events. To preliminarily predict the mainstream public reaction to these events, we study trendy response prediction to automatically generate top-liked user replies to social media events. While previous works focus on generating responses without factoring in popularity, we propose Popularity-Aligned Language Models (PopALM) to distinguish responses liked by a larger audience through reinforcement learning. Recognizing the noisy labels from user "likes", we tailor-make curriculum learning in proximal policy optimization (PPO) to help models capture the essential samples for easy-to-hard training. In experiments, we build a large-scale Weibo dataset for trendy response prediction, and its results show that PopALM can help boost the performance of advanced language models.</li>
<li><strong>摘要：</strong>社交媒体平台每天都会展示数百万个活动。为了初步预测主流公众对这些事件的反应，我们研究了流行响应预测，以自动生成对社交媒体事件最喜欢的用户回复。虽然之前的工作侧重于生成响应而不考虑受欢迎程度，但我们提出了流行度对齐语言模型（PopALM）来通过强化学习来区分更多受众喜欢的响应。识别出用户“喜欢”的嘈杂标签，我们在近端策略优化（PPO）中定制课程学习，以帮助模型捕获从简单到困难的训练的基本样本。在实验中，我们构建了一个大规模的微博数据集来进行流行响应预测，其结果表明 PopALM 可以帮助提高高级语言模型的性能。</li>
</ul>

<h3>Title: Generating, Reconstructing, and Representing Discrete and Continuous  Data: Generalized Diffusion with Learnable Encoding-Decoding</h3>
<ul>
<li><strong>Authors: </strong>Guangyi Liu, Yu Wang, Zeyu Feng, Qiyu Wu, Liping Tang, Yuan Gao, Zhen Li, Shuguang Cui, Julian McAuley, Eric P. Xing, Zichao Yang, Zhiting Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19009">https://arxiv.org/abs/2402.19009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19009">https://arxiv.org/pdf/2402.19009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19009]] Generating, Reconstructing, and Representing Discrete and Continuous  Data: Generalized Diffusion with Learnable Encoding-Decoding(https://arxiv.org/abs/2402.19009)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The vast applications of deep generative models are anchored in three core capabilities -- generating new instances, reconstructing inputs, and learning compact representations -- across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), autoregressive models, and diffusion models, generally excel in specific capabilities and data types but fall short in others. We introduce generalized diffusion with learnable encoder-decoder (DiLED), that seamlessly integrates the core capabilities for broad applicability and enhanced performance. DiLED generalizes the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding. Crucially, DiLED is compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder parameters jointly with diffusion. By choosing appropriate encoder/decoder (e.g., large language models), DiLED naturally applies to different data types. Extensive experiments on text, proteins, and images demonstrate DiLED's flexibility to handle diverse data and tasks and its strong improvement over various existing models.</li>
<li><strong>摘要：</strong>深度生成模型的广泛应用基于三个核心功能——生成新实例、重建输入和学习紧凑表示——跨各种数据类型，例如离散文本/蛋白质序列和连续图像。现有的模型系列，如变分自动编码器 (VAE)、生成对抗网络 (GAN)、自回归模型和扩散模型，通常在特定功能和数据类型方面表现出色，但在其他方面表现不佳。我们引入了具有可学习编码器解码器（DiLED）的广义扩散，它无缝集成了核心功能，以实现广泛的适用性和增强的性能。 DiLED 通过引入参数化编码-解码来推广标准扩散中的高斯噪声-去噪。至关重要的是，DiLED 与完善的扩散模型目标和训练配方兼容，允许与扩散一起有效学习编码器-解码器参数。通过选择合适的编码器/解码器（例如大型语言模型），DiLED自然适用于不同的数据类型。对文本、蛋白质和图像的大量实验证明了 DiLED 处理不同数据和任务的灵活性，以及​​对各种现有模型的强大改进。</li>
</ul>

<h3>Title: Exploring the Efficacy of Large Language Models in Summarizing Mental  Health Counseling Sessions: A Benchmark Study</h3>
<ul>
<li><strong>Authors: </strong>Prottay Kumar Adhikary, Aseem Srivastava, Shivani Kumar, Salam Michael Singh, Puneet Manuja, Jini K Gopinath, Vijay Krishnan, Swati Kedia, Koushik Sinha Deb, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19052">https://arxiv.org/abs/2402.19052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19052">https://arxiv.org/pdf/2402.19052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19052]] Exploring the Efficacy of Large Language Models in Summarizing Mental  Health Counseling Sessions: A Benchmark Study(https://arxiv.org/abs/2402.19052)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Comprehensive summaries of sessions enable an effective continuity in mental health counseling, facilitating informed therapy planning. Yet, manual summarization presents a significant challenge, diverting experts' attention from the core counseling process. This study evaluates the effectiveness of state-of-the-art Large Language Models (LLMs) in selectively summarizing various components of therapy sessions through aspect-based summarization, aiming to benchmark their performance. We introduce MentalCLOUDS, a counseling-component guided summarization dataset consisting of 191 counseling sessions with summaries focused on three distinct counseling components (aka counseling aspects). Additionally, we assess the capabilities of 11 state-of-the-art LLMs in addressing the task of component-guided summarization in counseling. The generated summaries are evaluated quantitatively using standard summarization metrics and verified qualitatively by mental health professionals. Our findings demonstrate the superior performance of task-specific LLMs such as MentalLlama, Mistral, and MentalBART in terms of standard quantitative metrics such as Rouge-1, Rouge-2, Rouge-L, and BERTScore across all aspects of counseling components. Further, expert evaluation reveals that Mistral supersedes both MentalLlama and MentalBART based on six parameters -- affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness. However, these models share the same weakness by demonstrating a potential for improvement in the opportunity costs and perceived effectiveness metrics.</li>
<li><strong>摘要：</strong>会议的全面总结可以实现心理健康咨询的有效连续性，促进知情的治疗计划。然而，手动总结提出了重大挑战，分散了专家对核心咨询过程的注意力。本研究评估了最先进的大型语言模型（LLM）在通过基于方面的总结选择性总结治疗课程的各个组成部分方面的有效性，旨在对其表现进行基准测试。我们引入了 MentalCLOUDS，这是一个咨询组件引导的总结数据集，由 191 个咨询会议组成，其摘要侧重于三个不同的咨询组件（也称为咨询方面）。此外，我们还评估了 11 名最先进的法学硕士在解决咨询中组件引导总结任务方面的能力。使用标准摘要指标对生成的摘要进行定量评估，并由心理健康专业人员进行定性验证。我们的研究结果表明，MentalLlama、Mistral 和 MentalBART 等特定任务的法学硕士在咨询组件各个方面的标准定量指标（如 Rouge-1、Rouge-2、Rouge-L 和 BERTScore）方面表现出色。此外，专家评估表明，Mistral 基于六个参数（情感态度、负担、道德、连贯性、机会成本和感知有效性）取代了 MentalLlama 和 MentalBART。然而，这些模型都具有相同的弱点，即展示了机会成本和感知有效性指标的改进潜力。</li>
</ul>

<h3>Title: Pointing out the Shortcomings of Relation Extraction Models with  Semantically Motivated Adversarials</h3>
<ul>
<li><strong>Authors: </strong>Gennaro Nolano, Moritz Blum, Basil Ell, Philipp Cimiano</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19076">https://arxiv.org/abs/2402.19076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19076">https://arxiv.org/pdf/2402.19076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19076]] Pointing out the Shortcomings of Relation Extraction Models with  Semantically Motivated Adversarials(https://arxiv.org/abs/2402.19076)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In recent years, large language models have achieved state-of-the-art performance across various NLP tasks. However, investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples. For instance, in the context of relation extraction (RE), we would expect a model to identify the same relation independently of the entities involved in it. For example, consider the sentence "Leonardo da Vinci painted the Mona Lisa" expressing the created(Leonardo_da_Vinci, Mona_Lisa) relation. If we substiute "Leonardo da Vinci" with "Barack Obama", then the sentence still expresses the created relation. A robust model is supposed to detect the same relation in both cases. In this work, we describe several semantically-motivated strategies to generate adversarial examples by replacing entity mentions and investigate how state-of-the-art RE models perform under pressure. Our analyses show that the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent on shortcuts, such as surface forms (or patterns therein) of entities, without making full use of the information present in the sentences.</li>
<li><strong>摘要：</strong>近年来，大型语言模型在各种 NLP 任务中取得了最先进的性能。然而，调查表明，这些模型往往依赖于快捷特征，导致预测不准确，并导致模型在泛化到分布外（OOD）样本时不可靠。例如，在关系提取（RE）的背景下，我们期望模型能够独立于其中涉及的实体来识别相同的关系。例如，考虑表达created(Leonardo_da_Vinci, Mona_Lisa)关系的句子“Leonardo da Vinci画了Mona Lisa”。如果我们用“巴拉克·奥巴马”代替“莱昂纳多·达·芬奇”，那么该句子仍然表达了所创建的关系。稳健的模型应该能够在两种情况下检测到相同的关系。在这项工作中，我们描述了几种语义驱动的策略，通过替换实体提及来生成对抗性示例，并研究最先进的 RE 模型在压力下的表现。我们的分析表明，这些模型的性能在修改后的数据集上显着恶化（F1 中的平均值为 -48.5%），这表明这些模型在很大程度上依赖于捷径，例如实体的表面形式（或其中的模式） ，没有充分利用句子中存在的信息。</li>
</ul>

<h3>Title: Controllable Preference Optimization: Toward Controllable  Multi-Objective Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19085">https://arxiv.org/abs/2402.19085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19085">https://arxiv.org/pdf/2402.19085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19085]] Controllable Preference Optimization: Toward Controllable  Multi-Objective Alignment(https://arxiv.org/abs/2402.19085)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the "alignment tax" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the "3H" (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving Pareto improvements in multi-objective alignment.</li>
<li><strong>摘要：</strong>人工智能的一致性追求模型响应与人类偏好和价值观之间的一致性。在实践中，人类偏好的多方面性质无意中引入了所谓的“一致性税”——一种妥协，即在一个目标（例如，无害）内增强一致性可能会降低其他目标（例如，乐于助人）的表现。然而，现有的对齐技术大多是单向的，导致对各种目标的权衡不理想且灵活性差。为了应对这一挑战，我们认为法学硕士具有明显偏好的重要性。我们引入了可控偏好优化（CPO），它明确指定不同目标的偏好分数，从而指导模型生成满足要求的响应。我们的实验分析表明，对齐的模型可以提供与“3H”（乐于助人、诚实、无害）需求中的各种偏好相匹配的响应。此外，通过引入多样化的数据和对齐目标，我们超越了单一目标对齐的基线方法，从而减轻了对齐税的影响，并在多目标对齐中实现了帕累托改进。</li>
</ul>

<h3>Title: Best Arm Identification with Resource Constraints</h3>
<ul>
<li><strong>Authors: </strong>Zitian Li, Wang Chi Cheung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19090">https://arxiv.org/abs/2402.19090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19090">https://arxiv.org/pdf/2402.19090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19090]] Best Arm Identification with Resource Constraints(https://arxiv.org/abs/2402.19090)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Motivated by the cost heterogeneity in experimentation across different alternatives, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem. The agent aims to identify the best arm under resource constraints, where resources are consumed for each arm pull. We make two novel contributions. We design and analyze the Successive Halving with Resource Rationing algorithm (SH-RR). The SH-RR achieves a near-optimal non-asymptotic rate of convergence in terms of the probability of successively identifying an optimal arm. Interestingly, we identify a difference in convergence rates between the cases of deterministic and stochastic resource consumption.</li>
<li><strong>摘要：</strong>受不同替代方案实验中成本异质性的推动，我们研究了资源约束下的最佳手臂识别（BAIwRC）问题。该代理的目标是在资源限制下确定最佳手臂，其中每次拉动手臂都会消耗资源。我们做出了两项新颖的贡献。我们设计并分析了资源配给连续减半算法（SH-RR）。 SH-RR 在连续识别最佳臂的概率方面实现了接近最佳的非渐近收敛率。有趣的是，我们发现确定性资源消耗和随机资源消耗情况之间的收敛率存在差异。</li>
</ul>

<h3>Title: TEncDM: Understanding the Properties of Diffusion Model in the Space of  Language Model Encodings</h3>
<ul>
<li><strong>Authors: </strong>Alexander Shabalin, Viacheslav Meshchaninov, Tingir Badmaev, Dmitry Molchanov, Grigory Bartosh, Sergey Markov, Dmitry Vetrov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19097">https://arxiv.org/abs/2402.19097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19097">https://arxiv.org/pdf/2402.19097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19097]] TEncDM: Understanding the Properties of Diffusion Model in the Space of  Language Model Encodings(https://arxiv.org/abs/2402.19097)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data. Despite these efforts, none of them has managed to achieve the quality of the large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority over existing non-autoregressive models.</li>
<li><strong>摘要：</strong>从各个领域的扩散模型的成功中汲取灵感，许多研究论文提出了将其适应文本数据的方法。尽管做出了这些努力，但他们都未能达到大型语言模型的质量。在本文中，我们对文本扩散模型的关键组成部分进行了全面分析，并介绍了一种称为文本编码扩散模型（TEncDM）的新方法。我们在语言模型编码的空间中训练我们的模型，而不是常用的标记嵌入空间。此外，我们建议使用基于 Transformer 的解码器，利用上下文信息进行文本重建。我们还分析了自调节，发现它增加了模型输出的幅度，从而减少了推理阶段的去噪步骤数量。对 TEncDM 对两个下游文本生成任务 QQP 和 XSum 的评估表明了其相对于现有非自回归模型的优越性。</li>
</ul>

<h3>Title: Whispers that Shake Foundations: Analyzing and Mitigating False Premise  Hallucinations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongbang Yuan, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19103">https://arxiv.org/abs/2402.19103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19103">https://arxiv.org/pdf/2402.19103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19103]] Whispers that Shake Foundations: Analyzing and Mitigating False Premise  Hallucinations in Large Language Models(https://arxiv.org/abs/2402.19103)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \textbf{FAITH} (\textbf{F}alse premise \textbf{A}ttention head constra\textbf{I}ining for mi\textbf{T}igating \textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately $1\%$ of the attention heads in the model yields a notable increase of nearly $20\%$ of model performance.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已经显示出令人印象深刻的能力，但仍然受到幻觉问题的困扰。这个问题的一个重要类型是错误前提幻觉，我们将其定义为法学硕士在面对错误前提问题时产生幻觉文本的现象。在本文中，我们对虚假前提幻觉进行了全面分析，并阐明了其内部工作机制：一小部分注意力头（我们将其称为虚假前提头）干扰了知识提取过程，导致虚假前提幻觉的发生。根据我们的分析，我们提出 \textbf{FAITH} （\textbf{F}也是前提 \textbf{A}ttention head constra\textbf{I}ining for mi\textbf{T}igating \textbf{H}allucinations），一种减轻虚假前提幻觉的新颖而有效的方法。它限制了模型推理过程中的错误前提注意头。令人印象深刻的是，大量实验表明，仅限制模型中大约 $1\%$ 的注意力头就能使模型性能显着提高近 $20\%$。</li>
</ul>

<h3>Title: How to Understand "Support"? An Implicit-enhanced Causal Inference  Approach for Weakly-supervised Phrase Grounding</h3>
<ul>
<li><strong>Authors: </strong>Jiamin Luo, Jianing Zhao, Jingjing Wang, Guodong Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19116">https://arxiv.org/abs/2402.19116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19116">https://arxiv.org/pdf/2402.19116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19116]] How to Understand "Support"? An Implicit-enhanced Causal Inference  Approach for Weakly-supervised Phrase Grounding(https://arxiv.org/abs/2402.19116)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting finding that IECI outperforms the advanced multimodal LLMs by a large margin on this implicit-enhanced dataset, which may facilitate more research to evaluate the multimodal LLMs in this direction.</li>
<li><strong>摘要：</strong>弱监督短语基础（WPG）是一项新兴任务，它推断细粒度的短语区域匹配，同时仅利用粗粒度的句子-图像对进行训练。然而，现有的WPG研究很大程度上忽略了隐含的短语区域匹配关系，这对于评估模型理解深层多模态语义的能力至关重要。为此，本文提出了一种隐式增强因果推理（IECI）方法来解决对隐式关系进行建模并在显式关系之外突出它们的挑战。具体来说，这种方法利用干预和反事实技术分别应对上述两个挑战。此外，还注释了高质量的隐式增强数据集来评估 IECI，详细的评估显示了 IECI 相对于最先进的基线的巨大优势。特别是，我们观察到一个有趣的发现，即 IECI 在这个隐式增强数据集上大大优于先进的多模态 LLM，这可能有助于更多研究在这个方向上评估多模态 LLM。</li>
</ul>

<h3>Title: Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale  Annotations</h3>
<ul>
<li><strong>Authors: </strong>Stephanie Brandl, Oliver Eberle, Tiago Ribeiro, Anders Søgaard, Nora Hollenstein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19133">https://arxiv.org/abs/2402.19133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19133">https://arxiv.org/pdf/2402.19133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19133]] Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale  Annotations(https://arxiv.org/abs/2402.19133)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Rationales in the form of manually annotated input spans usually serve as ground truth when evaluating explainability methods in NLP. They are, however, time-consuming and often biased by the annotation process. In this paper, we debate whether human gaze, in the form of webcam-based eye-tracking recordings, poses a valid alternative when evaluating importance scores. We evaluate the additional information provided by gaze data, such as total reading times, gaze entropy, and decoding accuracy with respect to human rationale annotations. We compare WebQAmGaze, a multilingual dataset for information-seeking QA, with attention and explainability-based importance scores for 4 different multilingual Transformer-based language models (mBERT, distil-mBERT, XLMR, and XLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily be applied to other tasks and languages. Our findings suggest that gaze data offers valuable linguistic insights that could be leveraged to infer task difficulty and further show a comparable ranking of explainability methods to that of human rationales.</li>
<li><strong>摘要：</strong>在评估 NLP 中的可解释性方法时，手动注释输入范围形式的基本原理通常用作基本事实。然而，它们非常耗时，并且常常受到注释过程的影响。在本文中，我们讨论基于网络摄像头的眼球跟踪记录形式的人类凝视在评估重要性分数时是否构成有效的替代方案。我们评估注视数据提供的附加信息，例如总阅读时间、注视熵和相对于人类基本原理注释的解码准确性。我们将 WebQAmGaze（用于信息寻求 QA 的多语言数据集）与 4 种不同的基于 Transformer 的多语言语言模型（mBERT、distil-mBERT、XLMR 和 XLMR-L）和 3 种语言（英语、英语、西班牙语和德语）。我们的管道可以轻松应用于其他任务和语言。我们的研究结果表明，注视数据提供了有价值的语言见解，可用于推断任务难度，并进一步显示可解释性方法与人类基本原理的可比排名。</li>
</ul>

<h3>Title: Beyond Language Models: Byte Models are Digital World Simulators</h3>
<ul>
<li><strong>Authors: </strong>Shangda Wu, Xu Tan, Zili Wang, Rui Wang, Xiaobing Li, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19155">https://arxiv.org/abs/2402.19155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19155">https://arxiv.org/pdf/2402.19155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19155]] Beyond Language Models: Byte Models are Digital World Simulators(https://arxiv.org/abs/2402.19155)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Traditional deep learning often overlooks bytes, the basic units of the digital world, where all forms of information and operations are encoded and manipulated in binary format. Inspired by the success of next token prediction in natural language processing, we introduce bGPT, a model with next byte prediction to simulate the digital world. bGPT matches specialized models in performance across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing algorithm or hardware behaviour. It has almost flawlessly replicated the process of converting symbolic music data, achieving a low error rate of 0.0011 bits per byte in converting ABC notation to MIDI format. In addition, bGPT demonstrates exceptional capabilities in simulating CPU behaviour, with an accuracy exceeding 99.99% in executing various operations. Leveraging next byte prediction, models like bGPT can directly learn from vast binary data, effectively simulating the intricate patterns of the digital world.</li>
<li><strong>摘要：</strong>传统的深度学习经常忽视字节，这是数字世界的基本单位，其中所有形式的信息和操作都以二进制格式进行编码和操作。受到自然语言处理中下一个标记预测成功的启发，我们引入了 bGPT，这是一个具有下一个字节预测功能的模型，用于模拟数字世界。 bGPT 与各种模式（包括文本、音频和图像）的性能专业模型相匹配，并为预测、模拟和诊断算法或硬件行为提供了新的可能性。它几乎完美地复制了符号音乐数据的转换过程，在将 ABC 符号转换为 MIDI 格式时实现了每字节 0.0011 位的低错误率。此外，bGPT 在模拟 CPU 行为方面表现出卓越的能力，执行各种操作的准确度超过 99.99%。利用下一个字节预测，bGPT 等模型可以直接从大量二进制数据中学习，有效地模拟数字世界的复杂模式。</li>
</ul>

<h3>Title: Teaching Large Language Models an Unseen Language on the Fly</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhang, Xiao Liu, Jiuheng Lin, Yansong Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19167">https://arxiv.org/abs/2402.19167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19167">https://arxiv.org/pdf/2402.19167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19167]] Teaching Large Language Models an Unseen Language on the Fly(https://arxiv.org/abs/2402.19167)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce \textsc{DiPMT++}, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and only 5K parallel sentences, \textsc{DiPMT++} significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity.</li>
<li><strong>摘要：</strong>现有的大型语言模型很难支持多种资源匮乏的语言，特别是那些资源极少的语言，其中可用于有效参数更新的训练数据最少。因此，我们调查法学硕士是否可以仅通过提示即时学习一门新语言。为了研究这个问题，我们收集了壮语的研究套件，目前尚无法学硕士支持这种语言。我们引入了 \textsc{DiPMT++}，一个通过上下文学习使法学硕士适应未见过的语言的框架。使用字典和仅 5K 并行句子，\textsc{DiPMT++} 显着增强了 GPT-4 汉语到壮语翻译的 BLEU 从 0 到 16 的性能，并实现了壮语到汉语翻译的 32 BLEU。此外，我们还展示了该框架在帮助人类翻译完全看不见的语言方面的实际效用，这可能有助于保护语言多样性。</li>
</ul>

<h3>Title: PeLLE: Encoder-based language models for Brazilian Portuguese based on  open data</h3>
<ul>
<li><strong>Authors: </strong>Guilherme Lamartine de Mello, Marcelo Finger, and Felipe Serras, Miguel de Mello Carpi, Marcos Menon Jose, Pedro Henrique Domingues, Paulo Cavalim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19204">https://arxiv.org/abs/2402.19204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19204">https://arxiv.org/pdf/2402.19204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19204]] PeLLE: Encoder-based language models for Brazilian Portuguese based on  open data(https://arxiv.org/abs/2402.19204)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper we present PeLLE, a family of large language models based on the RoBERTa architecture, for Brazilian Portuguese, trained on curated, open data from the Carolina corpus. Aiming at reproducible results, we describe details of the pretraining of the models. We also evaluate PeLLE models against a set of existing multilingual and PT-BR refined pretrained Transformer-based LLM encoders, contrasting performance of large versus smaller-but-curated pretrained models in several downstream tasks. We conclude that several tasks perform better with larger models, but some tasks benefit from smaller-but-curated data in its pretraining.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 PeLLE，这是一个基于 RoBERTa 架构的大型语言模型系列，适用于巴西葡萄牙语，并使用来自卡罗来纳语料库的精选开放数据进行训练。为了获得可重复的结果，我们描述了模型预训练的细节。我们还针对一组现有的多语言和 PT-BR 精炼的基于 Transformer 的 LLM 编码器评估 PeLLE 模型，比较大型预训练模型与较小但精心设计的预训练模型在多个下游任务中的性能。我们得出的结论是，一些任务在较大的模型中表现得更好，但有些任务在预训练中受益于较小但经过整理的数据。</li>
</ul>

<h3>Title: Memory-Augmented Generative Adversarial Transformers</h3>
<ul>
<li><strong>Authors: </strong>Stephan Raaijmakers, Roos Bakker, Anita Cremers, Roy de Kleijn, Tom Kouwenhoven, Tessa Verhoef</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19218">https://arxiv.org/abs/2402.19218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19218">https://arxiv.org/pdf/2402.19218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19218]] Memory-Augmented Generative Adversarial Transformers(https://arxiv.org/abs/2402.19218)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy. This paper investigates a possible route for addressing this problem. We propose to extend the standard Transformer architecture with an additional memory bank holding extra information (such as facts drawn from a knowledge base), and an extra attention layer for addressing this memory. We add this augmented memory to a Generative Adversarial Network-inspired Transformer architecture. This setup allows for implementing arbitrary felicity conditions on the generated language of the Transformer. We first demonstrate how this machinery can be deployed for handling factual questions in goal-oriented dialogues. Secondly, we demonstrate that our approach can be useful for applications like {\it style adaptation} as well: the adaptation of utterances according to certain stylistic (external) constraints, like social properties of human interlocutors in dialogues.</li>
<li><strong>摘要：</strong>依赖于大型语言模型的对话式人工智能系统（例如 Transformer）很难将外部数据（例如事实）与它们生成的语言交织在一起。 Vanilla Transformer 架构并不是为高精度回答事实问题而设计的。本文研究了解决该问题的可能途径。我们建议扩展标准 Transformer 架构，使用一个额外的内存库来保存额外的信息（例如从知识库中提取的事实），以及一个用于处理该内存的额外的关注层。我们将这种增强记忆添加到受生成对抗网络启发的 Transformer 架构中。此设置允许在 Transformer 生成的语言上实现任意适当条件。我们首先演示如何部署该机制来处理面向目标的对话中的事实问题。其次，我们证明我们的方法对于诸如风格适应之类的应用也很有用：根据某些风格（外部）约束对话语进行适应，例如对话中人类对话者的社会属性。</li>
</ul>

<h3>Title: Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question  Answering Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Zhikun Xu, Yinghui Li, Ruixue Ding, Xinyu Wang, Boli Chen, Yong Jiang, Xiaodong Deng, Jianxin Ma, Hai-Tao Zheng, Wenlian Lu, Pengjun Xie, Chang Zhou, Fei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19248">https://arxiv.org/abs/2402.19248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19248">https://arxiv.org/pdf/2402.19248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19248]] Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question  Answering Benchmark(https://arxiv.org/abs/2402.19248)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>How to better evaluate the capabilities of Large Language Models (LLMs) is the focal point and hot topic in current LLMs research. Previous work has noted that due to the extremely high cost of iterative updates of LLMs, they are often unable to answer the latest dynamic questions well. To promote the improvement of Chinese LLMs' ability to answer dynamic questions, in this paper, we introduce CDQA, a Chinese Dynamic QA benchmark containing question-answer pairs related to the latest news on the Chinese Internet. We obtain high-quality data through a pipeline that combines humans and models, and carefully classify the samples according to the frequency of answer changes to facilitate a more fine-grained observation of LLMs' capabilities. We have also evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA. Extensive experiments and valuable insights suggest that our proposed CDQA is challenging and worthy of more further study. We believe that the benchmark we provide will become the key data resource for improving LLMs' Chinese question-answering ability in the future.</li>
<li><strong>摘要：</strong>如何更好地评估大型语言模型（LLM）的能力是当前LLM研究的焦点和热点。之前的工作指出，由于LLM迭代更新的成本极高，他们往往无法很好地回答最新的动态问题。为了促进中国法学硕士回答动态问题的能力的提高，在本文中，我们引入了CDQA，一个包含与中国互联网最新新闻相关的问答对的中文动态问答基准。我们通过人类和模型相结合的管道获取高质量的数据，并根据答案变化的频率对样本进行仔细分类，以便于更细粒度地观察法学硕士的能力。我们还对中国主流和先进的法学硕士在CDQA上进行了评估和分析。大量的实验和有价值的见解表明，我们提出的 CDQA 具有挑战性，值得进一步研究。我们相信，我们提供的基准将成为未来提高法学硕士中文答疑能力的关键数据资源。</li>
</ul>

<h3>Title: GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of  LLMs as Mathematical Problem Solvers</h3>
<ul>
<li><strong>Authors: </strong>Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19255">https://arxiv.org/abs/2402.19255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19255">https://arxiv.org/pdf/2402.19255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19255]] GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of  LLMs as Mathematical Problem Solvers(https://arxiv.org/abs/2402.19255)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (\datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result. Code and data are available at \url{https://github.com/qtli/GSM-Plus}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种数学推理基准测试中取得了令人印象深刻的性能。然而，关于这些模型是否真正理解和应用数学知识，还是仅仅依赖数学推理的捷径，争论越来越多。一个重要且经常出现的证据是，当数学问题稍有改变时，法学硕士可能会表现不正确。这促使我们通过测试各种问题的变化来评估法学硕士数学推理能力的稳健性。我们介绍了对抗性小学数学 (\datasetname) 数据集，它是 GSM8K 的扩展，并增加了各种数学扰动。我们对 25 名法学硕士和 4 种提示技巧进行的实验表明，虽然法学硕士表现出不同水平的数学推理能力，但他们的表现远非稳健。特别是，即使对于 GSM8K 中已解决的问题，法学硕士在添加新陈述或更改问题目标时也可能会出错。我们还探讨了是否可以通过组合现有的提示方法来实现更鲁棒的性能，其中我们尝试一种迭代方法，根据其推理目标和计算结果生成并验证每个中间思想。代码和数据可在 \url{https://github.com/qtli/GSM-Plus} 获取。</li>
</ul>

<h3>Title: PlanGPT: Enhancing Urban Planning with Tailored Language Model and  Efficient Retrieval</h3>
<ul>
<li><strong>Authors: </strong>He Zhu, Wenjia Zhang, Nuoxian Huang, Boyang Li, Luyao Niu, Zipei Fan, Tianle Lun, Yicheng Tao, Junyou Su, Zhaoya Gong, Chenyu Fang, Xing Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19273">https://arxiv.org/abs/2402.19273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19273">https://arxiv.org/pdf/2402.19273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19273]] PlanGPT: Enhancing Urban Planning with Tailored Language Model and  Efficient Retrieval(https://arxiv.org/abs/2402.19273)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.</li>
<li><strong>摘要：</strong>在城市规划领域，通用大型语言模型往往难以满足规划者的特定需求。生成城市规划文本、检索相关信息和评估规划文件等任务带来了独特的挑战。为了提高城市专业人员的效率并克服这些障碍，我们推出了 PlanGPT，这是第一个为城市和空间规划量身定制的专业大型语言模型。 PlanGPT 通过与中国城市规划研究院等机构合作开发，利用定制的本地数据库检索框架、基础模型的特定领域微调以及先进的工具功能。实证测试表明，PlanGPT 已实现先进的性能，提供针对复杂的城市规划精确定制的优质响应。</li>
</ul>

<h3>Title: WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset</h3>
<ul>
<li><strong>Authors: </strong>Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia Yu, ChaoBin Zhang, Pei Chu, Yuan Qu, Runyu Peng, Zhiyuan Zeng, Huanze Tang, Ruiliang Xu, Wei Li, Hang Yan, Conghui He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19282">https://arxiv.org/abs/2402.19282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19282">https://arxiv.org/pdf/2402.19282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19282]] WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset(https://arxiv.org/abs/2402.19282)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper presents WanJuan-CC, a safe and high-quality open-sourced English webtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which require vast amounts of high-quality data. A comprehensive process was designed to handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data according to their needs. To evaluate the quality and utility of the dataset, we trained 1B-parameter and 3B-parameter models using WanJuan-CC and another dataset, RefinedWeb. Results show that WanJuan-CC performs better on validation datasets and downstream tasks.</li>
<li><strong>摘要：</strong>本文提出了 WanJuan-CC，一个源自 Common Crawl 数据的安全且高质量的开源英语网络文本数据集。该研究解决了为语言模型构建大规模预训练数据集的挑战，这需要大量高质量的数据。设计了一套全面的流程来处理Common Crawl数据，包括提取、启发式规则过滤、模糊去重、内容安全过滤和数据质量过滤。我们从约 680 亿份英文原始文档中，获得了 2.22T 安全数据 Token，并筛选出 1.0T 优质数据 Token 作为万卷 CC 的一部分。我们从这个数据集中开源了 300B 代币。论文还提供了与数据质量相关的统计信息，使用户能够根据自己的需要选择合适的数据。为了评估数据集的质量和实用性，我们使用 WanJuan-CC 和另一个数据集RefinedWeb 训练了 1B 参数和 3B 参数模型。结果表明，WanJuan-CC 在验证数据集和下游任务上表现更好。</li>
</ul>

<h3>Title: RL-GPT: Integrating Reinforcement Learning and Code-as-policy</h3>
<ul>
<li><strong>Authors: </strong>Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19299">https://arxiv.org/abs/2402.19299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19299">https://arxiv.org/pdf/2402.19299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19299]] RL-GPT: Integrating Reinforcement Learning and Code-as-policy(https://arxiv.org/abs/2402.19299)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all designated MineDojo tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已证明能够通过编码熟练地利用各种工具，但它们在处理复杂逻辑和精确控制方面面临限制。在具体任务中，高层规划适合直接编码，而低层行动通常需要针对特定​​任务的细化，例如强化学习（RL）。为了无缝集成这两种模式，我们引入了一个两级分层框架 RL-GPT，由慢速代理和快速代理组成。慢代理分析适合编码的动作，而快代理执行编码任务。这种分解有效地将每个代理集中在特定任务上，证明在我们的管道中非常高效。我们的方法优于传统的 RL 方法和现有的 GPT 代理，展现出卓越的效率。在《我的世界》游戏中，在RTX3090上，一天之内就可以快速获得钻石。此外，它在所有指定的 MineDojo 任务中都实现了 SOTA 性能。</li>
</ul>

<h3>Title: Learnability Gaps of Strategic Classification</h3>
<ul>
<li><strong>Authors: </strong>Lee Cohen, Yishay Mansour, Shay Moran, Han Shao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19303">https://arxiv.org/abs/2402.19303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19303">https://arxiv.org/pdf/2402.19303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19303]] Learnability Gaps of Strategic Classification(https://arxiv.org/abs/2402.19303)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In contrast with standard classification tasks, strategic classification involves agents strategically modifying their features in an effort to receive favorable predictions. For instance, given a classifier determining loan approval based on credit scores, applicants may open or close their credit cards to fool the classifier. The learning goal is to find a classifier robust against strategic manipulations. Various settings, based on what and when information is known, have been explored in strategic classification. In this work, we focus on addressing a fundamental question: the learnability gaps between strategic classification and standard learning. We essentially show that any learnable class is also strategically learnable: we first consider a fully informative setting, where the manipulation structure (which is modeled by a manipulation graph $G^\star$) is known and during training time the learner has access to both the pre-manipulation data and post-manipulation data. We provide nearly tight sample complexity and regret bounds, offering significant improvements over prior results. Then, we relax the fully informative setting by introducing two natural types of uncertainty. First, following Ahmadi et al. (2023), we consider the setting in which the learner only has access to the post-manipulation data. We improve the results of Ahmadi et al. (2023) and close the gap between mistake upper bound and lower bound raised by them. Our second relaxation of the fully informative setting introduces uncertainty to the manipulation structure. That is, we assume that the manipulation graph is unknown but belongs to a known class of graphs. We provide nearly tight bounds on the learning complexity in various unknown manipulation graph settings. Notably, our algorithm in this setting is of independent interest and can be applied to other problems such as multi-label learning.</li>
<li><strong>摘要：</strong>与标准分类任务相比，策略分类涉及代理策略性地修改其特征，以努力获得有利的预测。例如，假设分类器根据信用评分确定贷款批准，申请人可以打开或关闭他们的信用卡来欺骗分类器。学习目标是找到一个能够抵抗策略操纵的分类器。战略分类中已经探索了基于信息内容和信息已知时间的各种设置。在这项工作中，我们专注于解决一个基本问题：战略分类和标准学习之间的可学习性差距。我们本质上表明，任何可学习的类在策略上也是可学习的：我们首先考虑一个完全信息化的设置，其中操作结构（由操作图 $G^\star$ 建模）是已知的，并且在训练期间学习者可以访问包括处理前数据和处理后数据。我们提供了近乎严格的样本复杂性和后悔界限，与之前的结果相比有了显着的改进。然后，我们通过引入两种自然类型的不确定性来放松完全信息设置。首先，跟随艾哈迈迪等人。 （2023），我们考虑学习者只能访问操作后数据的设置。我们改进了 Ahmadi 等人的结果。 （2023）并缩小了他们提出的错误上限和下限之间的差距。我们对完全信息设置的第二次放宽给操纵结构带来了不确定性。也就是说，我们假设操作图是未知的，但属于已知的图类。我们对各种未知操作图设置中的学习复杂性提供了近乎严格的限制。值得注意的是，我们在这种情况下的算法具有独立的兴趣，可以应用于其他问题，例如多标签学习。</li>
</ul>

<h3>Title: Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge</h3>
<ul>
<li><strong>Authors: </strong>Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, Qiongkai Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19334">https://arxiv.org/abs/2402.19334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19334">https://arxiv.org/pdf/2402.19334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19334]] Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge(https://arxiv.org/abs/2402.19334)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The democratization of pre-trained language models through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies. However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability. This paper suggests that merging a backdoored model with other homogeneous models can remediate backdoor vulnerabilities even if such models are not entirely secure. In our experiments, we explore various models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks without additional resources or specific knowledge. Our approach consistently outperforms the other advanced baselines, leading to an average of 75% reduction in the attack success rate. Since model merging has been an established approach for improving model performance, the extra advantage it provides regarding defense can be seen as a cost-free bonus.</li>
<li><strong>摘要：</strong>通过开源计划实现的预训练语言模型的民主化迅速推动了创新并扩大了对尖端技术的获取。然而，这种开放性也带来了重大的安全风险，包括后门攻击，其中隐藏的恶意行为由特定输入触发，损害自然语言处理（NLP）系统的完整性和可靠性。本文建议，将后门模型与其他同类模型合并可以修复后门漏洞，即使这些模型并不完全安全。在我们的实验中，我们探索了各种模型（BERT-Base、RoBERTa-Large、Llama2-7B 和 Mistral-7B）和数据集（SST-2、OLID、AG News 和 QNLI）。与多种先进的防御方法相比，我们的方法提供了针对后门攻击的有效且高效的推理阶段防御，而无需额外的资源或特定知识。我们的方法始终优于其他先进基线，导致攻击成功率平均降低 75%。由于模型合并已成为提高模型性能的既定方法，因此它在防御方面提供的额外优势可以被视为免费的奖励。</li>
</ul>

<h3>Title: Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy,  Advances, and Outlook</h3>
<ul>
<li><strong>Authors: </strong>Xingchen Zou, Yibo Yan, Xixuan Hao, Yuehong Hu, Haomin Wen, Erdong Liu, Junbo Zhang, Yong Li, Tianrui Li, Yu Zheng, Yuxuan Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19348">https://arxiv.org/abs/2402.19348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19348">https://arxiv.org/pdf/2402.19348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19348]] Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy,  Advances, and Outlook(https://arxiv.org/abs/2402.19348)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applications into seven types: urban planning, transportation, economy, public safety, society, environment, and energy. Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications. Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field. We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community. The summary of the comprehensive and up-to-date paper list can be found at https://github.com/yoshall/Awesome-Multimodal-Urban-Computing.</li>
<li><strong>摘要：</strong>随着城市的不断发展，城市计算通过利用不同来源（例如地理、交通、社交媒体和环境数据）和模式（例如空间数据）的跨域数据融合的力量，成为可持续发展的关键学科。时间、视觉和文本模式）。最近，我们看到利用各种深度学习方法促进智慧城市跨领域数据融合的上升趋势。为此，我们提出了第一项调查，系统地回顾了为城市计算量身定制的基于深度学习的数据融合方法的最新进展。具体来说，我们首先深入研究数据视角，以理解每种模式和数据源的作用。其次，我们将该方法分为四个主要类别：基于特征、基于对齐、基于对比度和基于生成的融合方法。第三，我们将多模态城市应用进一步分为城市规划、交通、经济、公共安全、社会、环境和能源七类。与之前的调查相比，我们更关注深度学习方法与城市计算应用的协同作用。此外，我们阐明了大型语言模型（LLM）和城市计算之间的相互作用，提出了可能彻底改变该领域的未来研究方向。我们坚信，我们的调查中描绘的分类、进展和前景将极大地丰富研究界。全面且最新的论文列表摘要可以在 https://github.com/yoshall/Awesome-Multimodal-Urban-Computing 找到。</li>
</ul>

<h3>Title: Prompting Explicit and Implicit Knowledge for Multi-hop Question  Answering Based on Human Reading Process</h3>
<ul>
<li><strong>Authors: </strong>Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19350">https://arxiv.org/abs/2402.19350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19350">https://arxiv.org/pdf/2402.19350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19350]] Prompting Explicit and Implicit Knowledge for Multi-hop Question  Answering Based on Human Reading Process(https://arxiv.org/abs/2402.19350)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reasoning. Furthermore, our model incorporates type-specific reasoning via prompts, a form of implicit knowledge. Experimental results show that PEI performs comparably to the state-of-the-art on HotpotQA. Ablation studies confirm the efficacy of our model in bridging and integrating explicit and implicit knowledge.</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 利用思维链 (CoT) 来模拟人类推理和推理过程，从而在多跳 QA 中实现出色的性能。然而，在解决复杂问题时，PLM 的推理能力与人类之间仍然存在差距。心理学研究表明，段落中的明确信息与人类在阅读过程中的先验知识之间存在着重要的联系。然而，当前的研究从人类认知研究的角度对输入通道和 PLM 基于预训练的知识的联系给予了足够的重视。在这项研究中，我们引入了一个\textbf{P}提示\textbf{E}显性和\textbf{I}隐性知识（PEI）框架，该框架使用提示来连接显性和隐性知识，与人类阅读过程相一致，以实现多种跳质量检查。我们将输入段落视为显性知识，利用它们通过统一的即时推理来引出隐性知识。此外，我们的模型通过提示（一种隐性知识的形式）结合了特定类型的推理。实验结果表明，PEI 在 HotpotQA 上的表现与最先进的技术相当。消融研究证实了我们的模型在桥接和整合显性和隐性知识方面的有效性。</li>
</ul>

<h3>Title: OpenMedLM: Prompt engineering can out-perform fine-tuning in medical  question-answering with open-source large language models</h3>
<ul>
<li><strong>Authors: </strong>Jenish Maharjan, Anurag Garikipati, Navan Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, Ritankar Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19371">https://arxiv.org/abs/2402.19371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19371">https://arxiv.org/pdf/2402.19371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19371]] OpenMedLM: Prompt engineering can out-perform fine-tuning in medical  question-answering with open-source large language models(https://arxiv.org/abs/2402.19371)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-shot, few-shot, chain-of-thought (random selection and kNN selection), and ensemble/self-consistency voting. We found that OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks, surpassing the previous best performing OS models that leveraged computationally costly extensive fine-tuning. The model delivers a 72.6% accuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and achieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the first OS LLM to surpass 80% accuracy on this benchmark. Our results highlight medical-specific emergent properties in OS LLMs which have not yet been documented to date elsewhere, and showcase the benefits of further leveraging prompt engineering to improve the performance of accessible LLMs for medical applications.</li>
<li><strong>摘要：</strong>法学硕士完成一系列专业任务的能力越来越强，可以用来扩大医学知识的公平获取。大多数医学法学硕士都涉及广泛的微调，利用专门的医学数据和大量（因此成本高昂）的计算能力。许多表现最好的法学硕士都是专有的，只有极少数的研究小组才能访问它们。然而，开源 (OS) 模型代表了医学法学硕士的一个关键增长领域，因为其性能显着提高，并且具有提供医疗保健所需的透明度和合规性的固有能力。我们推出 OpenMedLM，这是一个提示平台，可为 OS LLM 在医学基准方面提供最先进的 (SOTA) 性能。我们根据四个医学基准（MedQA、MedMCQA、PubMedQA、MMLU 医学子集）评估了一系列操作系统基础法学硕士 (7B-70B)。我们采用了一系列的提示策略，包括零样本、少样本、思维链（随机选择和 kNN 选择）以及集成/自洽投票。我们发现 OpenMedLM 在三个常见的医学 LLM 基准上提供了操作系统 SOTA 结果，超越了之前利用计算成本高昂的广泛微调的最佳性能操作系统模型。该模型在 MedQA 基准上的准确率达到 72.6%，比之前的 SOTA 提高了 2.4%，并且在 MMLU 医学子集上达到了 81.7% 的准确率，成为第一个在此基准上超过 80% 准确率的 OS LLM。我们的结果强调了操作系统法学硕士中迄今为止尚未在其他地方记录的医疗特定的新兴特性，并展示了进一步利用即时工程来提高可访问的法学硕士在医疗应用中的性能的好处。</li>
</ul>

<h3>Title: On the Scaling Laws of Geographical Representation in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nathan Godey, Éric de la Clergerie, Benoît Sagot</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19406">https://arxiv.org/abs/2402.19406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19406">https://arxiv.org/pdf/2402.19406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19406]] On the Scaling Laws of Geographical Representation in Language Models(https://arxiv.org/abs/2402.19406)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Language models have long been shown to embed geographical information in their hidden representations. This line of work has recently been revisited by extending this result to Large Language Models (LLMs). In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when scaling language models. We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size. Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.</li>
<li><strong>摘要：</strong>长期以来，语言模型已被证明可以将地理信息嵌入到其隐藏表示中。最近，通过将此结果扩展到大型语言模型（LLM），人们重新审视了这一领域的工作。在本文中，我们建议通过观察在扩展语言模型时地理知识如何演变来填补成熟文献和最新文献之间的差距。我们证明，即使对于微小的模型，地理知识也是可观察的，并且随着我们增加模型大小，它会持续扩展。值得注意的是，我们观察到更大的语言模型无法减轻训练数据固有的地理偏差。</li>
</ul>

<h3>Title: Griffin: Mixing Gated Linear Recurrences with Local Attention for  Efficient Language Models</h3>
<ul>
<li><strong>Authors: </strong>Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar Gulcehre</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19427">https://arxiv.org/abs/2402.19427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19427">https://arxiv.org/pdf/2402.19427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19427]] Griffin: Mixing Gated Linear Recurrences with Local Attention for  Efficient Language Models(https://arxiv.org/abs/2402.19427)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.</li>
<li><strong>摘要：</strong>循环神经网络 (RNN) 在长序列上具有快速推理和高效扩展的能力，但它们难以训练且难以扩展。我们提出了 Hawk（一种具有门控线性递归的 RNN）和 Griffin（一种将门控线性递归与局部注意力混合的混合模型）。 Hawk 在下游任务上超过了 Mamba 报告的性能，而 Griffin 则与 Llama-2 的性能相当，尽管训练的 token 数量少了 6 倍多。我们还表明，格里芬可以推断出比训练期间看到的序列长得多的序列。我们的模型在训练期间与 Transformers 的硬件效率相匹配，并且在推理期间它们具有更低的延迟和显着更高的吞吐量。我们将 Griffin 扩展到 14B 参数，并解释如何对模型进行分片以进行高效的分布式训练。</li>
</ul>

<h3>Title: ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</h3>
<ul>
<li><strong>Authors: </strong>Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19446">https://arxiv.org/abs/2402.19446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19446">https://arxiv.org/pdf/2402.19446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19446]] ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL(https://arxiv.org/abs/2402.19446)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or "agent" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的一个广泛用例是目标导向的决策任务（或“代理”任务），其中 LLM 不仅需要为给定的提示生成补全，还需要针对多个问题做出智能决策。 -转向交互来完成任务（例如，与网络交互、使用工具或提供客户支持时）。强化学习 (RL) 提供了解决此类代理任务的通用范例，但目前用于法学硕士的 RL 方法主要侧重于优化单轮奖励。通过构建，大多数单轮强化学习方法无法赋予法学硕士在多轮中智能地寻找信息、执行信用分配或推理其过去行为的能力——所有这些在智能体任务中都至关重要。这就提出了一个问题：我们如何为法学硕士设计有效且高效的多轮强化学习算法？在本文中，我们开发了一个框架，用于构建用于微调 LLM 的多轮 RL 算法，该框架保留了 LLM 现有单轮 RL 方法的灵活性（例如，近端策略优化），同时适应多轮、长视野、并有效延迟奖励。为此，我们的框架采用分层 RL 方法，并并行运行两种 RL 算法：一种用于聚合话语奖励的基于离策略价值的高级 RL 算法，以及一种利用这种高级的低级 RL 算法。价值函数来训练每个话语或回合内的令牌策略。我们的分层框架，具有分层结构的 Actor-Critic Framework (ArCHer)，也可以产生其他 RL 方法。根据经验，我们发现 ArCHer 显着提高了代理任务的效率和性能，实现了现有方法约 100 倍的样本效率，同时还通过更大的模型容量（高达我们测试的 70 亿规模）进行了改进。</li>
</ul>

<h3>Title: Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent  on Language Models</h3>
<ul>
<li><strong>Authors: </strong>Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, Alberto Bietti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19449">https://arxiv.org/abs/2402.19449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19449">https://arxiv.org/pdf/2402.19449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19449]] Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent  on Language Models(https://arxiv.org/abs/2402.19449)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens. We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes. To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision CNNs, and linear models. We further study this phenomenon on a linear classification with cross-entropy loss, showing that heavy-tailed class imbalance leads to ill-conditioning, and that the normalization used by Adam can counteract it.</li>
<li><strong>摘要：</strong>Adam 已被证明在优化大型语言转换器方面优于梯度下降，并且比其他任务的优势更大，但尚不清楚为什么会发生这种情况。我们表明，语言建模任务中发现的重尾类不平衡会导致优化动态方面的困难。当使用梯度下降进行训练时，与不频繁单词相关的损失比与频繁单词相关的损失下降得慢。由于大多数样本来自相对不频繁的单词，平均损失随着梯度下降缓慢下降。另一方面，Adam 和基于符号的方法不会遇到这个问题，并且改进了所有类别的预测。为了确定这种行为确实是由类别不平衡引起的，我们凭经验证明它在语言转换器、视觉 CNN 和线性模型上的不同架构和数据类型中持续存在。我们在具有交叉熵损失的线性分类上进一步研究了这种现象，表明重尾类不平衡会导致病态，而 Adam 使用的归一化可以抵消它。</li>
</ul>

<h3>Title: Functional Benchmarks for Robust Evaluation of Reasoning Performance,  and the Reasoning Gap</h3>
<ul>
<li><strong>Authors: </strong>Saurabh Srivastava, Annarose M B, Anto P V, Shashank Menon, Ajay Sukumar, Adwaith Samod T, Alan Philipose, Stevin Prince, Sooraj Thomas</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19450">https://arxiv.org/abs/2402.19450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19450">https://arxiv.org/pdf/2402.19450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19450]] Functional Benchmarks for Robust Evaluation of Reasoning Performance,  and the Reasoning Gap(https://arxiv.org/abs/2402.19450)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building "gap 0" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.</li>
<li><strong>摘要：</strong>我们提出了一个使用基准的功能变体对语言模型的推理能力进行稳健评估的框架。与功能变体的快照相比，解决推理测试的模型在问题的静态版本上应该没有表现出性能差异。我们已将 MATH 基准的相关片段重写为其函数变体 MATH()，其他基准的函数化也将遵循。当通过 MATH() 的快照评估当前最先进的模型时，我们发现了一个推理差距——静态精度和函数精度之间的百分比差异。我们发现在静态基准上表现良好的最先进的封闭式和开放式权重模型之间的推理差距从 58.35% 到 80.31%，但需要注意的是，通过更复杂的提示策略，差距可能会更小。在这里，我们展示了那些据说在现实世界任务中具有良好推理性能的模型，具有可量化的较低差距，从而激发了构建“差距 0”模型的开放问题。评估代码和新评估数据集、三个 MATH() 快照可在 https://github.com/consequentai/fneval/ 上公开获取。</li>
</ul>

<h3>Title: Curiosity-driven Red-teaming for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja, James Glass, Akash Srivastava, Pulkit Agrawal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19464">https://arxiv.org/abs/2402.19464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19464">https://arxiv.org/pdf/2402.19464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19464]] Curiosity-driven Red-teaming for Large Language Models(https://arxiv.org/abs/2402.19464)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods. Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at \url{https://github.com/Improbable-AI/curiosity_redteam}</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在许多自然语言应用中具有巨大潜力，但存在生成不正确或有毒内容的风险。为了探究法学硕士何时生成不需要的内容，当前的范例是招募一个由人类测试人员组成的 \textit{red team} 来设计输入提示（即测试用例），从而引发法学硕士的不良反应。然而，仅依靠人类测试人员既昂贵又耗时。最近的工作通过强化学习（RL）训练一个单独的红队法学硕士来生成测试用例，最大限度地提高从目标法学硕士引出不良反应的机会，从而实现红队自动化。然而，当前的强化学习方法只能生成少量有效的测试用例，导致提示的覆盖范围较低，从而引起目标法学硕士的不良反应。为了克服这一限制，我们将增加生成的测试用例的覆盖范围的问题与经过充分研究的好奇心驱动的探索方法（优化新颖性）联系起来。与现有方法相比，我们的好奇心驱动的红队 (CRT) 方法实现了更大的测试用例覆盖范围，同时保持或提高了其有效性。我们的方法 CRT 成功地激发了 LLaMA2 模型的毒性反应，该模型已根据人类偏好进行了大量微调，以避免有毒输出。代码可在 \url{https://github.com/Improbable-AI/curiosity_redteam} 获取</li>
</ul>

<h3>Title: Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period  of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Qian, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong Liu, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19465">https://arxiv.org/abs/2402.19465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19465">https://arxiv.org/pdf/2402.19465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19465]] Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period  of Large Language Models(https://arxiv.org/abs/2402.19465)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression~\citep{shwartz2017opening}. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field. We will make our code publicly accessible at \url{https://github.com/ChnQ/TracingLLM}.</li>
<li><strong>摘要：</strong>确保大型语言模型 (LLM) 的可信度至关重要。大多数研究集中于经过充分预培训的法学硕士，以更好地理解和提高法学硕士的可信度。在本文中，为了揭示预训练尚未开发的潜力，我们率先探索了这一时期法学硕士的可信度，重点关注五个关键维度：可靠性、隐私性、毒性、公平性和鲁棒性。首先，我们将线性探测应用于法学硕士。高探测精度表明 \textit{LLM 在早期预训练中已经可以区分每个可信度维度中的概念}。因此，为了进一步揭示预训练隐藏的可能性，我们从LLM的预训练检查点中提取引导向量，以增强LLM的可信度。最后，受~\citet{choi2023理解}互信息估计受线性探测精度限制的启发，我们还利用互信息探测法学硕士，以研究预训练期间可信度的动态。我们是第一个观察到类似的两阶段现象：拟合和压缩~\citep{shwartz2017opening}。这项研究对法学硕士预培训期间的可信度建模进行了初步探索，旨在揭示新的见解并促进该领域的进一步发展。我们将在 \url{https://github.com/ChnQ/TracingLLM} 公开我们的代码。</li>
</ul>

<h3>Title: TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kate Sanders, Nathaniel Weir, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19467">https://arxiv.org/abs/2402.19467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19467">https://arxiv.org/pdf/2402.19467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19467]] TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning(https://arxiv.org/abs/2402.19467)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.</li>
<li><strong>摘要：</strong>对电视剪辑等复杂的多模式内容进行问答具有挑战性。部分原因是当前的视频语言模型依赖于单一模态推理，在长输入上的性能较低，并且缺乏互用性。我们提出了 TV-TREES，第一个多模态蕴涵树生成器。 TV-TREES 是一种视频理解方法，通过在视频直接蕴涵的简单前提和更高层次的结论之间生成蕴涵关系树，促进可解释的联合模态推理。然后，我们引入多模态蕴涵树生成的任务来评估此类方法的推理质量。我们的方法在具有挑战性的 TVQA 数据集上的实验结果证明了在完整视频剪辑上的不可解释的、最先进的零样本性能，说明了与黑盒方法的两全其美的对比。</li>
</ul>

<h3>Title: Loose LIPS Sink Ships: Asking Questions in Battleship with  Language-Informed Program Sampling</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Grand, Valerio Pepe, Jacob Andreas, Joshua B. Tenenbaum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.19471">https://arxiv.org/abs/2402.19471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.19471">https://arxiv.org/pdf/2402.19471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.19471]] Loose LIPS Sink Ships: Asking Questions in Battleship with  Language-Informed Program Sampling(https://arxiv.org/abs/2402.19471)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can leverage the statistics of language to capture human priors, while highlighting some shortcomings of pure LLMs as grounded reasoners.</li>
<li><strong>摘要：</strong>问题将我们对语言的掌握与我们对不确定性进行推理的卓越能力结合起来。在认知资源有限的情况下，人们如何驾驭巨大的假设空间来提出信息丰富的问题​​？我们通过基于棋盘游戏战舰的经典提问任务来研究这些权衡。我们的语言知情程序采样 (LIPS) 模型使用大型语言模型 (LLM) 生成自然语言问题，将其转换为符号程序，并评估其预期信息增益。我们发现，在资源预算出人意料地适度的情况下，这种简单的蒙特卡罗优化策略产生了反映人类在不同战舰棋盘场景中表现的信息丰富的问题​​。相比之下，仅限法学硕士的基线很难在董事会状态中解决问题；值得注意的是，GPT-4V 与非视觉基线相比没有任何改进。我们的结果说明了提问的贝叶斯模型如何利用语言统计来捕获人类先验，同时强调纯法学硕士作为扎根推理者的一些缺点。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
