<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-02-24</h1>
<h3>Title: Retrieval-augmented systems can be dangerous medical communicators</h3>
<ul>
<li><strong>Authors: </strong>Lionel Wong, Ayman Ali, Raymond Xiong, Shannon Zeijang Shen, Yoon Kim, Monica Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14898">https://arxiv.org/abs/2502.14898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14898">https://arxiv.org/pdf/2502.14898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14898]] Retrieval-augmented systems can be dangerous medical communicators(https://arxiv.org/abs/2502.14898)</code><input type="text"></li>
<li><strong>Keywords: </strong>hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Patients have long sought health information online, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. This paper argues that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations -- such as the incorporation of communication pragmatics and enhanced comprehension of source documents -- that could help mitigate these issues and extend beyond the medical domain.</li>
<li><strong>摘要：</strong>长期以来，患者一直在网上寻求健康信息，并且越来越多地转向生成的AI来回答与健康相关的查询。鉴于医疗领域的高赌注，诸如检索型生成和引文接地等技术已被广泛促进，作为减少幻觉和提高AI生成反应的准确性的方法，并已被广泛采用到搜索引擎中。本文认为，即使这些方法从幻觉中汲取了从源文档中汲取的字面意义上的内容，它们仍然可能具有很大的误导。患者可能与阅读原始资料相比，患者可以从AI生成的产出明显不同，更不用说咨询知识渊博的临床医生了。通过对包括有争议的诊断和程序安全在内的主题的大规模查询分析，我们通过定量和定性的证据来支持我们的论点，证明了当前系统所产生的次优答案。特别是，我们强调了这些模型如何倾向于将事实降低，忽略关键的相关来源，并加强患者的误解或偏见。我们提出了一系列建议，例如融入沟通语用学和增强对源文件的理解 - 可以帮助减轻这些问题并超越医疗领域。</li>
</ul>

<h3>Title: Can AI mimic the human ability to define neologisms?</h3>
<ul>
<li><strong>Authors: </strong>Georgios P. Georgiou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14900">https://arxiv.org/abs/2502.14900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14900">https://arxiv.org/pdf/2502.14900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14900]] Can AI mimic the human ability to define neologisms?(https://arxiv.org/abs/2502.14900)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>One ongoing debate in linguistics is whether Artificial Intelligence (AI) can effectively mimic human performance in language-related tasks. While much research has focused on various linguistic abilities of AI, little attention has been given to how it defines neologisms formed through different word formation processes. This study addresses this gap by examining the degree of agreement between human and AI-generated responses in defining three types of Greek neologisms: blends, compounds, and derivatives. The study employed an online experiment in which human participants selected the most appropriate definitions for neologisms, while ChatGPT received identical prompts. The results revealed fair agreement between human and AI responses for blends and derivatives but no agreement for compounds. However, when considering the majority response among humans, agreement with AI was high for blends and derivatives. These findings highlight the complexity of human language and the challenges AI still faces in capturing its nuances. In particular, they suggest a need for integrating more advanced semantic networks and contextual learning mechanisms into AI models to improve their interpretation of complex word formations, especially compounds.</li>
<li><strong>摘要：</strong>语言学中的一项持续辩论是，人工智能（AI）是否可以有效地模仿与语言相关的任务中的人类表现。尽管许多研究集中在AI的各种语言能力上，但很少有人关注它如何通过不同的单词形成过程形成的新词。这项研究通过研究人类和AI生成的反应之间在定义三种类型的希腊新系统方面的一致性来解决这一差距：混合，化合物和衍生物。该研究采用了一个在线实验，在该实验中，人类参与者选择了最适合的新博物主义定义，而Chatgpt则收到了相同的提示。结果揭示了人类和人工智能对混合物和衍生物的反应之间的公平协议，但没有对化合物的共识。但是，在考虑人类之间的多数反应时，与AI的共识对于混合和衍生物的同意很高。这些发现突出了人类语言的复杂性，AI在捕捉其细微差别方面仍然面临着挑战。特别是，他们建议需要将更先进的语义网络和上下文学习机制整合到AI模型中，以改善他们对复杂单词形成的解释，尤其是化合物。</li>
</ul>

<h3>Title: Reading the unreadable: Creating a dataset of 19th century English newspapers using image-to-text language models</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Bourne</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14901">https://arxiv.org/abs/2502.14901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14901">https://arxiv.org/pdf/2502.14901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14901]] Reading the unreadable: Creating a dataset of 19th century English newspapers using image-to-text language models(https://arxiv.org/abs/2502.14901)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Oscar Wilde said, "The difference between literature and journalism is that journalism is unreadable, and literature is not read." Unfortunately, The digitally archived journalism of Oscar Wilde's 19th century often has no or poor quality Optical Character Recognition (OCR), reducing the accessibility of these archives and making them unreadable both figuratively and literally. This paper helps address the issue by performing OCR on "The Nineteenth Century Serials Edition" (NCSE), an 84k-page collection of 19th-century English newspapers and periodicals, using Pixtral 12B, a pre-trained image-to-text language model. The OCR capability of Pixtral was compared to 4 other OCR approaches, achieving a median character error rate of 1%, 5x lower than the next best model. The resulting NCSE v2.0 dataset features improved article identification, high-quality OCR, and text classified into four types and seventeen topics. The dataset contains 1.4 million entries, and 321 million words. Example use cases demonstrate analysis of topic similarity, readability, and event tracking. NCSE v2.0 is freely available to encourage historical and sociological research. As a result, 21st-century readers can now share Oscar Wilde's disappointment with 19th-century journalistic standards, reading the unreadable from the comfort of their own computers.</li>
<li><strong>摘要：</strong>奥斯卡·王尔德（Oscar Wilde）说：“文学和新闻业之间的差异是新闻业不可读，文学不被阅读。”不幸的是，奥斯卡·王尔德（Oscar Wilde）19世纪的数字存档新闻通常没有质量的光学特征识别（OCR），从而降低了这些档案的可访问性，并使它们在象征和字面上使它们变得不可读。本文通过使用Pixtral 12B（一种预先训练的图像到文本语言模型），通过对19世纪的英语报纸和期刊进行了84K页的19世纪英语报纸和期刊来帮助解决问题。 。将PixTral的OCR能力与其他4种OCR方法进行了比较，其中位字符错误率为1％，比下一个最佳模型低5倍。由此产生的NCSE V2.0数据集具有改进的文章识别，高质量的OCR和文本分类为四种类型和十七个主题。数据集包含140万个条目和3.21亿个单词。示例用例展示了主题相似性，可读性和事件跟踪的分析。 NCSE v2.0可以自由使用，以鼓励历史和社会学研究。结果，21世纪的读者现在可以与19世纪的新闻标准分享奥斯卡·王尔德（Oscar Wilde）的失望，从他们自己的计算机的舒适性中读取了不可读的内容。</li>
</ul>

<h3>Title: PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths</h3>
<ul>
<li><strong>Authors: </strong>Boyu Chen, Zirui Guo, Zidan Yang, Yuluo Chen, Junze Chen, Zhenghao Liu, Chuan Shi, Cheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14902">https://arxiv.org/abs/2502.14902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14902">https://arxiv.org/pdf/2502.14902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14902]] PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths(https://arxiv.org/abs/2502.14902)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval augmented generation, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) improves the response quality of large language models (LLMs) by retrieving knowledge from external databases. Typical RAG approaches split the text database into chunks, organizing them in a flat structure for efficient searches. To better capture the inherent dependencies and structured relationships across the text database, researchers propose to organize textual information into an indexing graph, known asgraph-based RAG. However, we argue that the limitation of current graph-based RAG methods lies in the redundancy of the retrieved information, rather than its insufficiency. Moreover, previous methods use a flat structure to organize retrieved information within the prompts, leading to suboptimal performance. To overcome these limitations, we propose PathRAG, which retrieves key relational paths from the indexing graph, and converts these paths into textual form for prompting LLMs. Specifically, PathRAG effectively reduces redundant information with flow-based pruning, while guiding LLMs to generate more logical and coherent responses with path-based prompting. Experimental results show that PathRAG consistently outperforms state-of-the-art baselines across six datasets and five evaluation dimensions. The code is available at the following link: this https URL</li>
<li><strong>摘要：</strong>检索增强的生成（RAG）通过从外部数据库中检索知识来提高大语言模型（LLMS）的响应质量。典型的破布方法将文本数据库分为块，以平坦的结构组织它们以进行有效的搜索。为了更好地捕获整个文本数据库中固有的依赖关系和结构化关系，研究人员建议将文本信息组织到索引图中，基于Asgraph的抹布。但是，我们认为，当前基于图的抹布方法的局限性在于检索到的信息的冗余，而不是其不足。此外，以前的方法使用平坦的结构在提示中组织检索的信息，从而导致次优性能。为了克服这些局限性，我们提出了PathRag，该Pathrag从索引图中检索了关键的关系路径，并将这些路径转换为文本形式以提示LLM。具体而言，Pathrag通过基于流动的修剪有效地减少了冗余信息，同时指导LLMS通过基于路径的提示产生更合乎逻辑和相干的响应。实验结果表明，Pathrag始终在六个数据集和五个评估维度上胜过最先进的基线。该代码可在以下链接中可用：此HTTPS URL</li>
</ul>

<h3>Title: Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence</h3>
<ul>
<li><strong>Authors: </strong>Bhavik Agarwal, Ishan Joshi, Viktoria Rojkova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14905">https://arxiv.org/abs/2502.14905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14905">https://arxiv.org/pdf/2502.14905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14905]] Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence(https://arxiv.org/abs/2502.14905)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning dataset construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured-to-structured dataset, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation.</li>
<li><strong>摘要：</strong>在本文中，我们通过利用LLM推理能力来解决以大语言模型（LLM）生成严格依从性的挑战。在DeepSeek R1增强学习框架的基础上，我们的方法通过新的管道结合了合成推理数据集构造与小组相对策略优化（GRPO）下的自定义奖励功能的新型管道来训练1.5B参数模型的结构性推理技能。具体来说，我们首先在20K样本中执行R1强化学习，从而反映了原始的DeepSeek R1方法，以建立核心推理能力。随后，我们在单独的10K推理样本数据集上进行了监督的微调，重点是精炼下游任务的模式依从性。尽管训练范围相对较小，需要在8xH100 GPU群集上进行GRPO训练约20小时，而SFT的1XA100需要3小时，但我们的模型在实施模式一致性方面表现出了强劲的性能。我们将思想方法与原始DeepSeek R1（671b），DeepSeek R1（Qwen-1.5b和Qwen-7b）以及Gemini 2.0 Flash（70b）的蒸馏版进行了比较，展示了其在现实世界应用中的有效性。我们的结果强调了用于架构约束文本生成的资源有效框架的实际实用性。</li>
</ul>

<h3>Title: Beyond Words: Exploring Cultural Value Sensitivity in Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Srishti Yadav, Zhi Zhang, Daniel Hershcovich, Ekaterina Shutova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14906">https://arxiv.org/abs/2502.14906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14906">https://arxiv.org/pdf/2502.14906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14906]] Beyond Words: Exploring Cultural Value Sensitivity in Multimodal Models(https://arxiv.org/abs/2502.14906)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Investigating value alignment in Large Language Models (LLMs) based on cultural context has become a critical area of research. However, similar biases have not been extensively explored in large vision-language models (VLMs). As the scale of multimodal models continues to grow, it becomes increasingly important to assess whether images can serve as reliable proxies for culture and how these values are embedded through the integration of both visual and textual data. In this paper, we conduct a thorough evaluation of multimodal model at different scales, focusing on their alignment with cultural values. Our findings reveal that, much like LLMs, VLMs exhibit sensitivity to cultural values, but their performance in aligning with these values is highly context-dependent. While VLMs show potential in improving value understanding through the use of images, this alignment varies significantly across contexts highlighting the complexities and underexplored challenges in the alignment of multimodal models.</li>
<li><strong>摘要：</strong>基于文化背景的大语言模型（LLM）中的价值一致性已成为关键的研究领域。但是，在大型视觉模型（VLM）中尚未广泛探索类似的偏见。随着多模式模型的规模不断增长，评估图像是否可以作为文化的可靠代理以及如何通过整合视觉和文本数据来嵌入这些值变得越来越重要。在本文中，我们对不同尺度的多模式模型进行了彻底的评估，重点是它们与文化价值的一致性。我们的发现表明，与LLM一样，VLM对文化价值表现出敏感性，但是它们与这些价值一致性的性能高度依赖于上下文。尽管VLM通过使用图像显示了提高价值理解的潜力，但在多种模型对齐中，这种对齐方式在强调复杂性和毫无创伤的挑战的情况下差异很大。</li>
</ul>

<h3>Title: GneissWeb: Preparing High Quality Data for LLMs at Scale</h3>
<ul>
<li><strong>Authors: </strong>Hajar Emami Gohari, Swanand Ravindra Kadhe, Syed Yousaf Shah. Constantin Adam, Abdulhamid Adebayo, Praneet Adusumilli, Farhan Ahmed, Nathalie Baracaldo Angel, Santosh Borse, Yuan-Chi Chang, Xuan-Hong Dang, Nirmit Desai, Ravital Eres, Ran Iwamoto, Alexei Karve, Yan Koyfman, Wei-Han Lee, Changchang Liu, Boris Lublinsky, Takuyo Ohko, Pablo Pesce, Maroun Touma, Shiqiang Wang, Shalisha Witherspoon, Herbert Woisetschlager, David Wood, Kun-Lung Wu, Issei Yoshida, Syed Zawad, Petros Zerfos, Yi Zhou, Bishwaranjan Bhattacharjee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14907">https://arxiv.org/abs/2502.14907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14907">https://arxiv.org/pdf/2502.14907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14907]] GneissWeb: Preparing High Quality Data for LLMs at Scale(https://arxiv.org/abs/2502.14907)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Data quantity and quality play a vital role in determining the performance of Large Language Models (LLMs). High-quality data, in particular, can significantly boost the LLM's ability to generalize on a wide range of downstream tasks. Large pre-training datasets for leading LLMs remain inaccessible to the public, whereas many open datasets are small in size (less than 5 trillion tokens), limiting their suitability for training large models. In this paper, we introduce GneissWeb, a large dataset yielding around 10 trillion tokens that caters to the data quality and quantity requirements of training LLMs. Our GneissWeb recipe that produced the dataset consists of sharded exact sub-string deduplication and a judiciously constructed ensemble of quality filters. GneissWeb achieves a favorable trade-off between data quality and quantity, producing models that outperform models trained on state-of-the-art open large datasets (5+ trillion tokens). We show that models trained using GneissWeb dataset outperform those trained on FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed on a set of 11 commonly used benchmarks (both zero-shot and few-shot) for pre-training dataset evaluation. When the evaluation set is extended to 20 benchmarks (both zero-shot and few-shot), models trained using GneissWeb still achieve a 1.75 percentage points advantage over those trained on FineWeb-V1.1.0.</li>
<li><strong>摘要：</strong>数据数量和质量在确定大语言模型（LLMS）的性能中起着至关重要的作用。尤其是高质量的数据可以显着提高LLM在广泛的下游任务上概括的能力。领先的LLM的大型培训数据集对于公众来说仍然无法访问，而许多开放数据集的大小很小（少于5万亿代币），从而限制了其适合培训大型型号的能力。在本文中，我们介绍了Gneissweb，这是一个大约10万亿代币的大型数据集，可满足培训LLM的数据质量和数量要求。我们的Gneissweb食谱产生了数据集，包括精确的子字符串删除和明智地构建的质量过滤器合奏。 Gneissweb在数据质量和数量之间取得了一个良好的权衡，生产模型优于在最先进的开放大型数据集（5万亿代币）上训练的模型。我们表明，使用GneissWeb数据集训练的模型优于在FineWeb-V1.1.0上接受培训的模型，而在一组11个常用的基准测试中计算出的平均得分（零射击和很少射击），用于预先训练数据集评估。当评估集将其扩展到20个基准（零射门和少量射击）时，使用GneissWeb训练的型号仍然比在FineWeb-V1.1.0上训练的模型获得1.75个百分点的优势。</li>
</ul>

<h3>Title: EvoP: Robust LLM Inference via Evolutionary Pruning</h3>
<ul>
<li><strong>Authors: </strong>Shangyu Wu, Hongchao Du, Ying Xiong, Shuai Chen, Tei-wei Kuo, Nan Guan, Chun Jason Xue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14910">https://arxiv.org/abs/2502.14910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14910">https://arxiv.org/pdf/2502.14910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14910]] EvoP: Robust LLM Inference via Evolutionary Pruning(https://arxiv.org/abs/2502.14910)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, but their massive size and computational demands hinder their deployment in resource-constrained environments. Existing structured pruning methods address this issue by removing redundant structures (e.g., elements, channels, layers) from the model. However, these methods employ a heuristic pruning strategy, which leads to suboptimal performance. Besides, they also ignore the data characteristics when pruning the model. To overcome these limitations, we propose EvoP, an evolutionary pruning framework for robust LLM inference. EvoP first presents a cluster-based calibration dataset sampling (CCDS) strategy for creating a more diverse calibration dataset. EvoP then introduces an evolutionary pruning pattern searching (EPPS) method to find the optimal pruning pattern. Compared to existing structured pruning techniques, EvoP achieves the best performance while maintaining the best efficiency. Experiments across different LLMs and different downstream tasks validate the effectiveness of the proposed EvoP, making it a practical and scalable solution for deploying LLMs in real-world applications.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在自然语言处理任务中取得了巨大的成功，但是它们的庞大规模和计算要求阻碍了它们在资源受限环境中的部署。现有的结构化修剪方法通过从模型中删除冗余结构（例如元素，通道，层）来解决此问题。但是，这些方法采用了启发式修剪策略，从而导致次优性能。此外，在修剪模型时，它们还忽略了数据特征。为了克服这些局限性，我们提出了EVOP，这是一个强大的LLM推论的进化修剪框架。 EVOP首先提出了基于群集的校准数据集采样（CCD）策略，以创建更多样化的校准数据集。然后，EVOP引入了一种进化的修剪模式搜索（EPP）方法，以找到最佳的修剪模式。与现有的结构化修剪技术相比，EVOP在保持最佳效率的同时取得了最佳性能。跨不同LLM和不同下游任务的实验验证了提议的EVOP的有效性，使其成为在现实世界应用中部署LLM的实用且可扩展的解决方案。</li>
</ul>

<h3>Title: Batayan: A Filipino NLP benchmark for evaluating Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jann Railey Montalan, Jimson Paulo Layacan, David Demitri Africa, Richell Isaiah Flores, Michael T. Lopez II, Theresa Denise Magsajo, Anjanette Cayabyab, William Chandra Tjhi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14911">https://arxiv.org/abs/2502.14911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14911">https://arxiv.org/pdf/2502.14911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14911]] Batayan: A Filipino NLP benchmark for evaluating Large Language Models(https://arxiv.org/abs/2502.14911)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages; however, linguistic nuances of under-resourced languages remain unexplored. We introduce Batayan, a holistic Filipino benchmark designed to systematically evaluate LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. Batayan consolidates eight tasks, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven annotation process ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating a pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of multilingual LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pretraining corpora, the unique hurdles in modeling Filipino's rich morphology and construction, and the importance of explicit Filipino language support and instruction tuning. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public benchmark and leaderboard as a clear foundation for iterative, community-driven progress in Filipino NLP.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的最新进展表现出了广泛基准的高资源语言的显着功能。但是，资源不足的语言的语言细微差别仍未得到探索。我们介绍了Batayan，这是一种整体菲律宾基准测试，旨在系统地评估三种关键自然语言处理（NLP）能力的LLM：理解，推理和产生。 Batayan合并了八项任务，涵盖了他加禄语和代码开关的Taglish usteres。我们严格的言语驱动的注释过程可确保对菲律宾的复杂形态和句法结构的流利性和真实性，从而减轻了现有菲律宾语料库中普遍存在的翻译偏见。我们报告了各种多语言LLM的经验结果，强调了巨大的性能差距，这表明菲律宾人在训练训练中的代表性不足，这是对菲律宾人丰富的形态和构建建模的独特障碍，以及明确的菲律宾语言支持和指导调谐的重要性。此外，我们讨论了数据集构建中遇到的实际挑战，并提出了原则上的解决方案，以在代表性不足的语言上建立文化和语言信仰资源。我们还为菲律宾NLP的迭代，社区驱动的进步提供了公共基准和排行榜，作为明确的基础。</li>
</ul>

<h3>Title: OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and Consistency Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xiangjin Xie, Guangwei Xu, Lingyan Zhao, Ruijie Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14913">https://arxiv.org/abs/2502.14913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14913">https://arxiv.org/pdf/2502.14913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14913]] OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and Consistency Alignment(https://arxiv.org/abs/2502.14913)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, agent</a></li>
<li><strong>Abstract: </strong>Although multi-agent collaborative Large Language Models (LLMs) have achieved significant breakthroughs in the Text-to-SQL task, their performance is still constrained by various factors. These factors include the incompleteness of the framework, failure to follow instructions, and model hallucination problems. To address these problems, we propose OpenSearch-SQL, which divides the Text-to-SQL task into four main modules: Preprocessing, Extraction, Generation, and Refinement, along with an Alignment module based on a consistency alignment mechanism. This architecture aligns the inputs and outputs of agents through the Alignment module, reducing failures in instruction following and hallucination. Additionally, we designed an intermediate language called SQL-Like and optimized the structured CoT based on SQL-Like. Meanwhile, we developed a dynamic few-shot strategy in the form of self-taught Query-CoT-SQL. These methods have significantly improved the performance of LLMs in the Text-to-SQL task. In terms of model selection, we directly applied the base LLMs without any post-training, thereby simplifying the task chain and enhancing the framework's portability. Experimental results show that OpenSearch-SQL achieves an execution accuracy(EX) of 69.3% on the BIRD development set, 72.28% on the test set, and a reward-based validity efficiency score (R-VES) of 69.36%, with all three metrics ranking first at the time of submission. These results demonstrate the comprehensive advantages of the proposed method in both effectiveness and efficiency.</li>
<li><strong>摘要：</strong>尽管多机构协作大语模型（LLMS）在文本到SQL任务中取得了重大突破，但其性能仍然受到各种因素的限制。这些因素包括框架不完整，未能遵循说明和模型幻觉问题。为了解决这些问题，我们提出了OpenSearch-SQL，将文本到SQL任务划分为四个主要模块：预处理，提取，生成和改进，以及基于一致性比对机制的对齐模块。该体系结构通过对齐模块对准代理的输入和输出，从而减少了随后的指令和幻觉的故障。此外，我们设计了一种称为SQL样的中间语言，并基于SQL样的结构化COT进行了优化。同时，我们以自学成才的查询-COT-SQL的形式制定了一种动态的几次策略。这些方法显着改善了文本到SQL任务中LLM的性能。在模型选择方面，我们直接应用了基本LLM，而无需任何后培训，从而简化了任务链并增强了框架的可移植性。实验结果表明，OpenSearch-SQL的执行精度（EX）为69.3％，在鸟类开发集中达到72.28％，基于奖励的有效性效率评分（R-VES）为69.36％，这三个为69.36％指标在提交时排名第一。这些结果证明了该方法在有效性和效率方面具有全面的优势。</li>
</ul>

<h3>Title: MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD Coding for Chinese EMRs</h3>
<ul>
<li><strong>Authors: </strong>Xinxin You, Xien Liu, Xue Yang, Ziyi Wang, Ji Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14916">https://arxiv.org/abs/2502.14916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14916">https://arxiv.org/pdf/2502.14916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14916]] MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD Coding for Chinese EMRs(https://arxiv.org/abs/2502.14916)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The task of automatically coding the International Classification of Diseases (ICD) in the medical field has been well-established and has received much attention. Automatic coding of the ICD in the medical field has been successful in English but faces challenges when dealing with Chinese electronic medical records (EMRs). The first issue lies in the difficulty of extracting disease code-related information from Chinese EMRs, primarily due to the concise writing style and specific internal structure of the EMRs. The second problem is that previous methods have failed to leverage the disease-based multi-axial knowledge and lack of association with the corresponding clinical evidence. This paper introduces a novel framework called MKE-Coder: Multi-axial Knowledge with Evidence verification in ICD coding for Chinese EMRs. Initially, we identify candidate codes for the diagnosis and categorize each of them into knowledge under four coding this http URL, we retrieve corresponding clinical evidence from the comprehensive content of EMRs and filter credible evidence through a scoring model. Finally, to ensure the validity of the candidate code, we propose an inference module based on the masked language modeling strategy. This module verifies that all the axis knowledge associated with the candidate code is supported by evidence and provides recommendations accordingly. To evaluate the performance of our framework, we conduct experiments using a large-scale Chinese EMR dataset collected from various hospitals. The experimental results demonstrate that MKE-Coder exhibits significant superiority in the task of automatic ICD coding based on Chinese EMRs. In the practical evaluation of our method within simulated real coding scenarios, it has been demonstrated that our approach significantly aids coders in enhancing both their coding accuracy and speed.</li>
<li><strong>摘要：</strong>自动编码医学领域的国际疾病分类（ICD）的任务已经建立了良好的工作，并受到了很多关注。 ICD在医学领域的自动编码在英语方面已经成功，但是在处理中国电子病历（EMRS）时面临挑战。第一个问题在于很难从中国EMR中提取与疾病代码相关的信息，这主要是由于EMR的简洁写作风格和特定的内部结构。第二个问题是以前的方法未能利用基于疾病的多轴知识，并且与相应的临床证据缺乏关联。本文介绍了一个名为Mke-Coder的新型框架：在ICD编码中国EMR中的证据验证的多轴知识。最初，我们确定了诊断的候选代码，并将它们分类为四个编码此HTTP URL的知识，我们通过评分模型从EMR的综合内容中检索了相应的临床证据，并过滤了可靠的证据。最后，为了确保候选代码的有效性，我们根据蒙版语言建模策略提出了一个推论模块。该模块验证了与候选代码相关的所有轴心知识均由证据支持，并相应地提供了建议。为了评估框架的性能，我们使用从各个医院收集的大规模中国EMR数据集进行实验。实验结果表明，MKE-Coder在基于中国EMR的自动ICD编码任务中表现出显着优越性。在对我们模拟的真实编码方案中我们方法的实际评估中，已经证明我们的方法可以大大帮助编码者提高其编码准确性和速度。</li>
</ul>

<h3>Title: The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text</h3>
<ul>
<li><strong>Authors: </strong>Matthieu Meeus, Lukas Wutschitz, Santiago Zanella-Béguelin, Shruti Tople, Reza Shokri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14921">https://arxiv.org/abs/2502.14921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14921">https://arxiv.org/pdf/2502.14921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14921]] The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text(https://arxiv.org/abs/2502.14921)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>How much information about training samples can be gleaned from synthetic data generated by Large Language Models (LLMs)? Overlooking the subtleties of information flow in synthetic data generation pipelines can lead to a false sense of privacy. In this paper, we design membership inference attacks (MIAs) that target data used to fine-tune pre-trained LLMs that are then used to synthesize data, particularly when the adversary does not have access to the fine-tuned model but only to the synthetic data. We show that such data-based MIAs do significantly better than a random guess, meaning that synthetic data leaks information about the training data. Further, we find that canaries crafted to maximize vulnerability to model-based MIAs are sub-optimal for privacy auditing when only synthetic data is released. Such out-of-distribution canaries have limited influence on the model's output when prompted to generate useful, in-distribution synthetic data, which drastically reduces their vulnerability. To tackle this problem, we leverage the mechanics of auto-regressive models to design canaries with an in-distribution prefix and a high-perplexity suffix that leave detectable traces in synthetic data. This enhances the power of data-based MIAs and provides a better assessment of the privacy risks of releasing synthetic data generated by LLMs.</li>
<li><strong>摘要：</strong>从大语言模型（LLM）生成的合成数据中可以收集多少有关培训样本的信息？忽略合成数据生成管道中信息流的微妙之处可能会导致错误的隐私感。在本文中，我们设计了用于微调预训练的LLM的目标数据的成员推理攻击（MIA），然后用于合成数据，尤其是当对手无法访问微型模型，而仅当对手可以访问微型模型时合成数据。我们表明，这种基于数据的MIA比随机猜测要好得多，这意味着合成数据会泄漏有关培训数据的信息。此外，我们发现，在仅释放合成数据时，精心制作的金融旨在最大程度地提高基于模型的MIA的脆弱性，这是对隐私审核的最佳选择。当提示产生有用的，分布的合成数据时，这种分布的金丝雀对模型的输出的影响有限，从而大大降低了它们的脆弱性。为了解决这个问题，我们利用自动回归模型的力学来设计带有分布前缀和高充实后缀的金丝雀，这些后缀将可检测到的痕迹留在合成数据中。这增强了基于数据的MIA的功能，并更好地评估了释放LLMS生成的合成数据的隐私风险。</li>
</ul>

<h3>Title: SIFT: Grounding LLM Reasoning in Contexts via Stickers</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zeng, Xuyao Huang, Boxiu Li, Zhijie Deng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14922">https://arxiv.org/abs/2502.14922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14922">https://arxiv.org/pdf/2502.14922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14922]] SIFT: Grounding LLM Reasoning in Contexts via Stickers(https://arxiv.org/abs/2502.14922)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper identifies the misinterpretation of the context can be a significant issue during the reasoning process of large language models, spanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones like DeepSeek-R1. For example, in the phrase "10 dollars per kilo," LLMs might not recognize that "per" means "for each," leading to calculation errors. We introduce a novel, post-training approach called **Stick to the Facts (SIFT)** to tackle this. SIFT leverages increasing inference-time compute to ground LLM reasoning in contexts. At the core of SIFT lies the *Sticker*, which is generated by the model itself to explicitly emphasize the key information within the context. Given the curated Sticker, SIFT generates two predictions -- one from the original query and one from the query augmented with the Sticker. If they differ, the Sticker is sequentially refined via *forward* optimization (to better align the extracted facts with the query) and *inverse* generation (to conform with the model's inherent tendencies) for more faithful reasoning outcomes. Studies across diverse models (from 3B to 100B+) and benchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements. Notably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from 78.33% to **85.67**%, establishing a new state-of-the-art in the open-source community. The code is available at this https URL.</li>
<li><strong>摘要：</strong>本文确定了对上下文的误解可能是在大型语言模型的推理过程中的一个重要问题，从较小的模型等较小的模型等较小的模型到诸如deepSeek-r1之类的尖端。例如，在“每公斤10美元”一词中，LLMS可能无法认识到每个人的“均”表示“”，导致计算错误。我们介绍了一种小说，训练后的方法，称为**坚持事实（Sift）**解决这个问题。在上下文中，将推理时间计算增加到地面LLM推理。 SIFT的核心是 *贴纸 *，该 *由模型本身生成，以明确强调上下文中的关键信息。鉴于策划的贴纸，SIFT产生了两个预测 - 一个来自原始查询，另一个来自带有贴纸的查询。如果它们有所不同，则贴纸将通过 *正向 *优化（以更好地使提取的事实与查询更好地对齐）和 *逆 *生成（以符合模型的固有趋势），以实现更忠实的推理结果。跨不同模型（从3B到100B+）和基准测试（例如GSM8K，Math-500）的研究揭示了稳定的性能改善。值得注意的是，SIFT将AIME2024上DeepSeek-R1的通行证从78.33％提高到** 85.67 **％，在开源社区中建立了新的最先进。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Alabdulmohsin, Andreas Steiner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14924">https://arxiv.org/abs/2502.14924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14924">https://arxiv.org/pdf/2502.14924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14924]] A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?(https://arxiv.org/abs/2502.14924)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Language exhibits a fractal structure in its information-theoretic complexity (i.e. bits per token), with self-similarity across scales and long-range dependence (LRD). In this work, we investigate whether large language models (LLMs) can replicate such fractal characteristics and identify conditions-such as temperature setting and prompting method-under which they may fail. Moreover, we find that the fractal parameters observed in natural language are contained within a narrow range, whereas those of LLMs' output vary widely, suggesting that fractal parameters might prove helpful in detecting a non-trivial portion of LLM-generated texts. Notably, these findings, and many others reported in this work, are robust to the choice of the architecture; e.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset comprising of over 240,000 articles generated by various LLMs (both pretrained and instruction-tuned) with different decoding temperatures and prompting methods, along with their corresponding human-generated texts. We hope that this work highlights the complex interplay between fractal properties, prompting, and statistical mimicry in LLMs, offering insights for generating, evaluating and detecting synthetic texts.</li>
<li><strong>摘要：</strong>语言在其信息理论复杂性（即每个令牌）中表现出分形结构，跨尺度和远距离依赖性（LRD）具有自相似性。在这项工作中，我们研究了大型语言模型（LLMS）是否可以复制这种分形特征并确定诸如温度设置和提示未能失败的方法之类的条件。此外，我们发现在自然语言中观察到的分形参数包含在狭窄范围内，而LLMS的输出范围差异很大，这表明分形参数可能证明有助于检测LLM生成的文本的非平凡部分。值得注意的是，这些发现以及在这项工作中报道的许多其他发现，对建筑的选择是强大的。例如Gemini 1.0 Pro，Mistral-7b和Gemma-2B。我们还发布了一个数据集，该数据集由各种LLM（均经过验证和指导调整）产生的240,000篇文章，具有不同的解码温度和促使方法，以及它们相应的人类生成的文本。我们希望这项工作强调了LLMS中分形特性，提示和统计模仿之间的复杂相互作用，提供了生成，评估和检测合成文本的见解。</li>
</ul>

<h3>Title: Learning to Retrieve and Reason on Knowledge Graph through Active Self-Reflection</h3>
<ul>
<li><strong>Authors: </strong>Han Zhang, Langshi Zhou, Hanfang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14932">https://arxiv.org/abs/2502.14932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14932">https://arxiv.org/pdf/2502.14932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14932]] Learning to Retrieve and Reason on Knowledge Graph through Active Self-Reflection(https://arxiv.org/abs/2502.14932)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Extensive research has investigated the integration of large language models (LLMs) with knowledge graphs to enhance the reasoning process. However, understanding how models perform reasoning utilizing structured graph knowledge remains underexplored. Most existing approaches rely on LLMs or retrievers to make binary judgments regarding the utilization of knowledge, which is too coarse. Meanwhile, there is still a lack of feedback mechanisms for reflection and correction throughout the entire reasoning path. This paper proposes an Active self-Reflection framework for knowledge Graph reasoning ARG, introducing for the first time an end-to-end training approach to achieve iterative reasoning grounded on structured graphs. Within the framework, the model leverages special tokens to \textit{actively} determine whether knowledge retrieval is necessary, performs \textit{reflective} critique based on the retrieved knowledge, and iteratively reasons over the knowledge graph. The reasoning paths generated by the model exhibit high interpretability, enabling deeper exploration of the model's understanding of structured knowledge. Ultimately, the proposed model achieves outstanding results compared to existing baselines in knowledge graph reasoning tasks.</li>
<li><strong>摘要：</strong>广泛的研究调查了大语言模型（LLM）与知识图的整合，以增强推理过程。但是，了解模型如何使用结构化图形知识执行推理仍然没有充满反感。大多数现有的方法都依靠LLM或猎犬来对知识的利用进行二进制判断，而知识太粗糙了。同时，在整个推理路径中仍然缺乏反思和校正的反馈机制。本文提出了一个主动自我反省框架，用于知识图推理ARG，首次引入端到端训练方法，以实现基于结构化图的迭代推理。在框架内，该模型利用特殊令牌为\ textIt {apictively}确定知识检索是否是必要的，基于检索到的知识以及在知识图上进行迭代原因执行\ textit {反思}批评。该模型产生的推理路径具有很高的解释性，从而可以更深入地探索模型对结构化知识的理解。最终，与知识图推理任务中的现有基线相比，提出的模型取得了出色的结果。</li>
</ul>

<h3>Title: Lost in Space: Optimizing Tokens for Grammar-Constrained Decoding</h3>
<ul>
<li><strong>Authors: </strong>Sil Hamilton, David Mimno</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14969">https://arxiv.org/abs/2502.14969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14969">https://arxiv.org/pdf/2502.14969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14969]] Lost in Space: Optimizing Tokens for Grammar-Constrained Decoding(https://arxiv.org/abs/2502.14969)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>General-purpose language models are trained to produce varied natural language outputs, but for some tasks like annotation or classification we need more specific output formats. LLM systems increasingly support structured output, sampling tokens according to a grammar, which enforces a format but which can also reduce performance. We ask whether there are systematic differences between grammars that appear semantically similar to humans. To answer this question, we test four popular model families with five token formats on four NLP benchmarks. All models perform most accurately when instructed to classify with real numbers. Performance also improves by 5%-10% when models are instructed to return tokens incorporating leading whitespace, which we find can help models avoid structural deficiencies in subword token representations. Format-based differences are largest for smaller models that are often used for local laptop-scale inference. We present best practices for researchers using language models as zero-shot classifiers with structured output.</li>
<li><strong>摘要：</strong>通用语言模型经过训练以产生各种自然语言输出，但是对于某些任务，例如注释或分类，我们需要更具体的输出格式。 LLM系统越来越支持结构化的输出，根据语法进行采样代币，该语法会强制执行格式，但也可以降低性能。我们询问语法之间似乎与人类相似的语法之间存在系统的差异。为了回答这个问题，我们在四个NLP基准测试中测试了四个具有五个令牌格式的流行模型系列。当指示与实际数字分类时，所有模型的性能最准确。当指示模型返回融合领先的空格的令牌时，性能也会提高5％-10％，我们发现这可以帮助模型避免在子字代币表示中避免结构性缺陷。对于通常用于本地笔记本电脑规模推断的较小模型的基于格式的差异最大。我们为研究人员提供最佳实践，使用语言模型作为具有结构化输出的零击分类器。</li>
</ul>

<h3>Title: Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries</h3>
<ul>
<li><strong>Authors: </strong>David Noever, Grant Rosario</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14975">https://arxiv.org/abs/2502.14975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14975">https://arxiv.org/pdf/2502.14975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14975]] Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries(https://arxiv.org/abs/2502.14975)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>We present an open-source benchmark and evaluation framework for assessing emotional boundary handling in Large Language Models (LLMs). Using a dataset of 1156 prompts across six languages, we evaluated three leading LLMs (GPT-4o, Claude-3.5 Sonnet, and Mistral-large) on their ability to maintain appropriate emotional boundaries through pattern-matched response analysis. Our framework quantifies responses across seven key patterns: direct refusal, apology, explanation, deflection, acknowledgment, boundary setting, and emotional awareness. Results demonstrate significant variation in boundary-handling approaches, with Claude-3.5 achieving the highest overall score (8.69/10) and producing longer, more nuanced responses (86.51 words on average). We identified a substantial performance gap between English (average score 25.62) and non-English interactions (< 0.22), with English responses showing markedly higher refusal rates (43.20% vs. < 1% for non-English). Pattern analysis revealed model-specific strategies, such as Mistral's preference for deflection (4.2%) and consistently low empathy scores across all models (< 0.06). Limitations include potential oversimplification through pattern matching, lack of contextual understanding in response analysis, and binary classification of complex emotional responses. Future work should explore more nuanced scoring methods, expand language coverage, and investigate cultural variations in emotional boundary expectations. Our benchmark and methodology provide a foundation for systematic evaluation of LLM emotional intelligence and boundary-setting capabilities.</li>
<li><strong>摘要：</strong>我们提出了一个开源基准和评估框架，用于评估大型语言模型（LLMS）中的情感边界处理。使用六种语言的1156个提示的数据集，我们评估了三个领先的LLM（GPT-4O，Claude-3.5十四行诗和Mismtral-Large），以通过模式匹配的响应分析来保持适当的情感边界。我们的框架量化了七个关键模式的响应：直接拒绝，道歉，解释，挠度，确认，边界设置和情感意识。结果表明，边界处理方法的显着差异，Claude-3.5达到了最高的总分（8.69/10），并产生更长，更细微的响应（平均为86.51个单词）。我们确定了英语（平均得分25.62）和非英语相互作用（<0.22）之间的巨大性能差距，英语响应显示出明显更高的拒绝率（43.20％vs.非英语）。模式分析揭示了特定于模型的策略，例如Mistral对偏转的偏好（4.2％）和所有模型的同理心得分持续较低（<0.06）。局限性包括通过模式匹配的潜在过度简化，响应分析中缺乏上下文理解以及对复杂情绪反应的二进制分类。未来的工作应探索更细微的评分方法，扩大语言覆盖范围，并研究情绪边界期望的文化差异。我们的基准和方法论为LLM情商和边界设定功能的系统评估奠定了基础。</li>
</ul>

<h3>Title: A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organization Systems</h3>
<ul>
<li><strong>Authors: </strong>Lew Lefton, Kexin Rong, Chinar Dankhara, Lila Ghemri, Firdous Kausar, A. Hannibal Hamdallahi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15005">https://arxiv.org/abs/2502.15005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15005">https://arxiv.org/pdf/2502.15005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15005]] A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organization Systems(https://arxiv.org/abs/2502.15005)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval augmented generation, agent</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a Retrieval Augmented Generation (RAG) agent that maps natural language queries about research topics to precise, machine-interpretable semantic entities. Our approach combines RAG with Socratic dialogue to align a user's intuitive understanding of research topics with established Knowledge Organization Systems (KOSs). The proposed approach will effectively bridge "little semantics" (domain-specific KOS structures) with "big semantics" (broad bibliometric repositories), making complex academic taxonomies more accessible. Such agents have the potential for broad use. We illustrate with a sample application called CollabNext, which is a person-centric knowledge graph connecting people, organizations, and research topics. We further describe how the application design has an intentional focus on HBCUs and emerging researchers to raise visibility of people historically rendered invisible in the current science system.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一个检索增强发电（RAG）代理，该代理将有关研究主题的自然语言查询映射到精确的机器可解开的语义实体。我们的方法将抹布与苏格拉底对话结合在一起，以使用户对研究主题的直观理解与既定知识组织系统（KOSS）相结合。拟议的方法将有效地桥接“小语义”（特定于领域的KOS结构），上面有“大语义”（广泛的文献计量库），使复杂的学术分类法更加易于访问。这样的代理具有广泛使用的潜力。我们用一个名为Collagnext的示例应用程序进行了说明，该应用程序是一个以人为中心的知识图，将人，组织和研究主题连接起来。我们进一步描述了应用设计如何有意地关注HBCU和新兴的研究人员，以提高当前科学系统中有史以来看不见的人们的可见性。</li>
</ul>

<h3>Title: LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers</h3>
<ul>
<li><strong>Authors: </strong>Anton Razzhigaev, Matvey Mikhalchuk, Temurbek Rahmatullaev, Elizaveta Goncharova, Polina Druzhinina, Ivan Oseledets, Andrey Kuznetsov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15007">https://arxiv.org/abs/2502.15007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15007">https://arxiv.org/pdf/2502.15007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15007]] LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers(https://arxiv.org/abs/2502.15007)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens -- especially stopwords, articles, and commas -- consistently degrades performance on MMLU and BABILong-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer's embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of filler tokens in maintaining context. For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding.</li>
<li><strong>摘要：</strong>我们介绍了量化大型语言模型（LLMS）编码和存储上下文信息的方法，表明令牌通常被视为次要的（例如，确定词，标点符号）具有令人惊讶的高环境。值得注意的是，即使仅删除无关的令牌，删除这些代币（尤其是停止词，文章和逗号）也会始终降低对MMLU和Babilong-4K的性能。我们的分析还显示了上下文化与线性之间的密切相关性，其中线性度衡量了从一层的嵌入到下一个的转换如何通过单个线性映射近似。这些发现强调了填充令牌在维护上下文中的隐藏重要性。为了进一步探索，我们提出了LLM-Microscope，这是一种评估令牌​​级的非线性，评估上下文记忆，可视化中间层贡献（通过适应的logit镜头）并测量表示代表的内在维度。这个工具包阐明了看似微不足道的令牌对于远程理解至关重要。</li>
</ul>

<h3>Title: Contextualizing Search Queries In-Context Learning for Conversational Rewriting with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Raymond Wilson, Chase Carter, Cole Graham</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15009">https://arxiv.org/abs/2502.15009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15009">https://arxiv.org/pdf/2502.15009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15009]] Contextualizing Search Queries In-Context Learning for Conversational Rewriting with LLMs(https://arxiv.org/abs/2502.15009)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Conversational query rewriting is crucial for effective conversational search, yet traditional supervised methods require substantial labeled data, which is scarce in low-resource settings. This paper introduces Prompt-Guided In-Context Learning, a novel approach that leverages the in-context learning capabilities of Large Language Models (LLMs) for few-shot conversational query rewriting. Our method employs carefully designed prompts, incorporating task descriptions, input/output format specifications, and a small set of illustrative examples, to guide pre-trained LLMs to generate context-independent queries without explicit fine-tuning. Extensive experiments on benchmark datasets, TREC and Taskmaster-1, demonstrate that our approach significantly outperforms strong baselines, including supervised models and contrastive co-training methods, across various evaluation metrics such as BLEU, ROUGE-L, Success Rate, and MRR. Ablation studies confirm the importance of in-context examples, and human evaluations further validate the superior fluency, relevance, and context utilization of our generated rewrites. The results highlight the potential of prompt-guided in-context learning as an efficient and effective paradigm for low-resource conversational query rewriting, reducing the reliance on extensive labeled data and complex training procedures.</li>
<li><strong>摘要：</strong>会话查询重写对于有效的对话搜索至关重要，但是传统的监督方法需要大量标记的数据，这在低资源设置中很少。本文介绍了迅速引入的文化学习，这是一种新颖的方法，利用大型语言模型（LLMS）的文化学习能力（LLMS）进行了很少的对话性查询重写。我们的方法采用精心设计的提示，合并任务说明，输入/输出格式规格以及一小部分说明性示例，以指导预训练的LLM，以生成与上下文无关的查询，而无需明确调查。在基准数据集，TREC和TaskMaster-1上进行的广泛实验表明，我们的方法在BLEU，Rouge-L，成功率和MRR等各种评估指标上都显着优于强大的基准，包括监督模型和对比度共培训方法。消融研究证实了内在示例的重要性，人类评估进一步验证了我们生成的重写的卓越流利性，相关性和上下文利用。结果突出了迅速引入文化学习作为低资源对话查询重写的有效范式的潜力，从而减少了对广泛标记的数据和复杂培训程序的依赖。</li>
</ul>

<h3>Title: Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mark Russinovich, Ahmed Salem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15010">https://arxiv.org/abs/2502.15010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15010">https://arxiv.org/pdf/2502.15010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15010]] Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models(https://arxiv.org/abs/2502.15010)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent copyright agreements between AI companies and content creators have highlighted the need for precise control over language models' ability to reproduce copyrighted content. While existing approaches rely on either complete concept removal through unlearning or simple output filtering, we propose Obliviate, a novel post-training technique that selectively prevents verbatim reproduction of specific text while preserving semantic understanding. Obliviate operates by selecting tokens within memorized sequences and modifying the model's probability distribution to prevent exact reproduction while maintaining contextual understanding. We evaluate Obliviate on multiple large language models (LLaMA-3.1 8B, LLaMA-3.1-instruct 8B, Qwen-2.5-7B, and Yi-1.5 6B) across both synthetic memorization tasks and organic copyright content. Our results demonstrate that Obliviate achieves orders of magnitude reduction, e.g., 100x, in verbatim memorization while maintaining model performance within 1% of baseline on standard benchmarks (HellaSwag, MMLU, TruthfulQA, and Winogrande). This makes Obliviate particularly suitable for practical deployment scenarios where companies need to efficiently address copyright concerns in pretrained models without compromising their general capabilities.</li>
<li><strong>摘要：</strong>AI公司和内容创建者之间的最新版权协议强调了对语言模型重现受版权保护内容的能力的精确控制。尽管现有方法依赖于通过学习或简单的输出过滤的完全概念删除，但我们提出了一种遗忘，这是一种新颖的训练后技术，可以选择性地防止逐步阻止特定文本的逐步再现，同时保持语义理解。遗忘者通过在记忆序列中选择令牌并修改模型的概率分布以防止精确复制，同时保持上下文理解，从而进行操作。我们在多种大型语言模型（Llama-3.1 8B，Llama-3.1-Instruct 8b，Qwen-2.5-7b和Yi-1.5 6b）上评估遗忘，跨合成记忆任务和有机版权含量。我们的结果表明，在逐字记忆中，遗忘的降低了数量级的降低，同时将模型性能保持在标准基准（Hellaswag，MMLU，真实Fulthulqa和Winogrande）之内的1％之内。这使得遗忘特别适合于实用的部署方案，在这种情况下，公司需要有效地解决验证模型中的版权问题，而不会损害其一般能力。</li>
</ul>

<h3>Title: Using tournaments to calculate AUROC for zero-shot classification with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wonjin Yoon, Ian Bulovic, Timothy A. Miller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15018">https://arxiv.org/abs/2502.15018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15018">https://arxiv.org/pdf/2502.15018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15018]] Using tournaments to calculate AUROC for zero-shot classification with LLMs(https://arxiv.org/abs/2502.15018)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models perform surprisingly well on many zero-shot classification tasks, but are difficult to fairly compare to supervised classifiers due to the lack of a modifiable decision boundary. In this work, we propose and evaluate a method that converts binary classification tasks into pairwise comparison tasks, obtaining relative rankings from LLMs. Repeated pairwise comparisons can be used to score instances using the Elo rating system (used in chess and other competitions), inducing a confidence ordering over instances in a dataset. We evaluate scheduling algorithms for their ability to minimize comparisons, and show that our proposed algorithm leads to improved classification performance, while also providing more information than traditional zero-shot classification.</li>
<li><strong>摘要：</strong>大型语言模型在许多零摄像分类任务上表现出色，但是由于缺乏可修改的决策边界，很难与监督分类器进行公平比较。在这项工作中，我们提出并评估一种将二进制分类任务转换为成对比较任务的方法，从LLMS获得相对排名。重复的成对比较可用于使用ELO评分系统（用于国际象棋和其他比赛）来评分实例，从而诱导对数据集中实例的置信点。我们评估了调度算法最大程度地减少比较的能力，并表明我们提出的算法会提高分类性能的改善，同时还提供了比传统的零弹药分类更多的信息。</li>
</ul>

<h3>Title: A Meta-Evaluation of Style and Attribute Transfer Metrics</h3>
<ul>
<li><strong>Authors: </strong>Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15022">https://arxiv.org/abs/2502.15022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15022">https://arxiv.org/pdf/2502.15022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15022]] A Meta-Evaluation of Style and Attribute Transfer Metrics(https://arxiv.org/abs/2502.15022)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>LLMs make it easy to rewrite text in any style, be it more polite, persuasive, or more positive. We present a large-scale study of evaluation metrics for style and attribute transfer with a focus on content preservation; meaning content not attributed to the style shift is preserved. The de facto evaluation approach uses lexical or semantic similarity metrics often between source sentences and rewrites. While these metrics are not designed to distinguish between style or content differences, empirical meta-evaluation shows a reasonable correlation to human judgment. In fact, recent works find that LLMs prompted as evaluators are only comparable to semantic similarity metrics, even though intuitively, the LLM approach should better fit the task. To investigate this discrepancy, we benchmark 8 metrics for evaluating content preservation on existing datasets and additionally construct a new test set that better aligns with the meta-evaluation aim. Indeed, we then find that the empirical conclusion aligns with the intuition: content preservation metrics for style/attribute transfer must be conditional on the style shift. To support this, we propose a new efficient zero-shot evaluation method using the likelihood of the next token. We hope our meta-evaluation can foster more research on evaluating content preservation metrics, and also to ensure fair evaluation of methods for conducting style transfer.</li>
<li><strong>摘要：</strong>LLMS使以任何样式的文本重写变得容易，无论是更有礼貌，有说服力还是更积极。我们介绍了针对样式和属性转移的评估指标的大规模研究，重点是保存内容；表示不归因于样式转移的内容。事实上的评估方法使用源句和重写之间经常使用词汇或语义相似性指标。尽管这些指标并非旨在区分样式或内容差异，但经验元评估与人类判断有着合理的相关性。实际上，最近的作品发现，促使LLMS作为评估者的LLM仅与语义相似性指标相媲美，即使在直觉上，LLM方法也应该更好地适合任务。为了调查这种差异，我们基于8个指标来评估现有数据集上的内容保存，并构建了一个新的测试集，以更好地与元评估目标保持一致。确实，我们发现经验结论与直觉结合：样式/属性转移的内容保存指标必须以样式转移为条件。为了支持这一点，我们建议使用接下来令牌的可能性提出一种新的有效的零摄像评估方法。我们希望我们的元评估能够促进有关评估内容保存指标的更多研究，并确保对进行样式转移的方法进行公平评估。</li>
</ul>

<h3>Title: Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yun-Wei Chu, Kai Zhang, Christopher Malon, Martin Renqiang Min</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15040">https://arxiv.org/abs/2502.15040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15040">https://arxiv.org/pdf/2502.15040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15040]] Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation(https://arxiv.org/abs/2502.15040)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have shown impressive performance in vision and text tasks. However, hallucination remains a major challenge, especially in fields like healthcare where details are critical. In this work, we show how MLLMs may be enhanced to support Visual RAG (V-RAG), a retrieval-augmented generation framework that incorporates both text and visual data from retrieved images. On the MIMIC-CXR chest X-ray report generation and Multicare medical image caption generation datasets, we show that Visual RAG improves the accuracy of entity probing, which asks whether a medical entities is grounded by an image. We show that the improvements extend both to frequent and rare entities, the latter of which may have less positive training data. Downstream, we apply V-RAG with entity probing to correct hallucinations and generate more clinically accurate X-ray reports, obtaining a higher RadGraph-F1 score.</li>
<li><strong>摘要：</strong>多模式的大语言模型（MLLM）在视觉和文本任务中表现出令人印象深刻的表现。但是，幻觉仍然是一个重大挑战，尤其是在细节至关重要的医疗保健领域。在这项工作中，我们展示了如何增强MLLM来支持Visual Rag（V-RAG），这是一种检索型生成框架，该框架合并了来自检索的图像的文本和视觉数据。在模拟CXR胸部X射线报告生成和多层医疗图像字幕生成数据集上，我们表明视觉抹布提高了实体探测的准确性，这询问医疗实体是否以图像为基础。我们表明，这些改进既扩展到了频繁和稀有实体，后者的训练数据可能较少。在下游，我们将V-rag与实体探测一起应用以纠正幻觉并产生更临床准确的X射线报告，从而获得较高的radgraph-f1分数。</li>
</ul>

<h3>Title: Rare Disease Differential Diagnosis with Large Language Models at Scale: From Abdominal Actinomycosis to Wilson's Disease</h3>
<ul>
<li><strong>Authors: </strong>Elliot Schumacher, Dhruv Naik, Anitha Kannan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15069">https://arxiv.org/abs/2502.15069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15069">https://arxiv.org/pdf/2502.15069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15069]] Rare Disease Differential Diagnosis with Large Language Models at Scale: From Abdominal Actinomycosis to Wilson's Disease(https://arxiv.org/abs/2502.15069)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive capabilities in disease diagnosis. However, their effectiveness in identifying rarer diseases, which are inherently more challenging to diagnose, remains an open question. Rare disease performance is critical with the increasing use of LLMs in healthcare settings. This is especially true if a primary care physician needs to make a rarer prognosis from only a patient conversation so that they can take the appropriate next step. To that end, several clinical decision support systems are designed to support providers in rare disease identification. Yet their utility is limited due to their lack of knowledge of common disorders and difficulty of use. In this paper, we propose RareScale to combine the knowledge LLMs with expert systems. We use jointly use an expert system and LLM to simulate rare disease chats. This data is used to train a rare disease candidate predictor model. Candidates from this smaller model are then used as additional inputs to black-box LLM to make the final differential diagnosis. Thus, RareScale allows for a balance between rare and common diagnoses. We present results on over 575 rare diseases, beginning with Abdominal Actinomycosis and ending with Wilson's Disease. Our approach significantly improves the baseline performance of black-box LLMs by over 17% in Top-5 accuracy. We also find that our candidate generation performance is high (e.g. 88.8% on gpt-4o generated chats).</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在疾病诊断方面表现出了令人印象深刻的能力。但是，它们在识别稀有疾病方面的有效性，诊断本质上更具挑战性，仍然是一个悬而未决的问题。在医疗机构中，LLM的使用越来越多，罕见疾病的表现至关重要。如果初级保健医师只需要从患者对话中进行稀有预后，以便他们可以采取适当的下一步，尤其如此。为此，几个临床决策支持系统旨在为稀有疾病鉴定提供支持提供者。然而，由于缺乏对常见疾病和使用难度的了解，它们的效用受到限制。在本文中，我们建议将知识LLM与专家系统相结合。我们共同使用专家系统和LLM来模拟罕见疾病聊天。该数据用于训练一种罕见的候选预测模型。然后，该较小模型的候选人被用作Black-Box LLM的其他输入，以进行最终的差异诊断。因此，稀有的刻痕可以在稀有诊断和常见诊断之间保持平衡。我们提出了575种罕见疾病的结果，从腹部放线菌病开始，并以威尔逊氏病结束。我们的方法可显着提高黑盒LLM的基线性能，超过17％。我们还发现，我们的候选生成性能很高（例如，GPT-4O生成的聊天率为88.8％）。</li>
</ul>

<h3>Title: Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yeonjun In, Wonjoong Kim, Kanghoon Yoon, Sungchul Kim, Mehrab Tanjim, Kibum Kim, Chanyoung Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15086">https://arxiv.org/abs/2502.15086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15086">https://arxiv.org/pdf/2502.15086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15086]] Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models(https://arxiv.org/abs/2502.15086)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general standards, overlooking user-specific standards. However, safety standards for LLM may vary based on a user-specific profiles rather than being universally consistent across all users. This raises a critical research question: Do LLM agents act safely when considering user-specific safety standards? Despite its importance for safe LLM use, no benchmark datasets currently exist to evaluate the user-specific safety of LLMs. To address this gap, we introduce U-SAFEBENCH, the first benchmark designed to assess user-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs reveals current LLMs fail to act safely when considering user-specific safety standards, marking a new discovery in this field. To address this vulnerability, we propose a simple remedy based on chain-of-thought, demonstrating its effectiveness in improving user-specific safety. Our benchmark and code are available at this https URL.</li>
<li><strong>摘要：</strong>随着大语模型（LLM）代理商的使用继续增长，其安全漏洞变得越来越明显。广泛的基准测试基准通过严重依赖一般标准（可俯瞰特定用户的标准）来定义安全性的各个方面。但是，LLM的安全标准可能会根据特定于用户的配置文件而有所不同，而不是在所有用户中都普遍一致。这提出了一个关键的研究问题：在考虑特定用户的安全标准时，LLM代理会安全起作用吗？尽管对安全LLM使用的重要性很重要，但目前尚无基准数据集来评估LLMS的用户特定安全性。为了解决这一差距，我们介绍了U-Safebench，这是第一个旨在评估LLM安全用户特定方面的基准。我们对18种广泛使用LLM的评估表明，当前LLM在考虑特定于用户的安全标准时无法安全起作用，这标志着该领域的新发现。为了解决此漏洞，我们提出了一种基于经过思考链的简单补救措施，证明了其在改善用户特定安全性方面的有效性。我们的基准和代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans</h3>
<ul>
<li><strong>Authors: </strong>Masha Fedzechkina, Eleonora Gualdoni, Sinead Williamson, Katherine Metcalf, Skyler Seto, Barry-John Theobald</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15090">https://arxiv.org/abs/2502.15090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15090">https://arxiv.org/pdf/2502.15090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15090]] Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans(https://arxiv.org/abs/2502.15090)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Modern large language models (LLMs) achieve impressive performance on some tasks, while exhibiting distinctly non-human-like behaviors on others. This raises the question of how well the LLM's learned representations align with human representations. In this work, we introduce a novel approach to the study of representation alignment: we adopt a method from research on activation steering to identify neurons responsible for specific concepts (e.g., 'cat') and then analyze the corresponding activation patterns. Our findings reveal that LLM representations closely align with human representations inferred from behavioral data. Notably, this alignment surpasses that of word embeddings, which have been center stage in prior work on human and model alignment. Additionally, our approach enables a more granular view of how LLMs represent concepts. Specifically, we show that LLMs organize concepts in a way that reflects hierarchical relationships interpretable to humans (e.g., 'animal'-'dog').</li>
<li><strong>摘要：</strong>现代大型语言模型（LLMS）在某些任务上取得了令人印象深刻的表现，同时在其他任务上表现出明显的非人类行为。这就提出了一个问题，即LLM学到的表示与人类代表的一致性如何。在这项工作中，我们介绍了一种新颖的方法来研究表示对齐的研究：我们采用了一种从激活转向研究的方法来识别负责特定概念（例如“猫”）的神经元，然后分析相应的激活模式。我们的发现表明，LLM表示与行为数据推断的人类表示密切相符。值得注意的是，这种比对超过了单词嵌入的对齐，这是人类和模型对齐的先前工作中的中心阶段。此外，我们的方法可以使LLM如何代表概念的更详细的看法。具体来说，我们表明LLMS以反映对人类可解释的层次关系的方式组织概念（例如，'Animal'' - '狗'）。</li>
</ul>

<h3>Title: Optimizing Singular Spectrum for Large Language Model Compression</h3>
<ul>
<li><strong>Authors: </strong>Dengjie Li, Tiancheng Shen, Yao Zhou, Baisong Yang, Zhongying Liu, Masheng Yang, Bernard Ghanem, Yibo Yang, Yujie Zhong, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15092">https://arxiv.org/abs/2502.15092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15092">https://arxiv.org/pdf/2502.15092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15092]] Optimizing Singular Spectrum for Large Language Model Compression(https://arxiv.org/abs/2502.15092)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities, yet prohibitive parameter complexity often hinders their deployment. Existing singular value decomposition (SVD) based compression methods simply deem singular values as importance scores of decomposed components. However, this importance ordered by singular values does not necessarily correlate with the performance of a downstream task. In this work, we introduce SoCo (Singular spectrum optimization for large language model Compression), a novel compression framework that learns to rescale the decomposed components of SVD in a data-driven manner. Concretely, we employ a learnable diagonal matrix to assign importance scores for singular spectrum and develop a three-stage training process that progressively refines these scores from initial coarse compression to fine-grained sparsification-thereby striking an effective balance between aggressive model compression and performance preservation. Thanks to the learnable singular spectrum, SoCo adaptively prunes components according to the sparsified importance scores, rather than relying on the fixed order of singular values. More importantly, the remaining components with amplified importance scores can compensate for the loss of the pruned ones. Experimental evaluations across multiple LLMs and benchmarks demonstrate that SoCo surpasses the state-of-the-art methods in model compression.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）表现出了显着的功能，但过度的参数复杂性通常会阻碍其部署。基于现有的单数值分解（SVD）的压缩方法只是将单数视为分解组件的重要性得分。但是，单数值订购的这种重要性并不一定与下游任务的性能相关。在这项工作中，我们介绍了SOCO（大型语言模型压缩的奇异频谱优化），这是一个新型的压缩框架，以数据驱动的方式学会重新分解SVD的分解组件。具体而言，我们采用可学习的对角矩阵来分配奇异频谱的重要性得分，并开发了三阶段的训练过程，该过程逐步完善了这些分数，从初始粗大压缩到细粒度的稀疏性 - 在积极的模型压缩和绩效保护之间取得了有效的平衡。得益于可学习的奇异频谱，根据稀疏的重要性得分，SOCO自适应的原始组件，而不是依靠奇异值的固定顺序。更重要的是，其余的具有扩增重要性得分的组件可以补偿修剪成分的损失。跨多个LLM和基准测试的实验评估表明，SOCO超过了模型压缩中的最新方法。</li>
</ul>

<h3>Title: Judging It, Washing It: Scoring and Greenwashing Corporate Climate Disclosures using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Marianne Chuang, Gabriel Chuang, Cheryl Chuang, John Chuang</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15094">https://arxiv.org/abs/2502.15094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15094">https://arxiv.org/pdf/2502.15094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15094]] Judging It, Washing It: Scoring and Greenwashing Corporate Climate Disclosures using Large Language Models(https://arxiv.org/abs/2502.15094)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We study the use of large language models (LLMs) to both evaluate and greenwash corporate climate disclosures. First, we investigate the use of the LLM-as-a-Judge (LLMJ) methodology for scoring company-submitted reports on emissions reduction targets and progress. Second, we probe the behavior of an LLM when it is prompted to greenwash a response subject to accuracy and length constraints. Finally, we test the robustness of the LLMJ methodology against responses that may be greenwashed using an LLM. We find that two LLMJ scoring systems, numerical rating and pairwise comparison, are effective in distinguishing high-performing companies from others, with the pairwise comparison system showing greater robustness against LLM-greenwashed responses.</li>
<li><strong>摘要：</strong>我们研究了大型语言模型（LLM）来评估和绿色公司气候披露。首先，我们研究了使用LLM-AS-A-Gudge（LLMJ）方法的使用，以评分公司减少排放目标和进度的公司提交报告。其次，当提示LLM的行为提示以准确性和长度约束为由进行绿色响应时。最后，我们测试了LLMJ方法的鲁棒性，以针对使用LLM进行绿色的响应。我们发现，两个LLMJ评分系统，即数值等级和成对比较，可有效区分高性能的公司和其他公司，成对比较系统显示出针对LLM-GreeNASE的响应的更大鲁棒性。</li>
</ul>

<h3>Title: LUME: LLM Unlearning with Multitask Evaluations</h3>
<ul>
<li><strong>Authors: </strong>Anil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei Chang, Zhiqi Bu, Bhanukiran Vinzamuri, Volkan Cevher, Mingyi Hong, Rahul Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15097">https://arxiv.org/abs/2502.15097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15097">https://arxiv.org/pdf/2502.15097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15097]] LUME: LLM Unlearning with Multitask Evaluations(https://arxiv.org/abs/2502.15097)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Unlearning aims to remove copyrighted, sensitive, or private content from large language models (LLMs) without a full retraining. In this work, we develop a multi-task unlearning benchmark (LUME) which features three tasks: (1) unlearn synthetically generated creative short novels, (2) unlearn synthetic biographies with sensitive information, and (3) unlearn a collection of public biographies. We further release two fine-tuned LLMs of 1B and 7B parameter sizes as the target models. We conduct detailed evaluations of several recently proposed unlearning algorithms and present results on carefully crafted metrics to understand their behavior and limitations.</li>
<li><strong>摘要：</strong>未经学习的目的是从大型语言模型（LLMS）中删除受版权保护，敏感或私人内容，而无需完整的再培训。在这项工作中，我们开发了一个多任务学习的基准（LUME），其中包含三个任务：（1）未学习的综合产生的创意短篇小说，（2）带有敏感信息的未学习合成传记，以及（3）一系列公共传记集合。我们进一步释放了1B和7B参数尺寸的两个微调LLM作为目标模型。我们对最近提出的几种未学习算法进行了详细评估，并就精心制作的指标提出了结果，以了解其行为和局限性。</li>
</ul>

<h3>Title: Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning, and Interpretability through Attention Maps</h3>
<ul>
<li><strong>Authors: </strong>Yen-Che Hsiao, Abhishek Dutta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15120">https://arxiv.org/abs/2502.15120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15120">https://arxiv.org/pdf/2502.15120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15120]] Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning, and Interpretability through Attention Maps(https://arxiv.org/abs/2502.15120)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>This study investigates the in-context learning capabilities of various decoder-only transformer-based language models with different model sizes and training data, including GPT2, SmolLM2, OpenELM, TinyLlama, Stable LM, and Gemma 2. We identify a critical parameter threshold (~1.6 billion), beyond which reasoning performance improves significantly in tasks such as commonsense reasoning in multiple-choice question answering and deductive reasoning. Specifically, models above this threshold achieve better success rates in chain-of-thought (CoT) prompting for deductive reasoning tasks, especially those requiring longer reasoning chains, such as proof by contradiction and disjunction elimination. To address limitations in sub-threshold models, we demonstrate that fine-tuning with task-specific exemplars substantially enhances reasoning performance, enabling accurate CoT generation even without additional exemplars in the prompt for tasks with shorter reasoning chains. Finally, our analysis of attention maps reveals that models capable of generating correct CoTs exhibit higher token-level attention scores on subsequent correct tokens and the correct parts of speech, providing interpretability insights into reasoning processes. These findings collectively advance understanding of reasoning capabilities in decoder-only transformer-based models. The code is available at: this https URL.</li>
<li><strong>摘要：</strong>这项研究调查了具有不同模型大小和训练数据的各种仅解码器的语言模型的文化学习能力，包括GPT2，Smollm2，OpenElm，Tinyllama，Tinyllama，stable LM和Gemma 2。我们确定了关键参数阈值（ 〜16亿），除此之外，推理绩效在多项选择的问题回答和演绎推理等任务中有了显着改善。具体而言，高于此阈值的模型在促使推理推理任务的链接链（COT）中实现了更好的成功率，尤其是那些需要更长推理链的人，例如通过矛盾和脱节消除来证明。为了解决子阈值模型中的局限性，我们证明，对特定于任务的示例进行微调可以大大提高推理性能，即使没有其他示例，也可以在较短的推理链的任务提示中进行准确的COT生成。最后，我们对注意图的分析表明，能够生成正确的COTS的模型在随后的正确令牌和语音的正确部分表现出更高的令牌级别的注意力评分，从而为推理过程提供了可解释性的见解。这些发现共同提高了基于仅解码器的模型中的推理能力的理解。该代码可用：此HTTPS URL。</li>
</ul>

<h3>Title: CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from In-Context Demonstrations</h3>
<ul>
<li><strong>Authors: </strong>Vignesh Kothapalli, Hamed Firooz, Maziar Sanjabi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15132">https://arxiv.org/abs/2502.15132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15132">https://arxiv.org/pdf/2502.15132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15132]] CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from In-Context Demonstrations(https://arxiv.org/abs/2502.15132)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>We introduce CoT-ICL Lab, a framework and methodology to generate synthetic tokenized datasets and systematically study chain-of-thought (CoT) in-context learning (ICL) in language models. CoT-ICL Lab allows fine grained control over the complexity of in-context examples by decoupling (1) the causal structure involved in chain token generation from (2) the underlying token processing functions. We train decoder-only transformers (up to 700M parameters) on these datasets and show that CoT accelerates the accuracy transition to higher values across model sizes. In particular, we find that model depth is crucial for leveraging CoT with limited in-context examples, while more examples help shallow models match deeper model performance. Additionally, limiting the diversity of token processing functions throughout training improves causal structure learning via ICL. We also interpret these transitions by analyzing transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a simple yet powerful testbed for theoretical and empirical insights into ICL and CoT in language models.</li>
<li><strong>摘要：</strong>我们介绍了COT-ICL实验室，该实验室是一种框架和方法，用于生成综合令牌化数据集并系统地研究语言模型中的秘密链接链（COT）内部学习（ICL）。 COT-ICL实验室允许通过解耦（1）（1）与（2）基础令牌处理函数相关的因果结构（1）通过解耦（1）链接中示例的复杂性的细粒度控制。我们在这些数据集上训练仅解码器的变压器（最多700m参数），并表明COT加速了跨模型尺寸的精度过渡到更高的值。特别是，我们发现模型深度对于利用有限的内在示例的COT至关重要，而更多的示例有助于浅模型与更深的模型性能相匹配。此外，限制令牌处理功能的多样性在整个培训中都可以通过ICL改善因果结构学习。我们还通过分析变压器嵌入和注意图来解释这些转变。总体而言，COT-ICL实验室是对语言模型中ICL和COT的理论和经验见解的简单而强大的测试。</li>
</ul>

<h3>Title: Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG in Edge Device</h3>
<ul>
<li><strong>Authors: </strong>Juntae Lee, Jihwan Bang, Seunghan Yang, Kyuhong Shim, Simyung Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15134">https://arxiv.org/abs/2502.15134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15134">https://arxiv.org/pdf/2502.15134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15134]] Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG in Edge Device(https://arxiv.org/abs/2502.15134)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) with large language models (LLMs) is especially valuable in specialized domains, where precision is critical. To more specialize the LLMs into a target domain, domain-specific RAG has recently been developed by allowing the LLM to access the target domain early via finetuning. The domain-specific RAG makes more sense in resource-constrained environments like edge devices, as they should perform a specific task (e.g. personalization) reliably using only small-scale LLMs. While the domain-specific RAG is well-aligned with edge devices in this respect, it often relies on widely-used reasoning techniques like chain-of-thought (CoT). The reasoning step is useful to understand the given external knowledge, and yet it is computationally expensive and difficult for small-scale LLMs to learn it. Tackling this, we propose the Chain of Rank (CoR) which shifts the focus from intricate lengthy reasoning to simple ranking of the reliability of input external documents. Then, CoR reduces computational complexity while maintaining high accuracy, making it particularly suited for resource-constrained environments. We attain the state-of-the-art (SOTA) results in benchmarks, and analyze its efficacy.</li>
<li><strong>摘要：</strong>带有大语言模型（LLM）的检索增强生成（RAG）在精度至关重要的专用领域中特别有价值。为了使LLM更专业地进入目标域，最近通过允许LLM通过芬特登录来访问目标域，从而开发了域特异性的破布。特定于域的抹布在资源受限的环境（例如边缘设备）中更有意义，因为它们应该仅使用小规模的LLM可靠地执行特定任务（例如个性化）。尽管在这方面，特定于域特异性的抹布与边缘设备很好地保持一致，但它通常依赖于广泛使用的推理技术，例如经过思考链（COT）。推理步骤对于理解给定的外部知识很有用，但是对于小规模的LLM而言，它在计算上很昂贵且难以学习。解决这个问题时，我们提出了等级链（COR），将焦点从复杂的冗长推理转移到了输入外部文档的可靠性的简单排名。然后，COR降低了计算复杂性，同时保持高精度，使其特别适合资源受限的环境。我们达到了最新的（SOTA）的结果，并分析了其功效。</li>
</ul>

<h3>Title: Do LLMs Make Mistakes Like Students? Exploring Natural Alignment between Language Models and Human Error Patterns</h3>
<ul>
<li><strong>Authors: </strong>Naiming Liu, Shashank Sonkar, Richard G. Baraniuk</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15140">https://arxiv.org/abs/2502.15140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15140">https://arxiv.org/pdf/2502.15140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15140]] Do LLMs Make Mistakes Like Students? Exploring Natural Alignment between Language Models and Human Error Patterns(https://arxiv.org/abs/2502.15140)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in various educational tasks, yet their alignment with human learning patterns, particularly in predicting which incorrect options students are most likely to select in multiple-choice questions (MCQs), remains underexplored. Our work investigates the relationship between LLM generation likelihood and student response distributions in MCQs with a specific focus on distractor selections. We collect a comprehensive dataset of MCQs with real-world student response distributions to explore two fundamental research questions: (1). RQ1 - Do the distractors that students more frequently select correspond to those that LLMs assign higher generation likelihood to? (2). RQ2 - When an LLM selects a incorrect choice, does it choose the same distractor that most students pick? Our experiments reveals moderate correlations between LLM-assigned probabilities and student selection patterns for distractors in MCQs. Additionally, when LLMs make mistakes, they are more likley to select the same incorrect answers that commonly mislead students, which is a pattern consistent across both small and large language models. Our work provides empirical evidence that despite LLMs' strong performance on generating educational content, there remains a gap between LLM's underlying reasoning process and human cognitive processes in identifying confusing distractors. Our findings also have significant implications for educational assessment development. The smaller language models could be efficiently utilized for automated distractor generation as they demonstrate similar patterns in identifying confusing answer choices as larger language models. This observed alignment between LLMs and student misconception patterns opens new opportunities for generating high-quality distractors that complement traditional human-designed distractors.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在各种教育任务中都表现出了出色的能力，但是它们与人类学习模式的一致性，尤其是在预测哪些不正确的选择学生最有可能在多项选择问题中选择的选项（MCQ）仍然没有被忽略。我们的工作调查了LLM生成的可能性与MCQ中的学生反应分布之间的关系，并针对干扰物选择。我们收集了具有现实世界中学生反应分布的MCQ的全面数据集，以探讨两个基本的研究问题：（1）。 RQ1-学生选择更频繁选择的干扰因素是否对应于LLM分配更高生成可能性的分散因素？ （2）。 RQ2-当LLM选择不正确的选择时，它是否选择大多数学生选择的干扰器？我们的实验揭示了LLM分配的概率与MCQ中的分散障碍者的学生选择模式之间的中等相关性。此外，当LLMS犯错时，他们更喜欢选择通常误导学生的不正确答案，这在小型和大型语言模型中都是一致的模式。我们的工作提供了经验证据，表明尽管LLMS在产生教育内容方面表现出色，但LLM的基本推理过程与人类认知过程之间仍然存在差距，以识别令人困惑的干扰因素。我们的发现对教育评估发展也有重大影响。较小的语言模型可以有效地用于自动化分散分心器的生成，因为它们在识别出令人困惑的答案选择为较大语言模型时表现出相似的模式。观察到的LLM和学生误解模式之间的对齐方式为产生了补充传统人类设计的干扰因素的高质量干扰物的新机会。</li>
</ul>

<h3>Title: Latent Factor Models Meets Instructions:Goal-conditioned Latent Factor Discovery without Task Supervision</h3>
<ul>
<li><strong>Authors: </strong>Zhouhang Xie, Tushar Khot, Bhavana Dalvi Mishra, Harshit Surana, Julian McAuley, Peter Clark, Bodhisattwa Prasad Majumder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15147">https://arxiv.org/abs/2502.15147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15147">https://arxiv.org/pdf/2502.15147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15147]] Latent Factor Models Meets Instructions:Goal-conditioned Latent Factor Discovery without Task Supervision(https://arxiv.org/abs/2502.15147)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Instruction-following LLMs have recently allowed systems to discover hidden concepts from a collection of unstructured documents based on a natural language description of the purpose of the discovery (i.e., goal). Still, the quality of the discovered concepts remains mixed, as it depends heavily on LLM's reasoning ability and drops when the data is noisy or beyond LLM's knowledge. We present Instruct-LF, a goal-oriented latent factor discovery system that integrates LLM's instruction-following ability with statistical models to handle large, noisy datasets where LLM reasoning alone falls short. Instruct-LF uses LLMs to propose fine-grained, goal-related properties from documents, estimates their presence across the dataset, and applies gradient-based optimization to uncover hidden factors, where each factor is represented by a cluster of co-occurring properties. We evaluate latent factors produced by Instruct-LF on movie recommendation, text-world navigation, and legal document categorization tasks. These interpretable representations improve downstream task performance by 5-52% than the best baselines and were preferred 1.8 times as often as the best alternative, on average, in human evaluation.</li>
<li><strong>摘要：</strong>遵循指示性的LLM最近允许系统根据发现目的的自然语言描述（即目标），从非结构化文档的集合中发现隐藏的概念。尽管如此，发现的概念的质量仍然混杂，因为它在很大程度上取决于LLM的推理能力，并且当数据嘈杂或超越LLM知识时会下降。我们提出了指示LF，这是一种面向目标的潜在因子发现系统，该系统将LLM的指导跟随能力与统计模型集成在一起，以处理仅LLM推理的大型嘈杂数据集，仅LLM推理就不足。指令LF使用llms提出了来自文档的细粒度，与目标相关的属性，估算它们在整个数据集中的存在，并将基于梯度的优化应用于发现隐藏的因素，其中每个因素均由共同存在的属性簇表示。我们评估了指令LF对电影推荐，文本世界导航和法律文档分类任务产生的潜在因素。这些可解释的表示形式将下游任务绩效提高了5-52％，比最好的基线可获得优先的1.8倍，通常是人类评估中最好的替代方案。</li>
</ul>

<h3>Title: Investigating the Adaptive Robustness with Knowledge Conflicts in LLM-based Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Tianjie Ju, Bowen Wang, Hao Fei, Mong-Li Lee, Wynne Hsu, Yun Li, Qianren Wang, Pengzhou Cheng, Zongru Wu, Zhuosheng Zhang, Gongshen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15153">https://arxiv.org/abs/2502.15153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15153">https://arxiv.org/pdf/2502.15153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15153]] Investigating the Adaptive Robustness with Knowledge Conflicts in LLM-based Multi-Agent Systems(https://arxiv.org/abs/2502.15153)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have upgraded them from sophisticated text generators to autonomous agents capable of corporation and tool use in multi-agent systems (MASs). However, the robustness of these LLM-based MASs, especially under knowledge conflicts, remains unclear. In this paper, we design four comprehensive metrics to investigate the robustness of MASs when facing mild or task-critical knowledge conflicts. We first analyze mild knowledge conflicts introduced by heterogeneous agents and find that they do not harm system robustness but instead improve collaborative decision-making. Next, we investigate task-critical knowledge conflicts by synthesizing knowledge conflicts and embedding them into one of the agents. Our results show that these conflicts have surprisingly little to no impact on MAS robustness. Furthermore, we observe that MASs demonstrate certain self-repairing capabilities by reducing their reliance on knowledge conflicts and adopting alternative solution paths to maintain stability. Finally, we conduct ablation studies on the knowledge conflict number, agent number, and interaction rounds, finding that the self-repairing capability of MASs has intrinsic limits, and all findings hold consistently across various factors. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的最新进展已将它们从复杂的文本生成器升级为能够在多代理系统（MASS）中使用公司和工具使用的自主剂。但是，这些基于LLM的质量的鲁棒性，尤其是在知识冲突下，尚不清楚。在本文中，我们设计了四个综合指标，以调查面对轻度或关键任务知识冲突时质量的鲁棒性。我们首先分析了异质代理引入的轻度知识冲突，发现它们不会损害系统的鲁棒性，而是改善了协作决策。接下来，我们通过综合知识冲突并将其嵌入其中一个代理来研究关键任务知识冲突。我们的结果表明，这些冲突对MAS的鲁棒性几乎没有影响。此外，我们观察到，质量通过减少对知识冲突的依赖并采用替代解决方案路径来维持稳定性来证明某些自我修复能力。最后，我们对知识冲突数量，代理数量和互动回合进行消融研究，发现质量的自我修复能力具有内在限制，并且所有发现在各种因素中都始终存在。我们的代码在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: Extreme Speech Classification in the Era of LLMs: Exploring Open-Source and Proprietary Models</h3>
<ul>
<li><strong>Authors: </strong>Sarthak Mahajan, Nimmi Rangaswamy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15155">https://arxiv.org/abs/2502.15155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15155">https://arxiv.org/pdf/2502.15155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15155]] Extreme Speech Classification in the Era of LLMs: Exploring Open-Source and Proprietary Models(https://arxiv.org/abs/2502.15155)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>In recent years, widespread internet adoption and the growth in userbase of various social media platforms have led to an increase in the proliferation of extreme speech online. While traditional language models have demonstrated proficiency in distinguishing between neutral text and non-neutral text (i.e. extreme speech), categorizing the diverse types of extreme speech presents significant challenges. The task of extreme speech classification is particularly nuanced, as it requires a deep understanding of socio-cultural contexts to accurately interpret the intent of the language used by the speaker. Even human annotators often disagree on the appropriate classification of such content, emphasizing the complex and subjective nature of this task. The use of human moderators also presents a scaling issue, necessitating the need for automated systems for extreme speech classification. The recent launch of ChatGPT has drawn global attention to the potential applications of Large Language Models (LLMs) across a diverse variety of tasks. Trained on vast and diverse corpora, and demonstrating the ability to effectively capture and encode contextual information, LLMs emerge as highly promising tools for tackling this specific task of extreme speech classification. In this paper, we leverage the Indian subset of the extreme speech dataset from Maronikolakis et al. (2022) to develop an effective classification framework using LLMs. We evaluate open-source Llama models against closed-source OpenAI models, finding that while pre-trained LLMs show moderate efficacy, fine-tuning with domain-specific data significantly enhances performance, highlighting their adaptability to linguistic and contextual nuances. Although GPT-based models outperform Llama models in zero-shot settings, the performance gap disappears after fine-tuning.</li>
<li><strong>摘要：</strong>近年来，广泛的互联网采用和各种社交媒体平台的用户群的增长导致极端语音在线的扩散增加。尽管传统语言模型表现出熟练的区分中性文本和非中性文本（即极端语音），但对极端语音的多种类型进行分类带来了重大挑战。极端语音分类的任务特别细微，因为它需要对社会文化背景的深刻理解，以准确解释说话者使用的语言的意图。甚至人类注释者也经常不同意此类内容的适当分类，强调该任务的复杂和主观性质。人类主持人的使用还提出了一个扩展问题，因此需要对自动化系统进行极端语音分类。 Chatgpt最近推出的启动引起了全球对大型语言模型（LLM）在各种任务中的潜在应用的关注。在庞大而多样化的语料库中进行了培训，并证明了有效捕获和编码上下文信息的能力，LLM出现了作为应对这一特定的极端语音分类任务的高度有希望的工具。在本文中，我们利用了Maronikolakis等人的Extreme Speech数据集的印度子集。 （2022）使用LLMS开发有效的分类框架。我们根据封闭式OpenAI模型评估了开源的Llama模型，发现虽然预先训练的LLMS显示出适度的功效，但使用特定于域的数据进行了微调，可显着提高性能，突出其对语言和上下文细微差别的适应性。尽管基于GPT的模型在零拍设置中优于Llama模型，但在微调后，性能差距消失了。</li>
</ul>

<h3>Title: Scale-Free Graph-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianglin Lu, Yixuan Liu, Yitian Zhang, Yun Fu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15189">https://arxiv.org/abs/2502.15189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15189">https://arxiv.org/pdf/2502.15189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15189]] Scale-Free Graph-Language Models(https://arxiv.org/abs/2502.15189)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Graph-language models (GLMs) have demonstrated great potential in graph-based semi-supervised learning. A typical GLM consists of two key stages: graph generation and text embedding, which are usually implemented by inferring a latent graph and finetuning a language model (LM), respectively. However, the former often relies on artificial assumptions about the underlying edge distribution, while the latter requires extensive data annotations. To tackle these challenges, this paper introduces a novel GLM that integrates graph generation and text embedding within a unified framework. Specifically, for graph generation, we leverage an inherent characteristic of real edge distribution--the scale-free property--as a structural prior. We unexpectedly find that this natural property can be effectively approximated by a simple k-nearest neighbor (KNN) graph. For text embedding, we develop a graph-based pseudo-labeler that utilizes scale-free graphs to provide complementary supervision for improved LM finetuning. Extensive experiments on representative datasets validate our findings on the scale-free structural approximation of KNN graphs and demonstrate the effectiveness of integrating graph generation and text embedding with a real structural prior. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>图形模型（GLM）在基于图的半监督学习中表现出巨大的潜力。一个典型的GLM由两个关键阶段组成：图形生成和文本嵌入，通常通过分别推断潜在图和填充语言模型（LM）来实现。但是，前者通常依赖于对基础边缘分布的人工假设，而后者则需要广泛的数据注释。为了应对这些挑战，本文介绍了一种新颖的GLM，该GLM将图形生成和嵌入在统一框架中的文本嵌入。具体来说，对于图形生成，我们利用了实际边缘分布的固有特征（无尺度的属性）作为结构性的先验。我们出乎意料地发现，这种自然特性可以通过简单的k-neareb邻居（KNN）图有效地近似。对于文本嵌入，我们开发了一个基于图形的伪标记器，该伪标记使用了无标度图来提供互补的监督，以改善LM Finetuning。对代表性数据集的广泛实验验证了我们对KNN图的无尺度结构近似的发现，并证明了将图形生成和文本嵌入与实际结构之前的有效性。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Zhaoxuan Wu, Zijian Zhou, Arun Verma, Alok Prakash, Daniela Rus, Bryan Kian Hsiang Low</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15197">https://arxiv.org/abs/2502.15197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15197">https://arxiv.org/pdf/2502.15197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15197]] TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding(https://arxiv.org/abs/2502.15197)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We propose TETRIS, a novel method that optimizes the total throughput of batch speculative decoding in multi-request settings. Unlike existing methods that optimize for a single request or a group of requests as a whole, TETRIS actively selects the most promising draft tokens (for every request in a batch) to be accepted when verified in parallel, resulting in fewer rejected tokens and hence less wasted computing resources. Such an effective resource utilization to achieve fast inference in large language models (LLMs) is especially important to service providers with limited inference capacity. Compared to baseline speculative decoding, TETRIS yields a consistently higher acceptance rate and more effective utilization of the limited inference capacity. We show theoretically and empirically that TETRIS outperforms baseline speculative decoding and existing methods that dynamically select draft tokens, leading to a more efficient batch inference in LLMs.</li>
<li><strong>摘要：</strong>我们提出了Tetris，这是一种新颖的方法，可优化多要求设置中批次投机解码的总吞吐量。与现有的方法对单个请求进行优化或整个请求的一组请求不同，Tetris会积极选择最有前途的草稿令牌（对于批处理中的每个请求），并在并行验证时被接受，从而减少了拒绝令牌，从而减少了。浪费计算资源。这种有效的资源利用来实现大型语言模型（LLM）的快速推断对于推理能力有限的服务提供商尤为重要。与基线投机解码相比，俄罗斯方块的接受率始终如一，并且对有限推理能力的利用率更高。我们从理论和经验上表明，俄罗斯方块的表现优于基线投机解码和现有的方法，这些解码和现有的方法动态选择了草稿令牌，从而导致LLMS中更有效的批处理推断。</li>
</ul>

<h3>Title: Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing</h3>
<ul>
<li><strong>Authors: </strong>Zhilin Wang, Yafu Li, Jianhao Yan, Yu Cheng, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15208">https://arxiv.org/abs/2502.15208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15208">https://arxiv.org/pdf/2502.15208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15208]] Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing(https://arxiv.org/abs/2502.15208)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Dynamical systems theory provides a framework for analyzing iterative processes and evolution over time. Within such systems, repetitive transformations can lead to stable configurations, known as attractors, including fixed points and limit cycles. Applying this perspective to large language models (LLMs), which iteratively map input text to output text, provides a principled approach to characterizing long-term behaviors. Successive paraphrasing serves as a compelling testbed for exploring such dynamics, as paraphrases re-express the same underlying meaning with linguistic variation. Although LLMs are expected to explore a diverse set of paraphrases in the text space, our study reveals that successive paraphrasing converges to stable periodic states, such as 2-period attractor cycles, limiting linguistic diversity. This phenomenon is attributed to the self-reinforcing nature of LLMs, as they iteratively favour and amplify certain textual forms over others. This pattern persists with increasing generation randomness or alternating prompts and LLMs. These findings underscore inherent constraints in LLM generative capability, while offering a novel dynamical systems perspective for studying their expressive potential.</li>
<li><strong>摘要：</strong>动力学系统理论提供了一个框架，用于分析迭代过程和随着时间的流逝。在这样的系统中，重复转换可以导致稳定的配置，称为吸引子，包括固定点和极限周期。将此视角应用于大型语言模型（LLMS），将输入文本映射到输出文本，为表征长期行为提供了一种原则性的方法。连续的释义是探索这种动力学的引人注目的测试床，因为释义与语言差异重新表达了相同的基本含义。尽管LLM有望在文本空间中探索各种各样的释义，但我们的研究表明，连续的释义会融合到稳定的周期状态，例如2-周期吸引者周期，从而限制了语言多样性。这种现象归因于LLM的自我强化性质，因为它们在迭代中偏爱并扩大某些文本形式而不是其他形式。这种模式持续到发电的随机性或交替提示和LLM的增加。这些发现强调了LLM生成能力中的固有约束，同时提供了一种新型的动力学系统观点来研究其表达潜力。</li>
</ul>

<h3>Title: ESPnet-SpeechLM: An Open Speech Language Model Toolkit</h3>
<ul>
<li><strong>Authors: </strong>Jinchuan Tian, Jiatong Shi, William Chen, Siddhant Arora, Yoshiki Masuyama, Takashi Maekaku, Yihan Wu, Junyi Peng, Shikhar Bharadwaj, Yiwen Zhao, Samuele Cornell, Yifan Peng, Xiang Yue, Chao-Han Huck Yang, Graham Neubig, Shinji Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15218">https://arxiv.org/abs/2502.15218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15218">https://arxiv.org/pdf/2502.15218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15218]] ESPnet-SpeechLM: An Open Speech Language Model Toolkit(https://arxiv.org/abs/2502.15218)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>We present ESPnet-SpeechLM, an open toolkit designed to democratize the development of speech language models (SpeechLMs) and voice-driven agentic applications. The toolkit standardizes speech processing tasks by framing them as universal sequential modeling problems, encompassing a cohesive workflow of data preprocessing, pre-training, inference, and task evaluation. With ESPnet-SpeechLM, users can easily define task templates and configure key settings, enabling seamless and streamlined SpeechLM development. The toolkit ensures flexibility, efficiency, and scalability by offering highly configurable modules for every stage of the workflow. To illustrate its capabilities, we provide multiple use cases demonstrating how competitive SpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter model pre-trained on both text and speech tasks, across diverse benchmarks. The toolkit and its recipes are fully transparent and reproducible at: this https URL.</li>
<li><strong>摘要：</strong>我们提出ESPNET-SpeechLM，这是一种开放的工具包，旨在使语音语言模型（SecemlM）和语音驱动的代理应用的发展民主化。该工具包通过将其框架为通用的顺序建模问题，涵盖数据预处理，预训练，推理和任务评估的凝聚力工作流来使语音处理任务标准化。使用ESPNET-SpeechLM，用户可以轻松地定义任务模板并配置密钥设置，从而实现无缝和简化的SecedLM开发。该工具包通过为工作流的每个阶段提供高度可配置的模块来确保灵活性，效率和可扩展性。为了说明其功能，我们提供了多种用例，展示了如何使用ESPNET-SpeechLM构建竞争性语音LM，其中包括在文本和语音任务上预先培训的1.7B参数模型，跨不同的基准。该工具包及其配方完全透明且可重现：此HTTPS URL。</li>
</ul>

<h3>Title: Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews</h3>
<ul>
<li><strong>Authors: </strong>Mengqiao Liu, Tevin Wang, Cassandra A. Cohen, Sarah Li, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15226">https://arxiv.org/abs/2502.15226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15226">https://arxiv.org/pdf/2502.15226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15226]] Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews(https://arxiv.org/abs/2502.15226)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interacted with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, for example, the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our collected chat-and-interview logs will be released.</li>
<li><strong>摘要：</strong>哪种大型语言模型（LLM）更好？每个评估都讲述了一个故事，但是用户对当前LLM的真正看法是什么？本文介绍了线索，这是一位由LLM驱动的访调员，在用户与LLMS互动之后，可以进行瞬间的用户体验访谈，并自动从大量访谈日志中获得有关用户意见的见解。我们对数千名用户进行了一项研究，以了解主流LLM的用户意见，招募用户首先与目标LLM聊天，然后接受Clue的采访。我们的实验表明，线索捕获了有趣的用户意见，例如，对显示的DeepSeek-R1的推理过程的双相情感以及对信息新鲜度和多模式的需求。我们收集的聊天浏览日志将发布。</li>
</ul>

<h3>Title: LightMamba: Efficient Mamba Acceleration on FPGA with Quantization and Hardware Co-design</h3>
<ul>
<li><strong>Authors: </strong>Renjie Wei, Songqiang Xu, Linfeng Zhong, Zebin Yang, Qingyu Guo, Yuan Wang, Runsheng Wang, Meng Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15260">https://arxiv.org/abs/2502.15260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15260">https://arxiv.org/pdf/2502.15260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15260]] LightMamba: Efficient Mamba Acceleration on FPGA with Quantization and Hardware Co-design(https://arxiv.org/abs/2502.15260)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>State space models (SSMs) like Mamba have recently attracted much attention. Compared to Transformer-based large language models (LLMs), Mamba achieves linear computation complexity with the sequence length and demonstrates superior performance. However, Mamba is hard to accelerate due to the scattered activation outliers and the complex computation dependency, rendering existing LLM accelerators inefficient. In this paper, we propose LightMamba that co-designs the quantization algorithm and FPGA accelerator architecture for efficient Mamba inference. We first propose an FPGA-friendly post-training quantization algorithm that features rotation-assisted quantization and power-of-two SSM quantization to reduce the majority of computation to 4-bit. We further design an FPGA accelerator that partially unrolls the Mamba computation to balance the efficiency and hardware costs. Through computation reordering as well as fine-grained tiling and fusion, the hardware utilization and memory efficiency of the accelerator get drastically improved. We implement LightMamba on Xilinx Versal VCK190 FPGA and achieve 4.65x to 6.06x higher energy efficiency over the GPU baseline. When evaluated on Alveo U280 FPGA, LightMamba reaches 93 tokens/s, which is 1.43x that of the GPU baseline.</li>
<li><strong>摘要：</strong>像Mamba这样的国家空间模型（SSM）最近引起了很多关注。与基于变压器的大语言模型（LLM）相比，Mamba具有序列长度的线性计算复杂性，并表现出卓越的性能。但是，由于散射的激活异常值和复杂的计算依赖性，MAMBA很难加速，从而使现有的LLM加速器效率低下。在本文中，我们提出了LightMamba，该LightMamba共同设计了量化算法和FPGA加速器架构，以进行有效的MAMBA推断。我们首先提出了一种适合FPGA友好的训练后量化算法，该算法具有旋转辅助量化和两个SSM量化的功率，以将大多数计算降低到4位。我们进一步设计了一个FPGA加速器，该加速器部分展开了Mamba计算，以平衡效率和硬件成本。通过计算重新排序以及细粒的平铺和融合，加速器的硬件利用率和内存效率得到了巨大改进。我们在Xilinx Versal VCK190 FPGA上实施LightMamba，并在GPU基线上实现4.65倍至6.06倍的能源效率。当对ALVEO U280 FPGA进行评估时，LightMamba达到93个令牌/s，即GPU基线的1.43倍。</li>
</ul>

<h3>Title: Retrieval-Augmented Speech Recognition Approach for Domain Challenges</h3>
<ul>
<li><strong>Authors: </strong>Peng Shen, Xugang Lu, Hisashi Kawai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15264">https://arxiv.org/abs/2502.15264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15264">https://arxiv.org/pdf/2502.15264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15264]] Retrieval-Augmented Speech Recognition Approach for Domain Challenges(https://arxiv.org/abs/2502.15264)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Speech recognition systems often face challenges due to domain mismatch, particularly in real-world applications where domain-specific data is unavailable because of data accessibility and confidentiality constraints. Inspired by Retrieval-Augmented Generation (RAG) techniques for large language models (LLMs), this paper introduces a LLM-based retrieval-augmented speech recognition method that incorporates domain-specific textual data at the inference stage to enhance recognition performance. Rather than relying on domain-specific textual data during the training phase, our model is trained to learn how to utilize textual information provided in prompts for LLM decoder to improve speech recognition performance. Benefiting from the advantages of the RAG retrieval mechanism, our approach efficiently accesses locally available domain-specific documents, ensuring a convenient and effective process for solving domain mismatch problems. Experiments conducted on the CSJ database demonstrate that the proposed method significantly improves speech recognition accuracy and achieves state-of-the-art results on the CSJ dataset, even without relying on the full training data.</li>
<li><strong>摘要：</strong>语音识别系统通常由于领域不匹配而面临挑战，尤其是在由于数据可访问性和机密性约束而无法使用的现实世界应用中。受到大型语言模型（LLMS）的检索效果（RAG）技术的启发，本文介绍了一种基于LLM的检索式语音识别方法，该方法在推理阶段结合了特定于领域的文本数据以增强识别性能。我们的模型不是在训练阶段依靠特定领域的文本数据，而是对LLM解码器提示中提供的文本信息进行培训，以提高语音识别性能。我们的方法受益于抹布检索机制的优势，有效地访问了本地可用的特定领域文档，从而确保了解决域不匹配问题的方便有效过程。在CSJ数据库上进行的实验表明，即使不依靠完整的培训数据，该提出的方法显着提高了语音识别精度，并在CSJ数据集上实现了最先进的结果。</li>
</ul>

<h3>Title: A Training-free LLM-based Approach to General Chinese Character Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Houquan Zhou, Bo Zhang, Zhenghua Li, Ming Yan, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15266">https://arxiv.org/abs/2502.15266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15266">https://arxiv.org/pdf/2502.15266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15266]] A Training-free LLM-based Approach to General Chinese Character Error Correction(https://arxiv.org/abs/2502.15266)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Chinese spelling correction (CSC) is a crucial task that aims to correct character errors in Chinese text. While conventional CSC focuses on character substitution errors caused by mistyping, two other common types of character errors, missing and redundant characters, have received less attention. These errors are often excluded from CSC datasets during the annotation process or ignored during evaluation, even when they have been annotated. This issue limits the practicality of the CSC task. To address this issue, we introduce the task of General Chinese Character Error Correction (C2EC), which focuses on all three types of character errors. We construct a high-quality C2EC benchmark by combining and manually verifying data from CCTC and Lemon datasets. We extend the training-free prompt-free CSC method to C2EC by using Levenshtein distance for handling length changes and leveraging an additional prompt-based large language model (LLM) to improve performance. Experiments show that our method enables a 14B-parameter LLM to be on par with models nearly 50 times larger on both conventional CSC and C2EC tasks, without any fine-tuning.</li>
<li><strong>摘要：</strong>中国拼写校正（CSC）是一项至关重要的任务，旨在纠正中文文本中的字符错误。尽管常规的CSC专注于错误的字符替代错误，但其他两种常见的字符错误类型（缺失和冗余字符）受到了较少的关注。在注释过程中，这些错误通常被排除在CSC数据集中，或在评估过程中忽略，即使它们被注释。此问题限制了CSC任务的实用性。为了解决此问题，我们介绍了一般汉字错误校正（C2EC）的任务，该任务侧重于所有三种类型的字符错误。我们通过组合和手动验证CCTC和Lemon数据集的数据来构建高质量的C2EC基准。我们通过使用Levenshtein距离来处理长度变化并利用额外的基于及时的基于及时的大型语言模型（LLM）来提高性能，从而将无训练的无迅速CSC方法扩展到C2EC。实验表明，我们的方法使14B参数LLM在常规CSC和C2EC任务上都与近50倍的模型相当，而无需进行任何微调。</li>
</ul>

<h3>Title: Round Attention: A Novel Round-Level Attention Mechanism to Accelerate LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Yaohua Tang, Zhicheng Hu, Kun Cheng, Fan Mo, Qiheng Lv, Hua Wang, Zhi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15294">https://arxiv.org/abs/2502.15294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15294">https://arxiv.org/pdf/2502.15294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15294]] Round Attention: A Novel Round-Level Attention Mechanism to Accelerate LLM Inference(https://arxiv.org/abs/2502.15294)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The increasing context window size in large language models (LLMs) has improved their ability to handle complex, long-text tasks. However, as the conversation rounds continue, it is required to store a large amount of KV cache in GPU memory, which significantly affects the efficiency and even availability of the model serving systems. This paper analyzes dialogue data from real users and discovers that the LLM inference manifests a watershed layer, after which the distribution of round-level attention shows notable similarity. We propose Round Attention, a novel round-level attention mechanism that only recalls and computes the KV cache of the most relevant rounds. The experiments show that our method saves 55\% memory usage without compromising model performance.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）中不断增加的上下文窗口大小提高了其处理复杂的长篇文本任务的能力。但是，随着对话的继续，需要在GPU内存中存储大量的KV缓存，这显着影响模型服务系统的效率甚至可用性。本文分析了来自真实用户的对话数据，发现LLM推理表现出了一个分水岭，此后，圆形注意力的分布表现出显着的相似性。我们提出了圆形注意力，这是一种新型的圆形注意机制，只能回忆和计算最相关的回合的KV缓存。实验表明，我们的方法在不损害模型性能的情况下节省了55 \％的内存使用情况。</li>
</ul>

<h3>Title: Detecting Future-related Contexts of Entity Mentions</h3>
<ul>
<li><strong>Authors: </strong>Puneet Prashar, Krishna Mohan Shukla, Adam Jatowt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15332">https://arxiv.org/abs/2502.15332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15332">https://arxiv.org/pdf/2502.15332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15332]] Detecting Future-related Contexts of Entity Mentions(https://arxiv.org/abs/2502.15332)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The ability to automatically identify whether an entity is referenced in a future context can have multiple applications including decision making, planning and trend forecasting. This paper focuses on detecting implicit future references in entity-centric texts, addressing the growing need for automated temporal analysis in information processing. We first present a novel dataset of 19,540 sentences built around popular entities sourced from Wikipedia, which consists of future-related and non-future-related contexts in which those entities appear. As a second contribution, we evaluate the performance of several Language Models including also Large Language Models (LLMs) on the task of distinguishing future-oriented content in the absence of explicit temporal references.</li>
<li><strong>摘要：</strong>自动确定是否在未来上下文中引用实体的能力可以具有多个应用程序，包括决策，计划和趋势预测。本文着重于检测以实体为中心文本中的隐性未来参考，以解决信息处理中对自动化时间分析的日益增长的需求。我们首先介绍了一个围绕来自Wikipedia的流行实体而建立的19,540个句子的新颖数据集，该句子由这些实体出现的未来相关和与未来相关的上下文组成。作为第二个贡献，我们评估了几种语言模型的性能，包括大型语言模型（LLMS）在没有明确的时间参考的情况下区分未来的内容的任务。</li>
</ul>

<h3>Title: Stepwise Informativeness Search for Improving LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Wang, Enda Zhao, Zhongyu Wei, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15335">https://arxiv.org/abs/2502.15335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15335">https://arxiv.org/pdf/2502.15335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15335]] Stepwise Informativeness Search for Improving LLM Reasoning(https://arxiv.org/abs/2502.15335)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, prompt</a></li>
<li><strong>Abstract: </strong>Advances in Large Language Models (LLMs) have significantly improved multi-step reasoning through generating free-text rationales. However, recent studies show that LLMs tend to lose focus over the middle of long contexts. This raises concerns that as reasoning progresses, LLMs may overlook information in earlier steps when decoding subsequent steps, leading to generate unreliable and redundant rationales. To address this, we propose guiding LLMs to generate more accurate and concise step-by-step rationales by (1) proactively referencing information from underutilized prior steps, and (2) minimizing redundant information between new and existing steps. We introduce stepwise informativeness search, an inference-time tree search framework incorporating two selection heuristics: grounding-guided selection which prioritizes steps paying higher attention over underutilized steps; and novelty-guided selection which encourages steps with novel conclusions. During rationale generation, we use a self-grounding strategy that prompts LLMs to explicitly reference relevant prior steps to provide premises before deduction at each step. Experimental results on four reasoning datasets demonstrate that our approach improves reasoning accuracy by generating higher-quality rationales with reduced errors and redundancy.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的进步通过产生自由文本理性的多步骤推理可显着改善。但是，最近的研究表明，LLM倾向于在漫长背景下失去专注力。这引起了人们的担忧，即随着推理的进展，LLM可以在解码后续步骤时以早期步骤忽略信息，从而产生不可靠和冗余的理由。为了解决这个问题，我们提出指导LLMS，通过（1）主动从未充分利用的先前步骤中引用信息，以及（2）最大程度地减少新步骤和现有步骤之间的冗余信息，从而生成更准确和简洁的逐步理由。我们介绍了逐步信息搜索，这是一个推理时间树搜索框架，其中包含两个选择启发式方法：接地指导的选择，优先考虑步骤，而不是未充分利用的步骤。以及新颖的选择，可以通过新的结论来鼓励步骤。在基本原理生成期间，我们使用一种自我基础策略，该策略促使LLMS明确参考相关的先前步骤，以在每一步中扣除之前提供前提。四个推理数据集的实验结果表明，我们的方法通过减少错误和冗余的高质量理由来提高推理的准确性。</li>
</ul>

<h3>Title: Tokenization is Sensitive to Language Variation</h3>
<ul>
<li><strong>Authors: </strong>Anna Wegmann, Dong Nguyen, David Jurgens</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15343">https://arxiv.org/abs/2502.15343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15343">https://arxiv.org/pdf/2502.15343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15343]] Tokenization is Sensitive to Language Variation(https://arxiv.org/abs/2502.15343)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Variation in language is ubiquitous and often systematically linked to regional, social, and contextual factors. Tokenizers split texts into smaller units and might behave differently for less common linguistic forms. This might affect downstream LLM performance differently on two types of tasks: Tasks where the model should be robust to language variation (e.g., for semantic tasks like NLI, labels do not depend on whether a text uses British or American spelling) and tasks where the model should be sensitive to language variation (e.g., for form-based tasks like authorship verification, labels depend on whether a text uses British or American spelling). We pre-train BERT base models for the popular Byte-Pair Encoding algorithm to investigate how key algorithmic design choices impact downstream models' performances: fitting corpus, pre-tokenizer and vocabulary size. We find that the best tokenizer varies on the two task types -- with the pre-tokenizer having the biggest impact on performance. Further, we introduce a new approach to estimate tokenizer impact on downstream LLM performance, showing significant improvement over techniques like Rényi efficiency. We encourage more work on language variation and its relation to tokenizers and thus LLM performance.</li>
<li><strong>摘要：</strong>语言的变化无处不在，并且通常与区域，社会和上下文因素有系统地联系在一起。 Tokenizers将文本分为较小的单元，并且对于不太常见的语言形式可能会有所不同。这可能会在两种类型的任务上对下游LLM的性能有所不同：模型应适用语言变化的任务（例如，对于NLI等语义任务，标签不取决于文本是否使用英国拼写）和任务。模型应对语言变化敏感（例如，对于基于表格的任务（例如作者身份验证），标签取决于文本是否使用英国或美国拼写）。我们为流行的字节对编码算法的BERT基本模型预先培训，以调查关键算法设计选择如何影响下游模型的性能：拟合语料库，tokenizer和词汇大小。我们发现，最好的令牌仪在两种任务类型上有所不同 - 预遭到的对性能的影响最大。此外，我们引入了一种新的方法来估计令牌对下游LLM性能的影响，从而显示出对Rényi效率等技术的显着改善。我们鼓励更多关于语言差异及其与代币剂的关系，从而获得LLM的性能。</li>
</ul>

<h3>Title: Constructing a Norm for Children's Scientific Drawing: Distribution Features Based on Semantic Similarity of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yi Zhang, Fan Wei, Jingyi Li, Yan Wang, Yanyan Yu, Jianli Chen, Zipo Cai, Xinyu Liu, Wei Wang, Peng Wang, Zhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15348">https://arxiv.org/abs/2502.15348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15348">https://arxiv.org/pdf/2502.15348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15348]] Constructing a Norm for Children's Scientific Drawing: Distribution Features Based on Semantic Similarity of Large Language Models(https://arxiv.org/abs/2502.15348)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The use of children's drawings to examining their conceptual understanding has been proven to be an effective method, but there are two major problems with previous research: 1. The content of the drawings heavily relies on the task, and the ecological validity of the conclusions is low; 2. The interpretation of drawings relies too much on the subjective feelings of the researchers. To address this issue, this study uses the Large Language Model (LLM) to identify 1420 children's scientific drawings (covering 9 scientific themes/concepts), and uses the word2vec algorithm to calculate their semantic similarity. The study explores whether there are consistent drawing representations for children on the same theme, and attempts to establish a norm for children's scientific drawings, providing a baseline reference for follow-up children's drawing research. The results show that the representation of most drawings has consistency, manifested as most semantic similarity greater than 0.8. At the same time, it was found that the consistency of the representation is independent of the accuracy (of LLM's recognition), indicating the existence of consistency bias. In the subsequent exploration of influencing factors, we used Kendall rank correlation coefficient to investigate the effects of Sample Size, Abstract Degree, and Focus Points on drawings, and used word frequency statistics to explore whether children represented abstract themes/concepts by reproducing what was taught in class.</li>
<li><strong>摘要：</strong>使用儿童图纸检查他们的概念理解已被证明是一种有效的方法，但是先前的研究有两个主要问题：1。图纸的内容在很大程度上依赖于任务，结论的生态有效性是低的; 2。图纸的解释过于依赖研究人员的主观感觉。为了解决这个问题，本研究使用大型语言模型（LLM）来识别1420个儿童的科学图（涵盖9个科学主题/概念），并使用该词2VEC算法来计算其语义相似性。该研究探讨了是否有同一主题的儿童有一致的绘画表示，并试图为儿童的科学图制定规范，为后续儿童绘画研究提供了基线参考。结果表明，大多数图纸的表示形式具有一致性，表现为大多数语义相似性大于0.8。同时，发现表示的一致性独立于（LLM识别）的准确性，表明存在一致性偏差。在随后对影响因素的探索中，我们使用Kendall等级相关系数来研究样本量，抽象程度和重点点对图纸的影响，并使用单词频率统计量来探索孩子是否通过重现所教的内容来代表抽象的主题/概念在课堂上。</li>
</ul>

<h3>Title: AttentionEngine: A Versatile Framework for Efficient Attention Mechanisms on Diverse Hardware Platforms</h3>
<ul>
<li><strong>Authors: </strong>Feiyang Chen, Yu Cheng, Lei Wang, Yuqing Xia, Ziming Miao, Lingxiao Ma, Fan Yang, Jilong Xue, Zhi Yang, Mao Yang, Haibo Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15349">https://arxiv.org/abs/2502.15349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15349">https://arxiv.org/pdf/2502.15349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15349]] AttentionEngine: A Versatile Framework for Efficient Attention Mechanisms on Diverse Hardware Platforms(https://arxiv.org/abs/2502.15349)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Transformers and large language models (LLMs) have revolutionized machine learning, with attention mechanisms at the core of their success. As the landscape of attention variants expands, so too do the challenges of optimizing their performance, particularly across different hardware platforms. Current optimization strategies are often narrowly focused, requiring extensive manual intervention to accommodate changes in model configurations or hardware environments. In this paper, we introduce AttentionEngine, a comprehensive framework designed to streamline the optimization of attention mechanisms across heterogeneous hardware backends. By decomposing attention computation into modular operations with customizable components, AttentionEngine enables flexible adaptation to diverse algorithmic requirements. The framework further automates kernel optimization through a combination of programmable templates and a robust cross-platform scheduling strategy. Empirical results reveal performance gains of up to 10x on configurations beyond the reach of existing methods. AttentionEngine offers a scalable, efficient foundation for developing and deploying attention mechanisms with minimal manual tuning. Our code has been open-sourced and is available at this https URL.</li>
<li><strong>摘要：</strong>变形金刚和大型语言模型（LLM）彻底改变了机器学习，其关注机制是其成功的核心。随着注意力变体的景观随着优化其性能的挑战，尤其是在不同的硬件平台上的挑战。当前的优化策略通常是狭窄的集中精力，需要大量的手动干预以适应模型配置或硬件环境的变化。在本文中，我们介绍了注意力Engeine，这是一个综合框架，旨在简化异质硬件后端的注意机制的优化。通过将注意力计算分解为具有可自定义组件的模块化操作，注意力连接可以灵活适应各种算法要求。该框架通过可编程模板和强大的跨平台调度策略的组合进一步自动化内核优化。经验结果揭示了超出现有方法的配置上的绩效增长最高10倍。注意Engine为通过最少的手动调整开发和部署注意力机制提供了可扩展，有效的基础。我们的代码已开源，可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Evaluating Social Biases in LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Wu, Jinming Nian, Zhiqiang Tao, Yi Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15361">https://arxiv.org/abs/2502.15361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15361">https://arxiv.org/pdf/2502.15361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15361]] Evaluating Social Biases in LLM Reasoning(https://arxiv.org/abs/2502.15361)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In the recent development of AI reasoning, large language models (LLMs) are trained to automatically generate chain-of-thought reasoning steps, which have demonstrated compelling performance on math and coding tasks. However, when bias is mixed within the reasoning process to form strong logical arguments, it could cause even more harmful results and further induce hallucinations. In this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against their instruction tuned counterparts on the BBQ dataset, and investigated the bias that is elicited out and being amplified through reasoning steps. To the best of our knowledge, this empirical study is the first to assess bias issues in LLM reasoning.</li>
<li><strong>摘要：</strong>在最近的AI推理的最新发展中，大型语言模型（LLMS）经过培训以自动生成经过思考的推理步骤，这些步骤在数学和编码任务上表现出了令人信服的性能。但是，当在推理过程中混合偏见以形成强烈的逻辑论点时，它可能会导致更有害的结果并进一步引起幻觉。在本文中，我们根据烧烤数据集对DeepSeek-R1的8B和32B变体进行了评估，并研究了通过推理步骤引起并放大的偏差。据我们所知，这项实证研究是第一个评估LLM推理中偏见问题的研究。</li>
</ul>

<h3>Title: Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xuetao Ma, Wenbin Jiang, Hua Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15401">https://arxiv.org/abs/2502.15401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15401">https://arxiv.org/pdf/2502.15401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15401]] Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning(https://arxiv.org/abs/2502.15401)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be publicly available subsequently.</li>
<li><strong>摘要：</strong>内部文化学习（ICL）可以显着增强大语言模型（LLMS）的复杂推理能力，其中关键在于选择和顺序示例示例。以前的方法通常依赖于简单特征来测量示例之间的相关性。我们认为这些特征不足以反映示例之间的固有连接。在这项研究中，我们提出了一种以解决问题逻辑为指导的课程ICL策略。我们通过分析解决问题的逻辑并根据课程学习订购这些示例。具体而言，我们基于中断数据集构建了解决问题的逻辑指令集，并微调了语言模型来分析示例解决问题的逻辑。随后，我们根据解决问题的逻辑选择了适当的演示示例，并根据解决问题的步骤的数量评估了它们的难度。根据课程学习的原则，我们从易于到难以作为上下文提示的示例订购了示例。对多个基准测试的实验结果表明，我们的方法在性能和效率方面优于先前的ICL方法，从而有效增强了LLM的复杂推理能力。我们的项目随后将公开可用。</li>
</ul>

<h3>Title: HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings Filings</h3>
<ul>
<li><strong>Authors: </strong>Rasmus Aavang, Giovanni Rizzi, Rasmus Bøggild, Alexandre Iolov, Mike Zhang, Johannes Bjerva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15411">https://arxiv.org/abs/2502.15411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15411">https://arxiv.org/pdf/2502.15411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15411]] HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings Filings(https://arxiv.org/abs/2502.15411)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The U.S. Securities and Exchange Commission (SEC) requires that public companies file financial reports tagging numbers with the machine readable inline eXtensible Business Reporting Language (iXBRL) standard. However, the highly complex and highly granular taxonomy defined by iXBRL limits label transferability across domains. In this paper, we introduce the Hierarchical Financial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate numerical KPI extraction at specified levels of granularity from unstructured financial text. Our approach organizes a 218,126-label hierarchy using a taxonomy based grouping method, investigating which taxonomy layer provides the most meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M entities, each linked to a label in the iXBRL-specific calculation and presentation taxonomies. We provide baselines using encoder-based approaches and structured extraction using Large Language Models (LLMs). To simplify LLM inference and evaluation, we additionally release HiFi-KPI Lite, a manually curated subset with four expert-mapped labels. We publicly release all artifacts</li>
<li><strong>摘要：</strong>美国证券交易委员会（SEC）要求上市公司以机器可读的内联商业报告语言（IXBRL）标准提交财务报告标记数字。但是，由IXBRL限制了跨域的可转移性的高度复杂和高度颗粒状的分类法。在本文中，我们介绍了层次财务关键绩效指标（HIFI-KPI）数据集，该数据集旨在促进从非结构化财务文本中以指定的粒度水平提取数值KPI。我们的方法使用基于分类的分组方法组织了218,126标签的层次结构，研究哪个分类层提供了最有意义的结构。 HIFI-KPI包括约180万段和〜5M实体，每个实体都链接到IXBRL特异性计算和呈现分类法中的标签。我们使用基于编码器的方法和使用大语言模型（LLMS）的结构化提取提供基准。为了简化LLM的推理和评估，我们还释放了Hifi-Kpi Lite，这是一个带有四个专家映射标签的手动策划子集。我们公开释放所有工件</li>
</ul>

<h3>Title: Textual-to-Visual Iterative Self-Verification for Slide Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunqing Xu, Xinbei Ma, Jiyang Qiu, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15412">https://arxiv.org/abs/2502.15412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15412">https://arxiv.org/pdf/2502.15412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15412]] Textual-to-Visual Iterative Self-Verification for Slide Generation(https://arxiv.org/abs/2502.15412)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Generating presentation slides is a time-consuming task that urgently requires automation. Due to their limited flexibility and lack of automated refinement mechanisms, existing autonomous LLM-based agents face constraints in real-world applicability. We decompose the task of generating missing presentation slides into two key components: content generation and layout generation, aligning with the typical process of creating academic slides. First, we introduce a content generation approach that enhances coherence and relevance by incorporating context from surrounding slides and leveraging section retrieval strategies. For layout generation, we propose a textual-to-visual self-verification process using a LLM-based Reviewer + Refiner workflow, transforming complex textual layouts into intuitive visual formats. This modality transformation simplifies the task, enabling accurate and human-like review and refinement. Experiments show that our approach significantly outperforms baseline methods in terms of alignment, logical flow, visual appeal, and readability.</li>
<li><strong>摘要：</strong>生成演示幻灯片是一项耗时的任务，迫切需要自动化。由于它们的灵活性有限和缺乏自动改进机制，现有的基于自主LLM的代理在现实世界中的适用性中面临限制。我们将生成缺失的演示文稿幻灯片分解为两个关键组成部分：内容生成和布局生成，与创建学术幻灯片的典型过程保持一致。首先，我们介绍了一种内容生成方法，该方法通过结合周围幻灯片和利用部分检索策略的背景来增强连贯性和相关性。对于布局生成，我们使用基于LLM的审阅者 +炼油厂工作流程提出了一个文本到视觉的自我验证过程，将复杂的文本布局转化为直观的视觉格式。这种方式转换简化了任务，从而实现了准确和类似人类的审查和改进。实验表明，我们的方法在对齐，逻辑流，视觉吸引力和可读性方面显着优于基线方法。</li>
</ul>

<h3>Title: MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering Challenge for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Suraj Racha, Prashant Joshi, Anshika Raman, Nikita Jangid, Mridul Sharma, Ganesh Ramakrishnan, Nirmal Punjabi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15418">https://arxiv.org/abs/2502.15418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15418">https://arxiv.org/pdf/2502.15418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15418]] MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering Challenge for Language Models(https://arxiv.org/abs/2502.15418)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Mental health remains a challenging problem all over the world, with issues like depression, anxiety becoming increasingly common. Large Language Models (LLMs) have seen a vast application in healthcare, specifically in answering medical questions. However, there is a lack of standard benchmarking datasets for question answering (QA) in mental health. Our work presents a novel multiple choice dataset, MHQA (Mental Health Question Answering), for benchmarking Language models (LMs). Previous mental health datasets have focused primarily on text classification into specific labels or disorders. MHQA, on the other hand, presents question-answering for mental health focused on four key domains: anxiety, depression, trauma, and obsessive/compulsive issues, with diverse question types, namely, factoid, diagnostic, prognostic, and preventive. We use PubMed abstracts as the primary source for QA. We develop a rigorous pipeline for LLM-based identification of information from abstracts based on various selection criteria and converting it into QA pairs. Further, valid QA pairs are extracted based on post-hoc validation criteria. Overall, our MHQA dataset consists of 2,475 expert-verified gold standard instances called MHQA-gold and ~56.1k pairs pseudo labeled using external medical references. We report F1 scores on different LLMs along with few-shot and supervised fine-tuning experiments, further discussing the insights for the scores.</li>
<li><strong>摘要：</strong>心理健康仍然是全世界一个充满挑战的问题，抑郁症和焦虑等问题变得越来越普遍。大型语言模型（LLMS）在医疗保健中有了广泛的应用，特别是在回答医疗问题方面。但是，缺乏在心理健康方面的问题答案（QA）的标准基准数据集。我们的工作提出了一个新颖的多项选择数据集MHQA（心理健康问题答案），用于基准语言模型（LMS）。以前的心理健康数据集主要集中在特定标签或疾病中的文本分类。另一方面，MHQA提出了针对心理健康的提问，重点是四个关键领域：焦虑，抑郁，创伤和强迫性/强迫性问题，具有不同的问题类型，即，FACTOID，诊断，预后和预防性。我们使用PubMed摘要作为QA的主要来源。我们开发了一个严格的管道，以基于各种选择标准从摘要中基于LLM的信息识别，并将其转换为QA对。此外，根据事后验证标准提取有效的QA对。总体而言，我们的MHQA数据集由2,475个专家验证的黄金标准实例，称为MHQA-GOLD，〜56.1k对使用外部医学参考文献进行了标记。我们报告了不同LLM的F1分数以及几乎没有射击和监督的微调实验，进一步讨论了分数的见解。</li>
</ul>

<h3>Title: Beyond Translation: LLM-Based Data Generation for Multilingual Fact-Checking</h3>
<ul>
<li><strong>Authors: </strong>Yi-Ling Chung, Aurora Cobo, Pablo Serna</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15419">https://arxiv.org/abs/2502.15419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15419">https://arxiv.org/pdf/2502.15419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15419]] Beyond Translation: LLM-Based Data Generation for Multilingual Fact-Checking(https://arxiv.org/abs/2502.15419)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Robust automatic fact-checking systems have the potential to combat online misinformation at scale. However, most existing research primarily focuses on English. In this paper, we introduce MultiSynFact, the first large-scale multilingual fact-checking dataset containing 2.2M claim-source pairs designed to support Spanish, German, English, and other low-resource languages. Our dataset generation pipeline leverages Large Language Models (LLMs), integrating external knowledge from Wikipedia and incorporating rigorous claim validation steps to ensure data quality. We evaluate the effectiveness of MultiSynFact across multiple models and experimental settings. Additionally, we open-source a user-friendly framework to facilitate further research in multilingual fact-checking and dataset generation.</li>
<li><strong>摘要：</strong>强大的自动事实检查系统有可能大规模打击在线错误信息。但是，大多数现有研究主要集中在英语上。在本文中，我们介绍了Multisynfact，这是第一个大型多语言事实检查数据集，其中包含220万索赔源对，旨在支持西班牙，德语，英语和其他低资源语言。我们的数据集生成管道利用大型语言模型（LLMS），整合了Wikipedia的外部知识，并结合了严格的索赔验证步骤以确保数据质量。我们评估了多种模型和实验环境中多合同的有效性。此外，我们开源一个用户友好的框架，以促进多语言事实检查和数据集生成的进一步研究。</li>
</ul>

<h3>Title: Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable Explanations</h3>
<ul>
<li><strong>Authors: </strong>Lihu Chen, Shuojie Fu, Gabriel Freedman, Cemre Zor, Guy Martin, James Kinross, Uddhav Vaghela, Ovidiu Serban, Francesca Toni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15429">https://arxiv.org/abs/2502.15429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15429">https://arxiv.org/pdf/2502.15429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15429]] Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable Explanations(https://arxiv.org/abs/2502.15429)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool.</li>
<li><strong>摘要：</strong>发现大量发表的科学文章涉及欺诈性实践，对医学等领域的研究的信誉和安全构成了严重威胁。我们提出了Pub-Guard-Llm，这是第一个基于大型语言模型的系统，该系统量身定制，用于欺诈检测生物医学科学文章。我们提供了三种用于部署Pub-guard-llm的申请模式：香草推理，检索演示的生成和多代理辩论。每种模式都允许对预测进行文本说明。为了评估系统的性能，我们引入了一个开源基准PubMed Retraction，其中包括超过11K现实世界的生物医学文章，包括元数据和缩回标签。我们表明，在所有模式中，Pub-Guard-llm始终超过各种基准的性能，并提供了更可靠的解释，即与基本线所产生的解释相比，这些解释被认为更相关和相干，当通过多种评估方法评估时。通过提高科学欺诈检测中的检测性能和解释性，Pub-Guard-Llm通过一种新颖，有效的开源工具有助于保护研究完整性。</li>
</ul>

<h3>Title: Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhou, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15434">https://arxiv.org/abs/2502.15434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15434">https://arxiv.org/pdf/2502.15434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15434]] Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation(https://arxiv.org/abs/2502.15434)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Model merging integrates the parameters of multiple models into a unified model, combining their diverse capabilities. Existing model merging methods are often constrained by fixed parameter merging ratios. In this study, we propose Mixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data augmentation technique. This method merges the parameters of two large language models (LLMs) by randomly generating linear interpolation ratios, allowing for a more flexible and comprehensive exploration of the parameter space. Extensive experiments demonstrate the superiority of our proposed M$^3$ method in merging fine-tuned LLMs: (1) it significantly improves performance across multiple tasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and adversarial robustness, (3) it achieves superior results when combined with sparsification techniques such as DARE, and (4) it offers a simple yet efficient solution that does not require additional computational resources. In conclusion, M$^3$ is a simple yet effective model merging method that significantly enhances the performance of the merged model by randomly generating contribution ratios for two fine-tuned LLMs. The code is available at this https URL.</li>
<li><strong>摘要：</strong>模型合并将多个模型的参数集成到统一模型中，结合了它们的各种功能。现有的模型合并方法通常受固定参数合并比率的约束。在这项研究中，我们提出了混合模型合并（M $^3 $），这是一种受混合数据增强技术启发的创新方法。该方法通过随机生成线性插值比，将两个大语言模型（LLM）的参数融合在一起，从而可以对参数空间进行更灵活，更全面的探索。广泛的实验证明了我们提出的M $^3 $方法在合并微调LLMS的优势：（1）它显着提高了多个任务的性能，（2）它增强了LLMS的分布（OOD）鲁棒性和对抗性鲁棒性，（3）与诸如Dare之类的稀疏技术相结合时，它可以取得优越的结果，（4）它提供了一种简单而有效的解决方案，不需要额外计算资源。总之，M $^3 $是一种简单而有效的模型合并方法，可以通过随机生成两个微调LLM的贡献比大大提高合并模型的性能。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weilan Wang, Yu Mao, Dongdong Tang, Hongchao Du, Nan Guan, Chun Jason Xue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15443">https://arxiv.org/abs/2502.15443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15443">https://arxiv.org/pdf/2502.15443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15443]] When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models(https://arxiv.org/abs/2502.15443)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit excellent performance in various tasks. However, the memory requirements of LLMs present a great challenge when deploying on memory-limited devices, even for quantized LLMs. This paper introduces a framework to compress LLM after quantization further, achieving about 2.2x compression ratio. A compression-aware quantization is first proposed to enhance model weight compressibility by re-scaling the model parameters before quantization, followed by a pruning method to improve further. Upon this, we notice that decompression can be a bottleneck during practical scenarios. We then give a detailed analysis of the trade-off between memory usage and latency brought by the proposed method. A speed-adaptive method is proposed to overcome it. The experimental results show inference with the compressed model can achieve a 40% reduction in memory size with negligible loss in accuracy and inference speed.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在各种任务中表现出色。但是，LLM的内存需求在部署在内存限制的设备上，即使对于量化的LLMS时也会带来巨大的挑战。本文介绍了一个进一步量化后压缩LLM的框架，达到约2.2倍的压缩比。首先提出了一种压缩感知的量化，以通过在量化前重新缩放模型参数来增强模型权重的压缩性，然后再进行修剪方法以进一步改进。为此，我们注意到在实际情况下，减压可能是瓶颈。然后，我们详细分析了所提出的方法带来的记忆使用和潜伏期之间的权衡。提出了一种速度自适应方法来克服它。实验结果表明，与压缩模型的推断可以使记忆尺寸减少40％，而准确性和推理速度的损失可忽略不计。</li>
</ul>

<h3>Title: Enhancing RWKV-based Language Models for Long-Sequence Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinghan Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15485">https://arxiv.org/abs/2502.15485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15485">https://arxiv.org/pdf/2502.15485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15485]] Enhancing RWKV-based Language Models for Long-Sequence Text Generation(https://arxiv.org/abs/2502.15485)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper presents an enhanced RWKV-based language generation model designed to improve long-sequence text processing. We propose an adaptive token shift and gating mechanism to better capture long-range dependencies in text generation. Through a series of experiments, we compare the baseline RWKV model with the enhanced model, evaluating performance in terms of forward propagation time, text generation quality, and automatic evaluation metrics such as perplexity, BLEU, and ROUGE. Experimental results show that the enhanced model significantly improves generation quality, especially in BLEU and ROUGE scores, and demonstrates stronger context-capturing ability in long-text generation tasks.</li>
<li><strong>摘要：</strong>本文提出了一种增强的基于RWKV的语言生成模型，旨在改善长期序列文本处理。我们提出了一种自适应的令牌转移和门控机制，以更好地捕获文本生成中的远程依赖性。通过一系列实验，我们将基线RWKV模型与增强模型进行了比较，从而在远期传播时间，文本生成质量以及自动评估指标（例如困惑，BLEU和Rouge）中评估性能。实验结果表明，增强模型可显着提高发电质量，尤其是在BLEU和Rouge分数中，并在长文本生成任务中表现出更强的上下文捕获能力。</li>
</ul>

<h3>Title: ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Martina Miliani, Serenna Auriemma, Alessandro Bondielli, Emmanuele Chersoni, Lucia Passaro, Irene Sucameli, Alessandro Lenci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15487">https://arxiv.org/abs/2502.15487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15487">https://arxiv.org/pdf/2502.15487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15487]] ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models(https://arxiv.org/abs/2502.15487)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in different linguistic orders and explicitly expressed by linguistic connectives. The dataset is enriched with crowdsourced human acceptability ratings. We tested LLMs on ExpliCa through prompting and perplexity-based metrics. We assessed seven commercial and open-source LLMs, revealing that even top models struggle to reach 0.80 accuracy. Interestingly, models tend to confound temporal relations with causal ones, and their performance is also strongly influenced by the linguistic order of the events. Finally, perplexity-based scores and prompting performance are differently affected by model size.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地用于需要解释性和推论准确性的任务中。在本文中，我们介绍了Axplica，这是一个新的数据集，用于评估LLM的显式因果推理。显式唯一地整合了不同语言秩序中呈现的因果关系和时间关系，并通过语言缔合表示明确表达。该数据集具有众包人类的可接受性评级。我们通过提示和基于困惑的指标测试了LLMS的LLM。我们评估了七个商业和开源LLM，表明即使是顶级模型也很难达到0.80的精度。有趣的是，模型倾向于将时间关系与因果关系混淆，其性能也受到事件语言顺序的强烈影响。最后，基于困惑的分数和提示性能受模型大小的不同影响。</li>
</ul>

<h3>Title: Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ya Wang, Zhijian Zhuo, Yutao Zeng, Xun Zhou, Jian Yang, Xiaoqing Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15499">https://arxiv.org/abs/2502.15499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15499">https://arxiv.org/pdf/2502.15499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15499]] Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models(https://arxiv.org/abs/2502.15499)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that stabilizes training by explicitly decoupling the scale and distribution of the weight matrix in fully-connected layers. SDD applies a normalization mechanism to regulate activations and a learnable scaling vector to maintain well-conditioned gradients, effectively preventing $\textbf{gradient explosion and dissipation}$. This separation improves optimization efficiency, particularly in deep networks, by ensuring stable gradient propagation. Experimental results demonstrate that our method stabilizes training across various LLM architectures and outperforms existing techniques in different normalization configurations. Furthermore, the proposed method is lightweight and compatible with existing frameworks, making it a practical solution for stabilizing LLM training. Code is available at this https URL.</li>
<li><strong>摘要：</strong>训练稳定性是大语模型（LLMS）预训练的持续挑战，尤其是对于诸如公平后变压器等体系结构，这些结构容易爆炸和消散。在本文中，我们提出了比例分布解耦（SDD），一种新型方法，通过在完全连接的层中明确解耦权重矩阵的尺度和分布来稳定训练。 SDD采用标准化机制来调节激活和可学习的缩放矢量以维持条件良好的梯度，从而有效防止$ \ textbf {梯度爆炸和耗散} $。这种分离通过确保稳定的梯度传播来提高优化效率，尤其是在深层网络中。实验结果表明，我们的方法稳定了各种LLM架构的培训，并且在不同的归一化配置中均优于现有技术。此外，提出的方法是轻巧的，与现有框架兼容，使其成为稳定LLM培训的实用解决方案。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: SOTOPIA-Ω: Dynamic Strategy Injection Learning and Social Instrucion Following Evaluation for Social Agents</h3>
<ul>
<li><strong>Authors: </strong>Wenyuan Zhang, Tianyun Liu, Mengxiao Song, Xiaodong Li, Tingwen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15538">https://arxiv.org/abs/2502.15538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15538">https://arxiv.org/pdf/2502.15538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15538]] SOTOPIA-Ω: Dynamic Strategy Injection Learning and Social Instrucion Following Evaluation for Social Agents(https://arxiv.org/abs/2502.15538)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, agent</a></li>
<li><strong>Abstract: </strong>Despite the abundance of prior social strategies possessed by humans, there remains a paucity of research dedicated to their transfer and integration into social agents. Our proposed SOTOPIA-{\Omega} framework aims to address and bridge this gap, with a particular focus on enhancing the social capabilities of language agents. This framework dynamically injects multi-step reasoning strategies inspired by negotiation theory, along with two simple direct strategies, into expert agents, thereby automating the construction of high-quality social dialogue training corpus. Additionally, we introduce the concept of Social Instruction Following (S-IF) and propose two new S-IF evaluation metrics that are complementary to social capability. We demonstrate that several 7B models trained on high-quality corpus not only significantly surpass the expert agent (GPT-4) in achieving social goals but also enhance S-IF performance. Analysis and variant experiments validate the advantages of dynamic construction, which can especially break the agent's prolonged deadlock.</li>
<li><strong>摘要：</strong>尽管人类拥有的先前的社会策略丰富，但仍缺乏专门用于将其转移和融入社会代理人的研究。我们提出的Sotopia-{\ Omega}框架旨在解决和弥合这一差距，特别着眼于增强语言代理的社交能力。该框架动态地注入了受谈判理论启发的多步推理策略，以及两个简单的直接策略，分为专家代理，从而自动化高质量的社会对话培训培训语料库。此外，我们介绍了以下社会教学的概念（S-IF），并提出了两个与社会能力互补的新的S-IF评估指标。我们证明，接受过高质量语料库训练的几种7B模型不仅显着超过了实现社会目标的专家代理（GPT-4），而且还可以增强S-IF性能。分析和变体实验验证了动态构建的优势，这可能会尤其打破了代理的延长僵局。</li>
</ul>

<h3>Title: PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Huang, Zhenghao Liu, Yukun Yan, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15543">https://arxiv.org/abs/2502.15543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15543">https://arxiv.org/pdf/2502.15543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15543]] PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning(https://arxiv.org/abs/2502.15543)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Knowledge-Augmented Generation (KAG) has shown great promise in updating the internal memory of Large Language Models (LLMs) by integrating external knowledge. However, KAG inevitably faces knowledge conflicts when the internal memory contradicts external information. Current approaches to mitigating these conflicts mainly focus on improving external knowledge utilization. However, these methods have shown only limited effectiveness in mitigating the knowledge conflict problem, as internal knowledge continues to influence the generation process of LLMs. In this paper, we propose a ParametrIc Pruning-based Knowledge-Augmented Generation (PIP-KAG) approach, which prunes internal knowledge of LLMs and incorporates a plug-and-play adaptation module to help LLMs better leverage external sources. Additionally, we construct the CoConflictQA benchmark based on the hallucination of LLMs to better evaluate contextual faithfulness during answering questions. Experimental results on CoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts and improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by 13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes are available at this https URL.</li>
<li><strong>摘要：</strong>知识增强的生成（KAG）在通过集成外部知识来更新大语言模型（LLMS）的内部记忆方面表现出了巨大的希望。但是，当内部记忆与外部信息矛盾时，KAG不可避免地会面临知识冲突。缓解这些冲突的当前方法主要集中于改善外部知识利用率。但是，随着内部知识继续影响LLM的发电过程，这些方法仅显示在减轻知识冲突问题的有效性。在本文中，我们提出了一种基于参数修剪的知识增强生成（PIP-KAG）方法，该方法将修剪LLM的内部知识，并结合了插件适应模块，以帮助LLMS更好地利用外部来源。此外，我们基于LLM的幻觉来构建CoconflictQA基准测试，以更好地评估回答问题时上下文忠诚。 CoconflictQA的实验结果表明，PIP-KAG大大减少了知识冲突并改善了上下文的保真度。值得注意的是，PIP-KAG将LLM的参数降低了13％，从而提高了KAG框架内LLM的参数效率。所有代码均可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: DReSD: Dense Retrieval for Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Milan Gritta, Huiyin Xue, Gerasimos Lampouras</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15572">https://arxiv.org/abs/2502.15572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15572">https://arxiv.org/pdf/2502.15572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15572]] DReSD: Dense Retrieval for Speculative Decoding(https://arxiv.org/abs/2502.15572)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Speculative decoding (SD) accelerates Large Language Model (LLM) generation by using an efficient draft model to propose the next few tokens, which are verified by the LLM in a single forward call, reducing latency while preserving its outputs. We focus on retrieval-based SD where the draft model retrieves the next tokens from a non-parametric datastore. Sparse retrieval (REST), which operates on the surface form of strings, is currently the dominant paradigm due to its simplicity and scalability. However, its effectiveness is limited due to the usage of short contexts and exact string matching. Instead, we introduce Dense Retrieval for Speculative Decoding (DReSD), a novel framework that uses approximate nearest neighbour search with contextualised token embeddings to retrieve the most semantically relevant token sequences for SD. Extensive experiments show that DReSD achieves (on average) 87% higher acceptance rates, 65% longer accepted tokens and 19% faster generation speeds compared to sparse retrieval (REST).</li>
<li><strong>摘要：</strong>投机解码（SD）通过使用有效的草稿模型提出接下来的几个令牌，从而加速了大语言模型（LLM）的生成，LLM在单个正向呼叫中通过LLM验证，从而减少了延迟，同时保留其输出。我们专注于基于检索的SD，其中草案模型从非参数数据存储中检索了下一个令牌。在字符串表面形式下运行的稀疏检索（休息）由于其简单性和可扩展性而是主要的范式。但是，由于使用短上下文和精确的字符串匹配，其有效性受到限制。取而代之的是，我们引入了用于投机解码（DRESD）的密集检索，这是一个新颖的框架，它使用近似最近的邻居搜索，并带有上下文化令牌嵌入来检索SD的最相关的令牌序列。广泛的实验表明，与稀疏的检索相比，DRESD的接受率（平均平均）提高了87％的接受率，更长65％的接受令牌和生成速度快19％（休息）。</li>
</ul>

<h3>Title: Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Xuansheng Wu, Jiayi Yuan, Wenlin Yao, Xiaoming Zhai, Ninghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15576">https://arxiv.org/abs/2502.15576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15576">https://arxiv.org/pdf/2502.15576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15576]] Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders(https://arxiv.org/abs/2502.15576)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at handling human queries, but they can occasionally generate flawed or unexpected responses. Understanding their internal states is crucial for understanding their successes, diagnosing their failures, and refining their capabilities. Although sparse autoencoders (SAEs) have shown promise for interpreting LLM internal representations, limited research has explored how to better explain SAE features, i.e., understanding the semantic meaning of features learned by SAE. Our theoretical analysis reveals that existing explanation methods suffer from the frequency bias issue, where they emphasize linguistic patterns over semantic concepts, while the latter is more critical to steer LLM behaviors. To address this, we propose using a fixed vocabulary set for feature interpretations and designing a mutual information-based objective, aiming to better capture the semantic meaning behind these features. We further propose two runtime steering strategies that adjust the learned feature activations based on their corresponding explanations. Empirical results show that, compared to baselines, our method provides more discourse-level explanations and effectively steers LLM behaviors to defend against jailbreak attacks. These findings highlight the value of explanations for steering LLM behaviors in downstream applications. We will release our code and data once accepted.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在处理人类查询方面表现出色，但偶尔会产生有缺陷或意外的响应。了解他们的内部状态对于了解他们的成功，诊断失败并完善其能力至关重要。尽管稀疏的自动编码器（SAE）表现出了解释LLM内部表示形式的希望，但有限的研究探索了如何更好地解释SAE功能，即了解SAE学到的功能的语义含义。我们的理论分析表明，现有的解释方法遭受了频率偏见问题的困扰，在这种方法上，它们强调语言概念上的语言模式，而后者对引导LLM行为更为重要。为了解决这个问题，我们建议使用固定的词汇集用于特征解释并设计基于信息的目标，以便更好地捕获这些功能背后的语义含义。我们进一步提出了两种运行时转向策略，这些转向策略会根据其相应的解释来调整学习特征的激活。经验结果表明，与基准相比，我们的方法提供了更多的话语级别的解释，并有效地引导了LLM行为以防御越狱攻击。这些发现突出了下游应用程序中转向LLM行为的解释价值。我们将发布一旦接受的代码和数据。</li>
</ul>

<h3>Title: Chats-Grid: An Iterative Retrieval Q&A Optimization Scheme Leveraging Large Model and Retrieval Enhancement Generation in smart grid</h3>
<ul>
<li><strong>Authors: </strong>Yunfeng Li, Jiqun Zhang, Guofu Liao, Xue Shi, Junhong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15583">https://arxiv.org/abs/2502.15583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15583">https://arxiv.org/pdf/2502.15583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15583]] Chats-Grid: An Iterative Retrieval Q&A Optimization Scheme Leveraging Large Model and Retrieval Enhancement Generation in smart grid(https://arxiv.org/abs/2502.15583)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>With rapid advancements in artificial intelligence, question-answering (Q&A) systems have become essential in intelligent search engines, virtual assistants, and customer service platforms. However, in dynamic domains like smart grids, conventional retrieval-augmented generation(RAG) Q&A systems face challenges such as inadequate retrieval quality, irrelevant responses, and inefficiencies in handling large-scale, real-time data streams. This paper proposes an optimized iterative retrieval-based Q&A framework called Chats-Grid tailored for smart grid environments. In the pre-retrieval phase, Chats-Grid advanced query expansion ensures comprehensive coverage of diverse data sources, including sensor readings, meter records, and control system parameters. During retrieval, Best Matching 25(BM25) sparse retrieval and BAAI General Embedding(BGE) dense retrieval in Chats-Grid are combined to process vast, heterogeneous datasets effectively. Post-retrieval, a fine-tuned large language model uses prompt engineering to assess relevance, filter irrelevant results, and reorder documents based on contextual accuracy. The model further generates precise, context-aware answers, adhering to quality criteria and employing a self-checking mechanism for enhanced reliability. Experimental results demonstrate Chats-Grid's superiority over state-of-the-art methods in fidelity, contextual recall, relevance, and accuracy by 2.37%, 2.19%, and 3.58% respectively. This framework advances smart grid management by improving decision-making and user interactions, fostering resilient and adaptive smart grid infrastructures.</li>
<li><strong>摘要：</strong>随着人工智能方面的快速发展，提问（问答）系统在智能搜索引擎，虚拟助理和客户服务平台中已变得至关重要。但是，在智能网格等动态领域，传统的检索功能生成（RAG）问答系统面临挑战，例如检索质量不足，无关紧要的响应以及处理大型实时数据流的效率低下。本文提出了一个优化的基于迭代检索的问答框架，称为CATHS-GRID为智能电网环境量身定制。在退回阶段，CHATS-GRID高级查询扩展可确保对各种数据源的全面覆盖，包括传感器读数，仪表记录和控制系统参数。在检索过程中，将最佳匹配的25（BM25）稀疏检索和Baai通用嵌入（BGE）在聊天网格中的密集检索被合并，以有效地处理广泛的，异质的数据集。返回后的大型语言模型使用及时工程来评估相关性，过滤不相关的结果以及基于上下文准确性重新排序文档。该模型进一步生成了精确的上下文感知答案，遵守质量标准并采用自我检查机制来增强可靠性。实验结果表明，Chats-Grid在忠诚度，上下文召回，相关性和准确性方面的优势分别为2.37％，2.19％和3.58％。该框架通过改善决策和用户互动，促进弹性和适应性智能电网基础架构来提高智能电网管理。</li>
</ul>

<h3>Title: LightThinker: Thinking Step-by-Step Compression</h3>
<ul>
<li><strong>Authors: </strong>Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15589">https://arxiv.org/abs/2502.15589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15589">https://arxiv.org/pdf/2502.15589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15589]] LightThinker: Thinking Step-by-Step Compression(https://arxiv.org/abs/2502.15589)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window. This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance. Code will be released at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在复杂的推理任务中表现出色，但是与产生冗长令牌相关的大量内存和计算成本阻碍了它们的效率。在本文中，我们提出了LightThinker，这是一种使LLMS能够在推理过程中动态压缩中间思想的新颖方法。受到人类认知过程的启发，LightTinker将详细的表示序列置于紧凑的表示中并丢弃原始的推理链，从而大大减少了在上下文窗口中存储的令牌数量。这是通过训练模型何时以及如何通过数据构建执行压缩的模型，将隐藏状态映射到凝结要点令牌的情况下实现的，并创建专门的注意力面具。此外，我们引入了依赖性（DEP）度量，以通过衡量生成过程中对历史令牌的依赖来量化压缩程度。在四个数据集和两个模型上进行的大量实验表明，LightTinker会减少峰值存储器的使用和推理时间，同时保持竞争精度。我们的工作为提高LLM在复杂的推理任务中的效率而无需牺牲绩效而提供了一个新的方向。代码将在此HTTPS URL上发布。</li>
</ul>

<h3>Title: Generalizing From Short to Long: Effective Data Synthesis for Long-Context Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Zhu, Pinzhen Chen, Hanxu Hu, Shujian Huang, Fei Yuan, Jiajun Chen, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15592">https://arxiv.org/abs/2502.15592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15592">https://arxiv.org/pdf/2502.15592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15592]] Generalizing From Short to Long: Effective Data Synthesis for Long-Context Instruction Tuning(https://arxiv.org/abs/2502.15592)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Long-context modelling for large language models (LLMs) has been a key area of recent research because many real world use cases require reasoning over longer inputs such as documents. The focus of research into modelling long context has been on how to model position and there has been little investigation into other important aspects of language modelling such as instruction tuning. Long context training examples are challenging and expensive to create and use. In this paper, we investigate how to design instruction data for the post-training phase of a long context pre-trained model: how much and what type of context is needed for optimal and efficient post-training. Our controlled study reveals that models instruction-tuned on short contexts can effectively generalize to longer ones, while also identifying other critical factors such as instruction difficulty and context composition. Based on these findings, we propose context synthesis, a novel data synthesis framework that leverages off-the-shelf LLMs to generate extended background contexts for high-quality instruction-answer pairs. Experiment results on the document-level benchmark (LongBench) demonstrate that our proposed approach outperforms previous instruction synthesis approaches and comes close to the performance of human-annotated long-context instruction data. The project will be available at: this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）的长篇小说建模一直是最近研究的关键领域，因为许多现实世界中的用例需要推理较长的输入（例如文档）。研究建模的研究重点是如何建模位置，并且对语言建模（例如教学调整）的其他重要方面的研究很少。长上下文培训示例具有挑战性，创建和使用昂贵。在本文中，我们研究了如何为长上下文训练的模型的训练后阶段设计指导数据：最佳有效的训练后需要多少类型的上下文。我们的对照研究表明，在简短上下文上调整教学的模型可以有效地推广到较长的情况下，同时还可以识别其他关键因素，例如教学难度和上下文组成。基于这些发现，我们提出了上下文综合，这是一种新型的数据综合框架，利用现成的LLMS生成扩展背景上下文，以用于高质量的指令 - 答案对。文档级基准（Longbench）的实验结果表明，我们提出的方法的表现优于先前的指令合成方法，并且接近人类通知的长篇文章指令数据的性能。该项目将可用：此HTTPS URL。</li>
</ul>

<h3>Title: SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wu, Chen Chen, Chunyan Hou, Xiaojie Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15594">https://arxiv.org/abs/2502.15594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15594">https://arxiv.org/pdf/2502.15594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15594]] SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention(https://arxiv.org/abs/2502.15594)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the widespread real-world deployment of large language models (LLMs), ensuring their behavior complies with safety standards has become crucial. Jailbreak attacks exploit vulnerabilities in LLMs to induce undesirable behavior, posing a significant threat to LLM safety. Previous defenses often fail to achieve both effectiveness and efficiency simultaneously. Defenses from a representation perspective offer new insights, but existing interventions cannot dynamically adjust representations based on the harmfulness of the queries. To address this limitation while ensuring both effectiveness and efficiency, we propose SafeIntervention (SafeInt), a novel defense method that shields LLMs from jailbreak attacks through safety-aware representation intervention. SafeInt is built on our analysis of the representations of jailbreak samples. It adjusts representation distributions of jailbreak samples through intervention to align them with the representations of unsafe samples while minimizing unnecessary perturbations to jailbreak-irrelevant representations. We conduct comprehensive experiments covering six jailbreak attacks, two jailbreak datasets, and two utility benchmarks. Experimental results demonstrate that SafeInt outperforms all baselines in defending LLMs against jailbreak attacks while largely maintaining utility. Additionally, we evaluate SafeInt against adaptive attacks and verify its effectiveness in mitigating real-time attacks.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的广泛现实部署，确保其行为符合安全标准已经变得至关重要。越狱攻击利用LLMS中的漏洞引起了不良行为，对LLM安全构成了重大威胁。以前的防御能力通常无法同时实现有效性和效率。从表示的角度来防御提供了新的见解，但是现有的干预措施无法根据查询的有害性动态调整表示形式。为了解决这一限制，同时确保有效性和效率，我们提出了安全间歇性（Safeint），这是一种新型的防御方法，通过安全意识代表干预措施使LLMS免于越狱攻击。 SafeInt建立在我们对越狱样品表示的分析之上。它通过干预来调整越狱样本的代表性分布，以使其与不安全样本的表示形式保持一致，同时最大程度地减少对越狱的不必要的扰动。我们进行了全面的实验，涵盖了六次越狱攻击，两个越狱数据集和两个公用事业基准。实验结果表明，SafeInt在捍卫LLMS免受越狱攻击的同时，在很大程度上保持效用方面的表现都优于所有基准。此外，我们还评估了针对自适应攻击的安全int，并验证其在减轻实时攻击方面的有效性。</li>
</ul>

<h3>Title: Do Multilingual LLMs Think In English?</h3>
<ul>
<li><strong>Authors: </strong>Lisa Schut, Yarin Gal, Sebastian Farquhar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15603">https://arxiv.org/abs/2502.15603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15603">https://arxiv.org/pdf/2502.15603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15603]] Do Multilingual LLMs Think In English?(https://arxiv.org/abs/2502.15603)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have multilingual capabilities and can solve tasks across various languages. However, we show that current LLMs make key decisions in a representation space closest to English, regardless of their input and output languages. Exploring the internal representations with a logit lens for sentences in French, German, Dutch, and Mandarin, we show that the LLM first emits representations close to English for semantically-loaded words before translating them into the target language. We further show that activation steering in these LLMs is more effective when the steering vectors are computed in English rather than in the language of the inputs and outputs. This suggests that multilingual LLMs perform key reasoning steps in a representation that is heavily shaped by English in a way that is not transparent to system users.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）具有多语言功能，可以跨各种语言解决任务。但是，我们表明当前的LLM在最接近英语的表示空间中做出关键决策，而不论其输入和输出语言如何。在使用法语，德语，荷兰语和普通话的句子的句子中探索内部表示形式，我们表明LLM首先发出了靠近英语的表示语义上的单词，然后将其翻译成目标语言。我们进一步表明，当用英语而不是用输入和输出的语言计算转向向量时，这些LLM中的激活转向更有效。这表明多语言LLM在表示形式中执行关键的推理步骤，该步骤由英语塑造，以对系统用户不透明的方式。</li>
</ul>

<h3>Title: On the Robustness of Transformers against Context Hijacking for Linear Classification</h3>
<ul>
<li><strong>Authors: </strong>Tianle Li, Chenyang Zhang, Xingwu Chen, Yuan Cao, Difan Zou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15609">https://arxiv.org/abs/2502.15609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15609">https://arxiv.org/pdf/2502.15609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15609]] On the Robustness of Transformers against Context Hijacking for Linear Classification(https://arxiv.org/abs/2502.15609)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Transformer-based Large Language Models (LLMs) have demonstrated powerful in-context learning capabilities. However, their predictions can be disrupted by factually correct context, a phenomenon known as context hijacking, revealing a significant robustness issue. To understand this phenomenon theoretically, we explore an in-context linear classification problem based on recent advances in linear transformers. In our setup, context tokens are designed as factually correct query-answer pairs, where the queries are similar to the final query but have opposite labels. Then, we develop a general theoretical analysis on the robustness of the linear transformers, which is formulated as a function of the model depth, training context lengths, and number of hijacking context tokens. A key finding is that a well-trained deeper transformer can achieve higher robustness, which aligns with empirical observations. We show that this improvement arises because deeper layers enable more fine-grained optimization steps, effectively mitigating interference from context hijacking. This is also well supported by our numerical experiments. Our findings provide theoretical insights into the benefits of deeper architectures and contribute to enhancing the understanding of transformer architectures.</li>
<li><strong>摘要：</strong>基于变形金刚的大型语言模型（LLMS）已表现出强大的内在学习能力。但是，他们的预测可能会被事实正确的上下文（一种称为上下文劫持的现象）破坏，揭示了一个重大的鲁棒性问题。从理论上讲，我们基于线性变压器的最新进展来探索一个现象的线性分类问题。在我们的设置中，上下文令牌被设计为实际上正确的查询 - 答案对，其中查询与最终查询相似，但标签相反。然后，我们对线性变压器的鲁棒性进行了一般的理论分析，该分析是根据模型深度，训练上下文长度和劫持上下文令牌的数量提出的。一个关键的发现是，训练有素的更深的变压器可以达到更高的鲁棒性，这与经验观察一致。我们表明，出现了这种改进，因为更深的层实现了更细粒度的优化步骤，从而有效地减轻了上下文劫持的干扰。我们的数值实验也得到了很好的支持。我们的发现提供了对更深层次架构的好处的理论见解，并有助于增强对变压器体系结构的理解。</li>
</ul>

<h3>Title: Pastiche Novel Generation Creating: Fan Fiction You Love in Your Favorite Author's Style</h3>
<ul>
<li><strong>Authors: </strong>Xueran Han, Yuhan Liu, Mingzhe Li, Wei Liu, Sen Hu, Rui Yan, Zhiqiang Xu, Xiuying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15616">https://arxiv.org/abs/2502.15616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15616">https://arxiv.org/pdf/2502.15616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15616]] Pastiche Novel Generation Creating: Fan Fiction You Love in Your Favorite Author's Style(https://arxiv.org/abs/2502.15616)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Great novels create immersive worlds with rich character arcs, well-structured plots, and nuanced writing styles. However, current novel generation methods often rely on brief, simplistic story outlines and generate details using plain, generic language. To bridge this gap, we introduce the task of Pastiche Novel Generation, which requires the generated novels to imitate the distinctive features of the original work, including understanding character profiles, predicting plausible plot developments, and writing concrete details using vivid, expressive language. To achieve this, we propose WriterAgent, a novel generation system designed to master the core aspects of literary pastiche. WriterAgent is trained through a curriculum learning paradigm, progressing from low-level stylistic mastery to high-level narrative coherence. Its key tasks include language style learning, character modeling, plot planning, and stylish writing, ensuring comprehensive narrative control. To support this, WriterAgent leverages the WriterLoRA framework, an extension of LoRA with hierarchical and cumulative task-specific modules, each specializing in a different narrative aspect. We evaluate WriterAgent on multilingual classics like Harry Potter and Dream of the Red Chamber, demonstrating its superiority over baselines in capturing the target author's settings, character dynamics, and writing style to produce coherent, faithful narratives.</li>
<li><strong>摘要：</strong>伟大的小说创造了具有丰富角色弧，结构良好的情节和细微的写作风格的身临其境的世界。但是，当前的新颖生成方法通常依赖于简短，简单的故事概述，并使用普通的通用语言生成细节。为了弥合这一差距，我们介绍了Pastiche Novel Generation的任务，这需要生成的小说模仿原始作品的独特特征，包括了解角色概况，预测合理的情节发展以及使用生动，表现力的语言编写具体细节。为了实现这一目标，我们提出了一个旨在掌握文学兴趣的核心方面的新一代制度Writeragent。 Writeragent通过课程学习范式进行了训练，从低水平的掌握到高级叙事连贯性。它的关键任务包括语言风格学习，角色建模，情节计划和时尚写作，以确保全面的叙事控制。为了支持这一点，Writeragent利用了Writerlora框架，这是Lora的延伸，具有层次和累积的特定于任务的模块，每个模块都在不同的叙述方面进行专门研究。我们评估了诸如哈利·波特（Harry Potter）等多语言经典的作家，并展示了其优于基准的优越性，以捕捉目标作者的环境，角色动态和写作风格，以产生连贯的，忠实的叙事。</li>
</ul>

<h3>Title: Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing</h3>
<ul>
<li><strong>Authors: </strong>Qi Le, Enmao Diao, Ziyan Wang, Xinran Wang, Jie Ding, Li Yang, Ali Anwar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15618">https://arxiv.org/abs/2502.15618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15618">https://arxiv.org/pdf/2502.15618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15618]] Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing(https://arxiv.org/abs/2502.15618)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce Probe Pruning (PP), a novel framework for online, dynamic, structured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch effectively identifies crucial weights, enabling tailored dynamic pruning for different batches. It comprises three main stages: probing, history-informed pruning, and full inference. In the probing stage, PP selects a small yet crucial set of hidden states, based on residual importance, to run a few model layers ahead. During the history-informed pruning stage, PP strategically integrates the probing states with historical states. Subsequently, it structurally prunes weights based on the integrated states and the PP importance score, a metric developed specifically to assess the importance of each weight channel in maintaining performance. In the final stage, full inference is conducted on the remaining weights. A major advantage of PP is its compatibility with existing models, as it operates without requiring additional neural network modules or fine-tuning. Comprehensive evaluations of PP on LLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of FLOPs-can substantially enhance the efficiency of structured pruning of LLMs. For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56 times lower ratio of performance degradation per unit of runtime reduction compared to the state-of-the-art method at a 40% pruning ratio. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>我们介绍了探针修剪（PP），这是一个以批量方式应用的大型语言模型（LLMS）在线，动态，结构化修剪的新型框架。 PP利用了并非所有样本和令牌都对模型的输出的贡献，并探测每个批次的一小部分有效地识别关键权重，从而为不同的批次定制动态修剪。它包括三个主要阶段：探测，历史知识修剪和完整推断。在探测阶段，PP根据残留重要性选择了一组少量但至关重要的隐藏状态，以运行前方的几层模型层。在历史知名的修剪阶段，PP将探测状态与历史状态进行战略性融合。随后，它根据综合状态和PP重要性得分在结构上修剪权重，这是一个专门用于评估每个权重渠道在维持性能中的重要性的指标。在最后阶段，对剩余重量进行了全面推断。 PP的主要优点是它与现有模型的兼容性，因为它在不需要其他神经网络模块或微调的情况下运行。对PP在Llama-2/3和OPT模型上的全面评估表明，即使仅使用1.5％的Flops-Can进行最小的探测，也可以显着提高LLMS结构化修剪的效率。例如，当对Wikitext2的Llama-2-7b进行评估时，PP的每单位运行时降解比以40％的修剪比的比例达到了每单位运行时降解的2.56倍。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Extraction multi-étiquettes de relations en utilisant des couches de Transformer</h3>
<ul>
<li><strong>Authors: </strong>Ngoc Luyen Le, Gildas Tagny Ngompé</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15619">https://arxiv.org/abs/2502.15619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15619">https://arxiv.org/pdf/2502.15619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15619]] Extraction multi-étiquettes de relations en utilisant des couches de Transformer(https://arxiv.org/abs/2502.15619)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this article, we present the BTransformer18 model, a deep learning architecture designed for multi-label relation extraction in French texts. Our approach combines the contextual representation capabilities of pre-trained language models from the BERT family - such as BERT, RoBERTa, and their French counterparts CamemBERT and FlauBERT - with the power of Transformer encoders to capture long-term dependencies between tokens. Experiments conducted on the dataset from the TextMine'25 challenge show that our model achieves superior performance, particularly when using CamemBERT-Large, with a macro F1 score of 0.654, surpassing the results obtained with FlauBERT-Large. These results demonstrate the effectiveness of our approach for the automatic extraction of complex relations in intelligence reports.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了BtransFormer18模型，这是一种深入学习架构，旨在在法语文本中提取多标签关系。我们的方法结合了伯特家族的预训练语言模型的上下文表示能力，例如伯特，罗伯塔和他们的法国同行卡梅蒙德和弗劳伯特 - 与变形金刚编码者的力量捕获了代币之间的长期依赖性。 Textmine'25挑战赛在数据集上进行的实验表明，我们的模型实现了卓越的性能，尤其是在使用Camembert-Large时，宏F1得分为0.654，超过了使用Flaubert-Large获得的结果。这些结果证明了我们方法在情报报告中自动提取复杂关系的有效性。</li>
</ul>

<h3>Title: Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anirudh Sundar, Sinead Williamson, Katherine Metcalf, Barry-John Theobald, Skyler Seto, Masha Fedzechkina</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15639">https://arxiv.org/abs/2502.15639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15639">https://arxiv.org/pdf/2502.15639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15639]] Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models(https://arxiv.org/abs/2502.15639)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions -- a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM's activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval.</li>
<li><strong>摘要：</strong>跨语言的对齐表示是多语言大语言模型（MLLM）中所需的属性，因为对齐可以提高跨语性任务的性能。通常，对齐需要微调模型，该模型在计算上昂贵且相当大的语言数据，通常可能无法使用。微调替代品是模型干预措施 - 一种操纵模型激活以将生成转向所需方向的方法。我们分析了流行干预措施（寻找专家）对MLLM中跨语性表示的一致性的影响。我们确定操纵给定语言的神经元，并具有内省的嵌入MLLM前和操作后的嵌入空间。我们表明，修改MLLM的激活会改变其嵌入空间，从而增强了跨语性对准。此外，我们表明，嵌入空间的变化转化为检索任务上的下游性能的改善，跨语性检索的TOP-1准确性提高了2倍。</li>
</ul>

<h3>Title: Machine-generated text detection prevents language model collapse</h3>
<ul>
<li><strong>Authors: </strong>George Drayson, Vasileios Lampos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15654">https://arxiv.org/abs/2502.15654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15654">https://arxiv.org/pdf/2502.15654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15654]] Machine-generated text detection prevents language model collapse(https://arxiv.org/abs/2502.15654)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since web data is the primary resource for LLM pretraining, future models will be trained on an unknown portion of synthetic data. This will lead to model collapse, a degenerative process which causes models to reinforce their own errors and experience a drop in model performance. In this study, we investigate the impact of decoding strategy on model collapse, where we analyse the characteristics of the generated data during recursive training, its similarity to human references and the resulting model performance. Using the decoding strategies that lead to the most significant model degradation, we tackle the question: how to avoid model collapse when the origin (human or synthetic) of the training data is unknown. We design a novel methodology based on resampling the data distribution using importance weights from our machine-generated text detector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on the open-ended text generation task, demonstrating that we can successfully prevent model collapse and when there is enough human-authored data in the training dataset, our method improves model performance.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLMS）变得越来越普遍，它们的产生产生在整个网络上增殖，冒着机器生成的内容稀释人类实现的文本的未来风险。由于Web数据是LLM预处理的主要资源，因此将在未知部分的合成数据中培训未来的模型。这将导致模型崩溃，这是一个退化过程，它导致模型加强了自己的错误并体验到模型性能下降。在这项研究中，我们调查了解码策略对模型崩溃的影响，在该影响中，我们分析了递归训练期间生成的数据的特征，与人类参考的相似性和由此产生的模型性能。使用导致最重要的模型降级的解码策略，我们解决了一个问题：如何避免训练数据的起源（人类或合成）未知时避免模型崩溃。我们设计了一种新的方法，基于使用机器生成的文本检测器的重要性权重重新采样数据分布。我们的方法在开放式文本生成任务上的两个LLM变体（GPT-2和SMOLLM2）上进行了验证，这表明我们可以成功防止模型崩溃，并且当培训数据集中有足够的人为实现的数据时，我们的方法会改善模型。表现。</li>
</ul>

<h3>Title: Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing</h3>
<ul>
<li><strong>Authors: </strong>Shoumik Saha, Soheil Feizi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15666">https://arxiv.org/abs/2502.15666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15666">https://arxiv.org/pdf/2502.15666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15666]] Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing(https://arxiv.org/abs/2502.15666)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The growing use of large language models (LLMs) for text generation has led to widespread concerns about AI-generated content detection. However, an overlooked challenge is AI-polished text, where human-written content undergoes subtle refinements using AI tools. This raises a critical question: should minimally polished text be classified as AI-generated? Misclassification can lead to false plagiarism accusations and misleading claims about AI prevalence in online content. In this study, we systematically evaluate eleven state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation (APT-Eval) dataset, which contains $11.7K$ samples refined at varying AI-involvement levels. Our findings reveal that detectors frequently misclassify even minimally polished text as AI-generated, struggle to differentiate between degrees of AI involvement, and exhibit biases against older and smaller models. These limitations highlight the urgent need for more nuanced detection methodologies.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地用于文本生成，这引起了人们对AI生成的内容检测的广泛关注。但是，一个被忽视的挑战是AI抛光的文本，其中人写的内容使用AI工具进行了细微的改进。这提出了一个关键的问题：是否应该将最小的抛光文本归类为AI生成？错误分类可能会导致虚假的窃指控和关于在线内容中AI普遍存在的误导性主张。在这项研究中，我们使用我们的AI-POLED-TEXT评估（APT-Eval）数据集进行系统地评估11个最先进的AI-TEXT探测器，该数据集包含$ 11.7K $样品在不同的AI-INVOLVERMENT水平上精炼的$ 11.7K $。我们的发现表明，探测器经常将其分类为AI生成的最小抛光文本，难以区分AI的参与度，并对较旧的和小型模型表现出偏见。这些局限性凸显了迫切需要更细微的检测方法。</li>
</ul>

<h3>Title: FLEKE: Federated Locate-then-Edit Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Zongkai Zhao, Guozeng Xu, Xiuhua Li, Kaiwen Wei, Jiang Zhong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15677">https://arxiv.org/abs/2502.15677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15677">https://arxiv.org/pdf/2502.15677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15677]] FLEKE: Federated Locate-then-Edit Knowledge Editing(https://arxiv.org/abs/2502.15677)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating large language models (LLMs) without full retraining. However, existing methods assume a single-user setting and become inefficient in real-world multi-client scenarios, where decentralized organizations (e.g., hospitals, financial institutions) independently update overlapping knowledge, leading to redundant mediator knowledge vector (MKV) computations and privacy concerns. To address these challenges, we introduce Federated Locate-then-Edit Knowledge Editing (FLEKE), a novel task that enables multiple clients to collaboratively perform LEKE while preserving privacy and reducing computational overhead. To achieve this, we propose FedEdit, a two-stage framework that optimizes MKV selection and reuse. In the first stage, clients locally apply LEKE and upload the computed MKVs. In the second stage, rather than relying solely on server-based MKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine similarity, enabling knowledge re-edit and minimizing redundant computations. Experimental results on two benchmark datasets demonstrate that FedEdit retains over 96% of the performance of non-federated LEKE while significantly outperforming a FedAvg-based baseline by approximately twofold. Besides, we find that MEMIT performs more consistently than PMET in the FLEKE task with our FedEdit framework. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>定位 - 然后编辑知识编辑（LEKE）是在不完全重新培训的情况下更新大型语言模型（LLM）的关键技术。但是，现有方法采用单用户设置，并在现实世界中的多客户场景中效率低下，在该场景中，分散的组织（例如，医院，金融机构）独立地更新重叠的知识，从而导致冗余的调解员知识向量（MKV）计算和隐私计算关注。为了应对这些挑战，我们介绍了联合的定位 - 编辑知识编辑（Fleke），这是一项新颖的任务，使多个客户能够协作执行LEKE，同时保留隐私并减少计算开销。为了实现这一目标，我们提出了Fededit，这是一个两阶段的框架，可优化MKV选择和重复使用。在第一阶段，客户在本地应用LEKE并上传计算的MKV。在第二阶段，Fleke不仅依靠基于服务器的MKV共享，还允许客户根据余弦相似性检索相关的MKV，从而使知识重新编辑并最大程度地减少冗余计算。两个基准数据集的实验结果表明，FedEdit保留了非拟合LEKE的96％以上的表现，同时显着超过了基于FedAvg的基线大约两倍。此外，我们发现MEMIT在我们的FedEdit框架中的fleke任务中的表现比PMET更加一致。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training</h3>
<ul>
<li><strong>Authors: </strong>Jaydeep Borkar, Matthew Jagielski, Katherine Lee, Niloofar Mireshghallah, David A. Smith, Christopher A. Choquette-Choo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.15680">https://arxiv.org/abs/2502.15680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.15680">https://arxiv.org/pdf/2502.15680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.15680]] Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training(https://arxiv.org/abs/2502.15680)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Due to the sensitive nature of personally identifiable information (PII), its owners may have the authority to control its inclusion or request its removal from large-language model (LLM) training. Beyond this, PII may be added or removed from training datasets due to evolving dataset curation techniques, because they were newly scraped for retraining, or because they were included in a new downstream fine-tuning stage. We find that the amount and ease of PII memorization is a dynamic property of a model that evolves throughout training pipelines and depends on commonly altered design choices. We characterize three such novel phenomena: (1) similar-appearing PII seen later in training can elicit memorization of earlier-seen sequences in what we call assisted memorization, and this is a significant factor (in our settings, up to 1/3); (2) adding PII can increase memorization of other PII significantly (in our settings, as much as $\approx\!7.5\times$); and (3) removing PII can lead to other PII being memorized. Model creators should consider these first- and second-order privacy risks when training models to avoid the risk of new PII regurgitation.</li>
<li><strong>摘要：</strong>由于个人身份信息（PII）的敏感性，其所有者可能有权控制其包容性或要求其从大型语言模型（LLM）培训中删除。除此之外，由于不断发展的数据集策划技术，可以从训练数据集中添加或从训练数据集中删除PII，因为它们是新近刮擦以进行再培训的，或者是因为它们被包括在新的下游微型调整阶段。我们发现，PII记忆的数量和易度性是模型的动态特性，该模型在整个训练管道中都会发展，并取决于通常改变的设计选择。我们表征了三种这种新型现象：（1）稍后在训练中看到的类似表现的PII可以引起对我们所谓的辅助记忆的早期序列的记忆，这是一个重要因素（在我们的设置中，最高1/3） ; （2）添加PII可以大大增加其他PII的记忆（在我们的设置中，多达$ \ \！7.5 \ times $）; （3）删除PII可能会导致其他PII记忆。训练模型以避免新PII反流的风险时，模型创建者应考虑这些一阶和二阶隐私风险。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
