<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-08-21</h1>
<h3>Title: A Survey on Symbolic Knowledge Distillation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kamal Acharya, Alvaro Velasquez, Houbing Herbert Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10210">https://arxiv.org/abs/2408.10210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10210">https://arxiv.org/pdf/2408.10210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10210]] A Survey on Symbolic Knowledge Distillation of Large Language Models(https://arxiv.org/abs/2408.10210)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This survey paper delves into the emerging and critical area of symbolic knowledge distillation in Large Language Models (LLMs). As LLMs like Generative Pre-trained Transformer-3 (GPT-3) and Bidirectional Encoder Representations from Transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient Artificial Intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression towards more accessible and efficient AI systems.</li>
<li><strong>摘要：</strong>本综述论文深入探讨了大型语言模型 (LLM) 中符号知识提炼这一新兴且关键的领域。随着生成式预训练 Transformer-3 (GPT-3) 和 Transformer 的双向编码器表示 (BERT) 等 LLM 在规模和复杂性上不断扩大，有效利用其广泛知识的挑战变得至关重要。本综述集中于将这些模型中包含的复杂、通常隐性的知识提炼为更具符号性、更明确的形式的过程。这种转变对于增强 LLM 的可解释性、效率和适用性至关重要。我们根据方法和应用对现有研究进行分类，重点关注如何使用符号知识提炼来提高更小、更高效的人工智能 (AI) 模型的透明度和功能性。本综述讨论了核心挑战，包括以易于理解的形式保持知识深度，并探讨了该领域开发的各种方法和技术。我们确定了当前研究的差距和未来进步的潜在机会。本次调查旨在全面概述法学硕士 (LLM) 中的符号知识提炼，强调其在向更易于访问和更高效的 AI 系统发展中的重要性。</li>
</ul>

<h3>Title: Beyond Relevant Documents: A Knowledge-Intensive Approach for Query-Focused Summarization using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weijia Zhang, Jia-Hong Huang, Svitlana Vakulenko, Yumo Xu, Thilina Rajapakse, Evangelos Kanoulas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10357">https://arxiv.org/abs/2408.10357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10357">https://arxiv.org/pdf/2408.10357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10357]] Beyond Relevant Documents: A Knowledge-Intensive Approach for Query-Focused Summarization using Large Language Models(https://arxiv.org/abs/2408.10357)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Query-focused summarization (QFS) is a fundamental task in natural language processing with broad applications, including search engines and report generation. However, traditional approaches assume the availability of relevant documents, which may not always hold in practical scenarios, especially in highly specialized topics. To address this limitation, we propose a novel knowledge-intensive approach that reframes QFS as a knowledge-intensive task setup. This approach comprises two main components: a retrieval module and a summarization controller. The retrieval module efficiently retrieves potentially relevant documents from a large-scale knowledge corpus based on the given textual query, eliminating the dependence on pre-existing document sets. The summarization controller seamlessly integrates a powerful large language model (LLM)-based summarizer with a carefully tailored prompt, ensuring the generated summary is comprehensive and relevant to the query. To assess the effectiveness of our approach, we create a new dataset, along with human-annotated relevance labels, to facilitate comprehensive evaluation covering both retrieval and summarization performance. Extensive experiments demonstrate the superior performance of our approach, particularly its ability to generate accurate summaries without relying on the availability of relevant documents initially. This underscores our method's versatility and practical applicability across diverse query scenarios.</li>
<li><strong>摘要：</strong>以查询为中心的摘要 (QFS) 是自然语言处理中的一项基本任务，具有广泛的应用，包括搜索引擎和报告生成。然而，传统方法假设相关文档可用，但在实际场景中，尤其是在高度专业化的主题中，这可能并不总是成立。为了解决这一限制，我们提出了一种新颖的知识密集型方法，将 QFS 重新定义为知识密集型任务设置。这种方法包括两个主要组件：检索模块和摘要控制器。检索模块根据给定的文本查询从大规模知识语料库中高效检索潜在相关文档，消除了对预先存在的文档集的依赖。摘要控制器无缝集成了强大的基于大型语言模型 (LLM) 的摘要器和精心定制的提示，确保生成的摘要全面且与查询相关。为了评估我们方法的有效性，我们创建了一个新数据集以及人工注释的相关性标签，以便于全面评估检索和摘要性能。大量实验证明了我们的方法的卓越性能，特别是它能够在不依赖相关文档可用性的情况下生成准确的摘要。这凸显了我们的方法在各种查询场景中的多功能性和实用性。</li>
</ul>

<h3>Title: Value Alignment from Unstructured Text</h3>
<ul>
<li><strong>Authors: </strong>Inkit Padhi, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Manish Nagireddy, Pierre Dognin, Kush R. Varshney</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10392">https://arxiv.org/abs/2408.10392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10392">https://arxiv.org/pdf/2408.10392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10392]] Value Alignment from Unstructured Text(https://arxiv.org/abs/2408.10392)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) to value systems has emerged as a significant area of research within the fields of AI and NLP. Currently, this alignment process relies on the availability of high-quality supervised and preference data, which can be both time-consuming and expensive to curate or annotate. In this paper, we introduce a systematic end-to-end methodology for aligning LLMs to the implicit and explicit values represented in unstructured text data. Our proposed approach leverages the use of scalable synthetic data generation techniques to effectively align the model to the values present in the unstructured data. Through two distinct use-cases, we demonstrate the efficiency of our methodology on the Mistral-7B-Instruct model. Our approach credibly aligns LLMs to the values embedded within documents, and shows improved performance against other approaches, as quantified through the use of automatic metrics and win rates.</li>
<li><strong>摘要：</strong>将大型语言模型 (LLM) 与价值体系对齐已成为 AI 和 NLP 领域的一个重要研究领域。目前，这种对齐过程依赖于高质量监督和偏好数据的可用性，而这些数据的整理或注释既耗时又昂贵。在本文中，我们介绍了一种系统的端到端方法，用于将 LLM 与非结构化文本数据中表示的隐式和显式值对齐。我们提出的方法利用可扩展的合成数据生成技术来有效地将模型与非结构化数据中存在的值对齐。通过两个不同的用例，我们展示了我们的方法在 Mistral-7B-Instruct 模型上的效率。我们的方法可靠地将 LLM 与文档中嵌入的值对齐，并且通过使用自动指标和胜率来量化，与其他方法相比，其性能有所提高。</li>
</ul>

<h3>Title: Resolving Lexical Bias in Edit Scoping with Projector Editor Networks</h3>
<ul>
<li><strong>Authors: </strong>Hammad Rizwan, Domenic Rosati, Ga Wu, Hassan Sajjad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10411">https://arxiv.org/abs/2408.10411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10411">https://arxiv.org/pdf/2408.10411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10411]] Resolving Lexical Bias in Edit Scoping with Projector Editor Networks(https://arxiv.org/abs/2408.10411)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Weight-preserving model editing techniques heavily rely on the scoping mechanism that decides when to apply an edit to the base model. These scoping mechanisms utilize distance functions in the representation space to ascertain the scope of the edit. In this work, we show that distance-based scoping functions grapple with lexical biases leading to issues such as misfires with irrelevant prompts that share similar lexical characteristics. To address this problem, we introduce, Projector Editor Networks for Model Editing (PENME),is a model editing approach that employs a compact adapter with a projection network trained via a contrastive learning objective. We demonstrate the efficacy of PENME in achieving superior results while being compute efficient and flexible to adapt across model architectures.</li>
<li><strong>摘要：</strong>权重保留模型编辑技术在很大程度上依赖于决定何时对基础模型应用编辑的范围机制。这些范围机制利用表示空间中的距离函数来确定编辑的范围。在这项工作中，我们表明基于距离的范围函数可以解决词汇偏见问题，从而导致诸如具有相似词汇特征的无关提示失效等问题。为了解决这个问题，我们引入了模型编辑投影仪编辑器网络 (PENME)，这是一种模型编辑方法，它采用紧凑适配器和通过对比学习目标训练的投影网络。我们证明了 PENME 在实现卓越结果方面的有效性，同时具有计算效率和灵活性，可以适应各种模型架构。</li>
</ul>

<h3>Title: Goldfish: Monolingual Language Models for 350 Languages</h3>
<ul>
<li><strong>Authors: </strong>Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10441">https://arxiv.org/abs/2408.10441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10441">https://arxiv.org/pdf/2408.10441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10441]] Goldfish: Monolingual Language Models for 350 Languages(https://arxiv.org/abs/2408.10441)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than bigrams for many languages (e.g. 24% of languages in XGLM 4.5B; 43% in BLOOM 7.1B). To facilitate research that focuses on low-resource languages, we pre-train and release Goldfish, a suite of monolingual autoregressive Transformer language models up to 125M parameters for 350 languages. The Goldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98 of 204 FLORES languages, despite each Goldfish model being over 10x smaller. However, the Goldfish significantly underperform larger multilingual models on reasoning benchmarks, suggesting that for low-resource languages, multilinguality primarily improves general reasoning abilities rather than basic text generation. We release models trained on 5MB (350 languages), 10MB (288 languages), 100MB (166 languages), and 1GB (83 languages) of text data where available. The Goldfish models are available as baselines, fine-tuning sources, or augmentations to existing models in low-resource NLP research, and they are further useful for crosslinguistic studies requiring maximally comparable models across languages.</li>
<li><strong>摘要：</strong>对于许多资源匮乏的语言，唯一可用的语言模型是同时对多种语言进行训练的大型多语言模型。然而，使用 FLORES 困惑度作为指标，我们发现这些模型对许多语言的表现都比二元语法差（例如，XGLM 4.5B 中 24% 的语言；BLOOM 7.1B 中 43%）。为了促进针对资源匮乏语言的研究，我们预训练并发布了 Goldfish，这是一套单语自回归 Transformer 语言模型，最多 1.25 亿个参数，适用于 350 种语言。尽管每个 Goldfish 模型都小 10 倍以上，但在 204 种 FLORES 语言中的 98 种上，Goldfish 的 FLORES 困惑度低于 BLOOM、XGLM 和 MaLA-500。然而，Goldfish 在推理基准测试中的表现明显不如大型多语言模型，这表明对于资源匮乏的语言，多语言性主要提高的是一般推理能力，而不是基本的文本生成。我们发布使用 5MB（350 种语言）、10MB（288 种语言）、100MB（166 种语言）和 1GB（83 种语言）文本数据训练的模型（如有）。Goldfish 模型可用作低资源 NLP 研究中现有模型的基线、微调源或增强模型，并且对于需要跨语言最大程度可比的模型的跨语言研究也非常有用。</li>
</ul>

<h3>Title: Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Guanchen Li, Xiandong Zhao, Lian Liu, Zeping Li, Dong Li, Lu Tian, Jie He, Ashish Sirasao, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10473">https://arxiv.org/abs/2408.10473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10473">https://arxiv.org/pdf/2408.10473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10473]] Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism(https://arxiv.org/abs/2408.10473)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 被设计为在上下文理解方面具有强大的鲁棒性，并在各种自然语言处理任务中表现出色。然而，它们相当大的规模会产生大量的计算和存储成本。现代修剪策略采用一次性技术来压缩 PLM，而无需在特定于任务或其他一般数据上重新训练；然而，这些方法往往会导致性能不可避免地下降。在本文中，我们提出了 SDS，这是一个稀疏-密集-稀疏修剪框架，用于从权重分布优化的角度提高修剪后的 PLM 的性能。我们分三个步骤概述了修剪过程。首先，我们使用传统的一次性修剪方法修剪模型中不太重要的连接。接下来，我们通过使用稀疏正则化重新激活修剪后的连接来重建一个具有修剪友好权重分布的密集模型。最后，我们进行第二轮修剪，与初始修剪相比，得到一个更优秀的修剪模型。实验结果表明，在相同的稀疏度配置下，SDS 的表现优于最先进的修剪技术 SparseGPT 和 Wanda。例如，在 Raw-Wikitext2 上，SDS 将困惑度降低了 9.13，并在稀疏度为 2:4 的 OPT-125M 的多个零样本基准测试中将准确率平均提高了 2.05%。</li>
</ul>

<h3>Title: Analysis of Plan-based Retrieval for Grounded Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Ameya Godbole, Nicholas Monath, Seungyeon Kim, Ankit Singh Rawat, Andrew McCallum, Manzil Zaheer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10490">https://arxiv.org/abs/2408.10490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10490">https://arxiv.org/pdf/2408.10490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10490]] Analysis of Plan-based Retrieval for Grounded Text Generation(https://arxiv.org/abs/2408.10490)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>In text generation, hallucinations refer to the generation of seemingly coherent text that contradicts established knowledge. One compelling hypothesis is that hallucinations occur when a language model is given a generation task outside its parametric knowledge (due to rarity, recency, domain, etc.). A common strategy to address this limitation is to infuse the language models with retrieval mechanisms, providing the model with relevant knowledge for the task. In this paper, we leverage the planning capabilities of instruction-tuned LLMs and analyze how planning can be used to guide retrieval to further reduce the frequency of hallucinations. We empirically evaluate several variations of our proposed approach on long-form text generation tasks. By improving the coverage of relevant facts, plan-guided retrieval and generation can produce more informative responses while providing a higher rate of attribution to source documents.</li>
<li><strong>摘要：</strong>在文本生成中，幻觉是指生成看似连贯但与既定知识相矛盾的文本。一个令人信服的假设是，当语言模型被赋予其参数知识之外的生成任务时（由于稀有性、新近性、领域等），就会出现幻觉。解决这一限制的常见策略是将检索机制注入语言模型，为模型提供与任务相关的知识。在本文中，我们利用指令调整的 LLM 的规划功能，并分析如何使用规划来指导检索，以进一步降低幻觉的频率。我们在长文本生成任务上对我们提出的方法的几种变体进行了实证评估。通过提高相关事实的覆盖率，计划引导的检索和生成可以产生更具信息量的响应，同时提供更高的源文档归因率。</li>
</ul>

<h3>Title: QUITO-X: An Information Bottleneck-based Compression Algorithm with Cross-Attention</h3>
<ul>
<li><strong>Authors: </strong>Yihang Wang, Xu Huang, Bowen Tian, Yixing Fan, Jiafeng Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10497">https://arxiv.org/abs/2408.10497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10497">https://arxiv.org/pdf/2408.10497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10497]] QUITO-X: An Information Bottleneck-based Compression Algorithm with Cross-Attention(https://arxiv.org/abs/2408.10497)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, prompt</a></li>
<li><strong>Abstract: </strong>Generative LLM have achieved significant success in various industrial tasks and can effectively adapt to vertical domains and downstream tasks through ICL. However, with tasks becoming increasingly complex, the context length required by ICL is also getting longer, and two significant issues arise: (i) The excessively long context leads to high costs and inference delays. (ii) A substantial amount of task-irrelevant information introduced by long contexts exacerbates the "lost in the middle" problem. Recently, compressing prompts by removing tokens according to some metric obtained from some causal language models, such as llama-7b, has emerged as an effective approach to mitigate these issues. However, the metric used by prior method such as self-information or PPL do not fully align with the objective of distinuishing the most important tokens when conditioning on query. In this work, we introduce information bottleneck theory to carefully examine the properties required by the metric. Inspired by this, we use cross-attention in encoder-decoder architecture as a new metric. Our simple method leads to significantly better performance in smaller models with lower latency. We evaluate our method on four datasets: DROP, CoQA, SQuAD, and Quoref. The experimental results show that, while maintaining the same performance, our compression rate can improve by nearly 25% over previous SOTA. Remarkably, in experiments where 25% of the tokens are removed, our model's EM score for answers sometimes even exceeds that of the control group using uncompressed text as context.</li>
<li><strong>摘要：</strong>生成式 LLM 在各种工业任务中取得了显著的成功，并且可以通过 ICL 有效地适应垂直领域和下游任务。然而，随着任务变得越来越复杂，ICL 所需的上下文长度也越来越长，并且出现了两个重大问题：（i）过长的上下文导致高成本和推理延迟。（ii）长上下文引入的大量与任务无关的信息加剧了“迷失在中间”的问题。最近，根据从某些因果语言模型（如 llama-7b）获得的某些度量标准删除标记来压缩提示已成为缓解这些问题的有效方法。然而，先前方法使用的度量标准（如自信息或 PPL）与在条件查询时区分最重要的标记的目标并不完全一致。在这项工作中，我们引入了信息瓶颈理论来仔细检查度量标准所需的属性。受此启发，我们在编码器-解码器架构中使用交叉注意力作为新度量标准。我们的简单方法在较小的模型中显著提高了性能，并且延迟更低。我们在四个数据集上评估了我们的方法：DROP、CoQA、SQuAD 和 Quoref。实验结果表明，在保持相同性能的同时，我们的压缩率比之前的 SOTA 提高了近 25%。值得注意的是，在删除 25% 的标记的实验中，我们模型的答案 EM 分数有时甚至超过了使用未压缩文本作为上下文的对照组。</li>
</ul>

<h3>Title: Data Augmentation Integrating Dialogue Flow and Style to Adapt Spoken Dialogue Systems to Low-Resource User Groups</h3>
<ul>
<li><strong>Authors: </strong>Zhiyang Qi, Michimasa Inaba</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10516">https://arxiv.org/abs/2408.10516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10516">https://arxiv.org/pdf/2408.10516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10516]] Data Augmentation Integrating Dialogue Flow and Style to Adapt Spoken Dialogue Systems to Low-Resource User Groups(https://arxiv.org/abs/2408.10516)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study addresses the interaction challenges encountered by spoken dialogue systems (SDSs) when engaging with users who exhibit distinct conversational behaviors, particularly minors, in scenarios where data are scarce. We propose a novel data augmentation framework to enhance SDS performance for user groups with limited resources. Our approach leverages a large language model (LLM) to extract speaker styles and a pre-trained language model (PLM) to simulate dialogue act history. This method generates enriched and personalized dialogue data, facilitating improved interactions with unique user demographics. Extensive experiments validate the efficacy of our methodology, highlighting its potential to foster the development of more adaptive and inclusive dialogue systems.</li>
<li><strong>摘要：</strong>本研究解决了口语对话系统 (SDS) 在数据稀缺的情况下与表现出不同对话行为的用户（尤其是未成年人）互动时遇到的交互挑战。我们提出了一种新颖的数据增强框架，以增强资源有限的用户群体的 SDS 性能。我们的方法利用大型语言模型 (LLM) 来提取说话者风格，并利用预训练语言模型 (PLM) 来模拟对话行为历史。该方法生成丰富且个性化的对话数据，从而促进与独特用户群体的更好互动。大量实验验证了我们方法的有效性，凸显了其促进更具适应性和包容性的对话系统发展的潜力。</li>
</ul>

<h3>Title: Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Ruan, Xiang Lan, Jingying Ma, Yizhi Dong, Kai He, Mengling Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10548">https://arxiv.org/abs/2408.10548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10548">https://arxiv.org/pdf/2408.10548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10548]] Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution(https://arxiv.org/abs/2408.10548)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: this https URL.</li>
<li><strong>摘要：</strong>表格数据是各个领域中流行的数据类型，由于其异构性和复杂的结构关系，它带来了独特的挑战。在表格数据分析中实现高预测性能和稳健性对众多应用都具有重大意义。受自然语言处理（特别是 Transformer 架构）最新进展的影响，出现了新的表格数据建模方法。早期技术集中于从头开始预训练 Transformer，经常会遇到可扩展性问题。随后，开发了利用 BERT 等预训练语言模型的方法，这些方法需要的数据更少，但性能更高。最近出现的大型语言模型（如 GPT 和 LLaMA）进一步改变了该领域，以最少的微调促进了更先进、更多样化的应用。尽管人们对此越来越感兴趣，但对表格数据语言建模技术的全面调查仍然缺失。本文通过系统地回顾表格数据语言建模的发展来填补这一空白，内容包括：（1）不同表格数据结构和数据类型的分类； (2) 回顾模型训练中使用的关键数据集和用于评估的任务；(3) 总结建模技术，包括广泛采用的数据处理方法、流行的架构和训练目标；(4) 从采用传统的预训练/预训练语言模型到使用大型语言模型的演变；(5) 确定表格数据分析语言建模中持续存在的挑战和潜在的未来研究方向。与此调查相关的 GitHub 页面位于：此 https URL。</li>
</ul>

<h3>Title: Putting People in LLMs' Shoes: Generating Better Answers via Question Rewriter</h3>
<ul>
<li><strong>Authors: </strong>Junhao Chen, Bowen Wang, Zhouqiang jiang, Yuta Nakashima</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10573">https://arxiv.org/abs/2408.10573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10573">https://arxiv.org/pdf/2408.10573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10573]] Putting People in LLMs' Shoes: Generating Better Answers via Question Rewriter(https://arxiv.org/abs/2408.10573)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated significant capabilities, particularly in the domain of question answering (QA). However, their effectiveness in QA is often undermined by the vagueness of user questions. To address this issue, we introduce single-round instance-level prompt optimization, referred to as question rewriter. By enhancing the intelligibility of human questions for black-box LLMs, our question rewriter improves the quality of generated answers. The rewriter is optimized using direct preference optimization based on feedback collected from automatic criteria for evaluating generated answers; therefore, its training does not require costly human annotations. The experiments across multiple black-box LLMs and long-form question answering (LFQA) datasets demonstrate the efficacy of our method. This paper provides a practical framework for training question rewriters and sets a precedent for future explorations in prompt optimization within LFQA tasks. Code is available at \url{this https URL}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出强大的能力，尤其是在问答 (QA) 领域。然而，它们在问答中的有效性通常会因用户问题的模糊性而受到削弱。为了解决这个问题，我们引入了单轮实例级提示优化，称为问题重写器。通过提高黑盒 LLM 中人类问题的可理解性，我们的问题重写器提高了生成答案的质量。重写器使用直接偏好优化进行优化，该优化基于从评估生成答案的自动标准收集的反馈；因此，其训练不需要昂贵的人工注释。在多个黑盒 LLM 和长格式问答 (LFQA) 数据集上进行的实验证明了我们方法的有效性。本文为训练问题重写器提供了一个实用的框架，并为未来在 LFQA 任务中探索提示优化树立了先例。代码可在 \url{this https URL} 获得。</li>
</ul>

<h3>Title: An Efficient Sign Language Translation Using Spatial Configuration and Motion Dynamics with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Eui Jun Hwang, Sukmin Cho, Junmyeong Lee, Jong C. Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10593">https://arxiv.org/abs/2408.10593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10593">https://arxiv.org/pdf/2408.10593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10593]] An Efficient Sign Language Translation Using Spatial Configuration and Motion Dynamics with LLMs(https://arxiv.org/abs/2408.10593)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Gloss-free Sign Language Translation (SLT) converts sign videos directly into spoken language sentences without relying on glosses. Recently, Large Language Models (LLMs) have shown remarkable translation performance in gloss-free methods by harnessing their powerful natural language generation capabilities. However, these methods often rely on domain-specific fine-tuning of visual encoders to achieve optimal results. By contrast, this paper emphasizes the importance of capturing the spatial configurations and motion dynamics inherent in sign language. With this in mind, we introduce Spatial and Motion-based Sign Language Translation (SpaMo), a novel LLM-based SLT framework. The core idea of SpaMo is simple yet effective. We first extract spatial and motion features using off-the-shelf visual encoders and then input these features into an LLM with a language prompt. Additionally, we employ a visual-text alignment process as a warm-up before the SLT supervision. Our experiments demonstrate that SpaMo achieves state-of-the-art performance on two popular datasets, PHOENIX14T and How2Sign.</li>
<li><strong>摘要：</strong>无注解手语翻译 (SLT) 无需注解即可将手语视频直接转换为口语句子。最近，大型语言模型 (LLM) 利用其强大的自然语言生成功能，在无注解方法中表现出色。然而，这些方法通常依赖于特定领域的视觉编码器微调来实现最佳结果。相比之下，本文强调了捕捉手语固有的空间配置和运动动态的重要性。考虑到这一点，我们介绍了基于空间和运动的手语翻译 (SpaMo)，这是一种基于 LLM 的新型 SLT 框架。SpaMo 的核心思想简单而有效。我们首先使用现成的视觉编码器提取空间和运动特征，然后将这些特征输入带有语言提示的 LLM。此外，我们在 SLT 监督之前采用视觉文本对齐过程作为热身。我们的实验表明，SpaMo 在两个流行的数据集 PHOENIX14T 和 How2Sign 上实现了最佳性能。</li>
</ul>

<h3>Title: Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory</h3>
<ul>
<li><strong>Authors: </strong>Yongxin Deng (1), Xihe Qiu (1), Xiaoyu Tan (2), Jing Pan (3), Chen Jue (1), Zhijun Fang (4), Yinghui Xu (5), Wei Chu (2), Yuan Qi (5) ((1) Shanghai University of Engineering Science, (2) INF Technology (Shanghai) Co., Ltd., (3) Monash University, (4) Donghua University, (5) Fudan University)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10608">https://arxiv.org/abs/2408.10608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10608">https://arxiv.org/pdf/2408.10608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10608]] Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory(https://arxiv.org/abs/2408.10608)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are trained on extensive text corpora, which inevitably include biased information. Although techniques such as Affective Alignment can mitigate some negative impacts of these biases, existing prompt-based attack methods can still extract these biases from the model's weights. Moreover, these biases frequently appear subtly when LLMs are prompted to perform identical tasks across different demographic groups, thereby camouflaging their presence. To address this issue, we have formally defined the implicit bias problem and developed an innovative framework for bias removal based on Bayesian theory, Bayesian-Theory based Bias Removal (BTBR). BTBR employs likelihood ratio screening to pinpoint data entries within publicly accessible biased datasets that represent biases inadvertently incorporated during the LLM training phase. It then automatically constructs relevant knowledge triples and expunges bias information from LLMs using model editing techniques. Through extensive experimentation, we have confirmed the presence of the implicit bias problem in LLMs and demonstrated the effectiveness of our BTBR approach.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在大量文本语料库上进行训练，不可避免地包含有偏见的信息。尽管诸如情感对齐之类的技术可以减轻这些偏见的一些负面影响，但现有的基于提示的攻击方法仍然可以从模型的权重中提取这些偏见。此外，当 LLM 被提示对不同的人口群体执行相同的任务时，这些偏见经常会巧妙地出现，从而掩盖它们的存在。为了解决这个问题，我们正式定义了隐性偏见问题，并开发了一个基于贝叶斯理论的创新偏见消除框架，即基于贝叶斯理论的偏见消除 (BTBR)。BTBR 采用似然比筛选来精确定位可公开访问的有偏见数据集中的数据条目，这些数据条目代表了在 LLM 训练阶段无意中纳入的偏见。然后，它会自动构建相关知识三元组并使用模型编辑技术从 LLM 中删除偏见信息。通过大量实验，我们证实了 LLM 中存在隐性偏见问题，并证明了我们的 BTBR 方法的有效性。</li>
</ul>

<h3>Title: Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information</h3>
<ul>
<li><strong>Authors: </strong>Ming Jiang, Tingting Huang, Biao Guo, Yao Lu, Feng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10615">https://arxiv.org/abs/2408.10615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10615">https://arxiv.org/pdf/2408.10615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10615]] Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information(https://arxiv.org/abs/2408.10615)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In recent years, Large language models (LLMs) have garnered significant attention due to their superior performance in complex reasoning tasks. However, recent studies may diminish their reasoning capabilities markedly when problem descriptions contain irrelevant information, even with the use of advanced prompting techniques. To further investigate this issue, a dataset of primary school mathematics problems containing irrelevant information, named GSMIR, was constructed. Testing prominent LLMs and prompting techniques on this dataset revealed that while LLMs can identify irrelevant information, they do not effectively mitigate the interference it causes once identified. A novel automatic construction method, ATF, which enhances the ability of LLMs to identify and self-mitigate the influence of irrelevant information, is proposed to address this shortcoming. This method operates in two steps: first, analysis of irrelevant information, followed by its filtering. The ATF method, as demonstrated by experimental results, significantly improves the reasoning performance of LLMs and prompting techniques, even in the presence of irrelevant information on the GSMIR dataset.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 因其在复杂推理任务中的出色表现而备受关注。然而，最近的研究可能会在问题描述包含不相关信息时显著削弱其推理能力，即使使用高级提示技术也是如此。为了进一步研究这个问题，我们构建了一个包含不相关信息的小学数学问题数据集，名为 GSMIR。在这个数据集上测试了著名的 LLM 和提示技术，结果表明，虽然 LLM 可以识别不相关信息，但它们无法有效缓解识别后造成的干扰。为了解决这一缺点，我们提出了一种新颖的自动构建方法 ATF，该方法增强了 LLM 识别和自我缓解不相关信息影响的能力。该方法分为两个步骤：首先，分析不相关信息，然后对其进行过滤。实验结果表明，ATF 方法显著提高了 LLM 和提示技术的推理性能，即使在 GSMIR 数据集上存在不相关信息的情况下也是如此。</li>
</ul>

<h3>Title: Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge Representation Sharing in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Maxim Ifergan, Leshem Choshen, Roee Aharoni, Idan Szpektor, Omri Abend</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10646">https://arxiv.org/abs/2408.10646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10646">https://arxiv.org/pdf/2408.10646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10646]] Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge Representation Sharing in LLMs(https://arxiv.org/abs/2408.10646)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The veracity of a factoid is largely independent of the language it is written in. However, language models are inconsistent in their ability to answer the same factual question across languages. This raises questions about how LLMs represent a given fact across languages. We explore multilingual factual knowledge through two aspects: the model's ability to answer a query consistently across languages, and the ability to ''store'' answers in a shared representation for several languages. We propose a methodology to measure the extent of representation sharing across languages by repurposing knowledge editing methods. We examine LLMs with various multilingual configurations using a new multilingual dataset. We reveal that high consistency does not necessarily imply shared representation, particularly for languages with different scripts. Moreover, we find that script similarity is a dominant factor in representation sharing. Finally, we observe that if LLMs could fully share knowledge across languages, their accuracy in their best-performing language could benefit an increase of up to 150\% on average. These findings highlight the need for improved multilingual knowledge representation in LLMs and suggest a path for the development of more robust and consistent multilingual LLMs.</li>
<li><strong>摘要：</strong>事实的真实性在很大程度上与其所用的语言无关。然而，语言模型在不同语言中回答同一事实问题的能力并不一致。这引发了人们对 LLM 如何在不同语言中表示给定事实的疑问。我们从两个方面探索多语言事实知识：模型在不同语言中一致地回答查询的能力，以及将答案“存储”在多种语言的共享表示中的能力。我们提出了一种方法，通过重新利用知识编辑方法来衡量跨语言表示共享的程度。我们使用新的多语言数据集检查了具有各种多语言配置的 LLM。我们发现高一致性并不一定意味着共享表示，特别是对于具有不同脚本的语言。此外，我们发现脚本相似性是表示共享的主要因素。最后，我们观察到，如果 LLM 能够完全在不同语言之间共享知识，那么它们在表现最佳的语言中的准确性平均可以提高高达 150%。这些发现强调了改进法学硕士 (LLM) 中多语言知识表示的必要性，并为开发更为稳健、一致的多语言法学硕士 (LLM) 指明了方向。</li>
</ul>

<h3>Title: REInstruct: Building Instruction Data from Unlabeled Corpus</h3>
<ul>
<li><strong>Authors: </strong>Shu Chen, Xinyan Guan, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10663">https://arxiv.org/abs/2408.10663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10663">https://arxiv.org/pdf/2408.10663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10663]] REInstruct: Building Instruction Data from Unlabeled Corpus(https://arxiv.org/abs/2408.10663)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Manually annotating instruction data for large language models is difficult, costly, and hard to scale. Meanwhile, current automatic annotation methods typically rely on distilling synthetic data from proprietary LLMs, which not only limits the upper bound of the quality of the instruction data but also raises potential copyright issues. In this paper, we propose REInstruct, a simple and scalable method to automatically build instruction data from an unlabeled corpus without heavy reliance on proprietary LLMs and human annotation. Specifically, REInstruct first selects a subset of unlabeled texts that potentially contain well-structured helpful and insightful content and then generates instructions for these texts. To generate accurate and relevant responses for effective and robust training, REInstruct further proposes a rewriting-based approach to improve the quality of the generated instruction data. By training Llama-7b on a combination of 3k seed data and 32k synthetic data from REInstruct, fine-tuned model achieves a 65.41\% win rate on AlpacaEval leaderboard against text-davinci-003, outperforming other open-source, non-distilled instruction data construction methods. The code is publicly available at \url{this https URL}.</li>
<li><strong>摘要：</strong>手动注释大型语言模型的指令数据既困难又昂贵，而且难以扩展。同时，当前的自动注释方法通常依赖于从专有 LLM 中提取合成数据，这不仅限制了指令数据质量的上限，而且还引发了潜在的版权问题。在本文中，我们提出了 REInstruct，这是一种简单且可扩展的方法，可以自动从未标记的语料库中构建指令数据，而无需过度依赖专有 LLM 和人工注释。具体来说，REInstruct 首先选择一组可能包含结构良好、有用且有见地的内容的未标记文本子集，然后为这些文本生成指令。为了生成准确且相关的响应以进行有效且稳健的训练，REInstruct 进一步提出了一种基于重写的方法来提高生成的指令数据的质量。通过使用来自 REInstruct 的 3k 种子数据和 32k 合成数据的组合来训练 Llama-7b，经过微调的模型在 AlpacaEval 排行榜上与 text-davinci-003 的胜率达到 65.41\%，优于其他开源、非提炼的指令数据构建方法。代码可在 \url{此 https URL} 上公开获取。</li>
</ul>

<h3>Title: HMoE: Heterogeneous Mixture of Experts for Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>An Wang, Xingwu Sun, Ruobing Xie, Shuaipeng Li, Jiaqi Zhu, Zhen Yang, Pinxue Zhao, J.N.Han, Zhanhui Kang, Di Wang, Naoaki Okazaki, Cheng-zhong Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10681">https://arxiv.org/abs/2408.10681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10681">https://arxiv.org/pdf/2408.10681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10681]] HMoE: Heterogeneous Mixture of Experts for Language Modeling(https://arxiv.org/abs/2408.10681)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Mixture of Experts (MoE) offers remarkable performance and computational efficiency by selectively activating subsets of model parameters. Traditionally, MoE models use homogeneous experts, each with identical capacity. However, varying complexity in input data necessitates experts with diverse capabilities, while homogeneous MoE hinders effective expert specialization and efficient parameter utilization. In this study, we propose a novel Heterogeneous Mixture of Experts (HMoE), where experts differ in size and thus possess diverse capacities. This heterogeneity allows for more specialized experts to handle varying token complexities more effectively. To address the imbalance in expert activation, we propose a novel training objective that encourages the frequent activation of smaller experts, enhancing computational efficiency and parameter utilization. Extensive experiments demonstrate that HMoE achieves lower loss with fewer activated parameters and outperforms conventional homogeneous MoE models on various pre-training evaluation benchmarks. Codes will be released upon acceptance.</li>
<li><strong>摘要：</strong>专家混合 (MoE) 通过选择性激活模型参数子集，提供卓越的性能和计算效率。传统上，MoE 模型使用同质专家，每个专家都具有相同的能力。然而，输入数据的复杂性各不相同，需要具有不同能力的专家，而同质 MoE 会阻碍专家的有效专业化和高效的参数利用。在本研究中，我们提出了一种新颖的异质专家混合 (HMoE)，其中专家规模不同，因此具有不同的能力。这种异质性允许更专业的专家更有效地处理不同的 token 复杂性。为了解决专家激活不平衡的问题，我们提出了一种新颖的训练目标，鼓励频繁激活较小的专家，从而提高计算效率和参数利用率。大量实验表明，HMoE 以更少的激活参数实现了更低的损失，并且在各种预训练评估基准上优于传统的同质 MoE 模型。代码将在接受后发布。</li>
</ul>

<h3>Title: Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongbang Yuan, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10682">https://arxiv.org/abs/2408.10682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10682">https://arxiv.org/pdf/2408.10682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10682]] Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models(https://arxiv.org/abs/2408.10682)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>LLM have achieved success in many fields but still troubled by problematic content in the training corpora. LLM unlearning aims at reducing their influence and avoid undesirable behaviours. However, existing unlearning methods remain vulnerable to adversarial queries and the unlearned knowledge resurfaces after the manually designed attack queries. As part of a red-team effort to proactively assess the vulnerabilities of unlearned models, we design Dynamic Unlearning Attack (DUA), a dynamic and automated framework to attack these models and evaluate their robustness. It optimizes adversarial suffixes to reintroduce the unlearned knowledge in various scenarios. We find that unlearned knowledge can be recovered in $55.2\%$ of the questions, even without revealing the unlearned model's parameters. In response to this vulnerability, we propose Latent Adversarial Unlearning (LAU), a universal framework that effectively enhances the robustness of the unlearned process. It formulates the unlearning process as a min-max optimization problem and resolves it through two stages: an attack stage, where perturbation vectors are trained and added to the latent space of LLMs to recover the unlearned knowledge, and a defense stage, where previously trained perturbation vectors are used to enhance unlearned model's robustness. With our LAU framework, we obtain two robust unlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across multiple unlearning benchmarks and various models, and demonstrate that they improve the unlearning effectiveness by over $53.5\%$, cause only less than a $11.6\%$ reduction in neighboring knowledge, and have almost no impact on the model's general capabilities.</li>
<li><strong>摘要：</strong>LLM 在许多领域取得了成功，但仍然受到训练语料库中问题内容的困扰。LLM 反学习旨在减少它们的影响并避免不良行为。然而，现有的反学习方法仍然容易受到对抗性查询的攻击，并且未学习的知识会在手动设计的攻击查询之后重新浮出水面。作为红队主动评估未学习模型漏洞的一部分，我们设计了动态反学习攻击 (DUA)，这是一个动态的自动化框架来攻击这些模型并评估其鲁棒性。它优化了对抗性后缀以在各种场景中重新引入未学习的知识。我们发现，即使不透露未学习模型的参数，也可以在 $55.2\%$ 的问题中恢复未学习的知识。为了解决这个弱点，我们提出了潜在对抗性反学习 (LAU)，这是一个通用框架，可以有效增强反学习过程的鲁棒性。它将反学习过程公式化为最小-最大优化问题，并通过两个阶段解决：攻击阶段，训练扰动向量并将其添加到 LLM 的潜在空间以恢复未学习的知识；防御阶段，使用先前训练的扰动向量来增强未学习模型的鲁棒性。借助我们的 LAU 框架，我们获得了两种鲁棒的反学习方法，AdvGA 和 AdvNPO。我们在多个反学习基准和各种模型上进行了广泛的实验，并证明它们将反学习效果提高了超过 $53.5\%$，仅导致邻近知识减少不到 $11.6\%$，并且几乎不影响模型的一般能力。</li>
</ul>

<h3>Title: Unconditional Truthfulness: Learning Conditional Dependency for Uncertainty Quantification of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Artem Vazhentsev, Ekaterina Fadeeva, Rui Xing, Alexander Panchenko, Preslav Nakov, Timothy Baldwin, Maxim Panov, Artem Shelmanov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10692">https://arxiv.org/abs/2408.10692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10692">https://arxiv.org/pdf/2408.10692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10692]] Unconditional Truthfulness: Learning Conditional Dependency for Uncertainty Quantification of Large Language Models(https://arxiv.org/abs/2408.10692)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification (UQ) is a perspective approach to detecting Large Language Model (LLM) hallucinations and low quality output. In this work, we address one of the challenges of UQ in generation tasks that arises from the conditional dependency between the generation steps of an LLM. We propose to learn this dependency from data. We train a regression model, which target variable is the gap between the conditional and the unconditional generation confidence. During LLM inference, we use this learned conditional dependency model to modulate the uncertainty of the current generation step based on the uncertainty of the previous step. Our experimental evaluation on nine datasets and three LLMs shows that the proposed method is highly effective for uncertainty quantification, achieving substantial improvements over rivaling approaches.</li>
<li><strong>摘要：</strong>不确定性量化 (UQ) 是一种检测大型语言模型 (LLM) 幻觉和低质量输出的有效方法。在这项工作中，我们解决了生成任务中 UQ 面临的挑战之一，该挑战源于 LLM 生成步骤之间的条件依赖关系。我们建议从数据中学习这种依赖关系。我们训练一个回归模型，其目标变量是条件和无条件生成置信度之间的差距。在 LLM 推理期间，我们使用这个学习到的条件依赖模型根据上一步的不确定性来调节当前生成步骤的不确定性。我们对九个数据集和三个 LLM 进行的实验评估表明，所提出的方法对于不确定性量化非常有效，与其他方法相比取得了显着的改进。</li>
</ul>

<h3>Title: Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique</h3>
<ul>
<li><strong>Authors: </strong>Tej Deep Pala, Vernon Y.H. Toh, Rishabh Bhardwaj, Soujanya Poria</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10701">https://arxiv.org/abs/2408.10701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10701">https://arxiv.org/pdf/2408.10701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10701]] Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique(https://arxiv.org/abs/2408.10701)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In today's era, where large language models (LLMs) are integrated into numerous real-world applications, ensuring their safety and robustness is crucial for responsible AI usage. Automated red-teaming methods play a key role in this process by generating adversarial attacks to identify and mitigate potential vulnerabilities in these models. However, existing methods often struggle with slow performance, limited categorical diversity, and high resource demands. While Rainbow Teaming, a recent approach, addresses the diversity challenge by framing adversarial prompt generation as a quality-diversity search, it remains slow and requires a large fine-tuned mutator for optimal performance. To overcome these limitations, we propose Ferret, a novel approach that builds upon Rainbow Teaming by generating multiple adversarial prompt mutations per iteration and using a scoring function to rank and select the most effective adversarial prompt. We explore various scoring functions, including reward models, Llama Guard, and LLM-as-a-judge, to rank adversarial mutations based on their potential harm to improve the efficiency of the search for harmful mutations. Our results demonstrate that Ferret, utilizing a reward model as a scoring function, improves the overall attack success rate (ASR) to 95%, which is 46% higher than Rainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90% ASR by 15.2% compared to the baseline and generates adversarial prompts that are transferable i.e. effective on other LLMs of larger size. Our codes are available at this https URL.</li>
<li><strong>摘要：</strong>在当今时代，大型语言模型 (LLM) 被集成到众多实际应用中，确保其安全性和稳健性对于负责任地使用 AI 至关重要。自动红队方法在此过程中发挥着关键作用，它通过生成对抗性攻击来识别和缓解这些模型中的潜在漏洞。然而，现有方法通常面临性能缓慢、类别多样性有限和资源需求高的问题。虽然最近的方法 Rainbow Teaming 通过将对抗性提示生成定义为质量多样性搜索来解决多样性挑战，但它仍然很慢，并且需要大量微调的突变器才能获得最佳性能。为了克服这些限制，我们提出了 Ferret，这是一种基于 Rainbow Teaming 的新方法，它通过每次迭代生成多个对抗性提示突变并使用评分函数对最有效的对抗性提示进行排名和选择。我们探索了各种评分函数，包括奖励模型、Llama Guard 和 LLM-as-a-judge，以根据对抗性突变的潜在危害对其进行排名，从而提高有害突变搜索的效率。我们的结果表明，Ferret 利用奖励模型作为评分函数，将整体攻击成功率 (ASR) 提高到 95%，比 Rainbow Teaming 高出 46%。此外，与基线相比，Ferret 将实现 90% ASR 所需的时间缩短了 15.2%，并生成了可迁移的对抗提示，即对其他较大规模的 LLM 有效。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: MEGen: Generative Backdoor in Large Language Models via Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Jiyang Qiu, Xinbei Ma, Zhuosheng Zhang, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10722">https://arxiv.org/abs/2408.10722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10722">https://arxiv.org/pdf/2408.10722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10722]] MEGen: Generative Backdoor in Large Language Models via Model Editing(https://arxiv.org/abs/2408.10722)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities. Their powerful generative abilities enable flexible responses based on various queries or instructions. Emerging as widely adopted generalists for diverse tasks, LLMs are still vulnerable to backdoors. This paper proposes an editing-based generative backdoor, named MEGen, aiming to create a customized backdoor for NLP tasks with the least side effects. In our approach, we first leverage a language model to insert a trigger selected on fixed metrics into the input, then design a pipeline of model editing to directly embed a backdoor into an LLM. By adjusting a small set of local parameters with a mini-batch of samples, MEGen significantly enhances time efficiency and achieves high robustness. Experimental results indicate that our backdoor attack strategy achieves a high attack success rate on poison data while maintaining the model's performance on clean data. Notably, the backdoored model, when triggered, can freely output pre-set dangerous information while successfully completing downstream tasks. This suggests that future LLM applications could be guided to deliver certain dangerous information, thus altering the LLM's generative style. We believe this approach provides insights for future LLM applications and the execution of backdoor attacks on conversational AI systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出卓越的能力。其强大的生成能力使其能够根据各种查询或指令做出灵活的响应。LLM 已成为广泛采用的用于各种任务的通才，但仍容易受到后门的攻击。本文提出了一种基于编辑的生成后门 MEGen，旨在为 NLP 任务创建具有最小副作用的定制后门。在我们的方法中，我们首先利用语言模型将根据固定指标选择的触发器插入输入中，然后设计模型编辑管道以将后门直接嵌入 LLM。通过使用小批量样本调整一小组局部参数，MEGen 显著提高了时间效率并实现了高鲁棒性。实验结果表明，我们的后门攻击策略在毒药数据上实现了高攻击成功率，同时保持了模型在干净数据上的性能。值得注意的是，后门模型在被触发时可以自由输出预设的危险信息，同时成功完成下游任务。这表明，未来的 LLM 应用程序可能会被引导传递某些危险信息，从而改变 LLM 的生成风格。我们相信这种方法为未来的 LLM 应用程序和对对话式 AI 系统实施后门攻击提供了见解。</li>
</ul>

<h3>Title: Crafting Tomorrow's Headlines: Neural News Generation and Detection in English, Turkish, Hungarian, and Persian</h3>
<ul>
<li><strong>Authors: </strong>Cem Üyük, Danica Rovó, Shaghayegh Kolli, Rabia Varol, Georg Groh, Daryna Dementieva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10724">https://arxiv.org/abs/2408.10724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10724">https://arxiv.org/pdf/2408.10724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10724]] Crafting Tomorrow's Headlines: Neural News Generation and Detection in English, Turkish, Hungarian, and Persian(https://arxiv.org/abs/2408.10724)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>In the era dominated by information overload and its facilitation with Large Language Models (LLMs), the prevalence of misinformation poses a significant threat to public discourse and societal well-being. A critical concern at present involves the identification of machine-generated news. In this work, we take a significant step by introducing a benchmark dataset designed for neural news detection in four languages: English, Turkish, Hungarian, and Persian. The dataset incorporates outputs from multiple multilingual generators (in both, zero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and GPT-4. Next, we experiment with a variety of classifiers, ranging from those based on linguistic features to advanced Transformer-based models and LLMs prompting. We present the detection results aiming to delve into the interpretablity and robustness of machine-generated texts detectors across all target languages.</li>
<li><strong>摘要：</strong>在这个信息过载、大型语言模型 (LLM) 助推的时代，虚假信息的泛滥对公共话语和社会福祉构成了重大威胁。目前一个关键问题是机器生成新闻的识别。在这项工作中，我们迈出了重要的一步，引入了一个用于四种语言的神经新闻检测的基准数据集：英语、土耳其语、匈牙利语和波斯语。该数据集结合了来自多个多语言生成器（包括零样本和微调设置）的输出，例如 BloomZ、LLaMa-2、Mistral、Mixtral 和 GPT-4。接下来，我们尝试了各种分类器，从基于语言特征的分类器到基于 Transformer 的高级模型和 LLM 提示。我们展示了检测结果，旨在深入研究机器生成文本检测器在所有目标语言中的可解释性和鲁棒性。</li>
</ul>

<h3>Title: Towards Efficient Large Language Models for Scientific Text: A Review</h3>
<ul>
<li><strong>Authors: </strong>Huy Quoc To, Ming Liu, Guangyan Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10729">https://arxiv.org/abs/2408.10729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10729">https://arxiv.org/pdf/2408.10729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10729]] Towards Efficient Large Language Models for Scientific Text: A Review(https://arxiv.org/abs/2408.10729)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have ushered in a new era for processing complex information in various fields, including science. The increasing amount of scientific literature allows these models to acquire and understand scientific knowledge effectively, thus improving their performance in a wide range of tasks. Due to the power of LLMs, they require extremely expensive computational resources, intense amounts of data, and training time. Therefore, in recent years, researchers have proposed various methodologies to make scientific LLMs more affordable. The most well-known approaches align in two directions. It can be either focusing on the size of the models or enhancing the quality of data. To date, a comprehensive review of these two families of methods has not yet been undertaken. In this paper, we (I) summarize the current advances in the emerging abilities of LLMs into more accessible AI solutions for science, and (II) investigate the challenges and opportunities of developing affordable solutions for scientific domains using LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 开创了处理包括科学在内的各个领域复杂信息的新时代。越来越多的科学文献使这些模型能够有效地获取和理解科学知识，从而提高其在各种任务中的表现。由于 LLM 的强大功能，它们需要极其昂贵的计算资源、大量数据和训练时间。因此，近年来，研究人员提出了各种方法，以使科学 LLM 更实惠。最著名的方法有两个方向。它可以关注模型的大小，也可以提高数据的质量。到目前为止，尚未对这两类方法进行全面审查。在本文中，我们 (I) 总结了 LLM 新兴能力的最新进展，使其成为更易于获取的科学 AI 解决方案，以及 (II) 探讨了使用 LLM 为科学领域开发经济实惠的解决方案所面临的挑战和机遇。</li>
</ul>

<h3>Title: Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Chenhan Yuan, Fei Huang, Ru Peng, Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10764">https://arxiv.org/abs/2408.10764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10764">https://arxiv.org/pdf/2408.10764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10764]] Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model(https://arxiv.org/abs/2408.10764)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Transformer-based large language models (LLMs) exhibit limitations such as generating unsafe responses, unreliable reasoning, etc. Existing inference intervention approaches attempt to mitigate these issues by finetuning additional models to produce calibration signals (such as rewards) that guide the LLM's decoding process. However, this solution introduces substantial time and space overhead due to the separate models required. This work proposes Non-disruptive parameters insertion (Otter), inserting extra parameters into the transformer architecture to predict calibration signals along with the original LLM output. Otter offers state-of-the-art performance on multiple demanding tasks while saving up to 86.5\% extra space and 98.5\% extra time. Furthermore, Otter seamlessly integrates with existing inference engines, requiring only a one-line code change, and the original model response remains accessible after the parameter insertion. Our code is publicly available at \url{this https URL}</li>
<li><strong>摘要：</strong>基于 Transformer 的大型语言模型 (LLM) 存在一些局限性，例如生成不安全的响应、不可靠的推理等。现有的推理干预方法试图通过微调其他模型来产生指导 LLM 解码过程的校准信号（例如奖励）来缓解这些问题。然而，由于需要单独的模型，这种解决方案会带来大量的时间和空间开销。这项工作提出了无中断参数插入 (Otter)，将额外的参数插入到 Transformer 架构中，以预测校准信号以及原始 LLM 输出。Otter 在多个要求苛刻的任务上提供了最先进的性能，同时节省了高达 86.5% 的额外空间和 98.5% 的额外时间。此外，Otter 无缝集成到现有的推理引擎中，只需要更改一行代码，并且在插入参数后仍然可以访问原始模型响应。我们的代码在 \url{此 https URL} 上公开提供</li>
</ul>

<h3>Title: ColBERT Retrieval and Ensemble Response Scoring for Language Model Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Alex Gichamba, Tewodros Kederalah Idris, Brian Ebiyau, Eric Nyberg, Teruko Mitamura</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10808">https://arxiv.org/abs/2408.10808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10808">https://arxiv.org/pdf/2408.10808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10808]] ColBERT Retrieval and Ensemble Response Scoring for Language Model Question Answering(https://arxiv.org/abs/2408.10808)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Domain-specific question answering remains challenging for language models, given the deep technical knowledge required to answer questions correctly. This difficulty is amplified for smaller language models that cannot encode as much information in their parameters as larger models. The "Specializing Large Language Models for Telecom Networks" challenge aimed to enhance the performance of two small language models, Phi-2 and Falcon-7B in telecommunication question answering. In this paper, we present our question answering systems for this challenge. Our solutions achieved leading marks of 81.9% accuracy for Phi-2 and 57.3% for Falcon-7B. We have publicly released our code and fine-tuned models.</li>
<li><strong>摘要：</strong>对于语言模型来说，特定领域的问答仍然具有挑战性，因为正确回答问题需要深厚的技术知识。对于无法像大型模型那样在参数中编码大量信息的小型语言模型来说，这种困难更加严重。“专门为电信网络设计大型语言模型”挑战赛旨在提高两个小型语言模型 Phi-2 和 Falcon-7B 在电信问答方面的表现。在本文中，我们介绍了针对这一挑战赛的问答系统。我们的解决方案在 Phi-2 和 Falcon-7B 的准确率方面分别取得了 81.9% 和 57.3% 的领先成绩。我们已经公开发布了我们的代码和微调模型。</li>
</ul>

<h3>Title: Beyond English-Centric LLMs: What Language Do Multilingual Language Models Think in?</h3>
<ul>
<li><strong>Authors: </strong>Chengzhi Zhong, Fei Cheng, Qianying Liu, Junfeng Jiang, Zhen Wan, Chenhui Chu, Yugo Murawaki, Sadao Kurohashi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10811">https://arxiv.org/abs/2408.10811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10811">https://arxiv.org/pdf/2408.10811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10811]] Beyond English-Centric LLMs: What Language Do Multilingual Language Models Think in?(https://arxiv.org/abs/2408.10811)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this study, we investigate whether non-English-centric LLMs, despite their strong performance, `think' in their respective dominant language: more precisely, `think' refers to how the representations of intermediate layers, when un-embedded into the vocabulary space, exhibit higher probabilities for certain dominant languages during generation. We term such languages as internal $\textbf{latent languages}$. We examine the latent language of three typical categories of models for Japanese processing: Llama2, an English-centric model; Swallow, an English-centric model with continued pre-training in Japanese; and LLM-jp, a model pre-trained on balanced English and Japanese corpora. Our empirical findings reveal that, unlike Llama2 which relies exclusively on English as the internal latent language, Japanese-specific Swallow and LLM-jp employ both Japanese and English, exhibiting dual internal latent languages. For any given target language, the model preferentially activates the latent language most closely related to it. In addition, we explore how intermediate layers respond to questions involving cultural conflicts between latent internal and target output languages. We further explore how the language identity shifts across layers while keeping consistent semantic meaning reflected in the intermediate layer representations. This study deepens the understanding of non-English-centric large language models, highlighting the intricate dynamics of language representation within their intermediate layers.</li>
<li><strong>摘要：</strong>在本研究中，我们调查了非英语中心的 LLM 尽管性能强劲，但是否会用各自的主导语言“思考”：更准确地说，“思考”是指中间层的表示在未嵌入词汇空间时，在生成过程中对某些主导语言表现出更高的概率。我们将此类语言称为内部潜在语言。我们研究了三种典型的日语处理模型类别的潜在语言：Llama2，一个以英语为中心的模型；Swallow，一个以英语为中心的模型，并在日语中继续进行预训练；LLM-jp，一个在平衡的英语和日语语料库上进行预训练的模型。我们的实证结果表明，与完全依赖英语作为内部潜在语言的 Llama2 不同，特定于日语的 Swallow 和 LLM-jp 同时使用日语和英语，表现出双重内部潜在语言。对于任何给定的目标语言，该模型都会优先激活与其最密切相关的潜在语言。此外，我们还探索了中间层如何应对潜在内部语言和目标输出语言之间的文化冲突问题。我们进一步探索了语言身份如何在各层之间转变，同时保持中间层表示中反映的一致语义含义。这项研究加深了对非英语中心大型语言模型的理解，突出了中间层中语言表示的复杂动态。</li>
</ul>

<h3>Title: Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains</h3>
<ul>
<li><strong>Authors: </strong>Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, Yi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10819">https://arxiv.org/abs/2408.10819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10819">https://arxiv.org/pdf/2408.10819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10819]] Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains(https://arxiv.org/abs/2408.10819)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge graph completion (KGC) aims to identify missing triples in a knowledge graph (KG). This is typically achieved through tasks such as link prediction and instance completion. However, these methods often focus on either static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs), addressing only within-scope triples. This paper introduces a new generative completion framework called Generative Subgraph-based KGC (GS-KGC). GS-KGC employs a question-answering format to directly generate target entities, addressing the challenge of questions having multiple possible answers. We propose a strategy that extracts subgraphs centered on entities and relationships within the KG, from which negative samples and neighborhood information are separately obtained to address the one-to-many problem. Our method generates negative samples using known facts to facilitate the discovery of new information. Furthermore, we collect and refine neighborhood path data of known entities, providing contextual information to enhance reasoning in large language models (LLMs). Our experiments evaluated the proposed method on four SKGs and two TKGs, achieving state-of-the-art Hits@1 metrics on five datasets. Analysis of the results shows that GS-KGC can discover new triples within existing KGs and generate new facts beyond the closed KG, effectively bridging the gap between closed-world and open-world KGC.</li>
<li><strong>摘要：</strong>知识图谱补全 (KGC) 旨在识别知识图谱 (KG) 中缺失的三元组。这通常通过诸如链接预测和实例补全之类的任务来实现。然而，这些方法通常侧重于静态知识图谱 (SKG) 或时间知识图谱 (TKG)，仅处理范围内的三元组。本文介绍了一种新的生成式补全框架，称为基于生成子图的 KGC (GS-KGC)。GS-KGC 采用问答格式直接生成目标实体，解决了问题具有多个可能答案的挑战。我们提出了一种策略，该策略以 KG 中的实体和关系为中心提取子图，从中分别获取负样本和邻域信息以解决一对多问题。我们的方法使用已知事实生成负样本以促进新信息的发现。此外，我们收集和细化已知实体的邻域路径数据，提供上下文信息以增强大型语言模型 (LLM) 中的推理。我们在四个 SKG 和两个 TKG 上进行了实验，并在五个数据集上实现了最佳的 Hits@1 指标。结果分析表明，GS-KGC 可以在现有 KG 中发现新的三元组，并在封闭 KG 之外生成新的事实，从而有效地弥合了封闭世界和开放世界 KGC 之间的差距。</li>
</ul>

<h3>Title: Benchmarking Large Language Models for Math Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Kathrin Seßler, Yao Rong, Emek Gözlüklü, Enkelejda Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10839">https://arxiv.org/abs/2408.10839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10839">https://arxiv.org/pdf/2408.10839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10839]] Benchmarking Large Language Models for Math Reasoning Tasks(https://arxiv.org/abs/2408.10839)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>The use of Large Language Models (LLMs) in mathematical reasoning has become a cornerstone of related research, demonstrating the intelligence of these models and enabling potential practical applications through their advanced performance, such as in educational settings. Despite the variety of datasets and in-context learning algorithms designed to improve the ability of LLMs to automate mathematical problem solving, the lack of comprehensive benchmarking across different datasets makes it complicated to select an appropriate model for specific tasks. In this project, we present a benchmark that fairly compares seven state-of-the-art in-context learning algorithms for mathematical problem solving across five widely used mathematical datasets on four powerful foundation models. Furthermore, we explore the trade-off between efficiency and performance, highlighting the practical applications of LLMs for mathematical reasoning. Our results indicate that larger foundation models like GPT-4o and LLaMA 3-70B can solve mathematical reasoning independently from the concrete prompting strategy, while for smaller models the in-context learning approach significantly influences the performance. Moreover, the optimal prompt depends on the chosen foundation model. We open-source our benchmark code to support the integration of additional models in future research.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在数学推理中的应用已成为相关研究的基石，展示了这些模型的智能，并通过其先进的性能实现了潜在的实际应用，例如在教育环境中。尽管有各种各样的数据集和情境学习算法旨在提高 LLM 自动解决数学问题的能力，但缺乏对不同数据集的全面基准测试，使得为特定任务选择合适的模型变得复杂。在这个项目中，我们提出了一个基准测试，公平地比较了四个强大的基础模型上五个广泛使用的数学数据集中七种最先进的数学问题解决情境学习算法。此外，我们探索了效率和性能之间的权衡，强调了 LLM 在数学推理中的实际应用。我们的结果表明，像 GPT-4o 和 LLaMA 3-70B 这样的大型基础模型可以独立于具体的提示策略解决数学推理，而对于较小的模型，情境学习方法会显著影响性能。此外，最佳提示取决于所选的基础模型。我们开源我们的基准代码以支持未来研究中更多模型的集成。</li>
</ul>

<h3>Title: Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs</h3>
<ul>
<li><strong>Authors: </strong>John Mendonça, Isabel Trancoso, Alon Lavie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10902">https://arxiv.org/abs/2408.10902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10902">https://arxiv.org/pdf/2408.10902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10902]] Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs(https://arxiv.org/abs/2408.10902)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Although human evaluation remains the gold standard for open-domain dialogue evaluation, the growing popularity of automated evaluation using Large Language Models (LLMs) has also extended to dialogue. However, most frameworks leverage benchmarks that assess older chatbots on aspects such as fluency and relevance, which are not reflective of the challenges associated with contemporary models. In fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset, suggests that current chatbots may exhibit several recurring issues related to coherence and commonsense knowledge, but generally produce highly fluent and relevant responses. Noting the aforementioned limitations, this paper introduces Soda-Eval, an annotated dataset based on Soda that covers over 120K turn-level assessments across 10K dialogues, where the annotations were generated by GPT-4. Using Soda-Eval as a benchmark, we then study the performance of several open-access instruction-tuned LLMs, finding that dialogue evaluation remains challenging. Fine-tuning these models improves performance over few-shot inferences, both in terms of correlation and explanation.</li>
<li><strong>摘要：</strong>尽管人工评估仍然是开放领域对话评估的黄金标准，但使用大型语言模型 (LLM) 进行自动评估的日益普及也扩展到了对话领域。然而，大多数框架都利用了对旧聊天机器人的流畅性和相关性等方面进行评估的基准，而这些基准并不能反映当代模型所面临的挑战。事实上，对 GPT-3.5 生成的对话数据集 Soda 进行的定性分析表明，当前的聊天机器人可能会出现与连贯性和常识性知识相关的几个反复出现的问题，但通常会产生高度流畅和相关的回答。考虑到上述局限性，本文介绍了 Soda-Eval，这是一个基于 Soda 的带注释数据集，涵盖了 10K 对话中的 120K 多个回合级评估，其中注释由 GPT-4 生成。以 Soda-Eval 为基准，我们随后研究了几个开放获取的指令调整的 LLM 的性能，发现对话评估仍然具有挑战性。对这些模型进行微调可以提高小样本推理的性能，包括相关性和解释性。</li>
</ul>

<h3>Title: BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General Role-Playing Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yeyong Yu, Rusheng Yu, Haojie Wei, Zhanqiu Zhang, Quan Qian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10903">https://arxiv.org/abs/2408.10903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10903">https://arxiv.org/pdf/2408.10903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10903]] BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General Role-Playing Language Model(https://arxiv.org/abs/2408.10903)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has revolutionized role-playing, enabling the development of general role-playing models. However, current role-playing training has two significant issues: (I) Using a predefined role profile to prompt dialogue training for specific scenarios usually leads to inconsistencies and even conflicts between the dialogue and the profile, resulting in training biases. (II) The model learns to imitate the role based solely on the profile, neglecting profile-dialogue alignment at the sentence level. In this work, we propose a simple yet effective framework called BEYOND DIALOGUE, designed to overcome these hurdles. This framework innovatively introduces "beyond dialogue" tasks to align dialogue with profile traits based on each specific scenario, thereby eliminating biases during training. Furthermore, by adopting an innovative prompting mechanism that generates reasoning outcomes for training, the framework allows the model to achieve fine-grained alignment between profile and dialogue at the sentence level. The aforementioned methods are fully automated and low-cost. Additionally, the integration of automated dialogue and objective evaluation methods forms a comprehensive framework, paving the way for general role-playing. Experimental results demonstrate that our model excels in adhering to and reflecting various dimensions of role profiles, outperforming most proprietary general and specialized role-playing baselines. All code and datasets are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的快速发展彻底改变了角色扮演，使通用角色扮演模型的发展成为可能。然而，目前的角色扮演训练存在两个重大问题：（一）使用预定义的角色配置文件来提示特定场景的对话训练通常会导致对话与配置文件之间不一致甚至冲突，从而导致训练偏差。（二）该模型仅根据配置文件学习模仿角色，而忽略了句子级别的配置文件-对话对齐。在本文中，我们提出了一个简单而有效的框架，称为 BEYOND DIALOGUE，旨在克服这些障碍。该框架创新地引入了“超越对话”任务，根据每个特定场景将对话与配置文件特征对齐，从而消除训练过程中的偏差。此外，通过采用一种创新的提示机制来生成训练的推理结果，该框架允许模型在句子级别实现配置文件和对话之间的细粒度对齐。上述方法完全自动化且成本低廉。此外，自动对话和客观评估方法的整合形成了一个全面的框架，为一般角色扮演铺平了道路。实验结果表明，我们的模型在遵守和反映角色配置文件的各个维度方面表现出色，优于大多数专有的一般和专业角色扮演基线。所有代码和数据集均可在此 https URL 上找到。</li>
</ul>

<h3>Title: To Code, or Not To Code? Exploring Impact of Code in Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, Sara Hooker</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10914">https://arxiv.org/abs/2408.10914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10914">https://arxiv.org/pdf/2408.10914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10914]] To Code, or Not To Code? Exploring Impact of Code in Pre-training(https://arxiv.org/abs/2408.10914)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Including code in the pre-training data mixture, even for models not specifically designed for code, has become a common practice in LLMs pre-training. While there has been anecdotal consensus among practitioners that code data plays a vital role in general LLMs' performance, there is only limited work analyzing the precise impact of code on non-code tasks. In this work, we systematically investigate the impact of code data on general performance. We ask "what is the impact of code data used in pre-training on a large variety of downstream tasks beyond code generation". We conduct extensive ablations and evaluate across a broad range of natural language reasoning tasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for models with sizes ranging from 470M to 2.8B parameters. Across settings, we find a consistent results that code is a critical building block for generalization far beyond coding tasks and improvements to code quality have an outsized impact across all tasks. In particular, compared to text-only pre-training, the addition of code results in up to relative increase of 8.2% in natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement in generative win-rates, and a 12x boost in code performance respectively. Our work suggests investments in code quality and preserving code during pre-training have positive impacts.</li>
<li><strong>摘要：</strong>在预训练数据混合中包含代码，即使对于并非专门为代码设计的模型，也已成为 LLM 预训练的常见做法。虽然从业者普遍认为代码数据在一般 LLM 的性能中起着至关重要的作用，但分析代码对非代码任务的精确影响的工作却非常有限。在这项工作中，我们系统地研究了代码数据对一般性能的影响。我们问“预训练中使用的代码数据对代码生成以外的各种下游任务有何影响”。我们对范围广泛的自然语言推理任务、世界知识任务、代码基准和 LLM-as-a-judge 胜率进行了广泛的消融和评估，模型的大小从 4.7 亿到 2.8 亿个参数不等。在各种设置中，我们发现一致的结果是，代码是泛化的关键构建块，远远超出了编码任务的范围，并且代码质量的改进对所有任务都有巨大影响。具体而言，与纯文本预训练相比，添加代码可使自然语言 (NL) 推理相对提高 8.2%，世界知识相对提高 4.2%，生成胜率提高 6.6%，代码性能提高 12 倍。我们的研究表明，在预训练期间投资代码质量和保留代码会产生积极影响。</li>
</ul>

<h3>Title: CHECKWHY: Causal Fact Verification via Argument Structure</h3>
<ul>
<li><strong>Authors: </strong>Jiasheng Si, Yibo Zhao, Yingjie Zhu, Haiyang Zhu, Wenpeng Lu, Deyu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10918">https://arxiv.org/abs/2408.10918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10918">https://arxiv.org/pdf/2408.10918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10918]] CHECKWHY: Causal Fact Verification via Argument Structure(https://arxiv.org/abs/2408.10918)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>With the growing complexity of fact verification tasks, the concern with "thoughtful" reasoning capabilities is increasing. However, recent fact verification benchmarks mainly focus on checking a narrow scope of semantic factoids within claims and lack an explicit logical reasoning process. In this paper, we introduce CheckWhy, a challenging dataset tailored to a novel causal fact verification task: checking the truthfulness of the causal relation within claims through rigorous reasoning steps. CheckWhy consists of over 19K "why" claim-evidence-argument structure triplets with supports, refutes, and not enough info labels. Each argument structure is composed of connected evidence, representing the reasoning process that begins with foundational evidence and progresses toward claim establishment. Through extensive experiments on state-of-the-art models, we validate the importance of incorporating the argument structure for causal fact verification. Moreover, the automated and human evaluation of argument structure generation reveals the difficulty in producing satisfying argument structure by fine-tuned models or Chain-of-Thought prompted LLMs, leaving considerable room for future improvements.</li>
<li><strong>摘要：</strong>随着事实验证任务的复杂性日益增加，人们对“深思熟虑”的推理能力的关注度也日益增加。然而，最近的事实验证基准主要侧重于检查主张中范围狭窄的语义事实，缺乏明确的逻辑推理过程。在本文中，我们介绍了 CheckWhy，这是一个具有挑战性的数据集，专门用于一项新颖的因果事实验证任务：通过严格的推理步骤检查主张中因果关系的真实性。CheckWhy 包含超过 19K 个“为什么”主张-证据-论证结构三元组，其中包含支持、反驳和信息标签不足。每个论证结构都由相连的证据组成，代表从基础证据开始并逐步建立主张的推理过程。通过对最先进模型的大量实验，我们验证了将论证结构纳入因果事实验证的重要性。此外，对论证结构生成的自动和人工评估揭示了通过微调模型或思维链提示的 LLM 生成令人满意的论证结构的难度，为未来的改进留下了相当大的空间。</li>
</ul>

<h3>Title: LBC: Language-Based-Classifier for Out-Of-Variable Generalization</h3>
<ul>
<li><strong>Authors: </strong>Kangjun Noh, Baekryun Seong, Hoyoon Byun, Sungjin Song, Kyungwoo Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10923">https://arxiv.org/abs/2408.10923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10923">https://arxiv.org/pdf/2408.10923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10923]] LBC: Language-Based-Classifier for Out-Of-Variable Generalization(https://arxiv.org/abs/2408.10923)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have great success in natural language processing tasks such as response generation. However, their use in tabular data has been limited due to their inferior performance compared to traditional machine learning models (TMLs) such as XGBoost. We find that the pre-trained knowledge of LLMs enables them to interpret new variables that appear in a test without additional training, a capability central to the concept of Out-of-Variable (OOV). From the findings, we propose a Language-Based-Classifier (LBC), a classifier that maximizes the benefits of LLMs to outperform TMLs on OOV tasks. LBC employs three key methodological strategies: 1) Categorical changes to adjust data to better fit the model's understanding, 2) Advanced order and indicator to enhance data representation to the model, and 3) Using verbalizer to map logit scores to classes during inference to generate model predictions. These strategies, combined with the pre-trained knowledge of LBC, emphasize the model's ability to effectively handle OOV tasks. We empirically and theoretically validate the superiority of LBC. LBC is the first study to apply an LLM-based model to OOV tasks. The source code is at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在响应生成等自然语言处理任务中取得了巨大成功。然而，由于它们的性能不如 XGBoost 等传统机器学习模型 (TML)，因此它们在表格数据中的使用受到了限制。我们发现，LLM 的预训练知识使它们能够解释测试中出现的新变量而无需额外训练，这是变量外 (OOV) 概念的核心能力。根据研究结果，我们提出了一种基于语言的分类器 (LBC)，这是一种可以最大限度地发挥 LLM 优势的分类器，在 OOV 任务上的表现优于 TML。LBC 采用三种关键方法策略：1) 分类变化以调整数据以更好地适应模型的理解，2) 高级顺序和指标以增强数据对模型的表示，3) 在推理过程中使用言语化器将逻辑分数映射到类别以生成模型预测。这些策略与 LBC 的预训练知识相结合，强调了模型有效处理 OOV 任务的能力。我们通过实证和理论验证了 LBC 的优越性。LBC 是第一个将基于 LLM 的模型应用于 OOV 任务的研究。源代码位于此 https URL。</li>
</ul>

<h3>Title: SysBench: Can Large Language Models Follow System Messages?</h3>
<ul>
<li><strong>Authors: </strong>Yanzhao Qin, Tao Zhang, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan Zhou, Wentao Zhang, Bin Cui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10943">https://arxiv.org/abs/2408.10943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10943">https://arxiv.org/pdf/2408.10943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10943]] SysBench: Can Large Language Models Follow System Messages?(https://arxiv.org/abs/2408.10943)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become instrumental across various applications, with the customization of these models to specific scenarios becoming increasingly critical. System message, a fundamental component of LLMs, is consist of carefully crafted instructions that guide the behavior of model to meet intended goals. Despite the recognized potential of system messages to optimize AI-driven solutions, there is a notable absence of a comprehensive benchmark for evaluating how well different LLMs follow these system messages. To fill this gap, we introduce SysBench, a benchmark that systematically analyzes system message following ability in terms of three challenging aspects: constraint complexity, instruction misalignment and multi-turn stability. In order to enable effective evaluation, SysBench constructs multi-turn user conversations covering various interaction relationships, based on six common types of constraints from system messages in real-world scenarios. Our dataset contains 500 system messages from various domains, each paired with 5 turns of user conversations, which have been manually formulated and checked to guarantee high quality. SysBench provides extensive evaluation across various LLMs, measuring their ability to follow specified constraints given in system messages. The results highlight both the strengths and weaknesses of existing models, offering key insights and directions for future research. The open source library SysBench is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为各种应用中的重要工具，而针对特定场景定制这些模型变得越来越重要。系统消息是 LLM 的基本组成部分，由精心设计的指令组成，这些指令指导模型的行为以实现预期目标。尽管系统消息具有优化 AI 驱动解决方案的潜力，但目前仍缺乏全面的基准来评估不同 LLM 遵循这些系统消息的能力。为了填补这一空白，我们引入了 SysBench，这是一个基准，它从三个具有挑战性的方面系统地分析系统消息遵循能力：约束复杂性、指令错位和多轮稳定性。为了实现有效的评估，SysBench 根据现实场景中系统消息的六种常见约束类型构建了涵盖各种交互关系的多轮用户对话。我们的数据集包含来自不同领域的 500 条系统消息，每条消息都与 5 轮用户对话配对，这些对话都是手动制定和检查的，以确保高质量。SysBench 对各种 LLM 进行了广泛的评估，衡量了它们遵循系统消息中给出的指定约束的能力。结果突出了现有模型的优点和缺点，为未来研究提供了关键见解和方向。开源库 SysBench 可在此 https URL 上获取。</li>
</ul>

<h3>Title: CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Reinisch, Jianfeng He, Chenxi Liao, Sauleh Ahmad Siddiqui, Bei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10995">https://arxiv.org/abs/2408.10995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10995">https://arxiv.org/pdf/2408.10995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10995]] CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language Models(https://arxiv.org/abs/2408.10995)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>New medical treatment development requires multiple phases of clinical trials. Despite the significant human and financial costs of bringing a drug to market, less than 20% of drugs in testing will make it from the first phase to final approval. Recent literature indicates that the design of the trial protocols significantly contributes to trial performance. We investigated Clinical Trial Outcome Prediction (CTOP) using trial design documents to predict phase transitions automatically. We propose CTP-LLM, the first Large Language Model (LLM) based model for CTOP. We also introduce the PhaseTransition (PT) Dataset; which labels trials based on their progression through the regulatory process and serves as a benchmark for CTOP evaluation. Our fine-tuned GPT-3.5-based model (CTP-LLM) predicts clinical trial phase transition by analyzing the trial's original protocol texts without requiring human-selected features. CTP-LLM achieves a 67% accuracy rate in predicting trial phase transitions across all phases and a 75% accuracy rate specifically in predicting the transition from Phase~III to final approval. Our experimental performance highlights the potential of LLM-powered applications in forecasting clinical trial outcomes and assessing trial design.</li>
<li><strong>摘要：</strong>新医疗治疗方法的开发需要多个阶段的临床试验。尽管将药物推向市场需要大量的人力和财务成本，但只有不到 20% 的测试药物能够从第一阶段进入最终批准阶段。最近的文献表明，试验方案的设计对试验表现有重大影响。我们研究了临床试验结果预测 (CTOP)，使用试验设计文档自动预测阶段转变。我们提出了 CTP-LLM，这是第一个基于大型语言模型 (LLM) 的 CTOP 模型。我们还介绍了 PhaseTransition (PT) 数据集；它根据试验在监管过程中的进展对试验进行标记，并作为 CTOP 评估的基准。我们经过微调的基于 GPT-3.5 的模型 (CTP-LLM) 通过分析试验的原始方案文本来预测临床试验阶段转变，而无需人工选择的特征。CTP-LLM 在预测所有阶段的试验阶段转变方面实现了 67% 的准确率，在预测从第 III 阶段到最终批准的转变方面实现了 75% 的准确率。我们的实验表现凸显了 LLM 驱动的应用程序在预测临床试验结果和评估试验设计方面的潜力。</li>
</ul>

<h3>Title: While GitHub Copilot Excels at Coding, Does It Ensure Responsible Output?</h3>
<ul>
<li><strong>Authors: </strong>Wen Cheng, Ke Sun, Xinyu Zhang, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.11006">https://arxiv.org/abs/2408.11006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.11006">https://arxiv.org/pdf/2408.11006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.11006]] While GitHub Copilot Excels at Coding, Does It Ensure Responsible Output?(https://arxiv.org/abs/2408.11006)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The rapid development of large language models (LLMs) has significantly advanced code completion capabilities, giving rise to a new generation of LLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these tools possess unique workflows, integrating multiple information sources as input and prioritizing code suggestions over natural language interaction, which introduces distinct security challenges. Additionally, LCCTs often rely on proprietary code datasets for training, raising concerns about the potential exposure of sensitive data. This paper exploits these distinct characteristics of LCCTs to develop targeted attack methodologies on two critical security risks: jailbreaking and training data extraction attacks. Our experimental results expose significant vulnerabilities within LCCTs, including a 99.4% success rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate on Amazon Q. Furthermore, We successfully extracted sensitive user data from GitHub Copilot, including 54 real email addresses and 314 physical addresses associated with GitHub usernames. Our study also demonstrates that these code-based attack methods are effective against general-purpose LLMs, such as the GPT series, highlighting a broader security misalignment in the handling of code by modern LLMs. These findings underscore critical security challenges associated with LCCTs and suggest essential directions for strengthening their security frameworks. The example code and attack samples from our research are provided at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的快速发展显著提高了代码补全能力，催生了新一代基于 LLM 的代码补全工具 (LCCT)。与通用的 LLM 不同，这些工具具有独特的工作流程，集成多个信息源作为输入，并优先考虑代码建议而不是自然语言交互，这带来了明显的安全挑战。此外，LCCT 通常依赖专有代码数据集进行训练，这引发了对敏感数据潜在暴露的担忧。本文利用 LCCT 的这些独特特性，针对两个关键安全风险开发了针对性攻击方法：越狱和训练数据提取攻击。我们的实验结果揭示了 LCCT 中存在重大漏洞，包括对 GitHub Copilot 的越狱攻击成功率为 99.4%，对 Amazon Q 的成功率为 46.3%。此外，我们成功地从 GitHub Copilot 中提取了敏感用户数据，包括与 GitHub 用户名相关的 54 个真实电子邮件地址和 314 个物理地址。我们的研究还表明，这些基于代码的攻击方法对通用 LLM（例如 GPT 系列）有效，凸显了现代 LLM 在处理代码时存在更广泛的安全错位。这些发现强调了与 LCCT 相关的关键安全挑战，并提出了加强其安全框架的重要方向。我们研究中的示例代码和攻击样本在此 https URL 中提供。</li>
</ul>

<h3>Title: Athena: Safe Autonomous Agents with Verbal Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Tanmana Sadhu, Ali Pesaranghader, Yanan Chen, Dong Hoon Yi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.11021">https://arxiv.org/abs/2408.11021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.11021">https://arxiv.org/pdf/2408.11021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.11021]] Athena: Safe Autonomous Agents with Verbal Contrastive Learning(https://arxiv.org/abs/2408.11021)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Due to emergent capabilities, large language models (LLMs) have been utilized as language-based agents to perform a variety of tasks and make decisions with an increasing degree of autonomy. These autonomous agents can understand high-level instructions, interact with their environments, and execute complex tasks using a selection of tools available to them. As the capabilities of the agents expand, ensuring their safety and trustworthiness becomes more imperative. In this study, we introduce the Athena framework which leverages the concept of verbal contrastive learning where past safe and unsafe trajectories are used as in-context (contrastive) examples to guide the agent towards safety while fulfilling a given task. The framework also incorporates a critiquing mechanism to guide the agent to prevent risky actions at every step. Furthermore, due to the lack of existing benchmarks on the safety reasoning ability of LLM-based agents, we curate a set of 80 toolkits across 8 categories with 180 scenarios to provide a safety evaluation benchmark. Our experimental evaluation, with both closed- and open-source LLMs, indicates verbal contrastive learning and interaction-level critiquing improve the safety rate significantly.</li>
<li><strong>摘要：</strong>由于新兴能力，大型语言模型 (LLM) 已被用作基于语言的代理，以执行各种任务并以越来越高的自主性做出决策。这些自主代理可以理解高级指令、与环境交互并使用可用的一系列工具执行复杂任务。随着代理能力的扩展，确保其安全性和可信度变得更加重要。在本研究中，我们引入了 Athena 框架，该框架利用言语对比学习的概念，其中过去安全和不安全的轨迹被用作上下文（对比）示例，以在完成给定任务的同时引导代理走向安全。该框架还包含一个批评机制，以指导代理在每一步都防止冒险行为。此外，由于缺乏基于 LLM 的代理安全推理能力的现有基准，我们整理了一组 80 个工具包，涵盖 8 个类别和 180 个场景，以提供安全评估基准。我们对闭源和开源 LLM 进行的实验评估表明，言语对比学习和交互级别的批评显著提高了安全率。</li>
</ul>

<h3>Title: Scaling Law with Learning Rate Annealing</h3>
<ul>
<li><strong>Authors: </strong>Howe Tissue, Venus Wang, Lu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.11029">https://arxiv.org/abs/2408.11029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.11029">https://arxiv.org/pdf/2408.11029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.11029]] Scaling Law with Learning Rate Annealing(https://arxiv.org/abs/2408.11029)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps ($s$): $$L(s) = L_0 + A\cdot S_1^{-\alpha} - C\cdot S_2$$ Where $S_1$ is forward area and $S_2$ is learning rate annealing area. This formulation takes into account two factors: (1) The forward scaling defined as typical scaling law, and (2) the additional loss drop brought by LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training. Applying the scaling law with LR annealing and fitting only one or two training curves, we can accurately predict the loss of language model training at any given step and across any learning rate scheduler (LRS). Furthermore, this equation accurately describes the dynamics during training process, and provides a theoretical verification and explanation for numerous experimental findings of previous studies, particularly those focusing on LR schedule and LR annealing. The resulting insights, also serve as a guide for researchers to select critical LRS in advance by prediction using our equation. Most significantly, since all the points in a full training curve follow the equation, we can achieve accurate loss prediction at any given step across any learning rate scheduler, while expending less than 1\% of the computational cost required by the chinchilla scaling law to fit language modeling loss. This approach extremely democratizes scaling law fitting and predicting in developing large language models.</li>
<li><strong>摘要：</strong>我们发现，神经语言模型的交叉熵损失曲线在训练步骤（s$）上遵循具有学习率（LR）退火的缩放定律：$$L(s) = L_0 + A\cdot S_1^{-\alpha} - C\cdot S_2$$其中$S_1$是前向区域，$S_2$是学习率退火区域。该公式考虑了两个因素：（1）定义为典型缩放定律的前向缩放，（2）LR 退火带来的额外损失下降。因此，该公式可以描述每一步的完整损失曲线，而不是训练结束时的单个损失点。应用具有 LR 退火的缩放定律并仅拟合一条或两条训练曲线，我们可以准确预测任何给定步骤和任何学习率调度程序（LRS）的语言模型训练损失。此外，该方程准确地描述了训练过程中的动态，并为先前研究中的大量实验结果提供了理论验证和解释，特别是那些专注于 LR 调度和 LR 退火的研究。由此得出的见解也可作为研究人员通过使用我们的方程进行预测提前选择关键 LRS 的指南。最重要的是，由于完整训练曲线中的所有点都遵循该方程，我们可以在任何学习率调度程序的任何给定步骤中实现准确的损失预测，同时花费的计算成本不到 chinchilla 缩放定律拟合语言建模损失所需计算成本的 1\%。这种方法极大地使开发大型语言模型的缩放定律拟合和预测变得民主化。</li>
</ul>

<h3>Title: Inside the Black Box: Detecting Data Leakage in Pre-trained Language Encoders</h3>
<ul>
<li><strong>Authors: </strong>Yuan Xin, Zheng Li, Ning Yu, Dingfan Chen, Mario Fritz, Michael Backes, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.11046">https://arxiv.org/abs/2408.11046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.11046">https://arxiv.org/pdf/2408.11046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.11046]] Inside the Black Box: Detecting Data Leakage in Pre-trained Language Encoders(https://arxiv.org/abs/2408.11046)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite being prevalent in the general field of Natural Language Processing (NLP), pre-trained language models inherently carry privacy and copyright concerns due to their nature of training on large-scale web-scraped data. In this paper, we pioneer a systematic exploration of such risks associated with pre-trained language encoders, specifically focusing on the membership leakage of pre-training data exposed through downstream models adapted from pre-trained language encoders-an aspect largely overlooked in existing literature. Our study encompasses comprehensive experiments across four types of pre-trained encoder architectures, three representative downstream tasks, and five benchmark datasets. Intriguingly, our evaluations reveal, for the first time, the existence of membership leakage even when only the black-box output of the downstream model is exposed, highlighting a privacy risk far greater than previously assumed. Alongside, we present in-depth analysis and insights toward guiding future researchers and practitioners in addressing the privacy considerations in developing pre-trained language models.</li>
<li><strong>摘要：</strong>尽管预训练语言模型在自然语言处理 (NLP) 领域非常普遍，但由于其在从网络抓取的大规模数据上进行训练的性质，它本身就存在隐私和版权问题。在本文中，我们率先系统地探索了与预训练语言编码器相关的此类风险，特别关注通过从预训练语言编码器改编的下游模型暴露的预训练数据的成员泄漏 - 这是现有文献中很大程度上忽视的一个方面。我们的研究涵盖了四种类型的预训练编码器架构、三个代表性下游任务和五个基准数据集的全面实验。有趣的是，我们的评估首次揭示了即使只暴露下游模型的黑盒输出也存在成员泄漏，这突显了隐私风险远高于之前的假设。此外，我们还提供了深入的分析和见解，以指导未来的研究人员和从业者解决开发预训练语言模型时的隐私问题。</li>
</ul>

<h3>Title: MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.11049">https://arxiv.org/abs/2408.11049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.11049">https://arxiv.org/pdf/2408.11049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.11049]] MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding(https://arxiv.org/abs/2408.11049)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, chat, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency without sacrificing performance but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy speculative decoding more effectively for high throughput inference. Then, it leverages draft models with sparse KV cache to address the KV bottleneck that scales with both sequence length and batch size.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在长上下文应用程序（例如交互式聊天机器人、文档分析和代理工作流）中变得越来越普遍，但以低延迟和高吞吐量满足长上下文请求是一项挑战。推测解码 (SD) 是一种广泛使用的技术，可在不牺牲性能的情况下减少延迟，但传统观点认为其有效性仅限于小批量。在 MagicDec 中，我们展示了令人惊讶的是，即使对于中长序列的高吞吐量推理机制，SD 也可以实现加速。更有趣的是，根据我们严格的分析，智能起草策略可以随着批量大小的增加而实现更好的加速。MagicDec 首先识别出随着批量大小和序列长度的增加而发生的瓶颈变化，并利用这些见解更有效地部署推测解码以实现高吞吐量推理。然后，它利用具有稀疏 KV 缓存的起草模型来解决随着序列长度和批量大小而扩展的 KV 瓶颈。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
