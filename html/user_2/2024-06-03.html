<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-06-03</h1>
<h3>Title: Title:
          Small Language Models for Application Interactions: A Case Study</h3>
<ul>
<li><strong>Authors: </strong>Beibin Li, Yi Zhang, Sébastien Bubeck, Jeevan Pathuri, Ishai Menache</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Small Language Models for Application Interactions: A Case Study(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We study the efficacy of Small Language Models (SLMs) in facilitating application usage through natural language interactions. Our focus here is on a particular internal application used in Microsoft for cloud supply chain fulfilment. Our experiments show that small models can outperform much larger ones in terms of both accuracy and running time, even when fine-tuned on small datasets. Alongside these results, we also highlight SLM-based system design considerations.</li>
<li><strong>摘要：</strong>我们研究了小型语言模型 (SLM) 通过自然语言交互促进应用程序使用的有效性。我们重点研究了 Microsoft 用于云供应链履行的特定内部应用程序。我们的实验表明，即使在小型数据集上进行微调，小型模型在准确性和运行时间方面也可以胜过大型模型。除了这些结果之外，我们还强调了基于 SLM 的系统设计注意事项。</li>
</ul>

<h3>Title: Title:
          Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools</h3>
<ul>
<li><strong>Authors: </strong>Varun Magesh, Faiz Surani, Matthew Dahl, Mirac Suzgun, Christopher D. Manning, Daniel E. Ho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, hallucination, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to "hallucinate," or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as "eliminating" (Casetext, 2023) or "avoid[ing]" hallucinations (Thomson Reuters, 2023), or guaranteeing "hallucination-free" legal citations (LexisNexis, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. In this article, we design and report on the first preregistered empirical evaluation of AI-driven legal research tools. We demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), we find that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17% and 33% of the time. We also document substantial differences between systems in responsiveness and accuracy. Our article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law.</li>
<li><strong>摘要：</strong>法律实践中，人工智能 (AI) 产品的数量急剧增加。这些工具旨在协助完成一系列核心法律任务，从搜索和总结案例到文件起草。但这些工具中使用的大型语言模型容易“产生幻觉”，即编造虚假信息，因此在高风险领域使用它们存在风险。最近，某些法律研究提供商吹捧检索增强生成 (RAG) 等方法可以“消除”幻觉（Casetext，2023 年）或“避免”幻觉（Thomson Reuters，2023 年），或保证“无幻觉”的法律引文（LexisNexis，2023 年）。由于这些系统的封闭性，系统地评估这些说法具有挑战性。在本文中，我们设计并报告了对人工智能驱动的法律研究工具的首次预注册实证评估。我们证明提供商的说法是夸大其词。虽然与通用聊天机器人（GPT-4）相比，幻觉有所减少，但我们发现 LexisNexis（Lexis+ AI）和汤森路透（Westlaw AI-Assisted Research 和 Ask Practical Law AI）制作的人工智能研究工具均有 17% 至 33% 的时间出现幻觉。我们还记录了系统在响应能力和准确性方面的巨大差异。我们的文章做出了四个主要贡献。这是第一篇评估和报告基于 RAG 的专有法律人工智能工具性能的文章。其次，它引入了一个全面的、预先注册的数据集，用于识别和了解这些系统中的漏洞。第三，它提出了一种清晰的类型学来区分幻觉和准确的法律反应。最后，它提供了证据来告知法律专业人员在监督和验证人工智能输出方面的责任，这仍然是人工智能负责任地融入法律的一个核心悬而未决的问题。</li>
</ul>

<h3>Title: Title:
          XPrompt:Explaining Large Language Model's Generation via Joint Prompt Attribution</h3>
<ul>
<li><strong>Authors: </strong>Yurui Chang, Bochuan Cao, Yujia Wang, Jinghui Chen, Lu Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          XPrompt:Explaining Large Language Model's Generation via Joint Prompt Attribution(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive performances in complex text generation tasks. However, the contribution of the input prompt to the generated content still remains obscure to humans, underscoring the necessity of elucidating and explaining the causality between input and output pairs. Existing works for providing prompt-specific explanation often confine model output to be classification or next-word prediction. Few initial attempts aiming to explain the entire language generation often treat input prompt texts independently, ignoring their combinatorial effects on the follow-up generation. In this study, we introduce a counterfactual explanation framework based on joint prompt attribution, XPrompt, which aims to explain how a few prompt texts collaboratively influences the LLM's complete generation. Particularly, we formulate the task of prompt attribution for generation interpretation as a combinatorial optimization problem, and introduce a probabilistic algorithm to search for the casual input combination in the discrete space. We define and utilize multiple metrics to evaluate the produced explanations, demonstrating both faithfulness and efficiency of our framework.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在复杂的文本生成任务中表现出色。然而，输入提示对生成内容的贡献对人类来说仍然模糊不清，这强调了阐明和解释输入和输出对之间因果关系的必要性。现有的提供提示特定解释的工作通常将模型输出限制为分类或下一个单词预测。最初试图解释整个语言生成的一些尝试通常将输入提示文本独立处理，而忽略它们对后续生成的组合影响。在本研究中，我们引入了一个基于联合提示归因的反事实解释框架 XPrompt，旨在解释一些提示文本如何协同影响 LLM 的完整生成。具体而言，我们将生成解释的提示归因任务表述为组合优化问题，并引入一个概率算法来搜索离散空间中的因果输入组合。我们定义并使用多个指标来评估生成的解释，证明了我们框架的忠实性和效率。</li>
</ul>

<h3>Title: Title:
          SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Gong, Bandhav Veluri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.</li>
<li><strong>摘要：</strong>富有表现力的语音到语音翻译 (S2ST) 是无缝通信中的一个关键研究课题，其重点是在翻译语音中保留语义和说话者的声音风格。早期的研究合成了说话者风格对齐的语音，以便直接学习从语音到目标语音频谱图的映射。最近的研究不依赖于风格对齐的数据，而是利用语言建模 (LM) 的进步，并在语义和声学标记上构建级联 LM。这项工作提出了 SeamlessExpressiveLM，一个用于富有表现力的 S2ST 的单一语音语言模型。我们将复杂的源到目标语音映射分解为具有思路链提示的中间生成步骤。首先引导该模型翻译目标语义内容，然后将说话者风格转移到多流声学单元。在西班牙语到英语和匈牙利语到英语的翻译上进行评估，SeamlessExpressiveLM 在语义质量和风格转换方面均优于级联 LM，同时实现了更好的参数效率。</li>
</ul>

<h3>Title: Title:
          Automated Focused Feedback Generation for Scientific Writing Assistance</h3>
<ul>
<li><strong>Authors: </strong>Eric Chamoun, Michael Schlichktrull, Andreas Vlachos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Automated Focused Feedback Generation for Scientific Writing Assistance(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Scientific writing is a challenging task, particularly for novice researchers who often rely on feedback from experienced peers. Recent work has primarily focused on improving surface form and style rather than manuscript content. In this paper, we propose a novel task: automated focused feedback generation for scientific writing assistance. We present SWIF$^{2}$T: a Scientific WrIting Focused Feedback Tool. It is designed to generate specific, actionable and coherent comments, which identify weaknesses in a scientific paper and/or propose revisions to it. Our approach consists of four components - planner, investigator, reviewer and controller - leveraging multiple Large Language Models (LLMs) to implement them. We compile a dataset of 300 peer reviews citing weaknesses in scientific papers and conduct human evaluation. The results demonstrate the superiority in specificity, reading comprehension, and overall helpfulness of SWIF$^{2}$T's feedback compared to other approaches. In our analysis, we also identified cases where automatically generated reviews were judged better than human ones, suggesting opportunities for integration of AI-generated feedback in scientific writing.</li>
<li><strong>摘要：</strong>科学写作是一项具有挑战性的任务，尤其是对于经常依赖经验丰富的同行反馈的新手研究人员而言。最近的工作主要集中在改进表面形式和风格而不是手稿内容。在本文中，我们提出了一项新颖的任务：自动生成重点反馈以协助科学写作。我们介绍了 SWIF$^{2}$T：一种科学写作重点反馈工具。它旨在生成具体、可操作且连贯的评论，以识别科学论文中的弱点和/或提出修改意见。我们的方法由四个部分组成 - 规划者、调查者、审阅者和控制者 - 利用多个大型语言模型 (LLM) 来实现它们。我们汇编了一个包含 300 篇同行评审的数据集，其中引用了科学论文中的弱点并进行了人工评估。结果表明，与其他方法相比，SWIF$^{2}$T 的反馈在特异性、阅读理解和整体帮助方面具有优势。在我们的分析中，我们还发现了自动生成的评论被评判为比人工评论更好的案例，这表明有机会将人工智能生成的反馈整合到科学写作中。</li>
</ul>

<h3>Title: Title:
          Transfer Q Star: Principled Decoding for LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Souradip Chakraborty, Soumya Suvra Ghosal, Ming Yin, Dinesh Manocha, Mengdi Wang, Amrit Singh Bedi, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Transfer Q Star: Principled Decoding for LLM Alignment(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Aligning foundation models is essential for their safe and trustworthy deployment. However, traditional fine-tuning methods are computationally intensive and require updating billions of model parameters. A promising alternative, alignment via decoding, adjusts the response distribution directly without model updates to maximize a target reward $r$, thus providing a lightweight and adaptable framework for alignment. However, principled decoding methods rely on oracle access to an optimal Q-function ($Q^*$), which is often unavailable in practice. Hence, prior SoTA methods either approximate this $Q^*$ using $Q^{\pi_{\texttt{sft}}}$ (derived from the reference $\texttt{SFT}$ model) or rely on short-term rewards, resulting in sub-optimal decoding performance. In this work, we propose Transfer $Q^*$, which implicitly estimates the optimal value function for a target reward $r$ through a baseline model $\rho_{\texttt{BL}}$ aligned with a baseline reward $\rho_{\texttt{BL}}$ (which can be different from the target reward $r$). Theoretical analyses of Transfer $Q^*$ provide a rigorous characterization of its optimality, deriving an upper bound on the sub-optimality gap and identifying a hyperparameter to control the deviation from the pre-trained reference $\texttt{SFT}$ model based on user needs. Our approach significantly reduces the sub-optimality gap observed in prior SoTA methods and demonstrates superior empirical performance across key metrics such as coherence, diversity, and quality in extensive tests on several synthetic and real datasets.</li>
<li><strong>摘要：</strong>对齐基础模型对于其安全可靠的部署至关重要。然而，传统的微调方法计算量大，需要更新数十亿个模型参数。一种有前途的替代方法是通过解码进行对齐，它直接调整响应分布而无需模型更新以最大化目标奖励 $r$，从而为对齐提供了一个轻量级且适应性强的框架。然而，原则性解码方法依赖于对最优 Q 函数 ($Q^*$) 的 oracle 访问，这在实践中通常是不可用的。因此，先前的 SoTA 方法要么使用 $Q^{\pi_{\texttt{sft}}}$（源自参考 $\texttt{SFT}$ 模型）来近似这个 $Q^*$，要么依赖短期奖励，导致解码性能不佳。在本研究中，我们提出了 Transfer $Q^*$，它通过与基线奖励 $\rho_{\texttt{BL}}$（可能与目标奖励 $r$ 不同）对齐的基线模型 $\rho_{\texttt{BL}}$ 隐式估计目标奖励 $r$ 的最佳值函数。Transfer $Q^*$ 的理论分析对其最优性进行了严格的表征，推导出次优差距的上限，并根据用户需求确定超参数以控制与预训练参考 $\texttt{SFT}$ 模型的偏差。我们的方法显著减少了先前 SoTA 方法中观察到的次优差距，并在对多个合成和真实数据集的广泛测试中展示了关键指标（如连贯性、多样性和质量）的卓越经验性能。</li>
</ul>

<h3>Title: Title:
          SPOT: Text Source Prediction from Originality Score Thresholding</h3>
<ul>
<li><strong>Authors: </strong>Edouard Yvinec, Gabriel Kasser</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          SPOT: Text Source Prediction from Originality Score Thresholding(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The wide acceptance of large language models (LLMs) has unlocked new applications and social risks. Popular countermeasures aim at detecting misinformation, usually involve domain specific models trained to recognize the relevance of any information. Instead of evaluating the validity of the information, we propose to investigate LLM generated text from the perspective of trust. In this study, we define trust as the ability to know if an input text was generated by a LLM or a human. To do so, we design SPOT, an efficient method, that classifies the source of any, standalone, text input based on originality score. This score is derived from the prediction of a given LLM to detect other LLMs. We empirically demonstrate the robustness of the method to the architecture, training data, evaluation data, task and compression of modern LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的广泛接受开启了新的应用和社会风险。流行的对策旨在检测错误信息，通常涉及训练以识别任何信息相关性的特定领域模型。我们建议从信任的角度研究 LLM 生成的文本，而不是评估信息的有效性。在本研究中，我们将信任定义为知道输入文本是由 LLM 还是人生成的能力。为此，我们设计了一种有效的方法 SPOT，它根据原创性分数对任何独立文本输入的来源进行分类。该分数来自给定 LLM 检测其他 LLM 的预测。我们通过经验证明了该方法对现代 LLM 的架构、训练数据、评估数据、任务和压缩的稳健性。</li>
</ul>

<h3>Title: Title:
          How Multilingual Are Large Language Models Fine-Tuned for Translation?</h3>
<ul>
<li><strong>Authors: </strong>Aquia Richburg, Marine Carpuat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          How Multilingual Are Large Language Models Fine-Tuned for Translation?(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>A new paradigm for machine translation has recently emerged: fine-tuning large language models (LLM) on parallel text has been shown to outperform dedicated translation systems trained in a supervised fashion on much larger amounts of parallel data (Xu et al., 2024a; Alves et al., 2024). However, it remains unclear whether this paradigm can enable massively multilingual machine translation or whether it requires fine-tuning dedicated models for a small number of language pairs. How does translation fine-tuning impact the MT capabilities of LLMs for zero-shot languages, zero-shot language pairs, and translation tasks that do not involve English? To address these questions, we conduct an extensive empirical evaluation of the translation quality of the TOWER family of language models (Alves et al., 2024) on 132 translation tasks from the multi-parallel FLORES-200 data. We find that translation fine-tuning improves translation quality even for zero-shot languages on average, but that the impact is uneven depending on the language pairs involved. These results call for further research to effectively enable massively multilingual translation with LLMs.</li>
<li><strong>摘要：</strong>最近出现了一种新的机器翻译范式：在并行文本上微调大型语言模型 (LLM) 已被证明比在大量并行数据上以监督方式训练的专用翻译系统表现更好 (Xu et al., 2024a; Alves et al., 2024)。然而，尚不清楚这种范式是否可以实现大规模多语言机器翻译，或者是否需要对少量语言对的专用模型进行微调。翻译微调如何影响 LLM 对于零样本语言、零样本语言对和不涉及英语的翻译任务的 MT 能力？为了回答这些问题，我们对来自多并行 FLORES-200 数据的 132 个翻译任务对 TOWER 系列语言模型 (Alves et al., 2024) 的翻译质量进行了广泛的实证评估。我们发现，即使对于零样本语言，翻译微调也能提高翻译质量，但影响并不均匀，具体取决于所涉及的语言对。这些结果需要进一步研究，以便有效地使用 LLM 实现大规模多语言翻译。</li>
</ul>

<h3>Title: Title:
          Towards Ontology-Enhanced Representation Learning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Francesco Ronzano, Jay Nanavati</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Towards Ontology-Enhanced Representation Learning for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Taking advantage of the widespread use of ontologies to organise and harmonize knowledge across several distinct domains, this paper proposes a novel approach to improve an embedding-Large Language Model (embedding-LLM) of interest by infusing the knowledge formalized by a reference ontology: ontological knowledge infusion aims at boosting the ability of the considered LLM to effectively model the knowledge domain described by the infused ontology. The linguistic information (i.e. concept synonyms and descriptions) and structural information (i.e. is-a relations) formalized by the ontology are utilized to compile a comprehensive set of concept definitions, with the assistance of a powerful generative LLM (i.e. GPT-3.5-turbo). These concept definitions are then employed to fine-tune the target embedding-LLM using a contrastive learning framework. To demonstrate and evaluate the proposed approach, we utilize the biomedical disease ontology MONDO. The results show that embedding-LLMs enhanced by ontological disease knowledge exhibit an improved capability to effectively evaluate the similarity of in-domain sentences from biomedical documents mentioning diseases, without compromising their out-of-domain performance.</li>
<li><strong>摘要：</strong>利用本体在组织和协调多个不同领域的知识方面的广泛应用，本文提出了一种新方法，通过注入由参考本体形式化的知识来改进感兴趣的嵌入大型语言模型 (embedding-LLM)：本体知识注入旨在提高所考虑的 LLM 有效建模注入本体所描述的知识领域的能力。在强大的生成式 LLM（即 GPT-3.5-turbo）的帮助下，利用本体形式化的语言信息（即概念同义词和描述）和结构信息（即 is-a 关系）来编译一套全面的概念定义。然后使用这些概念定义通过对比学习框架对目标嵌入 LLM 进行微调。为了演示和评估所提出的方法，我们使用了生物医学疾病本体 MONDO。结果表明，通过本体疾病知识增强的嵌入 LLM 表现出更好的能力，可以有效评估提及疾病的生物医学文献中的域内句子的相似性，同时不会影响其域外性能。</li>
</ul>

<h3>Title: Title:
          Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Chanjun Park, Hyeonwoo Kim, Dahyun Kim, Seonghwan Cho, Sanghoon Kim, Sukyung Lee, Yungi Kim, Hwalsuk Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.</li>
<li><strong>摘要：</strong>本文介绍了开放 Ko-LLM 排行榜和 Ko-H5 基准，它们是评估韩语大型语言模型 (LLM) 的重要工具。通过结合私人测试集并模仿英语开放 LLM 排行榜，我们建立了一个强大的评估框架，该框架已在韩语 LLM 社区中得到很好的整合。我们进行了数据泄漏分析，以显示私人测试集的好处，同时进行了 Ko-H5 基准内的相关性研究和 Ko-H5 分数的时间分析。此外，我们还提供了实证支持，以证明需要超越既定基准。我们希望开放 Ko-LLM 排行榜能够为扩大 LLM 评估以促进更多语言多样性树立先例。</li>
</ul>

<h3>Title: Title:
          The Point of View of a Sentiment: Towards Clinician Bias Detection in Psychiatric Notes</h3>
<ul>
<li><strong>Authors: </strong>Alissa A. Valentine, Lauren A. Lepow, Alexander W. Charney, Isotta Landi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          The Point of View of a Sentiment: Towards Clinician Bias Detection in Psychiatric Notes(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>In psychiatry, negative patient descriptions and stigmatizing language can contribute to healthcare disparities in two ways: (1) read by patients they can harm their trust and engagement with the medical center; (2) read by future providers they may negatively influence the future perspective of a patient. By leveraging large language models, this work aims to identify the sentiment expressed in psychiatric clinical notes based on the reader's point of view. Extracting sentences from the Mount Sinai Health System's large and diverse clinical notes, we used prompts and in-context learning to adapt three large language models (GPT-3.5, Llama 2, Mistral) to classify the sentiment conveyed by the sentences according to the provider or non-provider point of view. Results showed that GPT-3.5 aligns best to provider point of view, whereas Mistral aligns best to non-provider point of view.</li>
<li><strong>摘要：</strong>在精神病学中，对患者的负面描述和污蔑性语言会通过两种方式造成医疗保健差距：（1）患者阅读时，可能会损害他们对医疗中心的信任和参与度；（2）未来的医疗服务提供者阅读时，可能会对患者的未来观点产生负面影响。通过利用大型语言模型，这项工作旨在根据读者的视角识别精神病临床笔记中表达的情绪。我们从西奈山医疗系统大量且多样化的临床笔记中提取句子，使用提示和上下文学习来调整三个大型语言模型（GPT-3.5、Llama 2、Mistral），以根据提供者或非提供者的视角对句子传达的情绪进行分类。结果表明，GPT-3.5 最符合提供者的观点，而 Mistral 最符合非提供者的观点。</li>
</ul>

<h3>Title: Title:
          GAMedX: Generative AI-based Medical Entity Data Extractor Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammed-Khalil Ghali, Abdelrahman Farrag, Hajar Sakai, Hicham El Baz, Yu Jin, Sarah Lam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          GAMedX: Generative AI-based Medical Entity Data Extractor Using Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving field of healthcare and beyond, the integration of generative AI in Electronic Health Records (EHRs) represents a pivotal advancement, addressing a critical gap in current information extraction techniques. This paper introduces GAMedX, a Named Entity Recognition (NER) approach utilizing Large Language Models (LLMs) to efficiently extract entities from medical narratives and unstructured text generated throughout various phases of the patient hospital visit. By addressing the significant challenge of processing unstructured medical text, GAMedX leverages the capabilities of generative AI and LLMs for improved data extraction. Employing a unified approach, the methodology integrates open-source LLMs for NER, utilizing chained prompts and Pydantic schemas for structured output to navigate the complexities of specialized medical jargon. The findings reveal significant ROUGE F1 score on one of the evaluation datasets with an accuracy of 98\%. This innovation enhances entity extraction, offering a scalable, cost-effective solution for automated forms filling from unstructured data. As a result, GAMedX streamlines the processing of unstructured narratives, and sets a new standard in NER applications, contributing significantly to theoretical and practical advancements beyond the medical technology sphere.</li>
<li><strong>摘要：</strong>在快速发展的医疗保健及其他领域，生成式人工智能与电子健康记录 (EHR) 的整合代表着一项关键进步，填补了当前信息提取技术的一个关键空白。本文介绍了 GAMedX，这是一种命名实体识别 (NER) 方法，利用大型语言模型 (LLM) 有效地从患者就诊各个阶段生成的医学叙述和非结构化文本中提取实体。通过解决处理非结构化医学文本的重大挑战，GAMedX 利用生成式人工智能和 LLM 的功能来改进数据提取。该方法采用统一的方法，将开源 LLM 集成到 NER 中，利用链式提示和 Pydantic 模式进行结构化输出，以应对专业医学术语的复杂性。研究结果显示，其中一个评估数据集的 ROUGE F1 得分显著，准确率为 98%。这项创新增强了实体提取，为从非结构化数据自动填写表格提供了一种可扩展、经济高效的解决方案。因此，GAMedX 简化了非结构化叙述的处理，并为 NER 应用树立了新标准，为医疗技术领域以外的理论和实践进步做出了重大贡献。</li>
</ul>

<h3>Title: Title:
          DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Taolin Zhang, Qizhou Chen, Dongyang Li, Chengyu Wang, Xiaofeng He, Longtao Huang, Hui Xue, Jun Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Recently, while large language models (LLMs) have demonstrated impressive results, they still suffer from hallucination, i.e., the generation of false information. Model editing is the task of fixing factual mistakes in LLMs; yet, most previous works treat it as a one-time task, paying little attention to ever-emerging mistakes generated by LLMs. We address the task of sequential model editing (SME) that aims to rectify mistakes continuously. A Dynamic Auxiliary Fusion Network (DAFNet) is designed to enhance the semantic interaction among the factual knowledge within the entire sequence, preventing catastrophic forgetting during the editing process of multiple knowledge triples. Specifically, (1) for semantic fusion within a relation triple, we aggregate the intra-editing attention flow into auto-regressive self-attention with token-level granularity in LLMs. We further leverage multi-layer diagonal inter-editing attention flow to update the weighted representations of the entire sequence-level granularity. (2) Considering that auxiliary parameters are required to store the knowledge for sequential editing, we construct a new dataset named \textbf{DAFSet}, fulfilling recent, popular, long-tail and robust properties to enhance the generality of sequential editing. Experiments show DAFNet significantly outperforms strong baselines in single-turn and sequential editing. The usage of DAFSet also consistently improves the performance of other auxiliary network-based methods in various scenarios</li>
<li><strong>摘要：</strong>最近，虽然大型语言模型 (LLM) 已经展示了令人瞩目的成果，但它们仍然存在幻觉，即生成虚假信息。模型编辑是修复 LLM 中的事实错误的任务；然而，之前的大多数工作都将其视为一次性任务，很少关注 LLM 不断产生的错误。我们解决了顺序模型编辑 (SME) 的任务，旨在持续纠正错误。动态辅助融合网络 (DAFNet) 旨在增强整个序列中事实知识之间的语义交互，防止在编辑多个知识三元组的过程中发生灾难性遗忘。具体而言，(1) 对于关系三元组中的语义融合，我们将编辑内注意流聚合到 LLM 中具有 token 级粒度的自回归自注意中。我们进一步利用多层对角线编辑间注意流来更新整个序列级粒度的加权表示。 (2) 考虑到需要辅助参数来存储顺序编辑的知识，我们构建了一个名为 \textbf{DAFSet} 的新数据集，该数据集满足近期、流行、长尾和稳健的特性，以增强顺序编辑的通用性。实验表明，DAFNet 在单转和顺序编辑方面的表现明显优于强基线。DAFSet 的使用也在各种场景中持续提高其他基于辅助网络的方法的性能</li>
</ul>

<h3>Title: Title:
          UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Hanzhang Zhou, Zijian Feng, Zixiao Zhu, Junlang Qian, Kezhi Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive capabilities in various tasks using the in-context learning (ICL) paradigm. However, their effectiveness is often compromised by inherent bias, leading to prompt brittleness, i.e., sensitivity to design settings such as example selection, order, and prompt formatting. Previous studies have addressed LLM bias through external adjustment of model outputs, but the internal mechanisms that lead to such bias remain unexplored. Our work delves into these mechanisms, particularly investigating how feedforward neural networks (FFNs) and attention heads result in the bias of LLMs. By Interpreting the contribution of individual FFN vectors and attention heads, we identify the biased LLM components that skew LLMs' prediction toward specific labels. To mitigate these biases, we introduce UniBias, an inference-only method that effectively identifies and eliminates biased FFN vectors and attention heads. Extensive experiments across 12 NLP datasets demonstrate that UniBias significantly enhances ICL performance and alleviates prompt brittleness of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已在使用上下文学习 (ICL) 范式的各种任务中展现出令人印象深刻的能力。然而，它们的有效性往往受到固有偏见的影响，导致提示脆弱性，即对设计设置（例如示例选择、顺序和提示格式）的敏感性。先前的研究通过对模型输出进行外部调整来解决 LLM 偏见，但导致这种偏见的内部机制仍未得到探索。我们的工作深入研究了这些机制，特别是研究了前馈神经网络 (FFN) 和注意力头如何导致 LLM 的偏见。通过解释单个 FFN 向量和注意力头的贡献，我们确定了导致 LLM 预测偏向特定标签的有偏见的 LLM 组件。为了减轻这些偏见，我们引入了 UniBias，这是一种仅推理的方法，可以有效识别和消除有偏见的 FFN 向量和注意力头。在 12 个 NLP 数据集上进行的大量实验表明，UniBias 显着提高了 ICL 性能并减轻了 LLM 的提示脆弱性。</li>
</ul>

<h3>Title: Title:
          FineRadScore: A Radiology Report Line-by-Line Evaluation Technique Generating Corrections with Severity Scores</h3>
<ul>
<li><strong>Authors: </strong>Alyssa Huang, Oishi Banerjee, Kay Wu, Eduardo Pontes Reis, Pranav Rajpurkar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          FineRadScore: A Radiology Report Line-by-Line Evaluation Technique Generating Corrections with Severity Scores(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The current gold standard for evaluating generated chest x-ray (CXR) reports is through radiologist annotations. However, this process can be extremely time-consuming and costly, especially when evaluating large numbers of reports. In this work, we present FineRadScore, a Large Language Model (LLM)-based automated evaluation metric for generated CXR reports. Given a candidate report and a ground-truth report, FineRadScore gives the minimum number of line-by-line corrections required to go from the candidate to the ground-truth report. Additionally, FineRadScore provides an error severity rating with each correction and generates comments explaining why the correction was needed. We demonstrate that FineRadScore's corrections and error severity scores align with radiologist opinions. We also show that, when used to judge the quality of the report as a whole, FineRadScore aligns with radiologists as well as current state-of-the-art automated CXR evaluation metrics. Finally, we analyze FineRadScore's shortcomings to provide suggestions for future improvements.</li>
<li><strong>摘要：</strong>目前，评估生成的胸部 X 光 (CXR) 报告的黄金标准是通过放射科医生的注释。然而，这个过程可能非常耗时且成本高昂，尤其是在评估大量报告时。在这项工作中，我们提出了 FineRadScore，这是一种基于大型语言模型 (LLM) 的生成 CXR 报告的自动评估指标。给定一份候选报告和一份真实报告，FineRadScore 会给出从候选报告到真实报告所需的最少逐行更正次数。此外，FineRadScore 会为每次更正提供错误严重程度评级，并生成注释来解释为什么需要更正。我们证明 FineRadScore 的更正和错误严重程度分数与放射科医生的意见一致。我们还表明，当用来判断整个报告的质量时，FineRadScore 与放射科医生以及当前最先进的自动 CXR 评估指标一致。最后，我们分析了 FineRadScore 的缺点，为未来的改进提供了建议。</li>
</ul>

<h3>Title: Title:
          Leveraging Large Language Models for Entity Matching</h3>
<ul>
<li><strong>Authors: </strong>Qianyu Huang, Tongfang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Leveraging Large Language Models for Entity Matching(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Entity matching (EM) is a critical task in data integration, aiming to identify records across different datasets that refer to the same real-world entities. Traditional methods often rely on manually engineered features and rule-based systems, which struggle with diverse and unstructured data. The emergence of Large Language Models (LLMs) such as GPT-4 offers transformative potential for EM, leveraging their advanced semantic understanding and contextual capabilities. This vision paper explores the application of LLMs to EM, discussing their advantages, challenges, and future research directions. Additionally, we review related work on applying weak supervision and unsupervised approaches to EM, highlighting how LLMs can enhance these methods.</li>
<li><strong>摘要：</strong>实体匹配 (EM) 是数据集成中的一项关键任务，旨在识别不同数据集中指向相同现实世界实体的记录。传统方法通常依赖于手动设计的特征和基于规则的系统，而这些系统难以处理多样化和非结构化数据。GPT-4 等大型语言模型 (LLM) 的出现为 EM 提供了变革潜力，利用了它们先进的语义理解和上下文功能。本愿景论文探讨了 LLM 在 EM 中的应用，讨论了它们的优势、挑战和未来的研究方向。此外，我们还回顾了将弱监督和无监督方法应用于 EM 的相关工作，重点介绍了 LLM 如何增强这些方法。</li>
</ul>

<h3>Title: Title:
          Reward-based Input Construction for Cross-document Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Byeonghu Na, Suhyeon Jo, Yeongmin Kim, Il-Chul Moon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Reward-based Input Construction for Cross-document Relation Extraction(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Relation extraction (RE) is a fundamental task in natural language processing, aiming to identify relations between target entities in text. While many RE methods are designed for a single sentence or document, cross-document RE has emerged to address relations across multiple long documents. Given the nature of long documents in cross-document RE, extracting document embeddings is challenging due to the length constraints of pre-trained language models. Therefore, we propose REward-based Input Construction (REIC), the first learning-based sentence selector for cross-document RE. REIC extracts sentences based on relational evidence, enabling the RE module to effectively infer relations. Since supervision of evidence sentences is generally unavailable, we train REIC using reinforcement learning with RE prediction scores as rewards. Experimental results demonstrate the superiority of our method over heuristic methods for different RE structures and backbones in cross-document RE. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>关系提取 (RE) 是自然语言处理中的一项基本任务，旨在识别文本中目标实体之间的关系。虽然许多 RE 方法是针对单个句子或文档而设计的，但跨文档 RE 已经出现以解决跨多个长文档的关系。鉴于跨文档 RE 中长文档的性质，由于预训练语言模型的长度限制，提取文档嵌入具有挑战性。因此，我们提出了基于奖励的输入构造 (REIC)，这是第一个基于学习的跨文档 RE 句子选择器。REIC 根据关系证据提取句子，使 RE 模块能够有效地推断关系。由于证据句子的监督通常不可用，我们使用强化学习来训练 REIC，并以 RE 预测分数作为奖励。实验结果表明，对于跨文档 RE 中的不同 RE 结构和主干，我们的方法优于启发式方法。我们的代码可在此 https URL 上公开获取。</li>
</ul>

<h3>Title: Title:
          Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Wu, Zhiyuan Peng, Sravanthi Rajanala, Hsin-Tai Wu, Yi Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Effective passage retrieval and reranking methods have been widely utilized to identify suitable candidates in open-domain question answering tasks, recent studies have resorted to LLMs for reranking the retrieved passages by the log-likelihood of the question conditioned on each passage. Although these methods have demonstrated promising results, the performance is notably sensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs can be computationally intensive and time-consuming. Furthermore, this approach limits the leverage of question-passage relevance pairs and passage-specific knowledge to enhance the ranking capabilities of LLMs. In this paper, we propose passage-specific prompt tuning for reranking in open-domain question answering (PSPT): a parameter-efficient method that fine-tunes learnable passage-specific soft prompts, incorporating passage-specific knowledge from a limited set of question-passage relevance pairs. The method involves ranking retrieved passages based on the log-likelihood of the model generating the question conditioned on each passage and the learned soft prompt. We conducted extensive experiments utilizing the Llama-2-chat-7B model across three publicly available open-domain question answering datasets and the results demonstrate the effectiveness of the proposed approach.</li>
<li><strong>摘要：</strong>有效的段落检索和重新排序方法已被广泛用于在开放域问答任务中识别合适的候选人，最近的研究已经利用 LLM 对检索到的段落进行重新排序，方法是根据以每个段落为条件的问题的对数似然性。虽然这些方法已经显示出有希望的结果，但其性能对人工编写的提示（或硬提示）特别敏感，并且微调 LLM 可能需要大量计算且耗时。此外，这种方法限制了问题-段落相关性对和段落特定知识的利用，无法增强 LLM 的排名能力。在本文中，我们提出了针对段落的提示调整以在开放域问答 (PSPT) 中进行重新排序：一种参数高效的方法，可以微调可学习的段落特定软提示，结合来自有限问题-段落相关性对的段落特定知识。该方法涉及根据以每个段落为条件生成问题的模型的对数似然性和学习到的软提示对检索到的段落进行排名。我们利用 Llama-2-chat-7B 模型在三个公开的开放域问答数据集上进行了广泛实验，结果证明了所提出方法的有效性。</li>
</ul>

<h3>Title: Title:
          DORY: Deliberative Prompt Recovery for LLM</h3>
<ul>
<li><strong>Authors: </strong>Lirong Gao, Ru Peng, Yiming Zhang, Junbo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DORY: Deliberative Prompt Recovery for LLM(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Prompt recovery in large language models (LLMs) is crucial for understanding how LLMs work and addressing concerns regarding privacy, copyright, etc. The trend towards inference-only APIs complicates this task by restricting access to essential outputs for recovery. To tackle this challenge, we extract prompt-related information from limited outputs and identify a strong(negative) correlation between output probability-based uncertainty and the success of prompt recovery. This finding led to the development of Deliberative PrOmpt RecoverY (DORY), our novel approach that leverages uncertainty to recover prompts accurately. DORY involves reconstructing drafts from outputs, refining these with hints, and filtering out noise based on uncertainty. Our evaluation across diverse LLMs and prompt benchmarks shows that DORY outperforms existing baselines, improving performance by approximately 10.82% and establishing a new state-of-the-art record in prompt recovery tasks. Significantly, DORY operates using a single LLM without any external resources or model, offering a cost-effective, user-friendly prompt recovery solution.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 中的快速恢复对于理解 LLM 的工作原理以及解决隐私、版权等问题至关重要。仅推理 API 的趋势通过限制对恢复基本输出的访问使这项任务变得复杂。为了应对这一挑战，我们从有限的输出中提取与提示相关的信息，并确定基于输出概率的不确定性与快速恢复成功之间的强（负）相关性。这一发现促成了我们利用不确定性准确恢复提示的新方法“深思熟虑的快速恢复 (DORY)”。DORY 涉及从输出重建草稿，使用提示对其进行细化，并根据不确定性过滤掉噪音。我们对各种 LLM 和快速基准的评估表明，DORY 的表现优于现有基线，性能提高了约 10.82%，并在快速恢复任务中创造了新的最先进记录。值得注意的是，DORY 使用单个 LLM 运行，无需任何外部资源或模型，提供了一种经济高效、用户友好的快速恢复解决方案。</li>
</ul>

<h3>Title: Title:
          Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Pengwei Zhan, Zhen Xu, Qian Tan, Jie Song, Ru Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 表现出卓越的指令遵循能力，可以完成各种下游任务。尽管这种令人印象深刻的能力使 LLM 成为灵活的任务解决者，但它们在解决任务方面的表现也严重依赖于指令。在本文中，我们揭示了 LLM 对任务指令中的词汇变化过于敏感，即使这些变化对人类来说是不可察觉的。通过为模型提供邻近指令，这些指令在潜在表示空间中位置紧密，并且只有一个语义相似的单词不同，下游任务的性能可能会有很大差异。根据这一特性，我们提出了一个用于提示词汇增强 (COPLE) 的黑盒组合优化框架。COPLE 根据一批代理任务的反馈执行迭代词汇优化，使用与单词影响相关的搜索策略。实验表明，即使是当前基准测试中广泛使用的人工制作的提示也会受到模型词汇敏感性的影响，而 COPLE 恢复了模型在指令遵循和解决下游任务方面的下降能力。</li>
</ul>

<h3>Title: Title:
          It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance</h3>
<ul>
<li><strong>Authors: </strong>Laura Cabello, Uchenna Akujuobi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Aspect-Based Sentiment Analysis (ABSA) involves extracting opinions from textual data about specific entities and their corresponding aspects through various complementary subtasks. Several prior research has focused on developing ad hoc designs of varying complexities for these subtasks. In this paper, we present a generative framework extensible to any ABSA subtask. We build upon the instruction tuned model proposed by Scaria et al. (2023), who present an instruction-based model with task descriptions followed by in-context examples on ABSA subtasks. We propose PFInstruct, an extension to this instruction learning paradigm by appending an NLP-related task prefix to the task description. This simple approach leads to improved performance across all tested SemEval subtasks, surpassing previous state-of-the-art (SOTA) on the ATE subtask (Rest14) by +3.28 F1-score, and on the AOOE subtask by an average of +5.43 F1-score across SemEval datasets. Furthermore, we explore the impact of the prefix-enhanced prompt quality on the ABSA subtasks and find that even a noisy prefix enhances model performance compared to the baseline. Our method also achieves competitive results on a biomedical domain dataset (ERSA).</li>
<li><strong>摘要：</strong>基于方面的情感分析 (ABSA) 涉及通过各种互补的子任务从文本数据中提取有关特定实体及其相应方面的意见。先前的一些研究专注于为这些子任务开发不同复杂度的临时设计。在本文中，我们提出了一个可扩展到任何 ABSA 子任务的生成框架。我们以 Scaria 等人 (2023) 提出的指令调整模型为基础，他们提出了一个基于指令的模型，该模型包含任务描述，后面是 ABSA 子任务的上下文示例。我们提出了 PFInstruct，这是对该指令学习范式的扩展，通过在任务描述中附加与 NLP 相关的任务前缀。这种简单的方法可以提高所有测试的 SemEval 子任务的性能，在 ATE 子任务 (Rest14) 上超过之前的最新 (SOTA) +3.28 F1 分数，在 AOOE 子任务上超过之前的平均 +5.43 F1 分数（在 SemEval 数据集上）。此外，我们探索了前缀增强提示质量对 ABSA 子任务的影响，发现即使是嘈杂的前缀也能提高模型性能，与基线相比。我们的方法在生物医学领域数据集 (ERSA) 上也取得了有竞争力的结果。</li>
</ul>

<h3>Title: Title:
          Improving code-mixed hate detection by native sample mixing: A case study for Hindi-English code-mixed scenario</h3>
<ul>
<li><strong>Authors: </strong>Debajyoti Mazumder, Aakash Kumar, Jasabanta Patro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Improving code-mixed hate detection by native sample mixing: A case study for Hindi-English code-mixed scenario(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Hate detection has long been a challenging task for the NLP community. The task becomes complex in a code-mixed environment because the models must understand the context and the hate expressed through language alteration. Compared to the monolingual setup, we see very less work on code-mixed hate as large-scale annotated hate corpora are unavailable to make the study. To overcome this bottleneck, we propose using native language hate samples. We hypothesise that in the era of multilingual language models (MLMs), hate in code-mixed settings can be detected by majorly relying on the native language samples. Even though the NLP literature reports the effectiveness of MLMs on hate detection in many cross-lingual settings, their extensive evaluation in a code-mixed scenario is yet to be done. This paper attempts to fill this gap through rigorous empirical experiments. We considered the Hindi-English code-mixed setup as a case study as we have the linguistic expertise for the same. Some of the interesting observations we got are: (i) adding native hate samples in the code-mixed training set, even in small quantity, improved the performance of MLMs for code-mixed hate detection, (ii) MLMs trained with native samples alone observed to be detecting code-mixed hate to a large extent, (iii) The visualisation of attention scores revealed that, when native samples were included in training, MLMs could better focus on the hate emitting words in the code-mixed context, and (iv) finally, when hate is subjective or sarcastic, naively mixing native samples doesn't help much to detect code-mixed hate. We will release the data and code repository to reproduce the reported results.</li>
<li><strong>摘要：</strong>仇恨检测长期以来一直是 NLP 社区面临的一项艰巨任务。在代码混合环境中，这项任务变得复杂，因为模型必须理解上下文和通过语言改变表达的仇恨。与单语设置相比，我们看到的关于代码混合仇恨的研究非常少，因为没有大规模注释仇恨语料库来进行研究。为了克服这一瓶颈，我们建议使用母语仇恨样本。我们假设，在多语言语言模型 (MLM) 时代，代码混合环境中的仇恨可以通过主要依赖母语样本来检测。尽管 NLP 文献报告了 MLM 在许多跨语言环境中对仇恨检测的有效性，但它们在代码混合场景中的广泛评估尚未完成。本文试图通过严格的实证实验来填补这一空白。我们将印地语-英语代码混合设置视为案例研究，因为我们拥有这方面的语言专业知识。我们得到了一些有趣的观察结果：（i）在代码混合训练集中添加本地仇恨样本，即使数量很少，也能提高 MLM 在代码混合仇恨检测方面的表现；（ii）仅使用本地样本训练的 MLM 观察到在很大程度上检测到了代码混合仇恨；（iii）注意力得分的可视化显示，当在训练中加入本地样本时，MLM 可以更好地关注代码混合上下文中散发仇恨的词语；（iv）最后，当仇恨是主观的或讽刺的，天真地混合本地样本对检测代码混合仇恨没有多大帮助。我们将发布数据和代码存储库以重现报告的结果。</li>
</ul>

<h3>Title: Title:
          Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent</h3>
<ul>
<li><strong>Authors: </strong>Guang Lin, Qibin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for defense, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.</li>
<li><strong>摘要：</strong>在过去的两年中，大型语言模型 (LLM) 的使用发展迅速。虽然这些 LLM 提供了相当大的便利，但它们也引发了安全问题，因为 LLM 容易受到一些精心设计的文本扰动的对抗性攻击。在本文中，我们介绍了一种名为大型语言模型哨兵 (LLAMOS) 的新型防御技术，旨在通过在将对抗性文本示例输入目标 LLM 之前对其进行净化来增强 LLM 的对抗性鲁棒性。我们的方法包括两个主要部分：a) 代理指令，它可以模拟一个新的代理进行对抗性防御，改变最少的字符以在防御攻击的同时保持句子的原始含义；b) 防御指导，它提供修改干净或对抗性示例的策略，以确保有效防御和目标 LLM 的准确输出。值得注意的是，即使没有从对抗性示例中学习，防御代理也表现出强大的防御能力。此外，我们还进行了一项有趣的对抗实验，我们开发了两个代理，一个用于防御，另一个用于防御，并让它们相互对抗。在对抗互动过程中，两个代理都没有完全击败对方。在开源和闭源 LLM 上进行的大量实验表明，我们的方法可以有效防御对抗攻击，从而增强对抗鲁棒性。</li>
</ul>

<h3>Title: Title:
          PGA-SciRE: Harnessing LLM on Data Augmentation for Enhancing Scientific Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhou, Shimin Shan, Hongkui Wei, Zhehuan Zhao, Wenshuo Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          PGA-SciRE: Harnessing LLM on Data Augmentation for Enhancing Scientific Relation Extraction(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Relation Extraction (RE) aims at recognizing the relation between pairs of entities mentioned in a text. Advances in LLMs have had a tremendous impact on NLP. In this work, we propose a textual data augmentation framework called PGA for improving the performance of models for RE in the scientific domain. The framework introduces two ways of data augmentation, utilizing a LLM to obtain pseudo-samples with the same sentence meaning but with different representations and forms by paraphrasing the original training set samples. As well as instructing LLM to generate sentences that implicitly contain information about the corresponding labels based on the relation and entity of the original training set samples. These two kinds of pseudo-samples participate in the training of the RE model together with the original dataset, respectively. The PGA framework in the experiment improves the F1 scores of the three mainstream models for RE within the scientific domain. Also, using a LLM to obtain samples can effectively reduce the cost of manually labeling data.</li>
<li><strong>摘要：</strong>关系抽取（RE）旨在识别文本中提到的实体对之间的关​​系。LLM 的进步对 NLP 产生了巨大的影响。在本文中，我们提出了一种文本数据增强框架 PGA，用于提高科学领域 RE 模型的性能。该框架引入了两种数据增强方式，利用 LLM 通过释义原始训练集样本来获取具有相同句子含义但具有不同表示和形式的伪样本。以及指示 LLM 根据原始训练集样本的关系和实体生成隐含相应标签信息的句子。这两类伪样本分别与原始数据集一起参与 RE 模型的训练。实验中的 PGA 框架提高了科学领域三种主流 RE 模型的 F1 分数。此外，使用 LLM 获取样本可以有效降低手动标记数据的成本。</li>
</ul>

<h3>Title: Title:
          Multilingual Text Style Transfer: Datasets & Models for Indian Languages</h3>
<ul>
<li><strong>Authors: </strong>Sourabrata Mukherjee, Atul Kr. Ojha, Akanksha Bansal, Deepak Alok, John P. McCrae, Ondřej Dušek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Multilingual Text Style Transfer: Datasets & Models for Indian Languages(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Text style transfer (TST) involves altering the linguistic style of a text while preserving its core content. This paper focuses on sentiment transfer, a vital TST subtask (Mukherjee et al., 2022a), across a spectrum of Indian languages: Hindi, Magahi, Malayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous work on English-Bangla sentiment transfer (Mukherjee et al., 2023). We introduce dedicated datasets of 1,000 positive and 1,000 negative style-parallel sentences for each of these eight languages. We then evaluate the performance of various benchmark models categorized into parallel, non-parallel, cross-lingual, and shared learning approaches, including the Llama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the significance of parallel data in TST and demonstrate the effectiveness of the Masked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel techniques. Moreover, cross-lingual and joint multilingual learning methods show promise, offering insights into selecting optimal models tailored to the specific language and task requirements. To the best of our knowledge, this work represents the first comprehensive exploration of the TST task as sentiment transfer across a diverse set of languages.</li>
<li><strong>摘要：</strong>文本风格迁移 (TST) 涉及在保留文本核心内容的同时改变其语言风格。本文重点研究情绪迁移，这是一项重要的 TST 子任务 (Mukherjee 等人，2022a)，涉及一系列印度语言：印地语、摩揭陀语、马拉雅拉姆语、马拉地语、旁遮普语、奥里亚语、泰卢固语和乌尔都语，扩展了之前关于英语-孟加拉语情绪迁移的研究 (Mukherjee 等人，2023)。我们为这八种语言中的每一种引入了 1,000 个正面和 1,000 个负面风格平行句子的专用数据集。然后，我们评估了各种基准模型的性能，这些模型分为并行、非并行、跨语言和共享学习方法，包括 Llama2 和 GPT-3.5 大型语言模型 (LLM)。我们的实验强调了并行数据在 TST 中的重要性，并证明了蒙版风格填充 (MSF) 方法 (Mukherjee 等人，2023) 在非并行技术中的有效性。此外，跨语言和联合多语言学习方法显示出良好的前景，为选择适合特定语言和任务要求的最佳模型提供了见解。据我们所知，这项工作代表了对 TST 任务作为跨多种语言的情感转移的首次全面探索。</li>
</ul>

<h3>Title: Title:
          An iterated learning model of language change that mixes supervised and unsupervised learning</h3>
<ul>
<li><strong>Authors: </strong>Jack Bunyan, Seth Bullock, Conor Houghton</a></li>
<li><strong>Subjects: </strong>cs.CL, nlin.AO, q-bio.PE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          An iterated learning model of language change that mixes supervised and unsupervised learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>The iterated learning model is an agent-based model of language change in which language is transmitted from a tutor to a pupil which itself becomes a tutor to a new pupil, and so on. Languages that are stable, expressive, and compositional arise spontaneously as a consequence of a language transmission bottleneck. Previous models have implemented an agent's mapping from signals to meanings using an artificial neural network decoder, but have relied on an unrealistic and computationally expensive process of obversion to implement the associated encoder, mapping from meanings to signals. Here, a new model is presented in which both decoder and encoder are neural networks, trained separately through supervised learning, and trained together through unsupervised learning in the form of an autoencoder. This avoids the substantial computational burden entailed in obversion and introduces a mixture of supervised and unsupervised learning as observed during human development.</li>
<li><strong>摘要：</strong>迭代学习模型是一种基于代理的语言变化模型，其中语言从导师传给学生，学生本身又成为新学生的导师，依此类推。由于语言传输瓶颈，稳定、富有表现力和结构化的语言自然产生。以前的模型使用人工神经网络解码器实现了代理从信号到含义的映射，但依赖于不切实际且计算成本高昂的反演过程来实现相关的编码器，从含义到信号的映射。这里提出了一个新模型，其中解码器和编码器都是神经网络，通过监督学习分别进行训练，并通过自动编码器形式的无监督学习一起进行训练。这避免了反演中大量的计算负担，并引入了人类发展过程中观察到的监督和无监督学习的混合。</li>
</ul>

<h3>Title: Title:
          Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yueqin Yin, Zhendong Wang, Yujia Xie, Weizhu Chen, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Traditional language model alignment methods, such as Direct Preference Optimization (DPO), are limited by their dependence on static, pre-collected paired preference data, which hampers their adaptability and practical applicability. To overcome this limitation, we introduce Self-Augmented Preference Optimization (SAPO), an effective and scalable training paradigm that does not require existing paired data. Building on the self-play concept, which autonomously generates negative responses, we further incorporate an off-policy learning pipeline to enhance data exploration and exploitation. Specifically, we employ an Exponential Moving Average (EMA) model in conjunction with a replay buffer to enable dynamic updates of response segments, effectively integrating real-time feedback with insights from historical data. Our comprehensive evaluations of the LLaMA3-8B and Mistral-7B models across benchmarks, including the Open LLM Leaderboard, IFEval, AlpacaEval 2.0, and MT-Bench, demonstrate that SAPO matches or surpasses established offline contrastive baselines, such as DPO and Odds Ratio Preference Optimization, and outperforms offline self-play methods like SPIN. Our code is available at this https URL</li>
<li><strong>摘要：</strong>传统的语言模型对齐方法（例如直接偏好优化 (DPO)）受限于对静态、预先收集的配对偏好数据的依赖，这妨碍了它们的适应性和实际适用性。为了克服这一限制，我们引入了自增强偏好优化 (SAPO)，这是一种有效且可扩展的训练范例，不需要现有的配对数据。在自主生成负面响应的自对弈概念的基础上，我们进一步整合了离策略学习管道以增强数据探索和利用。具体而言，我们结合使用指数移动平均 (EMA) 模型和重放缓冲区来实现响应段的动态更新，从而有效地将实时反馈与历史数据的洞察相结合。我们对 LLaMA3-8B 和 Mistral-7B 模型进行了全面的评估，这些评估涵盖了 Open LLM Leaderboard、IFEval、AlpacaEval 2.0 和 MT-Bench 等基准测试，结果表明 SAPO 达到或超越了 DPO 和 Odds Ratio Preference Optimization 等既定的离线对比基线，并且优于 SPIN 等离线自对弈方法。我们的代码可从此 https URL 获取</li>
</ul>

<h3>Title: Title:
          That's Optional: A Contemporary Exploration of "that" Omission in English Subordinate Clauses</h3>
<ul>
<li><strong>Authors: </strong>Ella Rabinovich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          That's Optional: A Contemporary Exploration of "that" Omission in English Subordinate Clauses(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The Uniform Information Density (UID) hypothesis posits that speakers optimize the communicative properties of their utterances by avoiding spikes in information, thereby maintaining a relatively uniform information profile over time. This paper investigates the impact of UID principles on syntactic reduction, specifically focusing on the optional omission of the connector "that" in English subordinate clauses. Building upon previous research, we extend our investigation to a larger corpus of written English, utilize contemporary large language models (LLMs) and extend the information-uniformity principles by the notion of entropy, to estimate the UID manifestations in the usecase of syntactic reduction choices.</li>
<li><strong>摘要：</strong>统一信息密度 (UID) 假设认为，说话者通过避免信息激增来优化其话语的交流特性，从而随着时间的推移保持相对统一的信息概况。本文研究了 UID 原则对句法简化的影响，特别关注英语从句中连接词“that”的可选省略。在先前研究的基础上，我们将研究扩展到更大的书面英语语料库，利用当代大型语言模型 (LLM) 并通过熵的概念扩展信息均匀性原则，以估计句法简化选择用例中的 UID 表现。</li>
</ul>

<h3>Title: Title:
          Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>A. Bavaresco, A. Testoni, R. Fernández</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Image-based advertisements are complex multimodal stimuli that often contain unusual visual elements and figurative language. Previous research on automatic ad understanding has reported impressive zero-shot accuracy of contrastive vision-and-language models (VLMs) on an ad-explanation retrieval task. Here, we examine the original task setup and show that contrastive VLMs can solve it by exploiting grounding heuristics. To control for this confound, we introduce TRADE, a new evaluation test set with adversarial grounded explanations. While these explanations look implausible to humans, we show that they "fool" four different contrastive VLMs. Our findings highlight the need for an improved operationalisation of automatic ad understanding that truly evaluates VLMs' multimodal reasoning abilities. We make our code and TRADE available at this https URL .</li>
<li><strong>摘要：</strong>基于图像的广告是复杂的多模态刺激，通常包含不寻常的视觉元素和比喻性语言。先前对自动广告理解的研究报告称，对比视觉和语言模型 (VLM) 在广告解释检索任务中具有令人印象深刻的零样本准确度。在这里，我们检查了原始任务设置，并表明对比 VLM 可以通过利用基础启发式方法来解决它。为了控制这种混淆，我们引入了 TRADE，这是一种具有对抗性基础解释的新评估测试集。虽然这些解释对人类来说看起来难以置信，但我们表明它们“欺骗”了四个不同的对比 VLM。我们的研究结果强调需要改进自动广告理解的操作化，以真正评估 VLM 的多模态推理能力。我们在此 https URL 上提供我们的代码和 TRADE。</li>
</ul>

<h3>Title: Title:
          Improving Reward Models with Synthetic Critiques</h3>
<ul>
<li><strong>Authors: </strong>Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, Matthias Gallé</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Improving Reward Models with Synthetic Critiques(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Reward models (RM) play a critical role in aligning language models through the process of reinforcement learning from human feedback. RMs are trained to predict a score reflecting human preference, which requires significant time and cost for human annotation. Additionally, RMs tend to quickly overfit on superficial features in the training set, hindering their generalization performance on unseen distributions. We propose a novel approach using synthetic natural language critiques generated by large language models to provide additional feedback, evaluating aspects such as instruction following, correctness, and style. This offers richer signals and more robust features for RMs to assess and score on. We demonstrate that high-quality critiques improve the performance and data efficiency of RMs initialized from different pretrained models. Conversely, we also show that low-quality critiques negatively impact performance. Furthermore, incorporating critiques enhances the interpretability and robustness of RM training.</li>
<li><strong>摘要：</strong>奖励模型 (RM) 在通过从人类反馈中进行强化学习的过程中，在语言模型对齐方面发挥着关键作用。RM 经过训练可以预测反映人类偏好的分数，这需要大量的时间和成本来进行人工注释。此外，RM 往往会在训练集中的浅显特征上快速过度拟合，从而阻碍其在未见分布上的泛化性能。我们提出了一种新方法，使用由大型语言模型生成的合成自然语言评论来提供额外的反馈，评估指令遵循、正确性和风格等方面。这为 RM 提供了更丰富的信号和更强大的功能，以供评估和评分。我们证明高质量的评论可以提高从不同预训练模型初始化的 RM 的性能和数据效率。相反，我们还表明低质量的评论会对性能产生负面影响。此外，加入评论可以增强 RM 训练的可解释性和稳健性。</li>
</ul>

<h3>Title: Title:
          clembench-2024: A Challenging, Dynamic, Complementary, Multilingual Benchmark and Underlying Flexible Framework for LLMs as Multi-Action Agents</h3>
<ul>
<li><strong>Authors: </strong>Anne Beyer, Kranti Chalamalasetti, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, David Schlangen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          clembench-2024: A Challenging, Dynamic, Complementary, Multilingual Benchmark and Underlying Flexible Framework for LLMs as Multi-Action Agents(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>It has been established in recent work that Large Language Models (LLMs) can be prompted to "self-play" conversational games that probe certain capabilities (general instruction following, strategic goal orientation, language understanding abilities), where the resulting interactive game play can be automatically scored. In this paper, we take one of the proposed frameworks for setting up such game-play environments, and further test its usefulness as an evaluation instrument, along a number of dimensions: We show that it can easily keep up with new developments while avoiding data contamination, we show that the tests implemented within it are not yet saturated (human performance is substantially higher than that of even the best models), and we show that it lends itself to investigating additional questions, such as the impact of the prompting language on performance. We believe that the approach forms a good basis for making decisions on model choice for building applied interactive systems, and perhaps ultimately setting up a closed-loop development environment of system and simulated evaluator.</li>
<li><strong>摘要：</strong>最近的研究表明，大型语言模型 (LLM) 可以被提示“自玩”对话游戏，以探索某些能力（一般指令遵循、战略目标导向、语言理解能力），其中产生的交互式游戏可以自动评分。在本文中，我们采用了一种建议的框架来设置此类游戏环境，并从多个维度进一步测试其作为评估工具的实用性：我们表明它可以轻松跟上新的发展，同时避免数据污染，我们表明其中实施的测试尚未饱和（人类的表现远远高于甚至最好的模型），我们表明它有助于研究其他问题，例如提示语言对性能的影响。我们相信，这种方法为构建应用交互系统的模型选择决策奠定了良好的基础，并可能最终建立系统和模拟评估器的闭环开发环境。</li>
</ul>

<h3>Title: Title:
          Large Language Models: A New Approach for Privacy Policy Analysis at Scale</h3>
<ul>
<li><strong>Authors: </strong>David Rodriguez, Ian Yang, Jose M. Del Alamo, Norman Sadeh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Large Language Models: A New Approach for Privacy Policy Analysis at Scale(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>The number and dynamic nature of web and mobile applications presents significant challenges for assessing their compliance with data protection laws. In this context, symbolic and statistical Natural Language Processing (NLP) techniques have been employed for the automated analysis of these systems' privacy policies. However, these techniques typically require labor-intensive and potentially error-prone manually annotated datasets for training and validation. This research proposes the application of Large Language Models (LLMs) as an alternative for effectively and efficiently extracting privacy practices from privacy policies at scale. Particularly, we leverage well-known LLMs such as ChatGPT and Llama 2, and offer guidance on the optimal design of prompts, parameters, and models, incorporating advanced strategies such as few-shot learning. We further illustrate its capability to detect detailed and varied privacy practices accurately. Using several renowned datasets in the domain as a benchmark, our evaluation validates its exceptional performance, achieving an F1 score exceeding 93%. Besides, it does so with reduced costs, faster processing times, and fewer technical knowledge requirements. Consequently, we advocate for LLM-based solutions as a sound alternative to traditional NLP techniques for the automated analysis of privacy policies at scale.</li>
<li><strong>摘要：</strong>网络和移动应用程序的数量和动态特性对评估它们是否符合数据保护法提出了重大挑战。在此背景下，符号和统计自然语言处理 (NLP) 技术已用于自动分析这些系统的隐私政策。但是，这些技术通常需要劳动密集型且容易出错的手动注释数据集进行训练和验证。本研究提出应用大型语言模型 (LLM) 作为替代方案，以有效、高效地从大规模隐私政策中提取隐私实践。特别是，我们利用 ChatGPT 和 Llama 2 等知名 LLM，并结合少量学习等高级策略，提供有关提示、参数和模型的最佳设计的指导。我们进一步说明了它准确检测详细和多样化隐私实践的能力。使用该领域中的几个知名数据集作为基准，我们的评估验证了其卓越的性能，实现了超过 93% 的 F1 分数。此外，它以更低的成本、更快的处理时间和更少的技术知识要求实现了这一点。因此，我们提倡基于 LLM 的解决方案作为传统 NLP 技术的合理替代方案，用于大规模自动分析隐私政策。</li>
</ul>

<h3>Title: Title:
          Preemptive Answer "Attacks" on Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Rongwu Xu, Zehan Qi, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Preemptive Answer "Attacks" on Chain-of-Thought Reasoning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) showcase impressive reasoning capabilities when coupled with Chain-of-Thought (CoT) prompting. However, the robustness of this approach warrants further investigation. In this paper, we introduce a novel scenario termed preemptive answers, where the LLM obtains an answer before engaging in reasoning. This situation can arise inadvertently or induced by malicious users by prompt injection attacks. Experiments reveal that preemptive answers significantly impair the model's reasoning capability across various CoT methods and a broad spectrum of datasets. To bolster the robustness of reasoning, we propose two measures aimed at mitigating this issue to some extent.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 与思维链 (CoT) 提示相结合时展现出令人印象深刻的推理能力。然而，这种方法的稳健性值得进一步研究。在本文中，我们介绍了一种称为抢先答案的新场景，其中 LLM 在进行推理之前获得答案。这种情况可能是无意中发生的，也可能是由恶意用户通过提示注入攻击引起的。实验表明，抢先答案会严重损害模型在各种 CoT 方法和广泛数据集中的推理能力。为了增强推理的稳健性，我们提出了两项​​措施，旨在在一定程度上缓解这一问题。</li>
</ul>

<h3>Title: Title:
          OR-Bench: An Over-Refusal Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Justin Cui, Wei-Lin Chiang, Ion Stoica, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          OR-Bench: An Over-Refusal Benchmark for Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where the LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that appear harmful but are benign. This study proposes a novel method for automatically generating large-scale sets of ``seemingly toxic prompts'' (benign prompts likely rejected by LLMs). Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 seemingly toxic prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 25 popular LLMs across 8 model families. Our datasets are available at this https URL and the corresponding demo can be found at this https URL. We hope this benchmark can help the community develop better safety aligned models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 需要仔细的安全调整以防止恶意输出。虽然大量研究都集中在减轻有害内容的生成上，但增强的安全性往往伴随着过度拒绝的副作用，即 LLM 可能会拒绝无害的提示并变得不那么有用。虽然过度拒绝的问题已经得到经验观察，但由于难以制作看似有害但实际上无害的提示，因此系统测量具有挑战性。这项研究提出了一种新方法，用于自动生成大规模“看似有毒的提示”（LLM 可能拒绝的良性提示）。利用这种技术，我们推出了第一个大规模过度拒绝基准 OR-Bench。OR-Bench 包含 10 个常见拒绝类别中的 80,000 个看似有毒的提示，一个大约 1,000 个硬提示的子集，即使对于最先进的 LLM 来说也具有挑战性，以及另外 600 个有毒提示以防止不加区分的反应。然后，我们进行了一项全面的研究，以衡量 8 个模型系列中 25 个流行 LLM 的过度拒绝率。我们的数据集可在此 https URL 上找到，相应的演示可在此 https URL 上找到。我们希望这个基准可以帮助社区开发更好的安全对齐模型。</li>
</ul>

<h3>Title: Title:
          Superlatives in Context: Explicit and Implicit Domain Restrictions for Superlative Frames</h3>
<ul>
<li><strong>Authors: </strong>Valentina Pyatkin, Bonnie Webber, Ido Dagan, Reut Tsarfaty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Superlatives in Context: Explicit and Implicit Domain Restrictions for Superlative Frames(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Superlatives are used to single out elements with a maximal/minimal property. Semantically, superlatives perform a set comparison: something (or some things) has the min/max property out of a set. As such, superlatives provide an ideal phenomenon for studying implicit phenomena and discourse restrictions. While this comparison set is often not explicitly defined, its (implicit) restrictions can be inferred from the discourse context the expression appears in. In this work we provide an extensive computational study on the semantics of superlatives. We propose a unified account of superlative semantics which allows us to derive a broad-coverage annotation schema. Using this unified schema we annotated a multi-domain dataset of superlatives and their semantic interpretations. We specifically focus on interpreting implicit or ambiguous superlative expressions, by analyzing how the discourse context restricts the set of interpretations. In a set of experiments we then analyze how well models perform at variations of predicting superlative semantics, with and without context. We show that the fine-grained semantics of superlatives in context can be challenging for contemporary models, including GPT-4.</li>
<li><strong>摘要：</strong>最高级用于选出具有最大/最小属性的元素。从语义上讲，最高级执行集合比较：某物（或某些事物）具有集合中的最小/最大属性。因此，最高级为研究隐性现象和话语限制提供了一种理想的现象。虽然这种比较集通常没有明确定义，但其（隐性）限制可以从表达式出现的话语上下文中推断出来。在这项工作中，我们对最高级的语义进行了广泛的计算研究。我们提出了一个统一的最高级语义说明，使我们能够得出一个覆盖范围广泛的注释模式。使用这个统一的模式，我们注释了一个多领域的最高级数据集及其语义解释。我们特别关注解释隐式或模棱两可的最高级表达，通过分析话语上下文如何限制解释集。在一组实验中，我们分析了模型在有上下文和无上下文的情况下预测最高级语义的变化时的表现。我们表明，上下文中最高级的细粒度语义对于包括 GPT-4 在内的当代模型来说可能具有挑战性。</li>
</ul>

<h3>Title: Title:
          SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales</h3>
<ul>
<li><strong>Authors: </strong>Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at \url{this https URL}.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常会生成不准确或虚假的信息，并且通常无法表明其置信度，这限制了它们的广泛应用。先前的研究通过直接或自洽提示，或构建特定数据集进行监督微调来从 LLM 中获取置信度。基于提示的方法性能较差，而基于训练的方法仅限于二元或不准确的组级置信度估计。在这项工作中，我们提出了高级 SaySelf，这是一个训练框架，可以教 LLM 表达更准确的细粒度置信度估计。此外，除了置信度分数之外，SaySelf 还启动了指导 LLM 产生自我反思原理的过程，这些原理可以清楚地识别其参数知识中的差距并解释其不确定性。这是通过使用 LLM 通过自然语言自动总结特定知识中的不确定性来实现的。总结基于对多个采样推理链中不一致性分析，并将结果数据用于监督微调。此外，我们利用强化学习和精心设计的奖励函数来校准置信度估计，激励 LLM 提供准确、高置信度的预测，并惩罚对错误输出的过度自信。在分布内和分布外数据集中的实验结果证明了 SaySelf 在减少置信度校准误差和保持任务性能方面的有效性。我们表明，生成的自我反思原理是合理的，并且可以进一步促进校准。代码在 \url{this https URL} 上公开。</li>
</ul>

<h3>Title: Title:
          You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet</h3>
<ul>
<li><strong>Authors: </strong>Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.</li>
<li><strong>摘要：</strong>线性注意机制因其线性计算复杂度和速度提升而在因果语言模型中占据重要地位。然而，线性注意机制固有的衰减机制在应用于多维序列建模任务（如图像处理和多模态学习）时带来了挑战。在这些场景中，利用顺序扫描建立全局感受野需要对多维数据进行多次扫描，从而导致效率低下。本文确定了乘法线性递归造成的效率低下，并提出了一种有效的替代加法线性递归来避免该问题，因为它可以在一次扫描中处理多维数据。我们进一步基于新的递归开发了一个高效的多维顺序建模框架 LightNet。此外，我们提出了两种新的多维线性相对位置编码方法，MD-TPE 和 MD-LRPE，以增强模型在多维场景中辨别位置信息的能力。我们对图像分类、图像生成、双向语言建模和自回归语言建模等各种任务进行的经验评估证明了 LightNet 的有效性，展示了其作为多维序列建模的多功能高效解决方案的潜力。</li>
</ul>

<h3>Title: Title:
          LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Elias Stengel-Eskin, Peter Hase, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>When answering questions, LLMs can convey not only an answer, but a level of confidence about the answer being correct. This includes explicit confidence markers (e.g. giving a numeric score) as well as implicit markers, like an authoritative tone or elaborating with additional knowledge. For LLMs to be trustworthy knowledge sources, the confidence they convey should match their actual expertise; however, most current models tend towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. We cast calibration as preference optimization, creating data via a two-agent game, where a speaker model's outputs are judged by a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the resulting models are better calibrated w.r.t. a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM's answers, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better confidence separation between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more and implicitly signals certainty when it is correct by using an authoritative tone or including details. Finally, LACIE finetuning leads to an emergent increase in model abstention (e.g. saying "I don't know") for answers that are likely wrong.</li>
<li><strong>摘要：</strong>在回答问题时，LLM 不仅可以传达答案，还可以传达对答案正确性的信心水平。这包括显式信心标记（例如给出数字分数）以及隐式标记，例如权威语气或用额外知识进行阐述。要使 LLM 成为值得信赖的知识来源，它们传达的信心应该与其实际专业知识相匹配；然而，大多数当前模型都倾向于过度自信。为了校准隐式和显式信心标记，我们引入了一种实用的、听众感知的微调方法 (LACIE)，该方法可以对听众进行建模，不仅考虑答案是否正确，还考虑听众是否会接受答案。我们将校准视为偏好优化，通过双智能体游戏创建数据，其中说话者模型的输出由模拟听众判断。然后，我们使用 LACIE 对三个 LLM（Mistral-7B、Llama3-8B、Llama3-70B）进行微调，并表明生成的模型相对于其他模型进行了更好的校准。模拟听众。至关重要的是，这些趋势会转移到人类听众身上，帮助他们正确预测模型的正确性：我们进行了人工评估，其中注释者接受或拒绝 LLM 的答案，发现使用 LACIE 进行训练可使接受的错误答案减少 47%，同时保持对正确答案的接受程度不变。此外，LACIE 可以推广到另一个数据集，在 TriviaQA 上进行训练时，TruthfulQA 上的真实性会大大提高。我们的分析表明，LACIE 可以更好地区分正确和错误的示例。从质量上讲，我们发现 LACIE 训练的模型会更多地进行对冲，并在正确时通过使用权威语气或包含细节来隐含地表示确定性。最后，LACIE 微调会导致模型对可能错误的答案弃权（例如说“我不知道”）的现象突然增加。</li>
</ul>

<h3>Title: Title:
          Direct Alignment of Language Models via Quality-Aware Self-Refinement</h3>
<ul>
<li><strong>Authors: </strong>Runsheng Yu, Yong Wang, Xiaoqi Jiao, Youzhi Zhang, James T. Kwok</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Direct Alignment of Language Models via Quality-Aware Self-Refinement(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has been commonly used to align the behaviors of Large Language Models (LLMs) with human preferences. Recently, a popular alternative is Direct Policy Optimization (DPO), which replaces an LLM-based reward model with the policy itself, thus obviating the need for extra memory and training time to learn the reward model. However, DPO does not consider the relative qualities of the positive and negative responses, and can lead to sub-optimal training outcomes. To alleviate this problem, we investigate the use of intrinsic knowledge within the on-the-fly fine-tuning LLM to obtain relative qualities and help to refine the loss function. Specifically, we leverage the knowledge of the LLM to design a refinement function to estimate the quality of both the positive and negative responses. We show that the constructed refinement function can help self-refine the loss function under mild assumptions. The refinement function is integrated into DPO and its variant Identity Policy Optimization (IPO). Experiments across various evaluators indicate that they can improve the performance of the fine-tuned models over DPO and IPO.</li>
<li><strong>摘要：</strong>人类反馈强化学习 (RLHF) 已广泛用于将大型语言模型 (LLM) 的行为与人类偏好相一致。最近，一种流行的替代方案是直接策略优化 (DPO)，它用策略本身取代基于 LLM 的奖励模型，从而消除了学习奖励模型所需的额外内存和训练时间。然而，DPO 不考虑正向和负向响应的相对质量，并且可能导致训练结果不理想。为了缓解这个问题，我们研究了在实时微调 LLM 中使用内在知识来获得相对质量并帮助改进损失函数。具体来说，我们利用 LLM 的知识来设计一个改进函数来估计正向和负向响应的质量。我们表明，构建的改进函数可以在温和的假设下帮助自我改进损失函数。改进函数集成到 DPO 及其变体身份策略优化 (IPO) 中。跨各种评估者的实验表明，它们可以提高微调模型相对于 DPO 和 IPO 的性能。</li>
</ul>

<h3>Title: Title:
          Code Pretraining Improves Entity Tracking Abilities of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Najoung Kim, Sebastian Schuster, Shubham Toshniwal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Code Pretraining Improves Entity Tracking Abilities of Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent work has provided indirect evidence that pretraining language models on code improves the ability of models to track state changes of discourse entities expressed in natural language. In this work, we systematically test this claim by comparing pairs of language models on their entity tracking performance. Critically, the pairs consist of base models and models trained on top of these base models with additional code data. We extend this analysis to additionally examine the effect of math training, another highly structured data type, and alignment tuning, an important step for enhancing the usability of models. We find clear evidence that models additionally trained on large amounts of code outperform the base models. On the other hand, we find no consistent benefit of additional math training or alignment tuning across various model families.</li>
<li><strong>摘要：</strong>最近的研究提供了间接证据，表明对代码进行预训练的语言模型可以提高模型跟踪自然语言表达的话语实体状态变化的能力。在这项研究中，我们通过比较语言模型对的实体跟踪性能，系统地测试了这一说法。至关重要的是，这些对由基础模型和在这些基础模型上使用附加代码数据训练的模型组成。我们扩展了这一分析，以进一步检查数学训练（另一种高度结构化的数据类型）和对齐调整（增强模型可用性的重要步骤）的效果。我们发现了明确的证据表明，在大量代码上额外训练的模型优于基础模型。另一方面，我们发现在各种模型系列中，额外的数学训练或对齐调整并没有带来一致的好处。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
