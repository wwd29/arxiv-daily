<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-12-12</h1>
<h3>Title: Multi-Response Preference Optimization with Augmented Ranking Dataset</h3>
<ul>
<li><strong>Authors: </strong>Hansle Gwon, Imjin Ahn, Young-Hak Kim, Sanghyun Park, Tae Joon Jun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.07812">https://arxiv.org/abs/2412.07812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.07812">https://arxiv.org/pdf/2412.07812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.07812]] Multi-Response Preference Optimization with Augmented Ranking Dataset(https://arxiv.org/abs/2412.07812)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have been remarkable, with new models consistently surpassing their predecessors. These advancements are underpinned by extensive research on various training mechanisms. Among these, Preference Optimization has played a significant role in improving the performance of LLMs by incorporating human preferences into the training process. However, constructing preference optimization datasets is challenging and the optimization process is highly sensitive to the dataset quality. In this study, we propose a novel approach to augment Preference Optimization datasets. Additionally, we introduce a Multi-response-based Preference Optimization training method that enables the simultaneous learning of multiple responses.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展令人瞩目，新模型不断超越其前辈。这些进步得益于对各种训练机制的广泛研究。其中，偏好优化通过将人类偏好纳入训练过程，在提高 LLM 性能方面发挥了重要作用。然而，构建偏好优化数据集具有挑战性，优化过程对数据集质量高度敏感。在本研究中，我们提出了一种增强偏好优化数据集的新方法。此外，我们介绍了一种基于多响应的偏好优化训练方法，可以同时学习多个响应。</li>
</ul>

<h3>Title: Rethinking Emotion Annotations in the Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Minxue Niu, Yara El-Tawil, Amrit Romana, Emily Mower Provost</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.07906">https://arxiv.org/abs/2412.07906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.07906">https://arxiv.org/pdf/2412.07906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.07906]] Rethinking Emotion Annotations in the Era of Large Language Models(https://arxiv.org/abs/2412.07906)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Modern affective computing systems rely heavily on datasets with human-annotated emotion labels, for training and evaluation. However, human annotations are expensive to obtain, sensitive to study design, and difficult to quality control, because of the subjective nature of emotions. Meanwhile, Large Language Models (LLMs) have shown remarkable performance on many Natural Language Understanding tasks, emerging as a promising tool for text annotation. In this work, we analyze the complexities of emotion annotation in the context of LLMs, focusing on GPT-4 as a leading model. In our experiments, GPT-4 achieves high ratings in a human evaluation study, painting a more positive picture than previous work, in which human labels served as the only ground truth. On the other hand, we observe differences between human and GPT-4 emotion perception, underscoring the importance of human input in annotation studies. To harness GPT-4's strength while preserving human perspective, we explore two ways of integrating GPT-4 into emotion annotation pipelines, showing its potential to flag low-quality labels, reduce the workload of human annotators, and improve downstream model learning performance and efficiency. Together, our findings highlight opportunities for new emotion labeling practices and suggest the use of LLMs as a promising tool to aid human annotation.</li>
<li><strong>摘要：</strong>现代情感计算系统在训练和评估方面严重依赖带有人工注释情感标签的数据集。然而，由于情感的主观性，人工注释的成本高昂、对研究设计敏感且难以进行质量控制。同时，大型语言模型 (LLM) 在许多自然语言理解任务中表现出色，成为一种有前途的文本注释工具。在这项工作中，我们分析了 LLM 背景下情感注释的复杂性，重点关注 GPT-4 作为领先模型。在我们的实验中，GPT-4 在一项人工评估研究中获得了高分，比以前的工作描绘了更积极的画面，在以前的工作中，人工标签是唯一的基本事实。另一方面，我们观察到人类和 GPT-4 情感感知之间的差异，强调了人类输入在注释研究中的重要性。为了在保留人类视角的同时发挥 GPT-4 的优势，我们探索了将 GPT-4 集成到情感注释管道中的两种方法，展示了其标记低质量标签、减少人工注释者的工作量以及提高下游模型学习性能和效率的潜力。总之，我们的研究结果强调了新的情绪标记实践的机会，并建议使用 LLM 作为辅助人类注释的有前途的工具。</li>
</ul>

<h3>Title: Asking Again and Again: Exploring LLM Robustness to Repeated Questions</h3>
<ul>
<li><strong>Authors: </strong>Sagi Shaier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.07923">https://arxiv.org/abs/2412.07923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.07923">https://arxiv.org/pdf/2412.07923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.07923]] Asking Again and Again: Exploring LLM Robustness to Repeated Questions(https://arxiv.org/abs/2412.07923)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>This study examines whether large language models (LLMs), such as ChatGPT, specifically the latest GPT-4o-mini, exhibit sensitivity to repeated prompts and whether repeating a question can improve response accuracy. We hypothesize that reiterating a question within a single prompt might enhance the model's focus on key elements of the query. To test this, we evaluate ChatGPT's performance on a large sample of two reading comprehension datasets under both open-book and closed-book settings, varying the repetition of each question to 1, 3, or 5 times per prompt. Our findings indicate that the model does not demonstrate sensitivity to repeated questions, highlighting its robustness and consistency in this context.</li>
<li><strong>摘要：</strong>本研究旨在检验大型语言模型 (LLM)（例如 ChatGPT，特别是最新的 GPT-4o-mini）是否对重复提示表现出敏感性，以及重复问题是否可以提高回答准确性。我们假设，在单个提示中重复问题可能会增强模型对查询关键元素的关注。为了测试这一点，我们在开卷和闭卷设置下对两个阅读理解数据集的大样本评估了 ChatGPT 的表现，每个问题的重复次数分别为每次提示 1、3 或 5 次。我们的研究结果表明，该模型对重复问题并不敏感，凸显了其在这种情况下的稳健性和一致性。</li>
</ul>

<h3>Title: Forking Paths in Neural Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Eric Bigelow, Ari Holtzman, Hidenori Tanaka, Tomer Ullman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.07961">https://arxiv.org/abs/2412.07961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.07961">https://arxiv.org/pdf/2412.07961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.07961]] Forking Paths in Neural Text Generation(https://arxiv.org/abs/2412.07961)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Estimating uncertainty in Large Language Models (LLMs) is important for properly evaluating LLMs, and ensuring safety for users. However, prior approaches to uncertainty estimation focus on the final answer in generated text, ignoring intermediate steps that might dramatically impact the outcome. We hypothesize that there exist key forking tokens, such that re-sampling the system at those specific tokens, but not others, leads to very different outcomes. To test this empirically, we develop a novel approach to representing uncertainty dynamics across individual tokens of text generation, and applying statistical models to test our hypothesis. Our approach is highly flexible: it can be applied to any dataset and any LLM, without fine tuning or accessing model weights. We use our method to analyze LLM responses on 7 different tasks across 4 domains, spanning a wide range of typical use cases. We find many examples of forking tokens, including surprising ones such as punctuation marks, suggesting that LLMs are often just a single token away from saying something very different.</li>
<li><strong>摘要：</strong>评估大型语言模型 (LLM) 中的不确定性对于正确评估 LLM 和确保用户安全非常重要。然而，先前的不确定性评估方法侧重于生成文本中的最终答案，而忽略了可能对结果产生巨大影响的中间步骤。我们假设存在关键的分叉标记，因此在这些特定标记（而不是其他标记）上重新采样系统会导致截然不同的结果。为了通过实证检验这一点，我们开发了一种新颖的方法来表示文本生成各个标记之间的不确定性动态，并应用统计模型来检验我们的假设。我们的方法非常灵活：它可以应用于任何数据集和任何 LLM，而无需微调或访问模型权重。我们使用我们的方法分析了 4 个领域中 7 个不同任务的 LLM 响应，涵盖了广泛的典型用例。我们发现了许多分叉标记的例子，包括令人惊讶的标记，例如标点符号，这表明 LLM 通常只需一个标记就能表达出截然不同的意思。</li>
</ul>

<h3>Title: HalluCana: Fixing LLM Hallucination with A Canary Lookahead</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Li, Erenay Dayanik, Shubhi Tyagi, Andrea Pierleoni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.07965">https://arxiv.org/abs/2412.07965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.07965">https://arxiv.org/pdf/2412.07965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.07965]] HalluCana: Fixing LLM Hallucination with A Canary Lookahead(https://arxiv.org/abs/2412.07965)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>In this paper, we present HalluCana, a canary lookahead to detect and correct factuality hallucinations of Large Language Models (LLMs) in long-form generation. HalluCana detects and intervenes as soon as traces of hallucination emerge, during and even before generation. To support timely detection, we exploit the internal factuality representation in the LLM hidden space, where we investigate various proxies to the LLMs' factuality self-assessment, and discuss its relation to the models' context familiarity from their pre-training. On biography generation, our method improves generation quality by up to 2.5x, while consuming over 6 times less compute.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 HalluCana，这是一种用于检测和纠正长篇生成中大型语言模型 (LLM) 事实性幻觉的金丝雀预测方法。HalluCana 可以在生成过程中甚至生成之前，在出现幻觉迹象时立即检测并干预。为了支持及时检测，我们利用 LLM 隐藏空间中的内部事实性表示，在其中我们研究了 LLM 事实性自我评估的各种代理，并讨论了其与模型在预训练中的上下文熟悉度的关系。在传记生成方面，我们的方法将生成质量提高了 2.5 倍，同时计算量减少了 6 倍以上。</li>
</ul>

<h3>Title: Concept Bottleneck Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chung-En Sun, Tuomas Oikarinen, Berk Ustun, Tsui-Wei Weng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.07992">https://arxiv.org/abs/2412.07992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.07992">https://arxiv.org/pdf/2412.07992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.07992]] Concept Bottleneck Large Language Models(https://arxiv.org/abs/2412.07992)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We introduce the Concept Bottleneck Large Language Model (CB-LLM), a pioneering approach to creating inherently interpretable Large Language Models (LLMs). Unlike traditional black-box LLMs that rely on post-hoc interpretation methods with limited neuron function insights, CB-LLM sets a new standard with its built-in interpretability, scalability, and ability to provide clear, accurate explanations. We investigate two essential tasks in the NLP domain: text classification and text generation. In text classification, CB-LLM narrows the performance gap with traditional black-box models and provides clear interpretability. In text generation, we show how interpretable neurons in CB-LLM can be used for concept detection and steering text generation. Our CB-LLMs enable greater interaction between humans and LLMs across a variety of tasks -- a feature notably absent in existing LLMs. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>我们引入了概念瓶颈大型语言模型 (CB-LLM)，这是一种创建固有可解释大型语言模型 (LLM) 的开创性方法。与依赖事后解释方法且神经元功能洞察力有限的传统黑盒 LLM 不同，CB-LLM 以其内置的可解释性、可扩展性和提供清晰、准确解释的能力树立了新标准。我们研究了 NLP 领域的两个基本任务：文本分类和文本生成。在文本分类中，CB-LLM 缩小了与传统黑盒模型的性能差距并提供了清晰的可解释性。在文本生成中，我们展示了如何使用 CB-LLM 中的可解释神经元进行概念检测和引导文本生成。我们的 CB-LLM 使人类和 LLM 在各种任务中实现更大的交互——这是现有 LLM 中明显缺乏的功能。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: TinyThinker: Distilling Reasoning through Coarse-to-Fine Knowledge Internalization with Self-Reflection</h3>
<ul>
<li><strong>Authors: </strong>Shengmin Piao, Sanghyun Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08024">https://arxiv.org/abs/2412.08024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08024">https://arxiv.org/pdf/2412.08024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08024]] TinyThinker: Distilling Reasoning through Coarse-to-Fine Knowledge Internalization with Self-Reflection(https://arxiv.org/abs/2412.08024)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large Language Models exhibit impressive reasoning capabilities across diverse tasks, motivating efforts to distill these capabilities into smaller models through generated reasoning data. However, direct training on such synthesized reasoning data may lead to superficial imitation of reasoning process, rather than fostering a genuine integration of reasoning capabilities with underlying knowledge. To address this, we propose TinyThinker, a framework introducing two novel approaches. First, we introduce a three-stage process that incrementally guides the student model through the reasoning process, progressively refining knowledge from coarse to fine granularity. Second, we develop a two-phase training framework comprising an initial reasoning acquisition phase followed by a self-reflection phase utilizing self-generated data. Experiments on commonsense reasoning benchmarks demonstrate that TinyThinker achieves superior performance compared to baselines. Ablation studies further validate the effectiveness of each component in our framework. TinyThinker is extendable to other knowledge-intensive reasoning tasks, offering an alternative strategy for developing effective reasoning capabilities in smaller language models. Codes are available at this https URL</li>
<li><strong>摘要：</strong>大型语言模型在各种任务中表现出令人印象深刻的推理能力，这促使人们努力通过生成的推理数据将这些能力提炼成更小的模型。然而，直接对这种合成的推理数据进行训练可能会导致对推理过程的肤浅模仿，而不是促进推理能力与底层知识的真正融合。为了解决这个问题，我们提出了 TinyThinker，这是一个引入两种新方法的框架。首先，我们引入了一个三阶段过程，逐步引导学生模型完成推理过程，逐步将知识从粗粒度细化到细粒度。其次，我们开发了一个两阶段训练框架，包括一个初始推理获取阶段，然后是一个利用自生成数据的自我反思阶段。在常识推理基准上的实验表明，TinyThinker 与基线相比实现了卓越的性能。消融研究进一步验证了我们框架中每个组件的有效性。TinyThinker 可扩展到其他知识密集型推理任务，为在较小的语言模型中开发有效的推理能力提供了一种替代策略。代码可在此 https URL 上找到</li>
</ul>

<h3>Title: M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified Sentiment and Emotion Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ao Li, Longwei Xu, Chen Ling, Jinghui Zhang, Pengwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08049">https://arxiv.org/abs/2412.08049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08049">https://arxiv.org/pdf/2412.08049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08049]] M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified Sentiment and Emotion Analysis(https://arxiv.org/abs/2412.08049)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Sentiment analysis and emotion recognition are crucial for applications such as human-computer interaction and depression detection. Traditional unimodal methods often fail to capture the complexity of emotional expressions due to conflicting signals from different modalities. Current Multimodal Large Language Models (MLLMs) also face challenges in detecting subtle facial expressions and addressing a wide range of emotion-related tasks. To tackle these issues, we propose M2SE, a Multistage Multitask Sentiment and Emotion Instruction Tuning Strategy for general-purpose MLLMs. It employs a combined approach to train models on tasks such as multimodal sentiment analysis, emotion recognition, facial expression recognition, emotion reason inference, and emotion cause-pair extraction. We also introduce the Emotion Multitask dataset (EMT), a custom dataset that supports these five tasks. Our model, Emotion Universe (EmoVerse), is built on a basic MLLM framework without modifications, yet it achieves substantial improvements across these tasks when trained with the M2SE strategy. Extensive experiments demonstrate that EmoVerse outperforms existing methods, achieving state-of-the-art results in sentiment and emotion tasks. These results highlight the effectiveness of M2SE in enhancing multimodal emotion perception. The dataset and code are available at this https URL.</li>
<li><strong>摘要：</strong>情绪分析和情绪识别对于人机交互和抑郁症检测等应用至关重要。传统的单模态方法通常无法捕捉情绪表达的复杂性，因为来自不同模态的信号相互冲突。当前的多模态大型语言模型 (MLLM) 在检测细微面部表情和处理各种与情绪相关的任务方面也面临挑战。为了解决这些问题，我们提出了 M2SE，这是一种用于通用 MLLM 的多阶段多任务情绪和情绪指令调整策略。它采用组合方法来训练模型完成多模态情绪分析、情绪识别、面部表情识别、情绪原因推理和情绪原因对提取等任务。我们还介绍了情绪多任务数据集 (EMT)，这是一个支持这五个任务的自定义数据集。我们的模型情绪宇宙 (EmoVerse) 建立在未经修改的基本 MLLM 框架上，但使用 M2SE 策略进行训练时，它在这些任务上取得了显着的改进。大量实验表明，EmoVerse 的表现优于现有方法，在情绪和情感任务中取得了最先进的成果。这些结果凸显了 M2SE 在增强多模态情感感知方面的有效性。数据集和代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic Alignment for Low-Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Ashutosh Bajpai, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08090">https://arxiv.org/abs/2412.08090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08090">https://arxiv.org/pdf/2412.08090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08090]] Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic Alignment for Low-Resource Languages(https://arxiv.org/abs/2412.08090)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The unwavering disparity in labeled resources between resource-rich languages and those considered low-resource remains a significant impediment for Large Language Models (LLMs). Recent strides in cross-lingual in-context learning (X-ICL), mainly through semantically aligned examples retrieved from multilingual pre-trained transformers, have shown promise in mitigating this issue. However, our investigation reveals that LLMs intrinsically reward in-language semantically aligned cross-lingual instances over direct cross-lingual semantic alignments, with a pronounced disparity in handling time-sensitive queries in the X-ICL setup. Such queries demand sound temporal reasoning ability from LLMs, yet the advancements have predominantly focused on English. This study aims to bridge this gap by improving temporal reasoning capabilities in low-resource languages. To this end, we introduce mTEMPREASON a temporal reasoning dataset aimed at the varied degrees of low-resource languages and propose Cross-Lingual Time-Sensitive Semantic Alignment (CLiTSSA), a novel method to improve temporal reasoning in these contexts. To facilitate this, we construct an extension of mTEMPREASON comprising pairs of parallel cross-language temporal queries along with their anticipated in-language semantic similarity scores. Our empirical evidence underscores the superior performance of CLiTSSA compared to established baselines across three languages - Romanian, German, and French, encompassing three temporal tasks and including a diverse set of four contemporaneous LLMs. This marks a significant step forward in addressing resource disparity in the context of temporal reasoning across languages.</li>
<li><strong>摘要：</strong>资源丰富的语言和资源匮乏的语言之间在标记资源方面存在着巨大的差异，这仍然是大型语言模型 (LLM) 的一个重大障碍。跨语言上下文学习 (X-ICL) 方面的最新进展，主要是通过从多语言预训练的 Transformer 中检索到的语义对齐示例，已显示出缓解这一问题的希望。然而，我们的调查显示，与直接跨语言语义对齐相比，LLM 本质上更青睐语言内语义对齐的跨语言实例，并且在 X-ICL 设置中处理时间敏感查询时存在明显差异。这类查询要求 LLM 具备良好的时间推理能力，但这些进步主要集中在英语上。本研究旨在通过提高低资源语言的时间推理能力来弥补这一差距。为此，我们引入了 mTEMPREASON，这是一个针对不同程度的低资源语言的时间推理数据集，并提出了跨语言时间敏感语义对齐 (CLiTSSA)，这是一种在这些情况下改进时间推理的新方法。为了实现这一点，我们构建了 mTEMPREASON 的扩展，其中包含成对的并行跨语言时间查询及其预期的语言内语义相似性分数。我们的实证证据强调了 CLiTSSA 与三种语言（罗马尼亚语、德语和法语）的既定基线相比具有优异的性能，涵盖三个时间任务并包括一组四个不同的同时期 LLM。这标志着在解决跨语言时间推理背景下的资源差异方面迈出了重要一步。</li>
</ul>

<h3>Title: Evil twins are not that evil: Qualitative insights into machine-generated prompts</h3>
<ul>
<li><strong>Authors: </strong>Nathanaël Carraz Rakotonirina, Corentin Kervadec, Francesca Franzon, Marco Baroni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08127">https://arxiv.org/abs/2412.08127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08127">https://arxiv.org/pdf/2412.08127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08127]] Evil twins are not that evil: Qualitative insights into machine-generated prompts(https://arxiv.org/abs/2412.08127)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>It has been widely observed that language models (LMs) respond in predictable ways to algorithmically generated prompts that are seemingly unintelligible. This is both a sign that we lack a full understanding of how LMs work, and a practical challenge, because opaqueness can be exploited for harmful uses of LMs, such as jailbreaking. We present the first thorough analysis of opaque machine-generated prompts, or autoprompts, pertaining to 3 LMs of different sizes and families. We find that machine-generated prompts are characterized by a last token that is often intelligible and strongly affects the generation. A small but consistent proportion of the previous tokens are fillers that probably appear in the prompt as a by-product of the fact that the optimization process fixes the number of tokens. The remaining tokens tend to have at least a loose semantic relation with the generation, although they do not engage in well-formed syntactic relations with it. We find moreover that some of the ablations we applied to machine-generated prompts can also be applied to natural language sequences, leading to similar behavior, suggesting that autoprompts are a direct consequence of the way in which LMs process linguistic inputs in general.</li>
<li><strong>摘要：</strong>人们普遍观察到，语言模型 (LM) 会以可预测的方式对看似难以理解的算法生成提示做出反应。这既表明我们缺乏对 LM 工作原理的全面了解，也是一种实际挑战，因为不透明性可能会被利用来对 LM 进行有害使用，例如越狱。我们首次对不透明的机器生成提示或自动提示进行了全面分析，涉及 3 个不同大小和系列的 LM。我们发现，机器生成的提示的特点是最后一个标记通常是可理解的，并且对生成有很强的影响。前面的标记中有一小部分但一致的比例是填充符，它们可能作为优化过程固定标记数量的副产品出现在提示中。剩余的标记往往与生成至少具有松散的语义关系，尽管它们与生成没有良好的句法关系。此外，我们发现，我们对机器生成的提示应用的某些消融也可以应用于自然语言序列，从而导致类似的行为，这表明自动提示是 LM 处理语言输入方式的直接结果。</li>
</ul>

<h3>Title: LCFO: Long Context and Long Form Output Dataset and Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Marta R. Costa-jussà, Pierre Andrews, Mariano Coria Meglioli, Joy Chen, Joe Chuang, David Dale, Christophe Ropers, Alexandre Mourachko, Eduardo Sánchez, Holger Schwenk, Tuan Tran, Arina Turkatenko, Carleigh Wood</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08268">https://arxiv.org/abs/2412.08268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08268">https://arxiv.org/pdf/2412.08268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08268]] LCFO: Long Context and Long Form Output Dataset and Benchmarking(https://arxiv.org/abs/2412.08268)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, long context</a></li>
<li><strong>Abstract: </strong>This paper presents the Long Context and Form Output (LCFO) benchmark, a novel evaluation framework for assessing gradual summarization and summary expansion capabilities across diverse domains. LCFO consists of long input documents (5k words average length), each of which comes with three summaries of different lengths (20%, 10%, and 5% of the input text), as well as approximately 15 questions and answers (QA) related to the input content. Notably, LCFO also provides alignments between specific QA pairs and corresponding summaries in 7 domains. The primary motivation behind providing summaries of different lengths is to establish a controllable framework for generating long texts from shorter inputs, i.e. summary expansion. To establish an evaluation metric framework for summarization and summary expansion, we provide human evaluation scores for human-generated outputs, as well as results from various state-of-the-art large language models (LLMs). GPT-4o-mini achieves best human scores among automatic systems in both summarization and summary expansion tasks (~ +10% and +20%, respectively). It even surpasses human output quality in the case of short summaries (~ +7%). Overall automatic metrics achieve low correlations with human evaluation scores (~ 0.4) but moderate correlation on specific evaluation aspects such as fluency and attribution (~ 0.6). The LCFO benchmark offers a standardized platform for evaluating summarization and summary expansion performance, as well as corresponding automatic metrics, thereby providing an important evaluation framework to advance generative AI.</li>
<li><strong>摘要：</strong>本文提出了长上下文和形式输出 (LCFO) 基准，这是一个用于评估跨不同领域的逐步摘要和摘要扩展能力的新颖评估框架。LCFO 由长输入文档（平均长度 5k 字）组成，每个文档都带有三个不同长度的摘要（分别占输入文本的 20%、10% 和 5%），以及大约 15 个与输入内容相关的问题和答案 (QA)。值得注意的是，LCFO 还提供了 7 个领域中特定 QA 对与相应摘要之间的对齐。提供不同长度的摘要的主要动机是建立一个可控的框架，用于从较短的输入生成长文本，即摘要扩展。为了建立摘要和摘要扩展的评估指标框架，我们为人工生成的输出提供了人工评估分数，以及各种最先进的大型语言模型 (LLM) 的结果。 GPT-4o-mini 在摘要和摘要扩展任务中均获得了自动系统中最佳的人类分数（分别约为 +10% 和 +20%）。在简短摘要的情况下，它甚至超过了人类的输出质量（约为 +7%）。总体而言，自动指标与人类评估分数的相关性较低（约为 0.4），但在流畅性和归因等特定评估方面具有中等相关性（约为 0.6）。LCFO 基准提供了一个标准化的平台来评估摘要和摘要扩展性能以及相应的自动指标，从而为推进生成式人工智能提供了重要的评估框架。</li>
</ul>

<h3>Title: Y-NQ: English-Yor\`ub\'a Evaluation dataset for Open-Book Reading Comprehension and Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Marta R. Costa-jussà, Joy Chen, Ifeoluwanimi Adebara, Joe Chuang, Christophe Ropers, Eduardo Sánchez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08279">https://arxiv.org/abs/2412.08279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08279">https://arxiv.org/pdf/2412.08279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08279]] Y-NQ: English-Yor\`ub\'a Evaluation dataset for Open-Book Reading Comprehension and Text Generation(https://arxiv.org/abs/2412.08279)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The purpose of this work is to share an English-Yorùbá evaluation dataset for open-book reading comprehension and text generation to assess the performance of models both in a high- and a low- resource language. The dataset contains 358 questions and answers on 338 English documents and 208 Yorùbá documents. The average document length is ~ 10k words for English and 430 words for Yorùbá. Experiments show a consistent disparity in performance between the two languages, with Yorùbá falling behind English for automatic metrics even if documents are much shorter for this language. For a small set of documents with comparable length, performance of Yorùbá drops by x2.5 times. When analyzing performance by length, we observe that Yorùbá decreases performance dramatically for documents that reach 1500 words while English performance is barely affected at that length. Our dataset opens the door to showcasing if English LLM reading comprehension capabilities extend to Yorùbá, which for the evaluated LLMs is not the case.</li>
<li><strong>摘要：</strong>这项工作的目的是分享一个英语-约鲁巴语评估数据集，用于开放式阅读理解和文本生成，以评估高资源语言和低资源语言模型的性能。该数据集包含 338 份英语文档和 208 份约鲁巴语文档的 358 个问题和答案。英语文档的平均长度约为 10k 个单词，约鲁巴语文档的平均长度约为 430 个单词。实验表明，这两种语言的性能存在一致的差异，即使约鲁巴语的文档短得多，它在自动度量方面也落后于英语。对于一小组长度相当的文档，约鲁巴语的性能下降了 2.5 倍。在按长度分析性能时，我们观察到，对于达到 1500 个单词的文档，约鲁巴语的性能会急剧下降，而英语性能几乎不受影响。我们的数据集为展示英语 LLM 阅读理解能力是否扩展到约鲁巴语打开了大门，而对于评估的 LLM 而言，情况并非如此。</li>
</ul>

<h3>Title: Adaptive Prompting for Continual Relation Extraction: A Within-Task Variance Perspective</h3>
<ul>
<li><strong>Authors: </strong>Minh Le, Tien Ngoc Luu, An Nguyen The, Thanh-Thien Le, Trang Nguyen, Thanh Tung Nguyen, Linh Ngo Van, Thien Huu Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08285">https://arxiv.org/abs/2412.08285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08285">https://arxiv.org/pdf/2412.08285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08285]] Adaptive Prompting for Continual Relation Extraction: A Within-Task Variance Perspective(https://arxiv.org/abs/2412.08285)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>To address catastrophic forgetting in Continual Relation Extraction (CRE), many current approaches rely on memory buffers to rehearse previously learned knowledge while acquiring new tasks. Recently, prompt-based methods have emerged as potent alternatives to rehearsal-based strategies, demonstrating strong empirical performance. However, upon analyzing existing prompt-based approaches for CRE, we identified several critical limitations, such as inaccurate prompt selection, inadequate mechanisms for mitigating forgetting in shared parameters, and suboptimal handling of cross-task and within-task variances. To overcome these challenges, we draw inspiration from the relationship between prefix-tuning and mixture of experts, proposing a novel approach that employs a prompt pool for each task, capturing variations within each task while enhancing cross-task variances. Furthermore, we incorporate a generative model to consolidate prior knowledge within shared parameters, eliminating the need for explicit data storage. Extensive experiments validate the efficacy of our approach, demonstrating superior performance over state-of-the-art prompt-based and rehearsal-free methods in continual relation extraction.</li>
<li><strong>摘要：</strong>为了解决持续关系提取 (CRE) 中的灾难性遗忘问题，许多当前方法都依赖于记忆缓冲区来复习以前学到的知识，同时学习新任务。最近，基于提示的方法已成为基于复习策略的有力替代方案，并表现出了强大的经验性能。然而，在分析现有的基于提示的 CRE 方法时，我们发现了几个关键的局限性，例如提示选择不准确、缓解共享参数中遗忘的机制不足以及对跨任务和任务内差异的处理不够理想。为了克服这些挑战，我们从前缀调整和专家混合之间的关系中汲取灵感，提出了一种新颖的方法，该方法为每个任务使用一个提示池，捕获每个任务内的变化，同时增强跨任务差异。此外，我们结合了一个生成模型来巩固共享参数中的先验知识，从而无需显式数据存储。大量实验验证了我们方法的有效性，表明在持续关系提取方面，其性能优于最先进的基于提示和无排练的方法。</li>
</ul>

<h3>Title: Code LLMs: A Taxonomy-based Survey</h3>
<ul>
<li><strong>Authors: </strong>Nishat Raihan, Christian Newman, Marcos Zampieri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08291">https://arxiv.org/abs/2412.08291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08291">https://arxiv.org/pdf/2412.08291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08291]] Code LLMs: A Taxonomy-based Survey(https://arxiv.org/abs/2412.08291)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks and have recently expanded their impact to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL). This taxonomy-based survey provides a comprehensive analysis of LLMs in the NL-PL domain, investigating how these models are utilized in coding tasks and examining their methodologies, architectures, and training processes. We propose a taxonomy-based framework that categorizes relevant concepts, providing a unified classification system to facilitate a deeper understanding of this rapidly evolving field. This survey offers insights into the current state and future directions of LLMs in coding tasks, including their applications and limitations.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种 NLP 任务中都表现出了卓越的能力，并且最近将其影响扩展到编码任务，从而弥合了自然语言 (NL) 和编程语言 (PL) 之间的差距。这项基于分类法的调查对 NL-PL 领域的 LLM 进行了全面分析，研究了这些模型如何在编码任务中使用，并检查了它们的方法、架构和训练过程。我们提出了一个基于分类法的框架，对相关概念进行分类，提供了一个统一的分类系统，以促进对这个快速发展的领域的更深入了解。这项调查提供了对 LLM 在编码任务中的现状和未来方向的见解，包括它们的应用和局限性。</li>
</ul>

<h3>Title: Large Language Models Still Face Challenges in Multi-Hop Reasoning with External Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Haotong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08317">https://arxiv.org/abs/2412.08317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08317">https://arxiv.org/pdf/2412.08317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08317]] Large Language Models Still Face Challenges in Multi-Hop Reasoning with External Knowledge(https://arxiv.org/abs/2412.08317)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>We carry out a series of experiments to test large language models' multi-hop reasoning ability from three aspects: selecting and combining external knowledge, dealing with non-sequential reasoning tasks and generalising to data samples with larger numbers of hops. We test the GPT-3.5 model on four reasoning benchmarks with Chain-of-Thought prompting (and its variations). Our results reveal that despite the amazing performance achieved by large language models on various reasoning tasks, models still suffer from severe drawbacks which shows a large gap with humans.</li>
<li><strong>摘要：</strong>我们开展了一系列实验，从三个方面测试大型语言模型的多跳推理能力：选择和组合外部知识、处理非连续推理任务以及推广到具有更多跳数的数据样本。我们在四个推理基准上对 GPT-3.5 模型进行了测试，并使用了思维链提示（及其变体）。我们的结果表明，尽管大型语言模型在各种推理任务上取得了惊人的表现，但模型仍然存在严重缺陷，与人类相比还存在很大差距。</li>
</ul>

<h3>Title: SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs</h3>
<ul>
<li><strong>Authors: </strong>Sultan Alrashed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08347">https://arxiv.org/abs/2412.08347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08347">https://arxiv.org/pdf/2412.08347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08347]] SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs(https://arxiv.org/abs/2412.08347)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present SmolTulu-1.7b-Instruct, referenced in this report as SmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAI's Tulu 3 post-training pipeline to enhance Huggingface's SmolLM2-1.7B base model. Through comprehensive empirical analysis using a 135M parameter model, we demonstrate that the relationship between learning rate and batch size significantly impacts model performance in a task-dependent manner. Our findings reveal a clear split: reasoning tasks like ARC and GSM8K benefit from higher learning rate to batch size ratios, while pattern recognition tasks such as HellaSwag and IFEval show optimal performance with lower ratios. These insights informed the development of SmolTulu, which achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval ($\Delta$11%), and mathematical reasoning with 51.6% on GSM8K ($\Delta$3.4%), with an alternate version achieving scoring 57.1% on ARC ($\Delta5.4%$). We release our model, training recipes, and ablation studies to facilitate further research in efficient model alignment, demonstrating that careful adaptation of optimization dynamics can help bridge the capability gap between small and large language models.</li>
<li><strong>摘要：</strong>我们介绍了 SmolTulu-1.7b-Instruct（本报告中称为 SmolTulu-DPO-1130），这是一种指令调整语言模型，可调整 AllenAI 的 Tulu 3 后训练管道以增强 Huggingface 的 SmolLM2-1.7B 基础模型。通过使用 135M 参数模型进行全面的实证分析，我们证明了学习率和批次大小之间的关系以任务相关的方式显著影响模型性能。我们的研究结果揭示了一个明显的分歧：ARC 和 GSM8K 等推理任务受益于更高的学习率与批次大小比率，而 HellaSwag 和 IFEval 等模式识别任务则以较低的比率表现出最佳性能。这些见解为 SmolTulu 的开发提供了参考，该模型在指令跟踪方面在 2B 以下参数模型中取得了最佳性能，在 IFEval 上得分为 67.7% ($\Delta$11%)，在 GSM8K 上数学推理得分为 51.6% ($\Delta$3.4%)，而替代版本在 ARC 上得分为 57.1% ($\Delta$5.4%$)。我们发布了我们的模型、训练方法和消融研究，以促进有效模型对齐方面的进一步研究，表明精心调整优化动态可以帮助缩小小型和大型语言模型之间的能力差距。</li>
</ul>

<h3>Title: NyayaAnumana & INLegalLlama: The Largest Indian Legal Judgment Prediction Dataset and Specialized Language Model for Enhanced Decision Analysis</h3>
<ul>
<li><strong>Authors: </strong>Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08385">https://arxiv.org/abs/2412.08385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08385">https://arxiv.org/pdf/2412.08385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08385]] NyayaAnumana & INLegalLlama: The Largest Indian Legal Judgment Prediction Dataset and Specialized Language Model for Enhanced Decision Analysis(https://arxiv.org/abs/2412.08385)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The integration of artificial intelligence (AI) in legal judgment prediction (LJP) has the potential to transform the legal landscape, particularly in jurisdictions like India, where a significant backlog of cases burdens the legal system. This paper introduces NyayaAnumana, the largest and most diverse corpus of Indian legal cases compiled for LJP, encompassing a total of 7,02,945 preprocessed cases. NyayaAnumana, which combines the words "Nyay" (judgment) and "Anuman" (prediction or inference) respectively for most major Indian languages, includes a wide range of cases from the Supreme Court, High Courts, Tribunal Courts, District Courts, and Daily Orders and, thus, provides unparalleled diversity and coverage. Our dataset surpasses existing datasets like PredEx and ILDC, offering a comprehensive foundation for advanced AI research in the legal domain. In addition to the dataset, we present INLegalLlama, a domain-specific generative large language model (LLM) tailored to the intricacies of the Indian legal system. It is developed through a two-phase training approach over a base LLaMa model. First, Indian legal documents are injected using continual pretraining. Second, task-specific supervised finetuning is done. This method allows the model to achieve a deeper understanding of legal contexts. Our experiments demonstrate that incorporating diverse court data significantly boosts model accuracy, achieving approximately 90% F1-score in prediction tasks. INLegalLlama not only improves prediction accuracy but also offers comprehensible explanations, addressing the need for explainability in AI-assisted legal decisions.</li>
<li><strong>摘要：</strong>人工智能 (AI) 与法律判决预测 (LJP) 的结合有可能改变法律格局，特别是在印度这样的司法管辖区，那里有大量积压案件给法律系统带来负担。本文介绍了 NyayaAnumana，这是为 LJP 编制的最大、最多样化的印度法律案例语料库，共包含 7,02,945 个预处理案例。NyayaAnumana 结合了大多数主要印度语言的单词“Nyay”（判决）和“Anuman”（预测或推断），包括来自最高法院、高等法院、审裁法院、地区法院和日常命令的广泛案例，因此提供了无与伦比的多样性和覆盖范围。我们的数据集超越了 PredEx 和 ILDC 等现有数据集，为法律领域的高级 AI 研究提供了全面的基础。除了数据集之外，我们还介绍了 INLegalLlama，这是一个针对印度法律体系的复杂性而量身定制的领域特定生成式大型语言模型 (LLM)。它是通过基于基础 LLaMa 模型的两阶段训练方法开发的。首先，使用持续预训练注入印度法律文件。其次，进行特定于任务的监督微调。这种方法使模型能够更深入地了解法律背景。我们的实验表明，结合不同的法庭数据可显著提高模型准确性，在预测任务中实现约 90% 的 F1 分数。INLegalLlama 不仅提高了预测准确性，而且还提供了易于理解的解释，满足了 AI 辅助法律决策对可解释性的需求。</li>
</ul>

<h3>Title: SweetieChat: A Strategy-Enhanced Role-playing Framework for Diverse Scenarios Handling Emotional Support Agent</h3>
<ul>
<li><strong>Authors: </strong>Jing Ye, Lu Xiang, Yaping Zhang, Chengqing Zong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08389">https://arxiv.org/abs/2412.08389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08389">https://arxiv.org/pdf/2412.08389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08389]] SweetieChat: A Strategy-Enhanced Role-playing Framework for Diverse Scenarios Handling Emotional Support Agent(https://arxiv.org/abs/2412.08389)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated promising potential in providing empathetic support during interactions. However, their responses often become verbose or overly formulaic, failing to adequately address the diverse emotional support needs of real-world scenarios. To tackle this challenge, we propose an innovative strategy-enhanced role-playing framework, designed to simulate authentic emotional support conversations. Specifically, our approach unfolds in two steps: (1) Strategy-Enhanced Role-Playing Interactions, which involve three pivotal roles -- Seeker, Strategy Counselor, and Supporter -- engaging in diverse scenarios to emulate real-world interactions and promote a broader range of dialogues; and (2) Emotional Support Agent Training, achieved through fine-tuning LLMs using our specially constructed dataset. Within this framework, we develop the \textbf{ServeForEmo} dataset, comprising an extensive collection of 3.7K+ multi-turn dialogues and 62.8K+ utterances. We further present \textbf{SweetieChat}, an emotional support agent capable of handling diverse open-domain scenarios. Extensive experiments and human evaluations confirm the framework's effectiveness in enhancing emotional support, highlighting its unique ability to provide more nuanced and tailored assistance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已显示出在互动过程中提供共情支持的巨大潜力。然而，它们的回应往往冗长或过于公式化，无法充分满足现实场景中多样化的情感支持需求。为了应对这一挑战，我们提出了一个创新的策略增强角色扮演框架，旨在模拟真实的情感支持对话。具体来说，我们的方法分为两个步骤：(1) 策略增强角色扮演互动，涉及三个关键角色——寻求者、策略顾问和支持者——参与不同的场景以模拟现实世界的互动并促进更广泛的对话；(2) 情感支持代理培训，通过使用我们专门构建的数据集对 LLM 进行微调来实现。在这个框架内，我们开发了 \textbf{ServeForEmo} 数据集，其中包含 3.7K+ 多轮对话和 62.8K+ 话语的大量集合。我们进一步介绍了 \textbf{SweetieChat}，这是一款能够处理各种开放域场景的情感支持代理。大量实验和人工评估证实了该框架在增强情感支持方面的有效性，凸显了其提供更细致入微和量身定制的帮助的独特能力。</li>
</ul>

<h3>Title: The Roles of English in Evaluating Multilingual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wessel Poelman, Miryam de Lhoneux</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08392">https://arxiv.org/abs/2412.08392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08392">https://arxiv.org/pdf/2412.08392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08392]] The Roles of English in Evaluating Multilingual Language Models(https://arxiv.org/abs/2412.08392)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Multilingual natural language processing is getting increased attention, with numerous models, benchmarks, and methods being released for many languages. English is often used in multilingual evaluation to prompt language models (LMs), mainly to overcome the lack of instruction tuning data in other languages. In this position paper, we lay out two roles of English in multilingual LM evaluations: as an interface and as a natural language. We argue that these roles have different goals: task performance versus language understanding. This discrepancy is highlighted with examples from datasets and evaluation setups. Numerous works explicitly use English as an interface to boost task performance. We recommend to move away from this imprecise method and instead focus on furthering language understanding.</li>
<li><strong>摘要：</strong>多语言自然语言处理正受到越来越多的关注，许多语言的模型、基准和方法都已发布。英语经常用于多语言评估以促进语言模型 (LM)，主要是为了克服其他语言缺乏指令调整数据的问题。在这篇立场文件中，我们列出了英语在多语言 LM 评估中的两个角色：作为界面和作为自然语言。我们认为这些角色有不同的目标：任务表现与语言理解。数据集和评估设置中的示例突出显示了这种差异。许多作品明确使用英语作为界面来提高任务表现。我们建议放弃这种不精确的方法，而是专注于进一步理解语言。</li>
</ul>

<h3>Title: Learning to Reason via Self-Iterative Process Feedback for Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Chen, Jin Wang, Xuejie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08393">https://arxiv.org/abs/2412.08393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08393">https://arxiv.org/pdf/2412.08393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08393]] Learning to Reason via Self-Iterative Process Feedback for Small Language Models(https://arxiv.org/abs/2412.08393)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Small language models (SLMs) are more efficient, cost-effective, and customizable than large language models (LLMs), though they often underperform in specific areas like reasoning. Past methods for enhancing SLMs' reasoning, such as supervised fine-tuning and distillation, often depend on costly external signals, resulting in SLMs being overly confident with limited supervision signals, thus limiting their abilities. Therefore, this study enables SLMs to learn to reason from self-iterative feedback. By combining odds ratio preference optimization (ORPO), we fine-tune and align SLMs using positive and negative signals generated by themselves. Additionally, we introduce process supervision for rewards in preference alignment by sampling-based inference simulation and process reward models. Compared to Supervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B by 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed method also demonstrated superior out-of-domain generalization capabilities on MMLU_Math and HumanEval.</li>
<li><strong>摘要：</strong>小语言模型 (SLM) 比大语言模型 (LLM) 更高效、更具成本效益且可定制性更强，尽管它们在推理等特定领域往往表现不佳。过去用于增强 SLM 推理能力的方法，例如监督微调和提炼，通常依赖于昂贵的外部信号，导致 SLM 对有限的监督信号过于自信，从而限制了它们的能力。因此，本研究使 SLM 能够从自我迭代反馈中学习推理。通过结合比值比偏好优化 (ORPO)，我们使用 SLM 自身生成的正信号和负信号对 SLM 进行微调和对齐。此外，我们通过基于采样的推理模拟和过程奖励模型引入了偏好对齐中奖励的过程监督。与监督微调 (SFT) 相比，我们的方法将 Gemma-2B 在 GSM8K 上的性能提高了 12.43 (Acc)，在 MBPP 上的性能提高了 3.95 (Pass@1)。此外，所提出的方法还在MMLU_Math和HumanEval上表现出卓越的域外泛化能力。</li>
</ul>

<h3>Title: Detecting Conversational Mental Manipulation with Intent-Aware Prompting</h3>
<ul>
<li><strong>Authors: </strong>Jiayuan Ma, Hongbin Na, Zimu Wang, Yining Hua, Yue Liu, Wei Wang, Ling Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08414">https://arxiv.org/abs/2412.08414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08414">https://arxiv.org/pdf/2412.08414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08414]] Detecting Conversational Mental Manipulation with Intent-Aware Prompting(https://arxiv.org/abs/2412.08414)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Mental manipulation severely undermines mental wellness by covertly and negatively distorting decision-making. While there is an increasing interest in mental health care within the natural language processing community, progress in tackling manipulation remains limited due to the complexity of detecting subtle, covert tactics in conversations. In this paper, we propose Intent-Aware Prompting (IAP), a novel approach for detecting mental manipulations using large language models (LLMs), providing a deeper understanding of manipulative tactics by capturing the underlying intents of participants. Experimental results on the MentalManip dataset demonstrate superior effectiveness of IAP against other advanced prompting strategies. Notably, our approach substantially reduces false negatives, helping detect more instances of mental manipulation with minimal misjudgment of positive cases. The code of this paper is available at this https URL.</li>
<li><strong>摘要：</strong>心理操纵会通过隐蔽地、负面地扭曲决策，严重损害心理健康。尽管自然语言处理社区对心理健康护理的兴趣日益浓厚，但由于检测对话中微妙、隐蔽的策略非常复杂，解决操纵问题的进展仍然有限。在本文中，我们提出了意图感知提示 (IAP)，这是一种使用大型语言模型 (LLM) 检测心理操纵的新方法，通过捕捉参与者的潜在意图，可以更深入地了解操纵策略。在 MentalManip 数据集上的实验结果表明，IAP 比其他高级提示策略更有效。值得注意的是，我们的方法大大减少了假阴性，有助于检测更多心理操纵实例，同时最大限度地减少对阳性案例的误判。本文的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Assessing Personalized AI Mentoring with Large Language Models in the Computing Field</h3>
<ul>
<li><strong>Authors: </strong>Xiao Luo, Sean O'Connell, Shamima Mithun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08430">https://arxiv.org/abs/2412.08430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08430">https://arxiv.org/pdf/2412.08430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08430]] Assessing Personalized AI Mentoring with Large Language Models in the Computing Field(https://arxiv.org/abs/2412.08430)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This paper provides an in-depth evaluation of three state-of-the-art Large Language Models (LLMs) for personalized career mentoring in the computing field, using three distinct student profiles that consider gender, race, and professional levels. We evaluated the performance of GPT-4, LLaMA 3, and Palm 2 using a zero-shot learning approach without human intervention. A quantitative evaluation was conducted through a custom natural language processing analytics pipeline to highlight the uniqueness of the responses and to identify words reflecting each student's profile, including race, gender, or professional level. The analysis of frequently used words in the responses indicates that GPT-4 offers more personalized mentoring compared to the other two LLMs. Additionally, a qualitative evaluation was performed to see if human experts reached similar conclusions. The analysis of survey responses shows that GPT-4 outperformed the other two LLMs in delivering more accurate and useful mentoring while addressing specific challenges with encouragement languages. Our work establishes a foundation for developing personalized mentoring tools based on LLMs, incorporating human mentors in the process to deliver a more impactful and tailored mentoring experience.</li>
<li><strong>摘要：</strong>本文使用考虑性别、种族和专业水平的三个不同学生档案，对三种最先进的大型语言模型 (LLM) 进行了深入评估，以用于计算领域的个性化职业指导。我们使用零样本学习方法评估了 GPT-4、LLaMA 3 和 Palm 2 的性能，无需人工干预。通过自定义自然语言处理分析管道进行定量评估，以突出响应的独特性并识别反映每个学生档案（包括种族、性别或专业水平）的单词。对响应中常用词的分析表明，与其他两个 LLM 相比，GPT-4 提供了更个性化的指导。此外，还进行了定性评估，以查看人类专家是否得出了类似的结论。对调查回复的分析表明，GPT-4 在提供更准确和有用的指导方面优于其他两个 LLM，同时通过鼓励语言解决特定挑战。我们的工作为开发基于法学硕士的个性化指导工具奠定了基础，并在过程中融入了人类导师，以提供更具影响力和针对性的指导体验。</li>
</ul>

<h3>Title: Mitigating Out-of-Entity Errors in Named Entity Recognition: A Sentence-Level Strategy</h3>
<ul>
<li><strong>Authors: </strong>Guochao Jiang, Ziqin Luo, Chengwei Hu, Zepeng Ding, Deqing Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08434">https://arxiv.org/abs/2412.08434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08434">https://arxiv.org/pdf/2412.08434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08434]] Mitigating Out-of-Entity Errors in Named Entity Recognition: A Sentence-Level Strategy(https://arxiv.org/abs/2412.08434)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Many previous models of named entity recognition (NER) suffer from the problem of Out-of-Entity (OOE), i.e., the tokens in the entity mentions of the test samples have not appeared in the training samples, which hinders the achievement of satisfactory performance. To improve OOE-NER performance, in this paper, we propose a new framework, namely S+NER, which fully leverages sentence-level information. Our S+NER achieves better OOE-NER performance mainly due to the following two particular designs. 1) It first exploits the pre-trained language model's capability of understanding the target entity's sentence-level context with a template set. 2) Then, it refines the sentence-level representation based on the positive and negative templates, through a contrastive learning strategy and template pooling method, to obtain better NER results. Our extensive experiments on five benchmark datasets have demonstrated that, our S+NER outperforms some state-of-the-art OOE-NER models.</li>
<li><strong>摘要：</strong>先前的许多命名实体识别 (NER) 模型都存在实体外 (OOE) 的问题，即测试样本的实体提及中的标记未出现在训练样本中，从而阻碍了令人满意的性能的实现。为了提高 OOE-NER 性能，本文提出了一个新的框架，即 S+NER，它充分利用了句子级信息。我们的 S+NER 实现了更好的 OOE-NER 性能，主要归功于以下两个特殊设计。1) 它首先利用预训练语言模型通过模板集理解目标实体的句子级上下文的能力。2) 然后，它通过对比学习策略和模板池化方法基于正负模板细化句子级表示，以获得更好的 NER 结果。我们在五个基准数据集上进行的大量实验表明，我们的 S+NER 优于一些最先进的 OOE-NER 模型。</li>
</ul>

<h3>Title: Comparative Opinion Mining in Product Reviews: Multi-perspective Prompt-based Learning</h3>
<ul>
<li><strong>Authors: </strong>Hai-Yen Thi Nguyen, Cam-Van Thi Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08508">https://arxiv.org/abs/2412.08508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08508">https://arxiv.org/pdf/2412.08508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08508]] Comparative Opinion Mining in Product Reviews: Multi-perspective Prompt-based Learning(https://arxiv.org/abs/2412.08508)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Comparative reviews are pivotal in understanding consumer preferences and influencing purchasing decisions. Comparative Quintuple Extraction (COQE) aims to identify five key components in text: the target entity, compared entities, compared aspects, opinions on these aspects, and polarity. Extracting precise comparative information from product reviews is challenging due to nuanced language and sequential task errors in traditional methods. To mitigate these problems, we propose MTP-COQE, an end-to-end model designed for COQE. Leveraging multi-perspective prompt-based learning, MTP-COQE effectively guides the generative model in comparative opinion mining tasks. Evaluation on the Camera-COQE (English) and VCOM (Vietnamese) datasets demonstrates MTP-COQE's efficacy in automating COQE, achieving superior performance with a 1.41% higher F1 score than the previous baseline models on the English dataset. Additionally, we designed a strategy to limit the generative model's creativity to ensure the output meets expectations. We also performed data augmentation to address data imbalance and to prevent the model from becoming biased towards dominant samples.</li>
<li><strong>摘要：</strong>比较评论对于理解消费者偏好和影响购买决策至关重要。比较五元组提取 (COQE) 旨在识别文本中的五个关键组成部分：目标实体、比较实体、比较方面、对这些方面的看法以及极性。由于传统方法中的语言细微差别和顺序任务错误，从产品评论中提取精确的比较信息具有挑战性。为了缓解这些问题，我们提出了 MTP-COQE，这是一种专为 COQE 设计的端到端模型。利用多视角提示式学习，MTP-COQE 有效地指导了比较意见挖掘任务中的生成模型。在 Camera-COQE（英语）和 VCOM（越南语）数据集上的评估证明了 MTP-COQE 在自动化 COQE 方面的有效性，在英语数据集上实现了比以前基线模型高 1.41% 的卓越性能。此外，我们设计了一种策略来限制生成模型的创造力，以确保输出符合预期。我们还进行了数据增强来解决数据不平衡问题并防止模型偏向主导样本。</li>
</ul>

<h3>Title: Bridging Relevance and Reasoning: Rationale Distillation in Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Pengyue Jia, Derong Xu, Xiaopeng Li, Zhaocheng Du, Xiangyang Li, Xiangyu Zhao, Yichao Wang, Yuhao Wang, Huifeng Guo, Ruiming Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08519">https://arxiv.org/abs/2412.08519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08519">https://arxiv.org/pdf/2412.08519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08519]] Bridging Relevance and Reasoning: Rationale Distillation in Retrieval-Augmented Generation(https://arxiv.org/abs/2412.08519)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The reranker and generator are two critical components in the Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking relevant documents and generating responses. However, due to differences in pre-training data and objectives, there is an inevitable gap between the documents ranked as relevant by the reranker and those required by the generator to support answering the query. To address this gap, we propose RADIO, a novel and practical preference alignment framework with RAtionale DIstillatiOn. Specifically, We first propose a rationale extraction method that leverages the reasoning capabilities of Large Language Models (LLMs) to extract the rationales necessary for answering the query. Subsequently, a rationale-based alignment process is designed to rerank the documents based on the extracted rationales, and fine-tune the reranker to align the preferences. We conduct extensive experiments on two tasks across three datasets to demonstrate the effectiveness of our approach compared to baseline methods. Our code is released online to ease reproduction.</li>
<li><strong>摘要：</strong>重排器和生成器是检索增强生成 (即 RAG) 流程中的两个关键组件，负责对相关文档进行排序并生成响应。然而，由于预训练数据和目标的差异，重排器评定为相关的文档与生成器支持回答查询所需的文档之间存在不可避免的差距。为了解决这一差距，我们提出了 RADIO，这是一个具有 RAtionale DIstillatiOn 的新颖实用的偏好对齐框架。具体来说，我们首先提出了一种原理提取方法，该方法利用大型语言模型 (LLM) 的推理能力来提取回答查询所需的原理。随后，设计了一个基于原理的对齐过程，根据提取的原理对文档进行重排，并微调重排器以对齐偏好。我们在三个数据集上对两个任务进行了广泛的实验，以证明我们的方法与基线方法相比的有效性。我们的代码已在线发布以方便复制。</li>
</ul>

<h3>Title: EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache Compression Based on Global-Local Importance</h3>
<ul>
<li><strong>Authors: </strong>Yingxin Li, Ye Li, Yuan Meng, Xinzhu Ma, Zihan Geng, Shutao Xia, Zhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08521">https://arxiv.org/abs/2412.08521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08521">https://arxiv.org/pdf/2412.08521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08521]] EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache Compression Based on Global-Local Importance(https://arxiv.org/abs/2412.08521)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to advance, the demand for higher quality and faster processing of long contexts across various applications is growing. KV cache is widely adopted as it stores previously generated key and value tokens, effectively reducing redundant computations during inference. However, as memory overhead becomes a significant concern, efficient compression of KV cache has gained increasing attention. Most existing methods perform compression from two perspectives: identifying important tokens and designing compression strategies. However, these approaches often produce biased distributions of important tokens due to the influence of accumulated attention scores or positional encoding. Furthermore, they overlook the sparsity and redundancy across different heads, which leads to difficulties in preserving the most effective information at the head level. To this end, we propose EMS to overcome these limitations, while achieving better KV cache compression under extreme compression ratios. Specifically, we introduce a Global-Local score that combines accumulated attention scores from both global and local KV tokens to better identify the token importance. For the compression strategy, we design an adaptive and unified Evict-then-Merge framework that accounts for the sparsity and redundancy of KV tokens across different heads. Additionally, we implement the head-wise parallel compression through a zero-class mechanism to enhance efficiency. Extensive experiments demonstrate our SOTA performance even under extreme compression ratios. EMS consistently achieves the lowest perplexity, improves scores by over 1.28 points across four LLMs on LongBench under a 256 cache budget, and preserves 95% retrieval accuracy with a cache budget less than 2% of the context length in the Needle-in-a-Haystack task.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的不断发展，各种应用程序对更高质量和更快处理长上下文的需求日益增长。KV 缓存被广泛采用，因为它可以存储先前生成的键和值标记，从而有效减少推理过程中的冗余计算。然而，随着内存开销成为一个重要问题，KV 缓存的有效压缩越来越受到关注。大多数现有方法从两个角度执行压缩：识别重要标记和设计压缩策略。然而，这些方法通常会由于累积注意力分数或位置编码的影响而产生重要标记的偏差分布。此外，它们忽略了不同头部之间的稀疏性和冗余性，这导致难以在头部级别保留最有效的信息。为此，我们提出了 EMS 来克服这些限制，同时在极端压缩率下实现更好的 KV 缓存压缩。具体来说，我们引入了一个全局-本地分数，该分数结合了全局和本地 KV 标记的累积注意力分数，以更好地识别标记的重要性。对于压缩策略，我们设计了一个自适应的统一 Evict-then-Merge 框架，该框架考虑了不同 head 之间 KV 标记的稀疏性和冗余性。此外，我们通过零类机制实现 head-wise 并行压缩以提高效率。大量实验表明，即使在极端压缩率下，我们的 SOTA 性能也很好。EMS 始终实现最低困惑度，在 256 缓存预算下在 LongBench 上的四个 LLM 中将分数提高了 1.28 分以上，并且在 Needle-in-a-Haystack 任务中以小于上下文长度 2% 的缓存预算保持了 95% 的检索准确率。</li>
</ul>

<h3>Title: Continual Learning for Encoder-only Language Models via a Discrete Key-Value Bottleneck</h3>
<ul>
<li><strong>Authors: </strong>Andor Diera, Lukas Galke, Fabian Karl, Ansgar Scherp</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08528">https://arxiv.org/abs/2412.08528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08528">https://arxiv.org/pdf/2412.08528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08528]] Continual Learning for Encoder-only Language Models via a Discrete Key-Value Bottleneck(https://arxiv.org/abs/2412.08528)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Continual learning remains challenging across various natural language understanding tasks. When models are updated with new training data, they risk catastrophic forgetting of prior knowledge. In the present work, we introduce a discrete key-value bottleneck for encoder-only language models, allowing for efficient continual learning by requiring only localized updates. Inspired by the success of a discrete key-value bottleneck in vision, we address new and NLP-specific challenges. We experiment with different bottleneck architectures to find the most suitable variants regarding language, and present a generic discrete key initialization technique for NLP that is task independent. We evaluate the discrete key-value bottleneck in four continual learning NLP scenarios and demonstrate that it alleviates catastrophic forgetting. We showcase that it offers competitive performance to other popular continual learning methods, with lower computational costs.</li>
<li><strong>摘要：</strong>在各种自然语言理解任务中，持续学习仍然具有挑战性。当使用新的训练数据更新模型时，它们可能会灾难性地遗忘先前的知识。在目前的研究中，我们为仅编码器语言模型引入了离散键值瓶颈，通过仅要求局部更新即可实现高效的持续学习。受到视觉中离散键值瓶颈成功的启发，我们解决了新的和特定于 NLP 的挑战。我们尝试了不同的瓶颈架构以找到最适合语言的变体，并提出了一种与任务无关的通用 NLP 离散键初始化技术。我们在四种持续学习 NLP 场景中评估了离散键值瓶颈，并证明它可以缓解灾难性遗忘。我们展示了它以更低的计算成本提供了与其他流行的持续学习方法相比具有竞争力的性能。</li>
</ul>

<h3>Title: Advancing Single- and Multi-task Text Classification through Large Language Model Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Hang Zhao, Qile P. Chen, Yijing Barry Zhang, Gang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08587">https://arxiv.org/abs/2412.08587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08587">https://arxiv.org/pdf/2412.08587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08587]] Advancing Single- and Multi-task Text Classification through Large Language Model Fine-tuning(https://arxiv.org/abs/2412.08587)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Both encoder-only models (e.g., BERT, RoBERTa) and large language models (LLMs, e.g., Llama3) have been widely used for text classification tasks. However, there is a lack of systematic studies comparing the performance of encoder-based models and LLMs in text classification, particularly when fine-tuning is involved. This study employed a diverse range of models and methods, varying in size and architecture, and including both fine-tuned and pre-trained approaches. We first assessed the performances of these LLMs on the 20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only RoBERTa models. Additionally, we explored the multi-task capabilities of both model types by combining multiple classification tasks, including intent detection and slot-filling, into a single model using data from both datasets. Our results indicate that fully fine-tuned Llama3-70B models outperform RoBERTa-large and other decoder LLMs across various classification tasks and datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the performance of dual-model setups in both tasks across both datasets. Overall, our study provides a comprehensive benchmark of encoder-only and LLM models on text classification tasks and demonstrates a method to combine two or more fully fine-tuned decoder LLMs for reduced latency and equivalent performance.</li>
<li><strong>摘要：</strong>仅编码器模型（例如 BERT、RoBERTa）和大型语言模型（LLM，例如 Llama3）都已广泛用于文本分类任务。然而，缺乏系统的研究来比较基于编码器的模型和 LLM 在文本分类中的表现，特别是在涉及微调时。这项研究采用了各种各样的模型和方法，大小和架构各不相同，包括微调和预训练方法。我们首先评估了这些 LLM 在 20 个新闻组 (20NG) 和 MASSIVE 数据集上的表现，并将它们与仅编码器的 RoBERTa 模型进行比较。此外，我们通过将多个分类任务（包括意图检测和槽填充）组合到使用两个数据集的数据的单个模型中，探索了这两种模型类型的多任务能力。我们的结果表明，经过完全微调的 Llama3-70B 模型在各种分类任务和数据集上的表现均优于 RoBERTa-large 和其他解码器 LLM。此外，合并后的多任务微调 LLM 在两个数据集上的两个任务中的表现都与双模型设置相当。总体而言，我们的研究为文本分类任务中的编码器和 LLM 模型提供了全面的基准，并展示了一种结合两个或多个完全微调的解码器 LLM 以减少延迟并获得同等性能的方法。</li>
</ul>

<h3>Title: Der Effizienz- und Intelligenzbegriff in der Lexikographie und kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte nachbilden?</h3>
<ul>
<li><strong>Authors: </strong>Ivan Arias-Arias, Maria Jose Dominguez Vazquez, Carlos Valcarcel Riveiro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08599">https://arxiv.org/abs/2412.08599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08599">https://arxiv.org/pdf/2412.08599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08599]] Der Effizienz- und Intelligenzbegriff in der Lexikographie und kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte nachbilden?(https://arxiv.org/abs/2412.08599)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>By means of pilot experiments for the language pair German and Galician, this paper examines the concept of efficiency and intelligence in lexicography and artificial intelligence, AI. The aim of the experiments is to gain empirically and statistically based insights into the lexicographical text type,dictionary article, in the responses of ChatGPT 3.5, as well as into the lexicographical data on which this chatbot was trained. Both quantitative and qualitative methods are used for this purpose. The analysis is based on the evaluation of the outputs of several sessions with the same prompt in ChatGPT 3.5. On the one hand, the algorithmic performance of intelligent systems is evaluated in comparison with data from lexicographical works. On the other hand, the ChatGPT data supplied is analysed using specific text passages of the aforementioned lexicographical text type. The results of this study not only help to evaluate the efficiency of this chatbot regarding the creation of dictionary articles, but also to delve deeper into the concept of intelligence, the thought processes and the actions to be carried out in both disciplines.</li>
<li><strong>摘要：</strong>本文通过对德语和加利西亚语进行试点实验，探讨了词典编纂和人工智能 (AI) 中的效率和智能概念。实验的目的是从经验和统计的角度深入了解 ChatGPT 3.5 的响应中的词典文本类型词典冠词，以及训练此聊天机器人的词典数据。为此，我们同时使用了定量和定性方法。分析基于对 ChatGPT 3.5 中具有相同提示的多个会话的输出的评估。一方面，通过与词典编纂作品中的数据进行比较来评估智能系统的算法性能。另一方面，使用上述词典文本类型的特定文本段落分析所提供的 ChatGPT 数据。这项研究的结果不仅有助于评估该聊天机器人在创建词典文章方面的效率，而且还有助于更深入地探讨智能的概念、思维过程以及这两个学科中要开展的行动。</li>
</ul>

<h3>Title: Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Li, Yongchang Hao, Haoyu Xu, Xing Wang, Yu Hong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08615">https://arxiv.org/abs/2412.08615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08615">https://arxiv.org/pdf/2412.08615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08615]] Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models(https://arxiv.org/abs/2412.08615)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Despite the advancements in training Large Language Models (LLMs) with alignment techniques to enhance the safety of generated content, these models remain susceptible to jailbreak, an adversarial attack method that exposes security vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG) method has demonstrated the ability to automatically generate adversarial suffixes that jailbreak state-of-the-art LLMs. However, the optimization process involved in GCG is highly time-consuming, rendering the jailbreaking pipeline inefficient. In this paper, we investigate the process of GCG and identify an issue of Indirect Effect, the key bottleneck of the GCG optimization. To this end, we propose the Model Attack Gradient Index GCG (MAGIC), that addresses the Indirect Effect by exploiting the gradient information of the suffix tokens, thereby accelerating the procedure by having less computation and fewer iterations. Our experiments on AdvBench show that MAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates (ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of 74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on GPT-3.5. Code is available at this https URL.</li>
<li><strong>摘要：</strong>尽管在使用对齐技术训练大型语言模型 (LLM) 以增强生成内容的安全性方面取得了进展，但这些模型仍然容易受到越狱的攻击，越狱是一种暴露 LLM 安全漏洞的对抗性攻击方法。值得注意的是，贪婪坐标梯度 (GCG) 方法已证明能够自动生成对抗性后缀，从而越狱最先进的 LLM。然而，GCG 所涉及的优化过程非常耗时，导致越狱流程效率低下。在本文中，我们研究了 GCG 的过程并确定了间接效应问题，这是 GCG 优化的关键瓶颈。为此，我们提出了模型攻击梯度指数 GCG (MAGIC)，它通过利用后缀标记的梯度信息来解决间接效应，从而通过减少计算和迭代来加速该过程。我们在 AdvBench 上的实验表明，MAGIC 实现了高达 1.5 倍的加速，同时保持与其他基准相当甚至更高的攻击成功率 (ASR)。我们的 MAGIC 在 Llama-2 上实现了 74% 的 ASR，在对 GPT-3.5 进行传输攻击时实现了 54% 的 ASR。代码可在此 https URL 上获取。</li>
</ul>

<h3>Title: Multimodal Latent Language Modeling with Next-Token Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08635">https://arxiv.org/abs/2412.08635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08635">https://arxiv.org/pdf/2412.08635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08635]] Multimodal Latent Language Modeling with Next-Token Diffusion(https://arxiv.org/abs/2412.08635)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers. Specifically, we employ a variational autoencoder (VAE) to represent continuous data as latent vectors and introduce next-token diffusion for autoregressive generation of these vectors. Additionally, we develop $\sigma$-VAE to address the challenges of variance collapse, which is crucial for autoregressive modeling. Extensive experiments demonstrate the effectiveness of LatentLM across various modalities. In image generation, LatentLM surpasses Diffusion Transformers in both performance and scalability. When integrated into multimodal large language models, LatentLM provides a general-purpose interface that unifies multimodal generation and understanding. Experimental results show that LatentLM achieves favorable performance compared to Transfusion and vector quantized models in the setting of scaling up training tokens. In text-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2 model in speaker similarity and robustness, while requiring 10x fewer decoding steps. The results establish LatentLM as a highly effective and scalable approach to advance large multimodal models.</li>
<li><strong>摘要：</strong>多模态生成模型需要一种统一的方法来处理离散数据（例如文本和代码）和连续数据（例如图像、音频、视频）。在这项工作中，我们提出了潜在语言模型 (LatentLM)，它使用因果 Transformers 无缝集成连续数据和离散数据。具体来说，我们采用变分自动编码器 (VAE) 将连续数据表示为潜在向量，并引入下一个标记扩散来自回归生成这些向量。此外，我们开发了 $\sigma$-VAE 来解决方差崩溃的挑战，这对于自回归建模至关重要。大量实验证明了 LatentLM 在各种模态中的有效性。在图像生成中，LatentLM 在性能和可扩展性方面都超越了 Diffusion Transformers。当集成到多模态大型语言模型中时，LatentLM 提供了一个统一多模态生成和理解的通用接口。实验结果表明，在扩大训练标记的设置中，LatentLM 与 Transfusion 和矢量量化模型相比取得了良好的性能。在文本转语音合成中，LatentLM 在说话人相似性和稳健性方面优于最先进的 VALL-E 2 模型，同时所需的解码步骤减少了 10 倍。结果表明，LatentLM 是一种高效且可扩展的方法，可用于推进大型多模态模型。</li>
</ul>

<h3>Title: Fast Prompt Alignment for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Khalil Mrini, Hanlin Lu, Linjie Yang, Weilin Huang, Heng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.08639">https://arxiv.org/abs/2412.08639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.08639">https://arxiv.org/pdf/2412.08639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.08639]] Fast Prompt Alignment for Text-to-Image Generation(https://arxiv.org/abs/2412.08639)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Text-to-image generation has advanced rapidly, yet aligning complex textual prompts with generated visuals remains challenging, especially with intricate object relationships and fine-grained details. This paper introduces Fast Prompt Alignment (FPA), a prompt optimization framework that leverages a one-pass approach, enhancing text-to-image alignment efficiency without the iterative overhead typical of current methods like OPT2I. FPA uses large language models (LLMs) for single-iteration prompt paraphrasing, followed by fine-tuning or in-context learning with optimized prompts to enable real-time inference, reducing computational demands while preserving alignment fidelity. Extensive evaluations on the COCO Captions and PartiPrompts datasets demonstrate that FPA achieves competitive text-image alignment scores at a fraction of the processing time, as validated through both automated metrics (TIFA, VQA) and human evaluation. A human study with expert annotators further reveals a strong correlation between human alignment judgments and automated scores, underscoring the robustness of FPA's improvements. The proposed method showcases a scalable, efficient alternative to iterative prompt optimization, enabling broader applicability in real-time, high-demand settings. The codebase is provided to facilitate further research: this https URL</li>
<li><strong>摘要：</strong>文本到图像的生成技术发展迅速，但将复杂的文本提示与生成的视觉效果对齐仍然具有挑战性，尤其是在对象关系错综复杂且细节细致的情况下。本文介绍了快速提示对齐 (FPA)，这是一种利用一次性方法的提示优化框架，可提高文本到图像的对齐效率，而无需当前方法（如 OPT2I）中常见的迭代开销。FPA 使用大型语言模型 (LLM) 进行单次迭代提示释义，然后使用优化的提示进行微调或上下文学习，以实现实时推理，从而减少计算需求，同时保持对齐保真度。对 COCO Captions 和 PartiPrompts 数据集的广泛评估表明，FPA 以极短的处理时间实现了具有竞争力的文本-图像对齐分数，这已通过自动指标（TIFA、VQA）和人工评估进行了验证。一项由专家注释者进行的人工研究进一步揭示了人工对齐判断与自动评分之间的强相关性，强调了 FPA 改进的稳健性。所提出的方法展示了一种可扩展、高效的迭代快速优化替代方案，使其在实时、高需求环境中具有更广泛的适用性。提供代码库以促进进一步研究：此 https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
