<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>language model</h2>
<h3>Title: Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models. (arXiv:2311.13628v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13628">http://arxiv.org/abs/2311.13628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13628]] Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models(http://arxiv.org/abs/2311.13628)</code></li>
<li>Summary: <p>The recent explosion in the capabilities of large language models has led to
a wave of interest in how best to prompt a model to perform a given task. While
it may be tempting to simply choose a prompt based on average performance on a
validation set, this can lead to a deployment where unexpectedly poor responses
are generated, especially for the worst-off users. To mitigate this prospect,
we propose Prompt Risk Control, a lightweight framework for selecting a prompt
based on rigorous upper bounds on families of informative risk measures. We
offer methods for producing bounds on a diverse set of metrics, including
quantities that measure worst-case responses and disparities in generation
quality across the population of users. In addition, we extend the underlying
statistical bounding techniques to accommodate the possibility of distribution
shifts in deployment. Experiments on applications such as open-ended chat,
medical question summarization, and code generation highlight how such a
framework can foster responsible deployment by reducing the risk of the worst
outcomes.
</p></li>
</ul>

<h3>Title: MAIRA-1: A specialised large multimodal model for radiology report generation. (arXiv:2311.13668v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13668">http://arxiv.org/abs/2311.13668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13668]] MAIRA-1: A specialised large multimodal model for radiology report generation(http://arxiv.org/abs/2311.13668)</code></li>
<li>Summary: <p>We present a radiology-specific multimodal model for the task for generating
radiological reports from chest X-rays (CXRs). Our work builds on the idea that
large language model(s) can be equipped with multimodal capabilities through
alignment with pre-trained vision encoders. On natural images, this has been
shown to allow multimodal models to gain image understanding and description
capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image
encoder in conjunction with a fine-tuned large language model based on
Vicuna-7B, and text-based data augmentation, to produce reports with
state-of-the-art quality. In particular, MAIRA-1 significantly improves on the
radiologist-aligned RadCliQ metric and across all lexical metrics considered.
Manual review of model outputs demonstrates promising fluency and accuracy of
generated reports while uncovering failure modes not captured by existing
evaluation practices. More information and resources can be found on the
project website: https://aka.ms/maira.
</p></li>
</ul>

<h3>Title: Towards More Likely Models for AI Planning. (arXiv:2311.13720v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13720">http://arxiv.org/abs/2311.13720</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13720]] Towards More Likely Models for AI Planning(http://arxiv.org/abs/2311.13720)</code></li>
<li>Summary: <p>This is the first work to look at the application of large language models
(LLMs) for the purpose of model space edits in automated planning tasks. To set
the stage for this sangam, we explore two different flavors of model space
problems that have been studied in the AI planning literature and explore the
effect of an LLM on those tasks. We empirically demonstrate how the performance
of an LLM contrasts with combinatorial search (CS) - an approach that has been
traditionally used to solve model space tasks in planning, both with the LLM in
the role of a standalone model space reasoner as well as in the role of a
statistical signal in concert with the CS approach as part of a two-stage
process. Our experiments show promising results suggesting further forays of
LLMs into the exciting world of model space reasoning for planning tasks in the
future.
</p></li>
</ul>

<h3>Title: A Cross Attention Approach to Diagnostic Explainability using Clinical Practice Guidelines for Depression. (arXiv:2311.13852v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13852">http://arxiv.org/abs/2311.13852</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13852]] A Cross Attention Approach to Diagnostic Explainability using Clinical Practice Guidelines for Depression(http://arxiv.org/abs/2311.13852)</code></li>
<li>Summary: <p>The lack of explainability using relevant clinical knowledge hinders the
adoption of Artificial Intelligence-powered analysis of unstructured clinical
dialogue. A wealth of relevant, untapped Mental Health (MH) data is available
in online communities, providing the opportunity to address the explainability
problem with substantial potential impact as a screening tool for both online
and offline applications. We develop a method to enhance attention in popular
transformer models and generate clinician-understandable explanations for
classification by incorporating external clinical knowledge. Inspired by how
clinicians rely on their expertise when interacting with patients, we leverage
relevant clinical knowledge to model patient inputs, providing meaningful
explanations for classification. This will save manual review time and engender
trust. We develop such a system in the context of MH using clinical practice
guidelines (CPG) for diagnosing depression, a mental health disorder of global
concern. We propose an application-specific language model called ProcesS
knowledge-infused cross ATtention (PSAT), which incorporates CPGs when
computing attention. Through rigorous evaluation on three expert-curated
datasets related to depression, we demonstrate application-relevant
explainability of PSAT. PSAT also surpasses the performance of nine baseline
models and can provide explanations where other baselines fall short. We
transform a CPG resource focused on depression, such as the Patient Health
Questionnaire (e.g. PHQ-9) and related questions, into a machine-readable
ontology using SNOMED-CT. With this resource, PSAT enhances the ability of
models like GPT-3.5 to generate application-relevant explanations.
</p></li>
</ul>

<h3>Title: Challenges of Large Language Models for Mental Health Counseling. (arXiv:2311.13857v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13857">http://arxiv.org/abs/2311.13857</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13857]] Challenges of Large Language Models for Mental Health Counseling(http://arxiv.org/abs/2311.13857)</code></li>
<li>Summary: <p>The global mental health crisis is looming with a rapid increase in mental
disorders, limited resources, and the social stigma of seeking treatment. As
the field of artificial intelligence (AI) has witnessed significant
advancements in recent years, large language models (LLMs) capable of
understanding and generating human-like text may be used in supporting or
providing psychological counseling. However, the application of LLMs in the
mental health domain raises concerns regarding the accuracy, effectiveness, and
reliability of the information provided. This paper investigates the major
challenges associated with the development of LLMs for psychological
counseling, including model hallucination, interpretability, bias, privacy, and
clinical effectiveness. We explore potential solutions to these challenges that
are practical and applicable to the current paradigm of AI. From our experience
in developing and deploying LLMs for mental health, AI holds a great promise
for improving mental health care, if we can carefully navigate and overcome
pitfalls of LLMs.
</p></li>
</ul>

<h3>Title: Minimizing Factual Inconsistency and Hallucination in Large Language Models. (arXiv:2311.13878v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13878">http://arxiv.org/abs/2311.13878</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13878]] Minimizing Factual Inconsistency and Hallucination in Large Language Models(http://arxiv.org/abs/2311.13878)</code></li>
<li>Summary: <p>Large Language Models (LLMs) are widely used in critical fields such as
healthcare, education, and finance due to their remarkable proficiency in
various language-related tasks. However, LLMs are prone to generating factually
incorrect responses or "hallucinations," which can lead to a loss of
credibility and trust among users. To address this issue, we propose a
multi-stage framework that generates the rationale first, verifies and refines
incorrect ones, and uses them as supporting references to generate the answer.
The generated rationale enhances the transparency of the answer and our
framework provides insights into how the model arrived at this answer, by using
this rationale and the references to the context. In this paper, we demonstrate
its effectiveness in improving the quality of responses to drug-related
inquiries in the life sciences industry. Our framework improves traditional
Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be
14-25% more faithful and 16-22% more accurate on two datasets. Furthermore,
fine-tuning samples based on our framework improves the accuracy of smaller
open-access LLMs by 33-42% and competes with RAG on commercial models.
</p></li>
</ul>

<h3>Title: Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach. (arXiv:2311.13884v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13884">http://arxiv.org/abs/2311.13884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13884]] Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach(http://arxiv.org/abs/2311.13884)</code></li>
<li>Summary: <p>The significant advancements in large language models (LLMs) have presented
novel opportunities for tackling planning and decision-making within
multi-agent systems. However, as the number of agents increases, the issues of
hallucination in LLMs and coordination in multi-agent systems (MAS) have become
increasingly pronounced. Additionally, the efficient utilization of tokens
becomes a critical consideration when employing LLMs to facilitate the
interactions of large numbers of agents. In this paper, we present a novel
framework aimed at enhancing coordination and decision-making capabilities of
LLMs within large-scale multi-agent environments. Our approach draws
inspiration from the actor-critic framework employed in multi-agent
reinforcement learning, and we develop a modular and token-efficient solution
that effectively addresses challenges presented by LLMs and MAS. Through
evaluations conducted in experiments involving system resource allocation and
robot grid transportation, we demonstrate the considerable advantages afforded
by our proposed approach.
</p></li>
</ul>

<h3>Title: General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level. (arXiv:2311.13892v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13892">http://arxiv.org/abs/2311.13892</a></li>
<li>Code URL: https://github.com/bingkangshi/general-phrase-debiaser</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13892]] General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level(http://arxiv.org/abs/2311.13892)</code></li>
<li>Summary: <p>The social biases and unwelcome stereotypes revealed by pretrained language
models are becoming obstacles to their application. Compared to numerous
debiasing methods targeting word level, there has been relatively less
attention on biases present at phrase level, limiting the performance of
debiasing in discipline domains. In this paper, we propose an automatic
multi-token debiasing pipeline called \textbf{General Phrase Debiaser}, which
is capable of mitigating phrase-level biases in masked language models.
Specifically, our method consists of a \textit{phrase filter stage} that
generates stereotypical phrases from Wikipedia pages as well as a \textit{model
debias stage} that can debias models at the multi-token level to tackle bias
challenges on phrases. The latter searches for prompts that trigger model's
bias, and then uses them for debiasing. State-of-the-art results on standard
datasets and metrics show that our approach can significantly reduce gender
biases on both career and multiple disciplines, across models with varying
parameter sizes.
</p></li>
</ul>

<h3>Title: Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions. (arXiv:2311.13982v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13982">http://arxiv.org/abs/2311.13982</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13982]] Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions(http://arxiv.org/abs/2311.13982)</code></li>
<li>Summary: <p>Large language models (LLMs) are capable of answering knowledge-intensive
complex questions with chain-of-thought (CoT) reasoning. However, they tend to
generate factually incorrect reasoning steps when the required knowledge is not
available or up-to-date in models' parameters. Recent works turn to retrieving
external knowledge to augment CoT reasoning. Despite being promising, these
chain-based methods suffer from: 1) Negative retrieval. Unnecessary or
incorrect retrieval may mislead the reasoning; 2) Limited sight. Lacking the
ability to look backward or forward, a local error in one step will propagate
along the chain.
</p>
<p>In this paper, we propose a novel approach: Probabilistic Tree-of-thought
Reasoning (ProbTree). First, LLMs translate a complex question into a query
tree, in which each non-root node denotes a sub-question of its parent node.
Then, probabilistic reasoning is conducted over the tree, by solving questions
from leaf to root considering the confidence of both question decomposing and
answering. During reasoning, for leaf nodes, LLMs choose a more confident
answer from Closed-book QA that employs parametric knowledge and Open-book QA
that employs retrieved external knowledge, thus eliminating the negative
retrieval problem. For non-leaf nodes, with the hierarchical structure, LLMs
have broader sights and are able to globally reason with the information from
child nodes, thus recovering from local errors. The experiments on three
Complex QA datasets under the open-domain setting show that our approach
outperforms SOTA methods significantly, demonstrating the effect of
probabilistic tree-of-thought reasoning.
</p></li>
</ul>

<h3>Title: Towards Explainable Strategy Templates using NLP Transformers. (arXiv:2311.14061v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14061">http://arxiv.org/abs/2311.14061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14061]] Towards Explainable Strategy Templates using NLP Transformers(http://arxiv.org/abs/2311.14061)</code></li>
<li>Summary: <p>This paper bridges the gap between mathematical heuristic strategies learned
from Deep Reinforcement Learning (DRL) in automated agent negotiation, and
comprehensible, natural language explanations. Our aim is to make these
strategies more accessible to non-experts. By leveraging traditional Natural
Language Processing (NLP) techniques and Large Language Models (LLMs) equipped
with Transformers, we outline how parts of DRL strategies composed of parts
within strategy templates can be transformed into user-friendly, human-like
English narratives. To achieve this, we present a top-level algorithm that
involves parsing mathematical expressions of strategy templates, semantically
interpreting variables and structures, generating rule-based primary
explanations, and utilizing a Generative Pre-trained Transformer (GPT) model to
refine and contextualize these explanations. Subsequent customization for
varied audiences and meticulous validation processes in an example illustrate
the applicability and potential of this approach.
</p></li>
</ul>

<h3>Title: A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14115">http://arxiv.org/abs/2311.14115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14115]] A density estimation perspective on learning from pairwise human preferences(http://arxiv.org/abs/2311.14115)</code></li>
<li>Summary: <p>Learning from human feedback (LHF) -- and in particular learning from
pairwise preferences -- has recently become a crucial ingredient in training
large language models (LLMs), and has been the subject of much research. Most
recent works frame it as a reinforcement learning problem, where a reward
function is learned from pairwise preference data and the LLM is treated as a
policy which is adapted to maximize the rewards, often under additional
regularization constraints. We propose an alternative interpretation which
centers on the generative process for pairwise preferences and treats LHF as a
density estimation problem. We provide theoretical and empirical results
showing that for a family of generative processes defined via preference
behavior distribution equations, training a reward function on pairwise
preferences effectively models an annotator's implicit preference distribution.
Finally, we discuss and present findings on "annotator misspecification" --
failure cases where wrong modeling assumptions are made about annotator
behavior, resulting in poorly-adapted models -- suggesting that approaches that
learn from pairwise human preferences could have trouble learning from a
population of annotators with diverse viewpoints.
</p></li>
</ul>

<h3>Title: Towards Auditing Large Language Models: Improving Text-based Stereotype Detection. (arXiv:2311.14126v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14126">http://arxiv.org/abs/2311.14126</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14126]] Towards Auditing Large Language Models: Improving Text-based Stereotype Detection(http://arxiv.org/abs/2311.14126)</code></li>
<li>Summary: <p>Large Language Models (LLM) have made significant advances in the recent past
becoming more mainstream in Artificial Intelligence (AI) enabled human-facing
applications. However, LLMs often generate stereotypical output inherited from
historical data, amplifying societal biases and raising ethical concerns. This
work introduces i) the Multi-Grain Stereotype Dataset, which includes 52,751
instances of gender, race, profession and religion stereotypic text and ii) a
novel stereotype classifier for English text. We design several experiments to
rigorously test the proposed model trained on the novel dataset. Our
experiments show that training the model in a multi-class setting can
outperform the one-vs-all binary counterpart. Consistent feature importance
signals from different eXplainable AI tools demonstrate that the new model
exploits relevant text features. We utilise the newly created model to assess
the stereotypic behaviour of the popular GPT family of models and observe the
reduction of bias over time. In summary, our work establishes a robust and
practical framework for auditing and evaluating the stereotypic bias in LLM.
</p></li>
</ul>

<h3>Title: Language Model Inversion. (arXiv:2311.13647v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13647">http://arxiv.org/abs/2311.13647</a></li>
<li>Code URL: https://github.com/jxmorris12/vec2text</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13647]] Language Model Inversion(http://arxiv.org/abs/2311.13647)</code></li>
<li>Summary: <p>Language models produce a distribution over the next token; can we use this
information to recover the prompt tokens? We consider the problem of language
model inversion and show that next-token probabilities contain a surprising
amount of information about the preceding text. Often we can recover the text
in cases where it is hidden from the user, motivating a method for recovering
unknown prompts given only the model's current distribution output. We consider
a variety of model access scenarios, and show how even without predictions for
every token in the vocabulary we can recover the probability vector through
search. On Llama-2 7b, our inversion method reconstructs prompts with a BLEU of
$59$ and token-level F1 of $78$ and recovers $27\%$ of prompts exactly. Code
for reproducing all experiments is available at
<a href="http://github.com/jxmorris12/vec2text.">this http URL</a>
</p></li>
</ul>

<h3>Title: Efficient Transformer Knowledge Distillation: A Performance Review. (arXiv:2311.13657v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13657">http://arxiv.org/abs/2311.13657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13657]] Efficient Transformer Knowledge Distillation: A Performance Review(http://arxiv.org/abs/2311.13657)</code></li>
<li>Summary: <p>As pretrained transformer language models continue to achieve
state-of-the-art performance, the Natural Language Processing community has
pushed for advances in model compression and efficient attention mechanisms to
address high computational requirements and limited input sequence length.
Despite these separate efforts, no investigation has been done into the
intersection of these two fields. In this work, we provide an evaluation of
model compression via knowledge distillation on efficient attention
transformers. We provide cost-performance trade-offs for the compression of
state-of-the-art efficient attention architectures and the gains made in
performance in comparison to their full attention counterparts. Furthermore, we
introduce a new long-context Named Entity Recognition dataset, GONERD, to train
and test the performance of NER models on long sequences. We find that
distilled efficient attention transformers can preserve a significant amount of
original model performance, preserving up to 98.6% across short-context tasks
(GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context
Question-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on
long-context Named Entity Recognition (GONERD), while decreasing inference
times by up to 57.8%. We find that, for most models on most tasks, performing
knowledge distillation is an effective method to yield high-performing
efficient attention models with low costs.
</p></li>
</ul>

<h3>Title: DaG LLM ver 1.0: Pioneering Instruction-Tuned Language Modeling for Korean NLP. (arXiv:2311.13784v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13784">http://arxiv.org/abs/2311.13784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13784]] DaG LLM ver 1(http://arxiv.org/abs/2311.13784)</code></li>
<li>Summary: <p>This paper presents the DaG LLM (David and Goliath Large Language Model), a
language model specialized for Korean and fine-tuned through Instruction Tuning
across 41 tasks within 13 distinct categories.
</p></li>
</ul>

<h3>Title: Dialogue Quality and Emotion Annotations for Customer Support Conversations. (arXiv:2311.13910v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13910">http://arxiv.org/abs/2311.13910</a></li>
<li>Code URL: https://github.com/johndmendonca/maia-dqe</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13910]] Dialogue Quality and Emotion Annotations for Customer Support Conversations(http://arxiv.org/abs/2311.13910)</code></li>
<li>Summary: <p>Task-oriented conversational datasets often lack topic variability and
linguistic diversity. However, with the advent of Large Language Models (LLMs)
pretrained on extensive, multilingual and diverse text data, these limitations
seem overcome. Nevertheless, their generalisability to different languages and
domains in dialogue applications remains uncertain without benchmarking
datasets. This paper presents a holistic annotation approach for emotion and
conversational quality in the context of bilingual customer support
conversations. By performing annotations that take into consideration the
complete instances that compose a conversation, one can form a broader
perspective of the dialogue as a whole. Furthermore, it provides a unique and
valuable resource for the development of text classification models. To this
end, we present benchmarks for Emotion Recognition and Dialogue Quality
Estimation and show that further research is needed to leverage these models in
a production setting.
</p></li>
</ul>

<h2>gpt</h2>
<h3>Title: Evaluating GPT-4's Vision Capabilities on Brazilian University Admission Exams. (arXiv:2311.14169v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14169">http://arxiv.org/abs/2311.14169</a></li>
<li>Code URL: https://github.com/piresramon/gpt-4-enem</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14169]] Evaluating GPT-4's Vision Capabilities on Brazilian University Admission Exams(http://arxiv.org/abs/2311.14169)</code></li>
<li>Summary: <p>Recent advancements in language models have showcased human-comparable
performance in academic entrance exams. However, existing studies often
overlook questions that require the integration of visual comprehension, thus
compromising the full spectrum and complexity inherent in real-world scenarios.
To address this gap, we present a comprehensive framework to evaluate language
models on entrance exams, which incorporates both textual and visual elements.
We evaluate the two most recent editions of Exame Nacional do Ensino M\'edio
(ENEM), the main standardized entrance examination adopted by Brazilian
universities. Our study not only reaffirms the capabilities of GPT-4 as the
state of the art for handling complex multidisciplinary questions, but also
pioneers in offering a realistic assessment of multimodal language models on
Portuguese examinations. One of the highlights is that text captions
transcribing visual content outperform the direct use of images, suggesting
that the vision model has room for improvement. Yet, despite improvements
afforded by images or captions, mathematical questions remain a challenge for
these state-of-the-art models. The code and data used on experiments are
available at https://github.com/piresramon/gpt-4-enem.
</p></li>
</ul>

<h3>Title: Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case. (arXiv:2311.13729v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13729">http://arxiv.org/abs/2311.13729</a></li>
<li>Code URL: https://github.com/shashank140195/raredis</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13729]] Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case(http://arxiv.org/abs/2311.13729)</code></li>
<li>Summary: <p>End-to-end relation extraction (E2ERE) is an important and realistic
application of natural language processing (NLP) in biomedicine. In this paper,
we aim to compare three prevailing paradigms for E2ERE using a complex dataset
focused on rare diseases involving discontinuous and nested entities. We use
the RareDis information extraction dataset to evaluate three competing
approaches (for E2ERE): NER $\rightarrow$ RE pipelines, joint sequence to
sequence models, and generative pre-trained transformer (GPT) models. We use
comparable state-of-the-art models and best practices for each of these
approaches and conduct error analyses to assess their failure modes. Our
findings reveal that pipeline models are still the best, while
sequence-to-sequence models are not far behind; GPT models with eight times as
many parameters are worse than even sequence-to-sequence models and lose to
pipeline models by over 10 F1 points. Partial matches and discontinuous
entities caused many NER errors contributing to lower overall E2E performances.
We also verify these findings on a second E2ERE dataset for chemical-protein
interactions. Although generative LM-based methods are more suitable for
zero-shot settings, when training data is available, our results show that it
is better to work with more conventional models trained and tailored for E2ERE.
More innovative methods are needed to marry the best of the both worlds from
smaller encoder-decoder pipeline models and the larger GPT models to improve
E2ERE. As of now, we see that well designed pipeline models offer substantial
performance gains at a lower cost and carbon footprint for E2ERE. Our
contribution is also the first to conduct E2ERE for the RareDis dataset.
</p></li>
</ul>

<h3>Title: Surpassing GPT-4 Medical Coding with a Two-Stage Approach. (arXiv:2311.13735v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13735">http://arxiv.org/abs/2311.13735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13735]] Surpassing GPT-4 Medical Coding with a Two-Stage Approach(http://arxiv.org/abs/2311.13735)</code></li>
<li>Summary: <p>Recent advances in large language models (LLMs) show potential for clinical
applications, such as clinical decision support and trial recommendations.
However, the GPT-4 LLM predicts an excessive number of ICD codes for medical
coding tasks, leading to high recall but low precision. To tackle this
challenge, we introduce LLM-codex, a two-stage approach to predict ICD codes
that first generates evidence proposals using an LLM and then employs an
LSTM-based verification stage. The LSTM learns from both the LLM's high recall
and human expert's high precision, using a custom loss function. Our model is
the only approach that simultaneously achieves state-of-the-art results in
medical coding accuracy, accuracy on rare codes, and sentence-level evidence
identification to support coding decisions without training on human-annotated
evidence according to experiments on the MIMIC dataset.
</p></li>
</ul>

<h3>Title: MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V. (arXiv:2311.13951v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13951">http://arxiv.org/abs/2311.13951</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13951]] MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V(http://arxiv.org/abs/2311.13951)</code></li>
<li>Summary: <p>In the pursuit of Artificial General Intelligence (AGI), the integration of
vision in language models has marked a significant milestone. The advent of
vision-language models (MLLMs) like GPT-4V have expanded AI applications,
aligning with the multi-modal capabilities of the human brain. However,
evaluating the efficacy of MLLMs poses a substantial challenge due to the
subjective nature of tasks that lack definitive answers. Existing automatic
evaluation methodologies on multi-modal large language models rely on objective
queries that have standard answers, inadequately addressing the nuances of
creative and associative multi-modal tasks. To address this, we introduce
MLLM-Bench, an innovative benchmark inspired by Vicuna, spanning a diverse
array of scenarios, including Perception, Understanding, Applying, Analyzing,
Evaluating, and Creation along with the ethical consideration. MLLM-Bench is
designed to reflect user experience more accurately and provide a more holistic
assessment of model performance. Comparative evaluations indicate a significant
performance gap between existing open-source models and GPT-4V. We posit that
MLLM-Bench will catalyze progress in the open-source community towards
developing user-centric vision-language models that meet a broad spectrum of
real-world applications. See online leaderboard in
\url{https://mllm-bench.llmzoo.com}.
</p></li>
</ul>

<h2>llm</h2>
<h3>Title: PrivateLoRA For Efficient Privacy Preserving LLM. (arXiv:2311.14030v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14030">http://arxiv.org/abs/2311.14030</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14030]] PrivateLoRA For Efficient Privacy Preserving LLM(http://arxiv.org/abs/2311.14030)</code></li>
<li>Summary: <p>End users face a choice between privacy and efficiency in current Large
Language Model (LLM) service paradigms. In cloud-based paradigms, users are
forced to compromise data locality for generation quality and processing speed.
Conversely, edge device paradigms maintain data locality but fail to deliver
satisfactory performance. In this work, we propose a novel LLM service paradigm
that distributes privacy-sensitive computation on edge devices and shared
computation in the cloud. Only activations are transmitted between the central
cloud and edge devices to ensure data locality. Our core innovation,
PrivateLoRA, addresses the challenging communication overhead by exploiting the
low rank of residual activations, achieving over 95% communication reduction.
Consequently, PrivateLoRA effectively maintains data locality and is extremely
resource efficient. Under standard 5G networks, PrivateLoRA achieves throughput
over 300% of device-only solutions for 7B models and over 80% of an A100 GPU
for 33B models. PrivateLoRA also provides tuning performance comparable to LoRA
for advanced personalization. Our approach democratizes access to
state-of-the-art generative AI for edge devices, paving the way for more
tailored LLM experiences for the general public. To our knowledge, our proposed
framework is the first efficient and privacy-preserving LLM solution in the
literature.
</p></li>
</ul>

<h3>Title: Auditing and Mitigating Cultural Bias in LLMs. (arXiv:2311.14096v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14096">http://arxiv.org/abs/2311.14096</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14096]] Auditing and Mitigating Cultural Bias in LLMs(http://arxiv.org/abs/2311.14096)</code></li>
<li>Summary: <p>Culture fundamentally shapes people's reasoning, behavior, and communication.
Generative artificial intelligence (AI) technologies may cause a shift towards
a dominant culture. As people increasingly use AI to expedite and even automate
various professional and personal tasks, cultural values embedded in AI models
may bias authentic expression. We audit large language models for cultural
bias, comparing their responses to nationally representative survey data, and
evaluate country-specific prompting as a mitigation strategy. We find that
GPT-4, 3.5 and 3 exhibit cultural values resembling English-speaking and
Protestant European countries. Our mitigation strategy reduces cultural bias in
recent models but not for all countries/territories. To avoid cultural bias in
generative AI, especially in high-stakes contexts, we suggest using culture
matching and ongoing cultural audits.
</p></li>
</ul>

<h2>long context</h2>
<h2>lora</h2>
<h2>hallucination</h2>
<h2>prompt</h2>
<h2>code</h2>
<h3>Title: Sample as You Infer: Predictive Coding With Langevin Dynamics. (arXiv:2311.13664v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13664">http://arxiv.org/abs/2311.13664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13664]] Sample as You Infer: Predictive Coding With Langevin Dynamics(http://arxiv.org/abs/2311.13664)</code></li>
<li>Summary: <p>We present a novel algorithm for parameter learning in generic deep
generative models that builds upon the predictive coding (PC) framework of
computational neuroscience. Our approach modifies the standard PC algorithm to
bring performance on-par and exceeding that obtained from standard variational
auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference
procedure we re-envision it as an overdamped Langevin sampling, which
facilitates optimisation with respect to a tight evidence lower bound (ELBO).
We improve the resultant encoder-free training method by incorporating an
encoder network to provide an amortised warm-start to our Langevin sampling and
test three different objectives for doing so. Finally, to increase robustness
to the sampling step size and reduce sensitivity to curvature, we validate a
lightweight and easily computable form of preconditioning, inspired by Riemann
Manifold Langevin and adaptive optimizers from the SGD literature. We compare
against VAEs by training like-for-like generative models using our technique
against those trained with standard reparameterisation-trick-based ELBOs. We
observe our method out-performs or matches performance across a number of
metrics, including sample quality, while converging in a fraction of the number
of SGD training iterations.
</p></li>
</ul>

<h3>Title: Scalable AI Generative Content for Vehicular Network Semantic Communication. (arXiv:2311.13782v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13782">http://arxiv.org/abs/2311.13782</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13782]] Scalable AI Generative Content for Vehicular Network Semantic Communication(http://arxiv.org/abs/2311.13782)</code></li>
<li>Summary: <p>Perceiving vehicles in a driver's blind spot is vital for safe driving. The
detection of potentially dangerous vehicles in these blind spots can benefit
from vehicular network semantic communication technology. However, efficient
semantic communication involves a trade-off between accuracy and delay,
especially in bandwidth-limited situations. This paper unveils a scalable
Artificial Intelligence Generated Content (AIGC) system that leverages an
encoder-decoder architecture. This system converts images into textual
representations and reconstructs them into quality-acceptable images,
optimizing transmission for vehicular network semantic communication. Moreover,
when bandwidth allows, auxiliary information is integrated. The encoder-decoder
aims to maintain semantic equivalence with the original images across various
tasks. Then the proposed approach employs reinforcement learning to enhance the
reliability of the generated contents. Experimental results suggest that the
proposed method surpasses the baseline in perceiving vehicles in blind spots
and effectively compresses communication data. While this method is
specifically designed for driving scenarios, this encoder-decoder architecture
also holds potential for wide use across various semantic communication
scenarios.
</p></li>
</ul>

<h3>Title: Exact Combinatorial Optimization with Temporo-Attentional Graph Neural Networks. (arXiv:2311.13843v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13843">http://arxiv.org/abs/2311.13843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13843]] Exact Combinatorial Optimization with Temporo-Attentional Graph Neural Networks(http://arxiv.org/abs/2311.13843)</code></li>
<li>Summary: <p>Combinatorial optimization finds an optimal solution within a discrete set of
variables and constraints. The field has seen tremendous progress both in
research and industry. With the success of deep learning in the past decade, a
recent trend in combinatorial optimization has been to improve state-of-the-art
combinatorial optimization solvers by replacing key heuristic components with
machine learning (ML) models. In this paper, we investigate two essential
aspects of machine learning algorithms for combinatorial optimization: temporal
characteristics and attention. We argue that for the task of variable selection
in the branch-and-bound (B&amp;B) algorithm, incorporating the temporal information
as well as the bipartite graph attention improves the solver's performance. We
support our claims with intuitions and numerical results over several standard
datasets used in the literature and competitions. Code is available at:
https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=047c6cf2-8463-40d7-b92f-7b2ca998e935
</p></li>
</ul>

<h3>Title: A DRL solution to help reduce the cost in waiting time of securing a traffic light for cyclists. (arXiv:2311.13905v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13905">http://arxiv.org/abs/2311.13905</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13905]] A DRL solution to help reduce the cost in waiting time of securing a traffic light for cyclists(http://arxiv.org/abs/2311.13905)</code></li>
<li>Summary: <p>Cyclists prefer to use infrastructure that separates them from motorized
traffic. Using a traffic light to segregate car and bike flows, with the
addition of bike-specific green phases, is a lightweight and cheap solution
that can be deployed dynamically to assess the opportunity of a heavier
infrastructure such as a separate bike lane. To compensate for the increased
waiting time induced by these new phases, we introduce in this paper a deep
reinforcement learning solution that adapts the green phase cycle of a traffic
light to the traffic. Vehicle counter data are used to compare the DRL approach
with the actuated traffic light control algorithm over whole days. Results show
that DRL achieves better minimization of vehicle waiting time at almost all
hours. Our DRL approach is also robust to moderate changes in bike traffic. The
code of this paper is available at
https://github.com/LucasMagnana/A-DRL-solution-to-help-reduce-the-cost-in-waiting-time-of-securing-a-traffic-light-for-cyclists.
</p></li>
</ul>

<h3>Title: Human Machine Co-Creation. A Complementary Cognitive Approach to Creative Character Design Process Using GANs. (arXiv:2311.13960v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13960">http://arxiv.org/abs/2311.13960</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13960]] Human Machine Co-Creation(http://arxiv.org/abs/2311.13960)</code></li>
<li>Summary: <p>Recent advances in Generative Adversarial Networks GANs applications continue
to attract the attention of researchers in different fields. In such a
framework, two neural networks compete adversely to generate new visual
contents indistinguishable from the original dataset. The objective of this
research is to create a complementary codesign process between humans and
machines to augment character designers abilities in visualizing and creating
new characters for multimedia projects such as games and animation. Driven by
design cognitive scaffolding, the proposed approach aims to inform the process
of perceiving, knowing, and making. The machine generated concepts are used as
a launching platform for character designers to conceptualize new characters. A
labelled dataset of 22,000 characters was developed for this work and deployed
using different GANs to evaluate the most suited for the context, followed by
mixed methods evaluation for the machine output and human derivations. The
discussed results substantiate the value of the proposed cocreation framework
and elucidate how the generated concepts are used as cognitive substances that
interact with designers competencies in a versatile manner to influence the
creative processes of conceptualizing novel characters.
</p></li>
</ul>

<h3>Title: Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training. (arXiv:2311.14109v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14109">http://arxiv.org/abs/2311.14109</a></li>
<li>Code URL: https://github.com/chengtan9907/mc-cot</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14109]] Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training(http://arxiv.org/abs/2311.14109)</code></li>
<li>Summary: <p>Multimodal reasoning is a challenging task that requires models to reason
across multiple modalities to answer questions. Existing approaches have made
progress by incorporating language and visual modalities into a two-stage
reasoning framework, separating rationale generation from answer inference.
However, these approaches often fall short due to the inadequate quality of the
generated rationales. In this work, we delve into the importance of rationales
in model reasoning. We observe that when rationales are completely accurate,
the model's accuracy significantly improves, highlighting the need for
high-quality rationale generation. Motivated by this, we propose MC-CoT, a
self-consistency training strategy that generates multiple rationales and
answers, subsequently selecting the most accurate through a voting process.
This approach not only enhances the quality of generated rationales but also
leads to more accurate and robust answers. Through extensive experiments, we
demonstrate that our approach significantly improves model performance across
various benchmarks. Remarkably, we show that even smaller base models, when
equipped with our proposed approach, can achieve results comparable to those of
larger models, illustrating the potential of our approach in harnessing the
power of rationales for improved multimodal reasoning. The code is available at
https://github.com/chengtan9907/mc-cot.
</p></li>
</ul>

<h3>Title: Density Distribution-based Learning Framework for Addressing Online Continual Learning Challenges. (arXiv:2311.13623v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13623">http://arxiv.org/abs/2311.13623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13623]] Density Distribution-based Learning Framework for Addressing Online Continual Learning Challenges(http://arxiv.org/abs/2311.13623)</code></li>
<li>Summary: <p>In this paper, we address the challenges of online Continual Learning (CL) by
introducing a density distribution-based learning framework. CL, especially the
Class Incremental Learning, enables adaptation to new test distributions while
continuously learning from a single-pass training data stream, which is more in
line with the practical application requirements of real-world scenarios.
However, existing CL methods often suffer from catastrophic forgetting and
higher computing costs due to complex algorithm designs, limiting their
practical use. Our proposed framework overcomes these limitations by achieving
superior average accuracy and time-space efficiency, bridging the performance
gap between CL and classical machine learning. Specifically, we adopt an
independent Generative Kernel Density Estimation (GKDE) model for each CL task.
During the testing stage, the GKDEs utilize a self-reported max probability
density value to determine which one is responsible for predicting incoming
test instances. A GKDE-based learning objective can ensure that samples with
the same label are grouped together, while dissimilar instances are pushed
farther apart. Extensive experiments conducted on multiple CL datasets validate
the effectiveness of our proposed framework. Our method outperforms popular CL
approaches by a significant margin, while maintaining competitive time-space
efficiency, making our framework suitable for real-world applications. Code
will be available at https://github.com/xxxx/xxxx.
</p></li>
</ul>

<h3>Title: Evaluating Pretrained models for Deployable Lifelong Learning. (arXiv:2311.13648v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13648">http://arxiv.org/abs/2311.13648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13648]] Evaluating Pretrained models for Deployable Lifelong Learning(http://arxiv.org/abs/2311.13648)</code></li>
<li>Summary: <p>We create a novel benchmark for evaluating a Deployable Lifelong Learning
system for Visual Reinforcement Learning (RL) that is pretrained on a curated
dataset, and propose a novel Scalable Lifelong Learning system capable of
retaining knowledge from the previously learnt RL tasks. Our benchmark measures
the efficacy of a deployable Lifelong Learning system that is evaluated on
scalability, performance and resource utilization. Our proposed system, once
pretrained on the dataset, can be deployed to perform continual learning on
unseen tasks. Our proposed method consists of a Few Shot Class Incremental
Learning (FSCIL) based task-mapper and an encoder/backbone trained entirely
using the pretrain dataset. The policy parameters corresponding to the
recognized task are then loaded to perform the task. We show that this system
can be scaled to incorporate a large number of tasks due to the small memory
footprint and fewer computational resources. We perform experiments on our DeLL
(Deployment for Lifelong Learning) benchmark on the Atari games to determine
the efficacy of the system.
</p></li>
</ul>

<h3>Title: BackboneLearn: A Library for Scaling Mixed-Integer Optimization-Based Machine Learning. (arXiv:2311.13695v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13695">http://arxiv.org/abs/2311.13695</a></li>
<li>Code URL: https://github.com/chziakas/backbone_learn</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13695]] BackboneLearn: A Library for Scaling Mixed-Integer Optimization-Based Machine Learning(http://arxiv.org/abs/2311.13695)</code></li>
<li>Summary: <p>We present BackboneLearn: an open-source software package and framework for
scaling mixed-integer optimization (MIO) problems with indicator variables to
high-dimensional problems. This optimization paradigm can naturally be used to
formulate fundamental problems in interpretable supervised learning (e.g.,
sparse regression and decision trees), in unsupervised learning (e.g.,
clustering), and beyond; BackboneLearn solves the aforementioned problems
faster than exact methods and with higher accuracy than commonly used
heuristics. The package is built in Python and is user-friendly and easily
extensible: users can directly implement a backbone algorithm for their MIO
problem at hand. The source code of BackboneLearn is available on GitHub (link:
https://github.com/chziakas/backbone_learn).
</p></li>
</ul>

<h3>Title: MedISure: Towards Assuring Machine Learning-based Medical Image Classifiers using Mixup Boundary Analysis. (arXiv:2311.13978v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13978">http://arxiv.org/abs/2311.13978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13978]] MedISure: Towards Assuring Machine Learning-based Medical Image Classifiers using Mixup Boundary Analysis(http://arxiv.org/abs/2311.13978)</code></li>
<li>Summary: <p>Machine learning (ML) models are becoming integral in healthcare
technologies, presenting a critical need for formal assurance to validate their
safety, fairness, robustness, and trustworthiness. These models are inherently
prone to errors, potentially posing serious risks to patient health and could
even cause irreparable harm. Traditional software assurance techniques rely on
fixed code and do not directly apply to ML models since these algorithms are
adaptable and learn from curated datasets through a training process. However,
adapting established principles, such as boundary testing using synthetic test
data can effectively bridge this gap. To this end, we present a novel technique
called Mix-Up Boundary Analysis (MUBA) that facilitates evaluating image
classifiers in terms of prediction fairness. We evaluated MUBA for two
important medical imaging tasks -- brain tumour classification and breast
cancer classification -- and achieved promising results. This research aims to
showcase the importance of adapting traditional assurance principles for
assessing ML models to enhance the safety and reliability of healthcare
technologies. To facilitate future research, we plan to publicly release our
code for MUBA.
</p></li>
</ul>

<h3>Title: Class Uncertainty: A Measure to Mitigate Class Imbalance. (arXiv:2311.14090v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14090">http://arxiv.org/abs/2311.14090</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14090]] Class Uncertainty: A Measure to Mitigate Class Imbalance(http://arxiv.org/abs/2311.14090)</code></li>
<li>Summary: <p>Class-wise characteristics of training examples affect the performance of
deep classifiers. A well-studied example is when the number of training
examples of classes follows a long-tailed distribution, a situation that is
likely to yield sub-optimal performance for under-represented classes. This
class imbalance problem is conventionally addressed by approaches relying on
the class-wise cardinality of training examples, such as data resampling. In
this paper, we demonstrate that considering solely the cardinality of classes
does not cover all issues causing class imbalance. To measure class imbalance,
we propose "Class Uncertainty" as the average predictive uncertainty of the
training examples, and we show that this novel measure captures the differences
across classes better than cardinality. We also curate SVCI-20 as a novel
dataset in which the classes have equal number of training examples but they
differ in terms of their hardness; thereby causing a type of class imbalance
which cannot be addressed by the approaches relying on cardinality. We
incorporate our "Class Uncertainty" measure into a diverse set of ten class
imbalance mitigation methods to demonstrate its effectiveness on long-tailed
datasets as well as on our SVCI-20. Code and datasets will be made available.
</p></li>
</ul>

<h2>chat</h2>
<h3>Title: Enhancing Task-Oriented Dialogues with Chitchat: a Comparative Study Based on Lexical Diversity and Divergence. (arXiv:2311.14067v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14067">http://arxiv.org/abs/2311.14067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14067]] Enhancing Task-Oriented Dialogues with Chitchat: a Comparative Study Based on Lexical Diversity and Divergence(http://arxiv.org/abs/2311.14067)</code></li>
<li>Summary: <p>As a recent development, task-oriented dialogues (TODs) have been enriched
with chitchat in an effort to make dialogues more diverse and engaging. This
enhancement is particularly valuable as TODs are often confined to narrow
domains, making the mitigation of repetitive and predictable responses a
significant challenge. This paper presents a comparative analysis of three
chitchat enhancements, aiming to identify the most effective approach in terms
of diversity. Additionally, we quantify the divergence between the added
chitchat, the original task-oriented language, and chitchat typically found in
chitchat datasets, highlighting the top 20 divergent keywords for each
comparison. Our findings drive a discussion on future enhancements for
augmenting TODs, emphasizing the importance of grounding dialogues beyond the
task to achieve more diverse and natural exchanges.
</p></li>
</ul>

<h3>Title: Searching for Snippets of Open-Domain Dialogue in Task-Oriented Dialogue Datasets. (arXiv:2311.14076v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14076">http://arxiv.org/abs/2311.14076</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14076]] Searching for Snippets of Open-Domain Dialogue in Task-Oriented Dialogue Datasets(http://arxiv.org/abs/2311.14076)</code></li>
<li>Summary: <p>Most existing dialogue corpora and models have been designed to fit into 2
predominant categories : task-oriented dialogues portray functional goals, such
as making a restaurant reservation or booking a plane ticket, while
chit-chat/open-domain dialogues focus on holding a socially engaging talk with
a user. However, humans tend to seamlessly switch between modes and even use
chitchat to enhance task-oriented conversations. To bridge this gap, new
datasets have recently been created, blending both communication modes into
conversation examples. The approaches used tend to rely on adding chit-chat
snippets to pre-existing, human-generated task-oriented datasets. Given the
tendencies observed in humans, we wonder however if the latter do not
\textit{already} hold chit-chat sequences. By using topic modeling and
searching for topics which are most similar to a set of keywords related to
social talk, we explore the training sets of Schema-Guided Dialogues and
MultiWOZ. Our study shows that sequences related to social talk are indeed
naturally present, motivating further research on ways chitchat is combined
into task-oriented dialogues.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
