<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-06</h1>
<h3>Title: Detection of Machine-Generated Text: Literature Survey</h3>
<ul>
<li><strong>Authors: </strong>Dmytro Valiaiev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01642">https://arxiv.org/abs/2402.01642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01642">https://arxiv.org/pdf/2402.01642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01642]] Detection of Machine-Generated Text: Literature Survey(https://arxiv.org/abs/2402.01642)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, agent</a></li>
<li><strong>Abstract: </strong>Since language models produce fake text quickly and easily, there is an oversupply of such content in the public domain. The degree of sophistication and writing style has reached a point where differentiating between human authored and machine-generated content is nearly impossible. As a result, works generated by language models rather than human authors have gained significant media attention and stirred controversy.Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes. Natural language generation (NLG) and generative pre-trained transformer (GPT) models have revolutionized a variety of sectors: the scope not only permeated throughout journalism and customer service but also reached academia. To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented, such as providing human agents with the capacity to distinguish between artificially made and human composed texts utilizing automated systems and possibly reverse-engineered language models. Furthermore, to ensure a balanced and responsible approach, it is critical to have a full grasp of the socio-technological ramifications of these breakthroughs. This literature survey aims to compile and synthesize accomplishments and developments in the aforementioned work, while also identifying future prospects. It also gives an overview of machine-generated text trends and explores the larger societal implications. Ultimately, this survey intends to contribute to the development of robust and effective approaches for resolving the issues connected with the usage and detection of machine-generated text by exploring the interplay between the capabilities of language models and their possible implications.</li>
<li><strong>摘要：</strong>由于语言模型可以快速、轻松地生成虚假文本，因此公共领域中此类内容供过于求。复杂程度和写作风格已经达到了几乎不可能区分人类创作和机器生成的内容的程度。因此，由语言模型而不是人类作者生成的作品获得了媒体的广泛关注并引发了争议。人们也开始担心高级语言模型对社会可能产生的影响，需要对这些过程有更全面的了解。自然语言生成（NLG）和生成式预训练变压器（GPT）模型已经彻底改变了各个领域：其范围不仅渗透到整个新闻业和客户服务领域，还延伸到了学术界。为了减轻使用这些模型可能产生的危险影响，必须实施预防措施，例如为人类代理提供利用自动化系统和可能的逆向工程语言模型区分人工生成和人类编写的文本的能力。此外，为了确保采取平衡和负责任的方法，充分掌握这些突破的社会技术影响至关重要。本文献综述旨在汇编和综合上述工作的成就和发展，同时也确定未来的前景。它还概述了机器生成的文本趋势，并探讨了更大的社会影响。最终，这项调查旨在通过探索语言模型的功能及其可能的含义之间的相互作用，为解决与机器生成文本的使用和检测相关的问题做出贡献。</li>
</ul>

<h3>Title: L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Md. Kowsher, Md. Shohanur Islam Sobuj, Asif Mahmud, Nusrat Jahan Prottasha, Prakash Bhat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01643">https://arxiv.org/abs/2402.01643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01643">https://arxiv.org/pdf/2402.01643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01643]] L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs(https://arxiv.org/abs/2402.01643)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with L-Tuning compared to traditional approaches, marking a promising advancement in fine-tuning LLMs for complex language tasks. \\ Code is available at: \textcolor{red}{\href{https://github.com/Kowsher/L-Tuning}{\texttt{https://github.com/Kowsher/L-Tuning}}}.</li>
<li><strong>摘要：</strong>针对特定任务有效地微调大型语言模型 (LLM) 在自然语言处理中提出了相当大的挑战。传统方法（例如提示或前缀调整）通常依赖于任意标记进行训练，导致训练时间延长以及跨各种类标签的通用标记使用。为了解决这些问题，本文引入了 L-Tuning，这是一种专为自然语言推理（NLI）框架内的分类任务而设计的高效微调方法。与传统方法不同，L-Tuning 专注于通过预先训练的 LLM 处理的标签标记进行微调，从而利用其预先存在的语义知识。该技术不仅提高了微调的准确性和效率，而且还有助于为每个类别生成不同的标签嵌入，从而增强模型训练的细微差别。我们的实验结果表明，与传统方法相比，L-Tuning 的训练效率和分类准确性有了显着提高，这标志着在复杂语言任务的 LLM 微调方面取得了有希望的进步。 \\ 代码位于：\textcolor{red}{\href{https://github.com/Kowsher/L-Tuning}{\texttt{https://github.com/Kowsher/L-Tuning}}} 。</li>
</ul>

<h3>Title: Tracing the Genealogies of Ideas with Large Language Model Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Lucian Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01661">https://arxiv.org/abs/2402.01661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01661">https://arxiv.org/pdf/2402.01661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01661]] Tracing the Genealogies of Ideas with Large Language Model Embeddings(https://arxiv.org/abs/2402.01661)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, I present a novel method to detect intellectual influence across a large corpus. Taking advantage of the unique affordances of large language models in encoding semantic and structural meaning while remaining robust to paraphrasing, we can search for substantively similar ideas and hints of intellectual influence in a computationally efficient manner. Such a method allows us to operationalize different levels of confidence: we can allow for direct quotation, paraphrase, or speculative similarity while remaining open about the limitations of each threshold. I apply an ensemble method combining General Text Embeddings, a state-of-the-art sentence embedding method optimized to capture semantic content and an Abstract Meaning Representation graph representation designed to capture structural similarities in argumentation style and the use of metaphor. I apply this method to vectorize sentences from a corpus of roughly 400,000 nonfiction books and academic publications from the 19th century for instances of ideas and arguments appearing in Darwin's publications. This functions as an initial evaluation and proof of concept; the method is not limited to detecting Darwinian ideas but is capable of detecting similarities on a large scale in a wide range of corpora and contexts.</li>
<li><strong>摘要：</strong>在本文中，我提出了一种新颖的方法来检测大型语料库中的智力影响力。利用大型语言模型在编码语义和结构意义方面的独特功能，同时保持对释义的鲁棒性，我们可以以计算有效的方式搜索实质上相似的想法和智力影响的暗示。这种方法使我们能够操作不同级别的置信度：我们可以允许直接引用、释义或推测相似性，同时对每个阈值的限制保持开放。我应用了一种结合了通用文本嵌入的集成方法，一种最先进的句子嵌入方法，经过优化以捕获语义内容，以及一种抽象含义表示图表示，旨在捕获论证风格和隐喻使用中的结构相似性。我应用这种方法对 19 世纪大约 400,000 本非小说类书籍和学术出版物的语料库中的句子进行向量化，以获取达尔文出版物中出现的思想和论点的实例。这起到了初步评估和概念验证的作用；该方法不仅限于检测达尔文思想，还能够在广泛的语料库和上下文中大规模地检测相似性。</li>
</ul>

<h3>Title: Language models align with human judgments on key grammatical  constructions</h3>
<ul>
<li><strong>Authors: </strong>Jennifer Hu, Kyle Mahowald, Gary Lupyan, Anna Ivanova, Roger Levy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01676">https://arxiv.org/abs/2402.01676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01676">https://arxiv.org/pdf/2402.01676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01676]] Language models align with human judgments on key grammatical  constructions(https://arxiv.org/abs/2402.01676)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Do Large Language Models (LLMs) make human-like linguistic generalizations? Dentella et al. (2023; "DGL") prompt several LLMs ("Is the following sentence grammatically correct in English?") to elicit grammaticality judgments of 80 English sentences, concluding that LLMs demonstrate a "yes-response bias" and a "failure to distinguish grammatical from ungrammatical sentences". We re-evaluate LLM performance using well-established practices and find that DGL's data in fact provide evidence for just how well LLMs capture human behaviors. Models not only achieve high accuracy overall, but also capture fine-grained variation in human linguistic judgments.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是否可以进行类似人类的语言概括？登特拉等人。 （2023；“DGL”）提示几位法学硕士（“以下句子在英语中语法上正确吗？”）引出了 80 个英语句子的语法判断，得出的结论是法学硕士表现出“是反应偏差”和“无法区分语法”来自不合语法的句子”。我们使用成熟的实践重新评估法学硕士的表现，发现 DGL 的数据实际上为法学硕士捕捉人类行为的能力提供了证据。模型不仅总体上实现了高精度，而且还捕获了人类语言判断的细粒度变化。</li>
</ul>

<h3>Title: Embedding Ontologies via Incoprorating Extensional and Intensional  Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Keyu Wang, Guilin Qi, Jiaoyan Chen, Tianxing Wu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01677">https://arxiv.org/abs/2402.01677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01677">https://arxiv.org/pdf/2402.01677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01677]] Embedding Ontologies via Incoprorating Extensional and Intensional  Knowledge(https://arxiv.org/abs/2402.01677)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can capture both structure information and textual information. Experimental results show that EIKE significantly outperforms state-of-the-art methods in three datasets for both triple classification and link prediction, indicating that EIKE provides a more comprehensive and representative perspective of the domain.</li>
<li><strong>摘要：</strong>本体包含丰富的领域知识，可分为外延知识和内涵知识两类。外延知识提供属于本体中特定概念的具体实例的信息，而内涵知识则详细描述概念之间的固有属性、特征和语义关联。然而，现有的本体嵌入方法未能同时充分考虑外延知识和内涵知识。在本文中，我们提出了一种新颖的本体嵌入方法，称为 EIKE（外延和内涵知识嵌入），通过在两个空间（称为外延空间和内涵空间）中表示本体。 EIKE提出了一个统一的框架，用于将实例、概念及其关系嵌入到本体中，应用基于几何的方法来建模外延知识，并应用预训练的语言模型来建模内涵知识，它可以捕获结构信息和文本信息。实验结果表明，EIKE 在三元组分类和链接预测的三个数据集中显着优于最先进的方法，表明 EIKE 提供了更全面和更具代表性的领域视角。</li>
</ul>

<h3>Title: StickerConv: Generating Multimodal Empathetic Responses from Scratch</h3>
<ul>
<li><strong>Authors: </strong>Yiqun Zhang, Fanheng Kong, Peidong Wang, Shuang Sun, Lingshuai Wang, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01679">https://arxiv.org/abs/2402.01679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01679">https://arxiv.org/pdf/2402.01679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01679]] StickerConv: Generating Multimodal Empathetic Responses from Scratch(https://arxiv.org/abs/2402.01679)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, rag, agent</a></li>
<li><strong>Abstract: </strong>Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research. In this paper, we introduce the Agent for StickerConv (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, StickerConv, which includes 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios, specifically designs to augment the generation of empathetic responses in a multimodal context. To leverage the richness of this dataset, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation model, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotionally resonant multimodal empathetic responses, contributing to the advancement of more nuanced and engaging empathetic dialogue systems. Our project page is available at https://neu-datamining.github.io/StickerConv .</li>
<li><strong>摘要：</strong>贴纸虽然被广泛认为可以增强在线互动中的同理心沟通，但在当前的同理心对话研究中仍未得到充分探索。在本文中，我们介绍了 Agent for StickerConv (Agent4SC)，它使用协作代理交互来通过贴纸的使用来真实地模拟人类行为，从而增强多模式同理心沟通。在此基础上，我们开发了一个多模态同理心对话数据集 StickerConv，其中包括 12.9K 个对话会话、5.8K 个独特贴纸和 2K 个不同的对话场景，专门设计用于增强多模态环境中同理心反应的生成。为了利用该数据集的丰富性，我们提出了 PErceive 和生成贴纸（PEGS），这是一种多模式同理心响应生成模型，并辅以一套基于 LLM 的全面的同理心评估指标。我们的实验证明了 PEGS 在生成上下文相关且情感共鸣的多模式共情反应方面的有效性，有助于发展更细致、更有吸引力的共情对话系统。我们的项目页面位于 https://neu-datamining.github.io/StickerConv 。</li>
</ul>

<h3>Title: Large Language Model based Multi-Agents: A Survey of Progress and  Challenges</h3>
<ul>
<li><strong>Authors: </strong>Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, Xiangliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01680">https://arxiv.org/abs/2402.01680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01680">https://arxiv.org/pdf/2402.01680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01680]] Large Language Model based Multi-Agents: A Survey of Progress and  Challenges(https://arxiv.org/abs/2402.01680)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在广泛的任务中取得了显着的成功。由于法学硕士令人印象深刻的规划和推理能力，他们已被用作自主代理来自动完成许多任务。最近，在使用一个LLM作为单一规划或决策代理的发展基础上，基于LLM的多代理系统在复杂问题解决和世界模拟方面取得了长足的进步。为了向社区提供对这个动态领域的概述，我们提出这项调查，以深入讨论基于法学硕士的多智能体系统的基本方面以及挑战。我们的目标是让读者获得对以下问题的深入见解：基于 LLM 的多智能体模拟哪些领域和环境？这些代理的概况如何以及他们如何沟通？哪些机制有助于代理人能力的增长？对于那些有兴趣深入研究这一领域的人，我们还总结了常用的数据集或基准，以方便他们访问。为了让研究人员了解最新研究，我们维护了一个开源 GitHub 存储库，专门概述基于 LLM 的多智能体系统的研究。</li>
</ul>

<h3>Title: Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social  Media Communications</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Zhou, Paiheng Xu, Xiyao Wang, Xuan Lu, Ge Gao, Wei Ai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01681">https://arxiv.org/abs/2402.01681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01681">https://arxiv.org/pdf/2402.01681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01681]] Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social  Media Communications(https://arxiv.org/abs/2402.01681)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code, chat, rag</a></li>
<li><strong>Abstract: </strong>Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji meanings can enhance clarity and transparency in online communications. Our findings indicate that ChatGPT has extensive knowledge of emojis. It is adept at elucidating the meaning of emojis across various application scenarios and demonstrates the potential to replace human annotators in a range of tasks.</li>
<li><strong>摘要：</strong>表情符号封装了超越单词或短语的语义，在社交网络通信中已经变得普遍。这激发了学术界对探索它们的属性和功能的兴趣。然而，表情符号相关的研究和应用面临两个主要挑战。首先，研究人员通常依靠众包来注释表情符号，以了解它们的情感、使用意图和语义。其次，用户的主观解读往往会导致对表情符号的误解，造成沟通障碍。大型语言模型 (LLM) 在各种注释任务中取得了巨大成功，ChatGPT 展示了跨多个领域的专业知识。在我们的研究中，我们评估了 ChatGPT 在处理先前注释的任务和下游任务方面的有效性。我们的目标是验证以下假设：ChatGPT 可以作为表情符号研究中人类注释器的可行替代方案，并且其解释表情符号含义的能力可以提高在线通信的清晰度和透明度。我们的研究结果表明 ChatGPT 对表情符号有广泛的了解。它擅长阐明表情符号在各种应用场景中的含义，并展示了在一系列任务中替代人类注释者的潜力。</li>
</ul>

<h3>Title: A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs  Using the CGC-LORA Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Chao Song, Zhihao Ye, Qiqiang Lin, Qiuying Peng, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01684">https://arxiv.org/abs/2402.01684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01684">https://arxiv.org/pdf/2402.01684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01684]] A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs  Using the CGC-LORA Algorithm(https://arxiv.org/abs/2402.01684)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain. In practice, there are two prevailing ways, in which the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently using the corresponding training samples from each task. (ii) An Integrated Model: Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. To address the high computing cost and seesawing issue simultaneously, we propose a unified framework that implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an innovative layer that contains two types of experts as additional trainable parameters to make LoRA be compatible with MTL. To comprehensively evaluate the proposed framework, we conduct well-designed experiments on two public datasets. The experimental results demonstrate that the unified framework with CGC-LoRA modules achieves higher evaluation scores than all benchmarks on both two datasets.</li>
<li><strong>摘要：</strong>随着自然语言处理（NLP）领域大语言模型（LLM）的富有成效的发展，人们付出了大量的努力来有效地微调常见的预训练LLM，以完成一个或多个特定领域的各种任务。在实践中，有两种流行的方法可以实现适应：（i）多个独立模型：使用每个任务中相应的训练样本对预训练的 LLM 进行多次独立微调。 (ii) 综合模型：使用所有任务的样本来统一微调预试验的法学硕士。为了同时解决高计算成本和跷跷板问题，我们提出了一个统一的框架，使用新颖的定制门控制（CGC）低秩适应（LoRA）算法在LLM中实现1+N多任务微调模式。我们的工作旨在利用 MTL（即 CGC）和 PEFT（即 LoRA）方案的优势。对于给定的任务集群，我们设计了一个创新层，其中包含两种类型的专家作为额外的可训练参数，以使 LoRA 与 MTL 兼容。为了全面评估所提出的框架，我们在两个公共数据集上进行了精心设计的实验。实验结果表明，带有 CGC-LoRA 模块的统一框架在两个数据集上获得了比所有基准更高的评估分数。</li>
</ul>

<h3>Title: SMUTF: Schema Matching Using Generative Tags and Hybrid Features</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Mei Di, Haozheng Luo, Chenwei Xu, Richard Tzong-Han Tsai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01685">https://arxiv.org/abs/2402.01685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01685">https://arxiv.org/pdf/2402.01685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01685]] SMUTF: Schema Matching Using Generative Tags and Hybrid Features(https://arxiv.org/abs/2402.01685)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models. Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated exceptional performance, surpassing existing state-of-the-art models in terms of accuracy and efficiency, and} improving the F1 score by 11.84% and the AUC of ROC by 5.08%.</li>
<li><strong>摘要：</strong>我们引入了 SMUTF，这是一种用于大规模表格数据模式匹配（SM）的独特方法，它假设监督学习不会影响开放域任务的性能，从而实现有效的跨域匹配。该系统独特地结合了基于规则的特征工程、预训练语言模型和生成式大语言模型。在受人道主义交换语言启发的创新改编中，我们为每个数据列部署“生成标签”，从而增强了 SM 的有效性。 SMUTF 具有广泛的多功能性，可与任何现有的预训练嵌入、分类方法和生成模型无缝协作。认识到缺乏广泛的公开可用的 SM 数据集，我们根据公共人道主义数据创建并开源了 HDXSM 数据集。我们相信这是目前可用的最详尽的 SM 数据集。在各种公共数据集和新颖的 HDXSM 数据集的评估中，SMUTF 表现出了卓越的性能，在准确性和效率方面超越了现有的最先进模型，并将 F1 分数提高了 11.84%，ROC 的 AUC 提高了 5.08 %。</li>
</ul>

<h3>Title: Linguistic-Based Mild Cognitive Impairment Detection Using Informative  Loss</h3>
<ul>
<li><strong>Authors: </strong>Ali Pourramezan Fard, Mohammad H. Mahoor, Muath Alsuhaibani, Hiroko H. Dodgec</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01690">https://arxiv.org/abs/2402.01690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01690">https://arxiv.org/pdf/2402.01690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01690]] Linguistic-Based Mild Cognitive Impairment Detection Using Informative  Loss(https://arxiv.org/abs/2402.01690)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat, rag</a></li>
<li><strong>Abstract: </strong>This paper presents a deep learning method using Natural Language Processing (NLP) techniques, to distinguish between Mild Cognitive Impairment (MCI) and Normal Cognitive (NC) conditions in older adults. We propose a framework that analyzes transcripts generated from video interviews collected within the I-CONECT study project, a randomized controlled trial aimed at improving cognitive functions through video chats. Our proposed NLP framework consists of two Transformer-based modules, namely Sentence Embedding (SE) and Sentence Cross Attention (SCA). First, the SE module captures contextual relationships between words within each sentence. Subsequently, the SCA module extracts temporal features from a sequence of sentences. This feature is then used by a Multi-Layer Perceptron (MLP) for the classification of subjects into MCI or NC. To build a robust model, we propose a novel loss function, called InfoLoss, that considers the reduction in entropy by observing each sequence of sentences to ultimately enhance the classification accuracy. The results of our comprehensive model evaluation using the I-CONECT dataset show that our framework can distinguish between MCI and NC with an average area under the curve of 84.75%.</li>
<li><strong>摘要：</strong>本文提出了一种使用自然语言处理（NLP）技术的深度学习方法，以区分老年人的轻度认知障碍（MCI）和正常认知（NC）状况。我们提出了一个框架，用于分析 I-CONECT 研究项目中收集的视频采访生成的文字记录，这是一项旨在通过视频聊天改善认知功能的随机对照试验。我们提出的 NLP 框架由两个基于 Transformer 的模块组成，即句子嵌入（SE）和句子交叉注意（SCA）。首先，SE 模块捕获每个句子中单词之间的上下文关系。随后，SCA 模块从句子序列中提取时间特征。然后，多层感知器 (MLP) 使用此功能将受试者分类为 MCI 或 NC。为了构建一个鲁棒的模型，我们提出了一种新颖的损失函数，称为 InfoLoss，它通过观察每个句子序列来考虑熵的减少，以最终提高分类准确性。我们使用 I-CONECT 数据集进行的综合模型评估结果表明，我们的框架可以区分 MCI 和 NC，平均曲线下面积为 84.75%。</li>
</ul>

<h3>Title: Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by  Self-Supervised Representation Mixing and Embedding Initialization</h3>
<ul>
<li><strong>Authors: </strong>Wei-Ping Huang, Sung-Feng Huang, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01692">https://arxiv.org/abs/2402.01692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01692">https://arxiv.org/pdf/2402.01692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01692]] Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by  Self-Supervised Representation Mixing and Embedding Initialization(https://arxiv.org/abs/2402.01692)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.</li>
<li><strong>摘要：</strong>本文提出了一种用于文本到语音系统中语言适应的有效迁移学习框架，重点是使用最少的标记和未标记数据来实现语言适应。虽然许多工作专注于减少标记数据的使用，但很少有人考虑最大限度地减少未标记数据的使用。通过在预训练阶段利用自监督特征，在微调期间用这些特征替换伪标签的噪声部分，并结合嵌入初始化技巧，与传统方法相比，我们的方法利用了更多来自未标记数据的信息。实验结果表明，我们的框架能够仅用 4 条标记数据和 15 分钟的未标记数据来合成看不见的语言的可理解语音。即使可以访问更多数据，我们的方法仍继续超越传统技术。这些发现凸显了我们的数据高效语言适应框架的潜力。</li>
</ul>

<h3>Title: Quality of Answers of Generative Large Language Models vs Peer Patients  for Interpreting Lab Test Results for Lay Patients: Evaluation Study</h3>
<ul>
<li><strong>Authors: </strong>Zhe He, Balu Bhasuran, Qiao Jin, Shubo Tian, Karim Hanna, Cindy Shavor, Lisbeth Garcia Arguello, Patrick Murray, Zhiyong Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01693">https://arxiv.org/abs/2402.01693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01693">https://arxiv.org/pdf/2402.01693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01693]] Quality of Answers of Generative Large Language Models vs Peer Patients  for Interpreting Lab Test Results for Lay Patients: Evaluation Study(https://arxiv.org/abs/2402.01693)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Lab results are often confusing and hard to understand. Large language models (LLMs) such as ChatGPT have opened a promising avenue for patients to get their questions answered. We aim to assess the feasibility of using LLMs to generate relevant, accurate, helpful, and unharmful responses to lab test-related questions asked by patients and to identify potential issues that can be mitigated with augmentation approaches. We first collected lab test results related question and answer data from Yahoo! Answers and selected 53 QA pairs for this study. Using the LangChain framework and ChatGPT web portal, we generated responses to the 53 questions from four LLMs including GPT-4, Meta LLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their answers using standard QA similarity-based evaluation metrics including ROUGE, BLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge whether a target model has higher quality in terms of relevance, correctness, helpfulness, and safety than the baseline model. Finally, we performed a manual evaluation with medical experts for all the responses to seven selected questions on the same four aspects. The results of Win Rate and medical expert evaluation both showed that GPT-4's responses achieved better scores than all the other LLM responses and human responses on all four aspects (relevance, correctness, helpfulness, and safety). However, LLM responses occasionally also suffer from a lack of interpretation in one's medical context, incorrect statements, and lack of references. We find that compared to other three LLMs and human answer from the Q&A website, GPT-4's responses are more accurate, helpful, relevant, and safer. However, there are cases which GPT-4 responses are inaccurate and not individualized. We identified a number of ways to improve the quality of LLM responses.</li>
<li><strong>摘要：</strong>实验室结果常常令人困惑且难以理解。 ChatGPT 等大型语言模型 (LLM) 为患者解答问题开辟了一条充满希望的途径。我们的目标是评估使用法学硕士对患者提出的实验室测试相关问题生成相关、准确、有用且无害的答复的可行性，并确定可以通过增强方法缓解的潜在问题。我们首先从 Yahoo! 收集了与实验室测试结果相关的问答数据。为本研究提供答案并选择 53 个 QA 对。使用 LangChain 框架和 ChatGPT 门户网站，我们生成了来自四位法学硕士（包括 GPT-4、Meta LLaMA 2、MedAlpaca 和 ORCA_mini）的 53 个问题的答案。我们首先使用基于标准 QA 相似性的评估指标（包括 ROUGE、BLEU、METEOR、BERTScore）评估他们答案的相似性。我们还利用基于 LLM 的评估器来判断目标模型在相关性、正确性、有用性和安全性方面是否比基线模型具有更高的质量。最后，我们与医学专家对同一四个方面的七个选定问题的所有回答进行了手动评估。胜率和医学专家评估的结果均表明，GPT-4 的回答在所有四个方面（相关性、正确性、有用性和安全性）都比所有其他 LLM 回答和人类回答取得了更好的分数。然而，法学硕士的回答有时也会受到缺乏医学背景的解释、不正确的陈述和缺乏参考资料的影响。我们发现，与其他三位法学硕士和问答网站的人工回答相比，GPT-4 的回答更加准确、有帮助、相关且安全。然而，在某些情况下，GPT-4 反应不准确且不个性化。我们确定了多种提高法学硕士回复质量的方法。</li>
</ul>

<h3>Title: ARGS: Alignment as Reward-Guided Search</h3>
<ul>
<li><strong>Authors: </strong>Maxim Khanov, Jirayu Burapacheep, Yixuan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01694">https://arxiv.org/abs/2402.01694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01694">https://arxiv.org/pdf/2402.01694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01694]] ARGS: Alignment as Reward-Guided Search(https://arxiv.org/abs/2402.01694)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, code, rag</a></li>
<li><strong>Abstract: </strong>Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing decoding-time alignment, paves the way for more responsive language models in the future. Code is publicly available at: \url{https://github.com/deeplearning-wisc/args}.</li>
<li><strong>摘要：</strong>让大型语言模型与人类目标保持一致至关重要，但包括 RLHF 在内的常见方法都受到不稳定且资源密集型训练的困扰。为了应对这一挑战，我们引入了 ARGS（Alignment as Reward-Guided Search），这是一种新颖的框架，它将对齐集成到解码过程中，从而消除了昂贵的 RL 训练的需要。通过使用奖励信号调整模型的概率预测，ARGS 生成具有语义多样性的文本，同时符合人类偏好，为对齐语言模型提供了一种有前途且灵活的解决方案。值得注意的是，与不同对齐任务和各种模型维度的基线相比，ARGS 表现出平均奖励的持续增强。例如，在相同的基于贪婪的解码策略下，我们的方法相对于基线将平均奖励提高了 19.56%，并在 GPT-4 评估中确保了 64.33% 的偏好或平局分数。我们相信，我们的框架强调解码时间对齐，为未来更具响应性的语言模型铺平了道路。代码可在以下位置公开获取：\url{https://github.com/deeplearning-wisc/args}。</li>
</ul>

<h3>Title: Language-Guided World Models: A Model-Based Approach to AI Control</h3>
<ul>
<li><strong>Authors: </strong>Alex Zhang, Khanh Nguyen, Jens Tuyls, Albert Lin, Karthik Narasimhan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01695">https://arxiv.org/abs/2402.01695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01695">https://arxiv.org/pdf/2402.01695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01695]] Language-Guided World Models: A Model-Based Approach to AI Control(https://arxiv.org/abs/2402.01695)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new language descriptions and environment dynamics. Our experiments reveal that the current state-of-the-art Transformer architecture performs poorly on this benchmark, motivating us to design a more robust architecture. To showcase the practicality of our proposed LWMs, we simulate a scenario where these models augment the interpretability and safety of an agent by enabling it to generate and discuss plans with a human before execution. By effectively incorporating language feedback on the plan, the models boost the agent performance in the real environment by up to three times without collecting any interactive experiences in this environment.</li>
<li><strong>摘要：</strong>将概率世界模型安装到人工代理中，为人类与这些代理进行通信和控制提供了有效的渠道。除了更新代理策略之外，人类还可以修改他们的内部世界模型以影响他们的决策。然而，挑战在于，目前现有的世界模型很难让人类适应，因为它们缺乏自然的沟通界面。为了解决这个缺点，我们开发了语言引导世界模型（LWM），它可以通过读取语言描述来捕获环境动态。这些模型提高了代理的沟通效率，允许人类通过简洁的语言反馈同时改变其在多个任务中的行为。它们还使智能体能够从最初为指导人类而编写的文本中进行自学习。为了促进 LWM 的开发，我们基于 MESSENGER 游戏设计了一个具有挑战性的基准（Hanjie et al., 2021），要求对新的语言描述和环境动态进行组合泛化。我们的实验表明，当前最先进的 Transformer 架构在此基准测试中表现不佳，这促使我们设计更强大的架构。为了展示我们提出的 LWM 的实用性，我们模拟了一个场景，其中这些模型通过使其能够在执行之前生成计划并与人类讨论来增强代理的可解释性和安全性。通过有效地将语言反馈纳入计划，这些模型将真实环境中的代理性能提高了三倍，而无需收集该环境中的任何交互体验。</li>
</ul>

<h3>Title: HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text  Classification</h3>
<ul>
<li><strong>Authors: </strong>Vidit Jain, Mukund Rungta, Yuchen Zhuang, Yue Yu, Zeyu Wang, Mu Gao, Jeffrey Skolnick, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01696">https://arxiv.org/abs/2402.01696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01696">https://arxiv.org/pdf/2402.01696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01696]] HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text  Classification(https://arxiv.org/abs/2402.01696)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicting Enzyme Commission (EC) numbers. Through extensive experiments on the ENZYME dataset and the widely recognized WOS and NYT datasets, our methodology demonstrates superior performance, surpassing existing approaches while efficiently handling data and mitigating class imbalance. The data and code will be released publicly.</li>
<li><strong>摘要：</strong>分层文本分类（HTC）是多标签文本分类下的复杂子任务，其特点是分层标签分类和数据不平衡。性能最佳的模型旨在通过结合文档和分层标签信息来学习静态表示。然而，文档部分的相关性可能会根据层次结构级别而变化，因此需要动态文档表示。为了解决这个问题，我们提出了 HiGen，一种基于文本生成的框架，利用语言模型对动态文本表示进行编码。我们引入了一个级别引导的损失函数来捕获文本和标签名称语义之间的关系。我们的方法结合了特定于任务的预训练策略，使语言模型适应领域内知识，并显着提高示例有限的类的性能。此外，我们还提出了一个名为 ENZYME 的新的有价值的数据集，该数据集是为 HTC 设计的，其中包含来自 PubMed 的文章，旨在预测酶委员会 (EC) 数字。通过对 ENZYME 数据集以及广泛认可的 WOS 和 NYT 数据集进行的广泛实验，我们的方法展示了卓越的性能，超越了现有方法，同时有效处理数据并减轻了类别不平衡。数据和代码将公开发布。</li>
</ul>

<h3>Title: APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data  Annotation</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zhu, Zhizhuo Yin, Ehsan-Ul Haq, Lik-Hang Lee, Gareth Tyson, Pan Hui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01697">https://arxiv.org/abs/2402.01697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01697">https://arxiv.org/pdf/2402.01697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01697]] APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data  Annotation(https://arxiv.org/abs/2402.01697)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt, chat, rag</a></li>
<li><strong>Abstract: </strong>Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by showing how it can be extended to support additional tuning mechanisms.</li>
<li><strong>摘要：</strong>最近的研究强调了 LLM 应用程序（例如 ChatGPT）在社交计算文本上执行标签注释的潜力。然而，众所周知，性能取决于输入提示的质量。为了解决这个问题，人们对提示调整进行了大量研究——试图提高提示质量的技术和指南。然而，这些在很大程度上依赖于手动工作和所注释数据集的先验知识。为了解决这个限制，我们提出了 APT-Pipe，一种自动提示调整管道。 APT-Pipe 旨在自动调整提示，以增强 ChatGPT 在任何给定数据集上的文本分类性能。我们实现 APT-Pipe 并在 12 个不同的文本分类数据集上对其进行测试。我们发现，APT-Pipe 调整的提示帮助 ChatGPT 在 12 个实验数据集中的 9 个上实现了更高的加权 F1 分数，平均提高了 7.01%。我们通过展示如何扩展 APT-Pipe 以支持其他调整机制，进一步强调了 APT-Pipe 作为框架的灵活性。</li>
</ul>

<h3>Title: Large language model empowered participatory urban planning</h3>
<ul>
<li><strong>Authors: </strong>Zhilun Zhou, Yuming Lin, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01698">https://arxiv.org/abs/2402.01698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01698">https://arxiv.org/pdf/2402.01698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01698]] Large language model empowered participatory urban planning(https://arxiv.org/abs/2402.01698)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.</li>
<li><strong>摘要：</strong>参与式城市规划是现代城市规划的主流，需要不同利益相关者的积极参与。然而，传统的参与范式遇到了时间和人力的挑战，而生成规划工具无法提供可调整和包容的解决方案。这项研究引入了一种创新的城市规划方法，将大语言模型（LLM）整合到参与过程中。该框架基于精心设计的 LLM 代理，由角色扮演、协作生成和反馈迭代组成，解决了满足 1000 个不同兴趣的社区级土地利用任务。在不同城市社区的实证实验展示了法学硕士在不同规划场景中的适应性和有效性。结果通过四个指标进行评估，在满意度和包容性方面超越了人类专家，并在服务和生态方面与最先进的强化学习方法相媲美。进一步的分析显示了LLM代理在提供具有自然语言推理和强大可扩展性的可调整和包容性解决方案方面的优势。在实施模拟人类行为规划方面的最新进展的同时，这项工作设想规划者和公民都受益于低成本、高效的法学硕士代理人，这对于加强参与和实现参与式城市规划至关重要。</li>
</ul>

<h3>Title: Fluent dreaming for language models</h3>
<ul>
<li><strong>Authors: </strong>T. Ben Thompson (1), Zygimantas Straznickas (1), Michael Sklar (1) ((1) Confirm Labs)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01702">https://arxiv.org/abs/2402.01702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01702">https://arxiv.org/pdf/2402.01702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01702]] Fluent dreaming for language models(https://arxiv.org/abs/2402.01702)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code</a></li>
<li><strong>Abstract: </strong>Feature visualization, also known as "dreaming", offers insights into vision models by optimizing the inputs to maximize a neuron's activation or other internal component. However, dreaming has not been successfully applied to language models because the input space is discrete. We extend Greedy Coordinate Gradient, a method from the language model adversarial attack literature, to design the Evolutionary Prompt Optimization (EPO) algorithm. EPO optimizes the input prompt to simultaneously maximize the Pareto frontier between a chosen internal feature and prompt fluency, enabling fluent dreaming for language models. We demonstrate dreaming with neurons, output logits and arbitrary directions in activation space. We measure the fluency of the resulting prompts and compare language model dreaming with max-activating dataset examples. Critically, fluent dreaming allows automatically exploring the behavior of model internals in reaction to mildly out-of-distribution prompts. Code for running EPO is available at https://github.com/Confirm-Solutions/dreamy. A companion page demonstrating code usage is at https://confirmlabs.org/posts/dreamy.html</li>
<li><strong>摘要：</strong>特征可视化，也称为“梦想”，通过优化输入以最大化神经元的激活或其他内部组件来提供对视觉模型的见解。然而，由于输入空间是离散的，梦想尚未成功应用于语言模型。我们扩展了贪婪坐标梯度（一种来自语言模型对抗攻击文献的方法）来设计进化提示优化（EPO）算法。 EPO 优化输入提示，以同时最大化所选内部特征和提示流畅性之间的帕累托边界，从而实现语言模型的流畅梦想。我们用神经元、输出逻辑和激活空间中的任意方向来演示做梦。我们测量结果提示的流畅性，并将语言模型梦想与最大激活数据集示例进行比较。至关重要的是，流畅的梦允许自动探索模型内部的行为，以对轻微的不符合分布提示做出反应。运行 EPO 的代码可在 https://github.com/Confirm-Solutions/dreamy 获取。演示代码使用情况的配套页面位于 https://confirmlabs.org/posts/dreamy.html</li>
</ul>

<h3>Title: States as Strings as Strategies: Steering Language Models with  Game-Theoretic Solvers</h3>
<ul>
<li><strong>Authors: </strong>Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari Dasagi, Luke Marris, Georgios Piliouras, Karl Tuyls</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01704">https://arxiv.org/abs/2402.01704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01704">https://arxiv.org/pdf/2402.01704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01704]] States as Strings as Strategies: Steering Language Models with  Game-Theoretic Solvers(https://arxiv.org/abs/2402.01704)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag, agent</a></li>
<li><strong>Abstract: </strong>Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new dialogue scenarios, which are grounded in real world applications. In this work, we present one possible binding from dialogue to game theory as well as generalizations of existing equilibrium finding algorithms to this setting. In addition, by exploiting LLMs generation capabilities along with our proposed binding, we can synthesize a large repository of formally-defined games in which one can study and test game-theoretic solution concepts. We also demonstrate how one can combine LLM-driven game generation, game-theoretic solvers, and imitation learning to construct a process for improving the strategic capabilities of LLMs.</li>
<li><strong>摘要：</strong>博弈论是对理性主体之间战略互动的数学模型的研究。语言是人类互动的关键媒介，尽管历史证明很难对对话及其策略动机进行数学建模。与语言交互相关的参与者、策略和收益的合适模型（即与博弈论的传统符号逻辑的结合）将使现有的博弈论算法能够在语言空间中提供策略解决方案。换句话说，绑定可以提供一种在对话中计算稳定、合理的对话策略的途径。大型语言模型（LLM）可以说已经达到了这样的程度：它们的生成能力可以实现真实的、类人的自然对话模拟。通过以各种方式提示他们，我们可以引导他们对不同的输出话语做出反应。利用自然语言的表达能力，法学硕士还可以帮助我们快速生成基于现实世界应用的新对话场景。在这项工作中，我们提出了从对话到博弈论的一种可能的结合，以及现有平衡发现算法对此设置的概括。此外，通过利用 LLM 生成功能以及我们提出的绑定，我们可以合成一个大型的正式定义的游戏存储库，在其中可以研究和测试游戏理论解决方案概念。我们还演示了如何将法学硕士驱动的游戏生成、博弈论求解器和模仿学习结合起来，构建一个提高法学硕士战略能力的流程。</li>
</ul>

<h3>Title: MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse  Worlds</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong Jin, Zhuo Zhang, Xiangyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01706">https://arxiv.org/abs/2402.01706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01706">https://arxiv.org/pdf/2402.01706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01706]] MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse  Worlds(https://arxiv.org/abs/2402.01706)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both effectiveness and efficiency. In addition, our results indicate that existing LLMs are extremely vulnerable to nesting worlds and programming language worlds. They imply that existing alignment training focuses on the real-world and is lacking in various (virtual) worlds where LLMs can be exploited.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 调整旨在确保 LLM 输出与人类价值观相匹配。研究人员已经通过大量越狱技术证明了对齐问题的严重性，这些技术可能导致法学硕士在对话期间产生恶意内容。找到相应的越狱提示通常需要大量的人类智慧或计算资源。在本文中，我们报告法学硕士在不同背景下具有不同程度的一致性。因此，通过系统地构建许多称为世界的上下文，利用描述可能的世界（例如时间、位置、角色、动作和语言）的领域特定语言以及相应的编译器，我们可以经济高效地暴露潜在的对齐问题。鉴于我们方法的成本较低，我们能够针对不同世界中的法学硕士对齐问题进行大规模研究。我们的结果表明，我们的方法在有效性和效率方面都优于最先进的越狱技术。此外，我们的结果表明，现有的法学硕士极易受到嵌套世界和编程语言世界的影响。它们意味着现有的一致性培训侧重于现实世界，缺乏法学硕士可以利用的各种（虚拟）世界。</li>
</ul>

<h3>Title: Socially Aware Synthetic Data Generation for Suicidal Ideation Detection  Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hamideh Ghanadian, Isar Nejadgholi, Hussein Al Osman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01712">https://arxiv.org/abs/2402.01712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01712">https://arxiv.org/pdf/2402.01712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01712]] Socially Aware Synthetic Data Generation for Suicidal Ideation Detection  Using Large Language Models(https://arxiv.org/abs/2402.01712)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat, rag</a></li>
<li><strong>Abstract: </strong>Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed by social factors, offers consistent F1-scores of 0.82 for both models, suggesting that the richness of topics in synthetic data can bridge the performance gap across different model complexities. Most impressively, when we combined a mere 30% of the UMD dataset with our synthetic data, we witnessed a substantial increase in performance, achieving an F1-score of 0.88 on the UMD test set. Such results underscore the cost-effectiveness and potential of our approach in confronting major challenges in the field, such as data scarcity and the quest for diversity in data representation.</li>
<li><strong>摘要：</strong>自杀意念检测是一个重要的研究领域，在改善心理健康支持系统方面具有巨大的潜力。然而，自杀相关数据的敏感性给访问训练有效的机器学习模型所需的大规模带注释数据集带来了挑战。为了解决这一限制，我们引入了一种创新策略，利用生成式人工智能模型（例如 ChatGPT、Flan-T5 和 Llama）的功能来创建用于自杀意念检测的合成数据。我们的数据生成方法基于从心理学文献中提取的社会因素，旨在确保覆盖与自杀意念相关的基本信息。在我们的研究中，我们以最先进的 NLP 分类模型为基准，特别是那些以 BERT 系列结构为中心的模型。当在现实世界数据集 UMD 上进行训练时，这些传统模型往往会产生 0.75 到 0.87 之间的 F1 分数。我们的合成数据驱动方法根据社会因素提供了一致的 F1 分数 0.82，这表明合成数据中主题的丰富性可以弥合不同模型复杂性之间的性能差距。最令人印象深刻的是，当我们将 30% 的 UMD 数据集与我们的合成数据相结合时，我们见证了性能的大幅提升，在 UMD 测试集上实现了 0.88 的 F1 分数。这些结果强调了我们的方法在应对该领域重大挑战（例如数据稀缺和追求数据表示多样性）方面的成本效益和潜力。</li>
</ul>

<h3>Title: Prompting Large Language Models for Zero-Shot Clinical Prediction with  Structured Longitudinal Electronic Health Record Data</h3>
<ul>
<li><strong>Authors: </strong>Yinghao Zhu, Zixiang Wang, Junyi Gao, Yuning Tong, Jingkun An, Weibin Liao, Ewen M. Harrison, Liantao Ma, Chengwei Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01713">https://arxiv.org/abs/2402.01713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01713">https://arxiv.org/pdf/2402.01713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01713]] Prompting Large Language Models for Zero-Shot Clinical Prediction with  Structured Longitudinal Electronic Health Record Data(https://arxiv.org/abs/2402.01713)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with our elaborately designed prompting framework, LLMs can improve prediction performance in key tasks such as mortality, length-of-stay, and 30-day readmission by about 35\%, surpassing ML models in few-shot settings. Our research underscores the potential of LLMs in enhancing clinical decision-making, especially in urgent healthcare situations like the outbreak of emerging diseases with no labeled data. The code is publicly available at https://github.com/yhzhu99/llm4healthcare for reproducibility.</li>
<li><strong>摘要：</strong>结构化纵向电子健康记录 (EHR) 数据固有的复杂性在与传统上为自然语言处理量身定制的大型语言模型 (LLM) 集成时提出了重大挑战。在新疾病爆发期间，传统的预测模型常常因缺乏历史数据而失败，因此迫切需要做出快速决策，本研究调查了法学硕士（如 GPT-4）对 EHR 数据的适应性。我们特别关注它们的零样本能力，这使它们能够在未经明确训练的场景中做出预测。针对 EHR 数据的纵向、稀疏和知识注入的性质，我们的提示方法包括考虑特定的 EHR 特征，例如单位和参考范围，并采用与临床背景相一致的上下文学习策略。我们对 MIMIC-IV 和 TJH 数据集的综合实验表明，通过我们精心设计的提示框架，法学硕士可以将死亡率、住院时间和 30 天再入院等关键任务的预测性能提高约 35%，超过少样本设置中的 ML 模型。我们的研究强调了法学硕士在增强临床决策方面的潜力，特别是在紧急医疗保健情况下，例如没有标记数据的新发疾病的爆发。该代码可在 https://github.com/yhzhu99/llm4healthcare 上公开获取，以确保可重复性。</li>
</ul>

<h3>Title: TrICy: Trigger-guided Data-to-text Generation with Intent aware  Attention-Copy</h3>
<ul>
<li><strong>Authors: </strong>Vibhav Agarwal, Sourav Ghosh, Harichandana BSS, Himanshu Arora, Barath Raj Kandur Raja</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01714">https://arxiv.org/abs/2402.01714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01714">https://arxiv.org/pdf/2402.01714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01714]] TrICy: Trigger-guided Data-to-text Generation with Intent aware  Attention-Copy(https://arxiv.org/abs/2402.01714)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, rag</a></li>
<li><strong>Abstract: </strong>Data-to-text (D2T) generation is a crucial task in many natural language understanding (NLU) applications and forms the foundation of task-oriented dialog systems. In the context of conversational AI solutions that can work directly with local data on the user's device, architectures utilizing large pre-trained language models (PLMs) are impractical for on-device deployment due to a high memory footprint. To this end, we propose TrICy, a novel lightweight framework for an enhanced D2T task that generates text sequences based on the intent in context and may further be guided by user-provided triggers. We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L: 70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom dataset related to text messaging applications, showcase our architecture's effectiveness. Moreover, we show that by leveraging an optional trigger input, data-to-text generation quality increases significantly and achieves the new SOTA score of 69.29% BLEU for E2E NLG. Furthermore, our analyses show that TrICy achieves at least 24% and 3% improvement in BLEU and METEOR respectively over LLMs like GPT-3, ChatGPT, and Llama 2. We also demonstrate that in some scenarios, performance improvement due to triggers is observed even when they are absent in training.</li>
<li><strong>摘要：</strong>数据到文本（D2T）生成是许多自然语言理解（NLU）应用中的一项关键任务，并且构成了面向任务的对话系统的基础。在可以直接处理用户设备上的本地数据的对话式 AI 解决方案中，由于内存占用较高，利用大型预训练语言模型 (PLM) 的架构对于设备上部署来说是不切实际的。为此，我们提出了 TrICy，这是一种用于增强型 D2T 任务的新型轻量级框架，它根据上下文中的意图生成文本序列，并可以进一步由用户提供的触发器进行指导。我们利用注意力复制机制来准确预测词汇外（OOV）单词。对 E2E NLG 数据集（BLEU：66.43％，ROUGE-L：70.14％）、WebNLG 数据集（BLEU：Seen 64.08％，Unseen 52.35％）以及与短信应用程序相关的自定义数据集的性能分析展示了我们的架构的有效性。此外，我们还表明，通过利用可选的触发输入，数据到文本的生成质量显着提高，并且 E2E NLG 的新 SOTA 得分为 69.29% BLEU。此外，我们的分析表明，与 GPT-3、ChatGPT 和 Llama 2 等 LLM 相比，TrICy 在 BLEU 和 METEOR 方面分别实现了至少 24% 和 3% 的改进。我们还证明，在某些情况下，甚至可以观察到由于触发器而导致的性能改进当他们缺席训练时。</li>
</ul>

<h3>Title: ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Alessio Buscemi, Daniele Proverbio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01715">https://arxiv.org/abs/2402.01715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01715">https://arxiv.org/pdf/2402.01715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01715]] ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis(https://arxiv.org/abs/2402.01715)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Automated sentiment analysis using Large Language Model (LLM)-based models like ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic research and in industrial applications. However, assessment and validation of their performance in case of ambiguous or ironic text is still poor. In this study, we constructed nuanced and ambiguous scenarios, we translated them in 10 languages, and we predicted their associated sentiment using popular LLMs. The results are validated against post-hoc human responses. Ambiguous scenarios are often well-coped by ChatGPT and Gemini, but we recognise significant biases and inconsistent performance across models and evaluated human languages. This work provides a standardised methodology for automated sentiment analysis evaluation and makes a call for action to further improve the algorithms and their underlying data, to improve their performance, interpretability and applicability.</li>
<li><strong>摘要：</strong>使用 ChatGPT、Gemini 或 LLaMA2 等基于大型语言模型 (LLM) 的模型进行自动情感分析正在学术研究和工业应用中变得越来越普遍。然而，在含糊或讽刺性文本的情况下，对其表现的评估和验证仍然很差。在这项研究中，我们构建了微妙且模糊的场景，将它们翻译成 10 种语言，并使用流行的法学硕士预测了它们的相关情绪。结果根据事后人类反应进行验证。 ChatGPT 和 Gemini 通常可以很好地应对模棱两可的场景，但我们认识到模型和评估的人类语言之间存在显着的偏差和不一致的性能。这项工作为自动情感分析评估提供了一种标准化方法，并呼吁采取行动进一步改进算法及其基础数据，以提高其性能、可解释性和适用性。</li>
</ul>

<h3>Title: From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical  Regulatory Compliance Process</h3>
<ul>
<li><strong>Authors: </strong>Jaewoong Kim (Sungkyunkwan University), Moohong Min (Sungkyunkwan University)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01717">https://arxiv.org/abs/2402.01717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01717">https://arxiv.org/pdf/2402.01717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01717]] From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical  Regulatory Compliance Process(https://arxiv.org/abs/2402.01717)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further research and development.</li>
<li><strong>摘要：</strong>制药行业的监管合规性需要遵循复杂而大量的指导方针，通常需要大量的人力资源。为了应对这些挑战，我们的研究引入了一种利用生成人工智能和检索增强生成（RAG）方法的聊天机器人模型。该聊天机器人旨在搜索与用户查询相关的指南文档，并根据检索到的指南提供答案。认识到该领域对高可靠性的内在需求，我们提出了问答检索增强生成（QA-RAG）模型。在比较实验中，QA-RAG 模型表现出准确性的显着提高，优于包括传统 RAG 方法在内的所有其他基线。本文详细介绍了 QA-RAG 的结构和绩效评估，强调了其在制药行业及其他行业监管合规领域的潜力。我们已将我们的工作公开以供进一步研究和开发。</li>
</ul>

<h3>Title: Measuring Moral Inconsistencies in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Vamshi Krishna Bonagiri, Sreeram Vennam, Manas Gaur, Ponnurangam Kumaraguru</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01719">https://arxiv.org/abs/2402.01719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01719">https://arxiv.org/pdf/2402.01719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01719]] Measuring Moral Inconsistencies in Large Language Models(https://arxiv.org/abs/2402.01719)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies and propose improvements.</li>
<li><strong>摘要：</strong>如果语义等效的提示产生语义等效的响应，则大型语言模型（LLM）被认为是一致的。尽管最近的进展展示了法学硕士在会话系统中令人印象深刻的能力，但我们表明，即使是最先进的法学硕士在其世代中也高度不一致，质疑其可靠性。先前的研究试图通过特定任务的准确性来衡量这一点。然而，这种方法不适合道德场景，例如电车难题，没有“正确”答案。为了解决这个问题，我们提出了一种新的信息论测量方法，称为语义图熵（SGE）来测量法学硕士在道德场景中的一致性。我们利用“经验法则”（RoT）来解释模型的决策策略并进一步增强我们的指标。与现有的一致性指标相比，SGE 与五个法学硕士的人类判断具有更好的相关性。未来，我们的目标是调查LLM不一致的根本原因并提出改进建议。</li>
</ul>

<h3>Title: Enhancing Large Language Model Performance To Answer Questions and  Extract Information More Accurately</h3>
<ul>
<li><strong>Authors: </strong>Liang Zhang, Katherine Jijo, Spurthi Setty, Eden Chung, Fatima Javid, Natan Vidra, Tommy Clifford</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01722">https://arxiv.org/abs/2402.01722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01722">https://arxiv.org/pdf/2402.01722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01722]] Enhancing Large Language Model Performance To Answer Questions and  Extract Information More Accurately(https://arxiv.org/abs/2402.01722)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）生成对问题的回答；然而，它们的有效性常常受到答案质量次优和偶尔无法对问题提供准确答复的阻碍。为了应对这些挑战，采用了微调过程，其中包括反馈和示例来完善模型。目标是通过持续反馈循环增强 AI 模型，利用余弦相似度、LLM 评估和 Rouge-L 分数等指标来评估模型。利用 GPT-3.5、GPT4ALL、LLaMA2 和 Claude 等法学硕士，该方法以金融数据集（包括 FinanceBench 和 RAG Instruct 基准测试仪数据集）为基准，说明了微调的必要性。结果展示了微调模型的能力超越了零样本法学硕士的准确性，提供了卓越的问答能力。值得注意的是，事实证明，将 LLM 微调与检索增强生成 (RAG) 过程相结合可以提高生成响应的准确性。</li>
</ul>

<h3>Title: An Empirical Study on Large Language Models in Accuracy and Robustness  under Chinese Industrial Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Zongjie Li, Wenying Qiu, Pingchuan Ma, Yichen Li, You Li, Sijia He, Baozheng Jiang, Shuai Wang, Weixi Gu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01723">https://arxiv.org/abs/2402.01723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01723">https://arxiv.org/pdf/2402.01723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01723]] An Empirical Study on Large Language Models in Accuracy and Robustness  under Chinese Industrial Scenarios(https://arxiv.org/abs/2402.01723)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent years have witnessed the rapid development of large language models (LLMs) in various domains. To better serve the large number of Chinese users, many commercial vendors in China have adopted localization strategies, training and providing local LLMs specifically customized for Chinese users. Furthermore, looking ahead, one of the key future applications of LLMs will be practical deployment in industrial production by enterprises and users in those sectors. However, the accuracy and robustness of LLMs in industrial scenarios have not been well studied. In this paper, we present a comprehensive empirical study on the accuracy and robustness of LLMs in the context of the Chinese industrial production area. We manually collected 1,200 domain-specific problems from 8 different industrial sectors to evaluate LLM accuracy. Furthermore, we designed a metamorphic testing framework containing four industrial-specific stability categories with eight abilities, totaling 13,631 questions with variants to evaluate LLM robustness. In total, we evaluated 9 different LLMs developed by Chinese vendors, as well as four different LLMs developed by global vendors. Our major findings include: (1) Current LLMs exhibit low accuracy in Chinese industrial contexts, with all LLMs scoring less than 0.6. (2) The robustness scores vary across industrial sectors, and local LLMs overall perform worse than global ones. (3) LLM robustness differs significantly across abilities. Global LLMs are more robust under logical-related variants, while advanced local LLMs perform better on problems related to understanding Chinese industrial terminology. Our study results provide valuable guidance for understanding and promoting the industrial domain capabilities of LLMs from both development and industrial enterprise perspectives. The results further motivate possible research directions and tooling support.</li>
<li><strong>摘要：</strong>近年来，大型语言模型（LLM）在各个领域的快速发展。为了更好地服务大量的中国用户，中国的许多商业供应商都采取了本地化策略，培训和提供专门为中国用户定制的本地LLM。此外，展望未来，法学硕士未来的关键应用之一将是这些行业的企业和用户在工业生产中的实际部署。然而，LLM在工业场景中的准确性和稳健性尚未得到很好的研究。在本文中，我们对中国工业生产领域的法学硕士的准确性和稳健性进行了全面的实证研究。我们手动收集了来自 8 个不同行业部门的 1,200 个特定领域问题来评估 LLM 的准确性。此外，我们设计了一个变质测试框架，包含四个特定行业的稳定性类别和八种能力，总共 13,631 个问题及其变体，用于评估 LLM 的稳健性。我们总共评估了中国供应商开发的 9 个不同的 LLM，以及全球供应商开发的 4 个不同的 LLM。我们的主要发现包括： (1) 目前的法学硕士在中国工业环境中的准确性较低，所有法学硕士的得分均低于 0.6。 (2) 不同行业的稳健性得分有所不同，本地法学硕士总体表现低于全球法学硕士。 (3) LLM 稳健性因能力而异。全球法学硕士在逻辑相关变体下更加稳健，而高级本地法学硕士在与理解中国工业术语相关的问题上表现更好。我们的研究结果为从发展和工业企业角度理解和提升法学硕士的工业领域能力提供了宝贵的指导。结果进一步激发了可能的研究方向和工具支持。</li>
</ul>

<h3>Title: Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing  Security in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yunhong He, Jianling Qiu, Wei Zhang, Zhengqing Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01725">https://arxiv.org/abs/2402.01725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01725">https://arxiv.org/pdf/2402.01725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01725]] Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing  Security in Large Language Models(https://arxiv.org/abs/2402.01725)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have significantly enhanced capabilities in natural language processing and artificial intelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionized text generation, translation, and question-answering tasks due to the transformative Transformer model. Despite their widespread use, LLMs present challenges such as ethical dilemmas when models are compelled to respond inappropriately, susceptibility to phishing attacks, and privacy violations. This paper addresses these challenges by introducing a multi-pronged approach that includes: 1) filtering sensitive vocabulary from user input to prevent unethical responses; 2) detecting role-playing to halt interactions that could lead to 'prison break' scenarios; 3) implementing custom rule engines to restrict the generation of prohibited content; and 4) extending these methodologies to various LLM derivatives like Multi-Model Large Language Models (MLLMs). Our approach not only fortifies models against unethical manipulations and privacy breaches but also maintains their high performance across tasks. We demonstrate state-of-the-art performance under various attack prompts, without compromising the model's core functionalities. Furthermore, the introduction of differentiated security levels empowers users to control their personal data disclosure. Our methods contribute to reducing social risks and conflicts arising from technological abuse, enhance data protection, and promote social equity. Collectively, this research provides a framework for balancing the efficiency of question-answering systems with user privacy and ethical standards, ensuring a safer user experience and fostering trust in AI technology.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 的最新进展显着增强了自然语言处理和人工智能的能力。这些模型（包括 GPT-3.5 和 LLaMA-2）由于变革性的 Transformer 模型而彻底改变了文本生成、翻译和问答任务。尽管法学硕士被广泛使用，但它也带来了一些挑战，例如模型被迫做出不当反应时的道德困境、容易受到网络钓鱼攻击和侵犯隐私等。本文通过引入多管齐下的方法来解决这些挑战，其中包括：1）从用户输入中过滤敏感词汇，以防止不道德的响应； 2) 检测角色扮演以停止可能导致“越狱”场景的互动； 3）实施自定义规则引擎来限制禁止内容的生成； 4) 将这些方法扩展到各种 LLM 衍生产品，例如多模型大型语言模型 (MLLM)。我们的方法不仅增强了模型免受不道德操纵和隐私侵犯的能力，而且还保持了模型在任务中的高性能。我们在各种攻击提示下展示了最先进的性能，而不会影响模型的核心功能。此外，差异化安全级别的引入使用户能够控制其个人数据的泄露。我们的方法有助于减少因技术滥用而产生的社会风险和冲突，加强数据保护，促进社会公平。总的来说，这项研究提供了一个平衡问答系统效率与用户隐私和道德标准的框架，确保更安全的用户体验并培养对人工智能技术的信任。</li>
</ul>

<h3>Title: Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of  Text Messages</h3>
<ul>
<li><strong>Authors: </strong>N'yoma Diamond</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01726">https://arxiv.org/abs/2402.01726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01726">https://arxiv.org/pdf/2402.01726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01726]] Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of  Text Messages(https://arxiv.org/abs/2402.01726)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone, clarity, and ability to convey intent, we find that there is no statistically significant evidence that the belief that AI is utilized alters recipient perceptions. This provides hopeful evidence that LLM-based text message composition assistance can be implemented without the risk of counter-productive outcomes.</li>
<li><strong>摘要：</strong>对于许多人来说，焦虑、抑郁和其他社会和心理因素可能使撰写短信成为一项积极的挑战。为了解决这个问题，大型语言模型（LLM）可能被证明是帮助用户的完美工具，否则他们会发现发短信很困难或有压力。然而，尽管法学硕士的使用迅速普及，但尚未探讨其在短信撰写中的辅助使用的考虑因素。关于法学硕士使用的一个主要担忧是，公众对人工智能的不良情绪可能会损害人们对人工智能辅助短信的看法，从而产生适得其反的效果。为了（或）验证这种可能性，我们探讨了文本消息在写作中是否接受人工智能协助的信念如何改变其感知的语气、清晰度和传达意图的能力。在这项研究中，我们调查了 26 名参与者对 18 条随机标记的预先编写的短信的看法。在分析参与者对消息语气、清晰度和传达意图的能力的评分时，我们发现没有统计上显着的证据表明使用人工智能的信念会改变接收者的看法。这提供了充满希望的证据，证明基于法学硕士的短信撰写协助可以在不存在适得其反的风险的情况下实施。</li>
</ul>

<h3>Title: Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain  Specific Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Weimin Fu, Shijie Li, Yifang Zhao, Haocheng Ma, Raj Dutta, Xuan Zhang, Kaichen Yang, Yier Jin, Xiaolong Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01728">https://arxiv.org/abs/2402.01728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01728">https://arxiv.org/pdf/2402.01728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01728]] Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain  Specific Knowledge(https://arxiv.org/abs/2402.01728)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving semiconductor industry, where research, design, verification, and manufacturing are intricately linked, the potential of Large Language Models to revolutionize hardware design and security verification is immense. The primary challenge, however, lies in the complexity of hardware specific issues that are not adequately addressed by the natural language or software code knowledge typically acquired during the pretraining stage. Additionally, the scarcity of datasets specific to the hardware domain poses a significant hurdle in developing a foundational model. Addressing these challenges, this paper introduces Hardware Phi 1.5B, an innovative large language model specifically tailored for the hardware domain of the semiconductor industry. We have developed a specialized, tiered dataset comprising small, medium, and large subsets and focused our efforts on pretraining using the medium dataset. This approach harnesses the compact yet efficient architecture of the Phi 1.5B model. The creation of this first pretrained, hardware domain specific large language model marks a significant advancement, offering improved performance in hardware design and verification tasks and illustrating a promising path forward for AI applications in the semiconductor sector.</li>
<li><strong>摘要：</strong>在快速发展的半导体行业中，研究、设计、验证和制造错综复杂地联系在一起，大型语言模型彻底改变硬件设计和安全验证的潜力是巨大的。然而，主要的挑战在于硬件特定问题的复杂性，通常在预训练阶段获得的自然语言或软件代码知识无法充分解决这些问题。此外，特定于硬件领域的数据集的稀缺给开发基础模型带来了重大障碍。为了解决这些挑战，本文介绍了 Hardware Phi 1.5B，这是一种专为半导体行业硬件领域量身定制的创新型大型语言模型。我们开发了一个专门的分层数据集，包括小型、中型和大型子集，并将我们的工作重点放在使用中型数据集进行预训练上。该方法利用了 Phi 1.5B 模型紧凑而高效的架构。第一个预训练的、特定于硬件领域的大语言模型的创建标志着一项重大进步，它提高了硬件设计和验证任务的性能，并为半导体领域的人工智能应用展示了一条充满希望的前进道路。</li>
</ul>

<h3>Title: Contextualization Distillation from Large Language Model for Knowledge  Graph Completion</h3>
<ul>
<li><strong>Authors: </strong>Dawei Li, Zhen Tan, Tianlong Chen, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01729">https://arxiv.org/abs/2402.01729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01729">https://arxiv.org/pdf/2402.01729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01729]] Contextualization Distillation from Large Language Model for Knowledge  Graph Completion(https://arxiv.org/abs/2402.01729)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into generating path selection, as well as the choosing of suitable distillation tasks. All the code and data in this work will be released at https://github.com/David-Li0406/Contextulization-Distillation</li>
<li><strong>摘要：</strong>虽然文本信息显着增强了知识图补全 (KGC) 中预训练语言模型 (PLM) 的性能，但从维基百科文章或同义词集定义收集的现有语料库的静态和噪声性质通常限制了基于 PLM 的 KGC 模型的潜力。为了克服这些挑战，我们引入了上下文化蒸馏策略，这是一种与判别式和生成式 KGC 框架兼容的多功能即插即用方法。我们的方法首先指示大型语言模型（LLM）将紧凑的结构三元组转换为上下文丰富的片段。随后，我们引入了两个定制的辅助任务：重建和情境化，允许较小的 KGC 模型吸收这些丰富的三元组的见解。跨不同数据集和 KGC 技术的综合评估突出了我们方法的有效性和适应性，无论底层管道或架构如何，都揭示了一致的性能增强。此外，我们的分析使我们的方法更具可解释性，并提供了生成路径选择以及选择合适的蒸馏任务的见解。本工作中的所有代码和数据都将发布在https://github.com/David-Li0406/Contextulization-Distillation</li>
</ul>

<h3>Title: Evaluating LLM - Generated Multimodal Diagnosis from Medical Images and  Symptom Analysis</h3>
<ul>
<li><strong>Authors: </strong>Dimitrios P. Panagoulias, Maria Virvou, George A. Tsihrintzis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01730">https://arxiv.org/abs/2402.01730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01730">https://arxiv.org/pdf/2402.01730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01730]] Evaluating LLM - Generated Multimodal Diagnosis from Medical Images and  Symptom Analysis(https://arxiv.org/abs/2402.01730)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) constitute a breakthrough state-of-the-art Artificial Intelligence technology which is rapidly evolving and promises to aid in medical diagnosis. However, the correctness and the accuracy of their returns has not yet been properly evaluated. In this work, we propose an LLM evaluation paradigm that incorporates two independent steps of a novel methodology, namely (1) multimodal LLM evaluation via structured interactions and (2) follow-up, domain-specific analysis based on data extracted via the previous interactions. Using this paradigm, (1) we evaluate the correctness and accuracy of LLM-generated medical diagnosis with publicly available multimodal multiple-choice questions(MCQs) in the domain of Pathology and (2) proceed to a systemic and comprehensive analysis of extracted results. We used GPT-4-Vision-Preview as the LLM to respond to complex, medical questions consisting of both images and text, and we explored a wide range of diseases, conditions, chemical compounds, and related entity types that are included in the vast knowledge domain of Pathology. GPT-4-Vision-Preview performed quite well, scoring approximately 84\% of correct diagnoses. Next, we further analyzed the findings of our work, following an analytical approach which included Image Metadata Analysis, Named Entity Recognition and Knowledge Graphs. Weaknesses of GPT-4-Vision-Preview were revealed on specific knowledge paths, leading to a further understanding of its shortcomings in specific areas. Our methodology and findings are not limited to the use of GPT-4-Vision-Preview, but a similar approach can be followed to evaluate the usefulness and accuracy of other LLMs and, thus, improve their use with further optimization.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 构成了一种突破性的最先进的人工智能技术，该技术正在迅速发展，有望帮助医学诊断。然而，其回报的正确性和准确性尚未得到适当评估。在这项工作中，我们提出了一种 LLM 评估范式，它结合了一种新颖方法的两个独立步骤，即（1）通过结构化交互进行多模式 LLM 评估和（2）基于通过先前交互提取的数据进行后续、特定领域的分析。使用这种范式，（1）我们使用病理学领域公开的多模态多项选择题（MCQ）评估法学硕士生成的医学诊断的正确性和准确性，以及（2）对提取的结果进行系统和全面的分析。我们使用 GPT-4-Vision-Preview 作为法学硕士来回答由图像和文本组成的复杂医学问题，并且我们探索了广泛的疾病、状况、化合物和相关实体类型，这些实体类型包含在广泛的数据中。病理学知识领域。 GPT-4-Vision-Preview 表现相当不错，正确诊断率约为 84%。接下来，我们采用图像元数据分析、命名实体识别和知识图谱等分析方法，进一步分析了我们的工作结果。 GPT-4-Vision-Preview的弱点在具体的知识路径上被揭示，从而进一步了解其在特定领域的缺点。我们的方法和研究结果不仅限于 GPT-4-Vision-Preview 的使用，而是可以遵循类似的方法来评估其他 LLM 的有用性和准确性，从而通过进一步优化来改进其使用。</li>
</ul>

<h3>Title: Development and Testing of Retrieval Augmented Generation in Large  Language Models -- A Case Study Report</h3>
<ul>
<li><strong>Authors: </strong>YuHe Ke, Liyuan Jin, Kabilan Elangovan, Hairil Rizal Abdullah, Nan Liu, Alex Tiong Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat Ling Ong, Daniel Shu Wei Ting</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01733">https://arxiv.org/abs/2402.01733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01733">https://arxiv.org/pdf/2402.01733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01733]] Development and Testing of Retrieval Augmented Generation in Large  Language Models -- A Case Study Report(https://arxiv.org/abs/2402.01733)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine. Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison. Results: The LLM-RAG model generated answers within an average of 15-20 seconds, significantly faster than the 10 minutes typically required by humans. Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This accuracy was further increased to 91.4% when the model was enhanced with RAG. Compared to the human-generated instructions, which had an accuracy of 86.3%, the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610). Conclusions: In this case study, we demonstrated a LLM-RAG model for healthcare implementation. The pipeline shows the advantages of grounded knowledge, upgradability, and scalability as important aspects of healthcare LLM deployment.</li>
<li><strong>摘要：</strong>目的：大型语言模型 (LLM) 为医疗应用带来了巨大的希望。检索增强生成（RAG）作为法学硕士定制领域知识的一种有前途的方法而出现。本案例研究介绍了专为医疗保健量身定制的 LLM-RAG 管道的开发和评估，特别关注术前医学。方法：我们使用 35 条术前指南开发了 LLM-RAG 模型，并针对人类生成的反应对其进行了测试，总共评估了 1260 个反应。 RAG 过程涉及使用 LangChain 和 Llamaindex 等基于 Python 的框架将临床文档转换为文本，并将这些文本处理成块以进行嵌入和检索。向量存储技术和选择的嵌入模型来优化数据检索，使用 Pinecone 进行向量存储，维数为 1536，损失指标使用余弦相似度。由初级医生提供的人类生成的答案被用作比较。结果：LLM-RAG 模型平均在 15-20 秒内生成答案，明显快于人类通常需要的 10 分钟。在基础LLM中，GPT4.0的准确率最高，达到80.1%。当使用 RAG 增强模型时，这一准确率进一步提高到 91.4%。与准确率为 86.3% 的人类生成指令相比，GPT4.0 RAG 模型的性能表现出非劣效性 (p=0.610)。结论：在本案例研究中，我们展示了用于医疗保健实施的 LLM-RAG 模型。该管道展示了基础知识、可升级性和可扩展性的优势，作为医疗保健法学硕士部署的重要方面。</li>
</ul>

<h3>Title: VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large  Models</h3>
<ul>
<li><strong>Authors: </strong>Yi Zhao, Yilin Zhang, Rong Xiang, Jing Li, Hillming Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01735">https://arxiv.org/abs/2402.01735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01735">https://arxiv.org/pdf/2402.01735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01735]] VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large  Models(https://arxiv.org/abs/2402.01735)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Visually Impaired Assistance (VIA) aims to automatically help visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (\textbf{VIALM}). In this task, given an \textit{image} illustrating the physical environments and a \textit{linguistic request} from a VI user, VIALM aims to output step-by-step \textit{guidance} to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities in VIA. The results indicate that while LMs can augment VIA, their output cannot be well \textit{grounded} in reality (i.e., 25.7\% GPT-4's responses) and lacks \textit{fine-grained} guidance (i.e., 32.1\% GPT-4's responses).</li>
<li><strong>摘要：</strong>视障援助 (VIA) 旨在自动帮助视障 (VI) 处理日常活动。 VIA 的进步主要取决于计算机视觉 (CV) 和自然语言处理 (NLP) 的发展，这两者都展示了大型模型 (LM) 的前沿范例。此外，语言模型还表现出了卓越的多模式能力，可以解决具有挑战性的物理任务，例如实体机器人。为了研究最先进 (SOTA) 语言模型在 VIA 应用中的潜力和局限性，我们针对使用语言模型 (\textbf{VIALM}) 的 VIA 任务进行了广泛的研究。在此任务中，给定一个说明物理环境的 \textit{image} 和来自 VI 用户的 \textit{linguistic request}，VIALM 旨在输出逐步的 \textit{guidance} 以帮助 VI 用户完成任务要求植根于环境。该研究包括回顾近期 LM 研究的调查和检查所选 LM 在 VIA 中的能力的基准实验。结果表明，虽然 LM 可以增强 VIA，但它们的输出在现实中不能很好地 \textit{grounded} （即 25.7\% GPT-4 的响应）并且缺乏 \textit{fine-grained} 指导（即 32.1\% GPT -4的回应）。</li>
</ul>

<h3>Title: SADAS: A Dialogue Assistant System Towards Remediating Norm Violations  in Bilingual Socio-Cultural Conversations</h3>
<ul>
<li><strong>Authors: </strong>Yuncheng Hua, Zhuang Li, Linhao Luo, Kadek Ananta Satriadi, Tao Feng, Haolan Zhan, Lizhen Qu, Suraj Sharma, Ingrid Zukerman, Zhaleh Semnani-Azad, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01736">https://arxiv.org/abs/2402.01736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01736">https://arxiv.org/pdf/2402.01736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01736]] SADAS: A Dialogue Assistant System Towards Remediating Norm Violations  in Bilingual Socio-Cultural Conversations(https://arxiv.org/abs/2402.01736)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>In today's globalized world, bridging the cultural divide is more critical than ever for forging meaningful connections. The Socially-Aware Dialogue Assistant System (SADAS) is our answer to this global challenge, and it's designed to ensure that conversations between individuals from diverse cultural backgrounds unfold with respect and understanding. Our system's novel architecture includes: (1) identifying the categories of norms present in the dialogue, (2) detecting potential norm violations, (3) evaluating the severity of these violations, (4) implementing targeted remedies to rectify the breaches, and (5) articulates the rationale behind these corrective actions. We employ a series of State-Of-The-Art (SOTA) techniques to build different modules, and conduct numerous experiments to select the most suitable backbone model for each of the modules. We also design a human preference experiment to validate the overall performance of the system. We will open-source our system (including source code, tools and applications), hoping to advance future research. A demo video of our system can be found at:~\url{https://youtu.be/JqetWkfsejk}. We have released our code and software at:~\url{https://github.com/AnonymousEACLDemo/SADAS}.</li>
<li><strong>摘要：</strong>在当今全球化的世界中，弥合文化鸿沟对于建立有意义的联系比以往任何时候都更加重要。社交意识对话辅助系统 (SADAS) 是我们应对这一全球挑战的答案，它旨在确保来自不同文化背景的个人之间的对话以尊重和理解的方式展开。我们系统的新颖架构包括：（1）识别对话中存在的规范类别，（2）检测潜在的规范违规行为，（3）评估这些违规行为的严重性，（4）实施有针对性的补救措施来纠正违规行为，以及（ 5) 阐明这些纠正措施背后的理由。我们采用一系列最先进（SOTA）技术来构建不同的模块，并进行大量实验来为每个模块选择最合适的骨干模型。我们还设计了人类偏好实验来验证系统的整体性能。我们将开源我们的系统（包括源代码、工具和应用程序），希望能够推进未来的研究。我们系统的演示视频可以在以下位置找到：~\url{https://youtu.be/JqetWkfsejk}。我们已经在以下位置发布了我们的代码和软件：~\url{https://github.com/AnonymousEACLDemo/SADAS}。</li>
</ul>

<h3>Title: Assistive Large Language Model Agents for Socially-Aware Negotiation  Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Yuncheng Hua, Lizhen Qu, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01737">https://arxiv.org/abs/2402.01737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01737">https://arxiv.org/pdf/2402.01737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01737]] Assistive Large Language Model Agents for Socially-Aware Negotiation  Dialogues(https://arxiv.org/abs/2402.01737)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, agent</a></li>
<li><strong>Abstract: </strong>In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.</li>
<li><strong>摘要：</strong>在这项工作中，我们的目标是开发法学硕士代理，以减少多代理环境中谈判中违反社会规范的情况。我们通过让两个大型语言模型 (LLM) 在每次对话中扮演两个谈判者的角色来模拟现实世界的谈判。第三位法学硕士充当补救代理，重写违反规范的言论，以改善谈判结果。由于这是一项新颖的任务，因此没有可用的手动构建的数据。为了解决这一限制，我们引入了一种基于价值影响的情境学习（ICL）方法，为基于 LLM 的补救代理识别高质量的 ICL 示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与政策学习的联系，并提供了丰富的经验证据来证明其在三个不同主题的谈判中的有效性：产品销售、房价和薪资谈判。源代码和生成的数据集将在接受后公开。</li>
</ul>

<h3>Title: C4Q: A Chatbot for Quantum</h3>
<ul>
<li><strong>Authors: </strong>Yaiza Aragonés-Soria, Manuel Oriol</a></li>
<li><strong>Subjects: </strong>cs.CL, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01738">https://arxiv.org/abs/2402.01738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01738">https://arxiv.org/pdf/2402.01738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01738]] C4Q: A Chatbot for Quantum(https://arxiv.org/abs/2402.01738)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, code, chat, agent</a></li>
<li><strong>Abstract: </strong>Quantum computing is a growing field that promises many real-world applications such as quantum cryptography or quantum finance. The number of people able to use quantum computing is however still very small. This limitation comes from the difficulty to understand the concepts and to know how to start coding. Therefore, there is a need for tools that can assist non-expert in overcoming this complexity. One possibility would be to use existing conversational agents. Unfortunately ChatGPT and other Large-Language Models produce inaccurate results. This article presents C4Q, a chatbot that answers accurately basic questions and guides users when trying to code quantum programs. Contrary to other approaches C4Q uses a pre-trained large language model only to discover and classify user requests. It then generates an accurate answer using an own engine. Thanks to this architectural design, C4Q's answers are always correct, and thus C4Q can become a support tool that makes quantum computing more available to non-experts.</li>
<li><strong>摘要：</strong>量子计算是一个不断发展的领域，有望实现许多现实世界的应用，例如量子密码学或量子金融。然而，能够使用量子计算的人数仍然很少。这种限制来自于理解概念和知道如何开始编码的困难。因此，需要能够帮助非专家克服这种复杂性的工具。一种可能性是使用现有的对话代理。不幸的是，ChatGPT 和其他大语言模型产生的结果不准确。本文介绍了 C4Q，这是一个聊天机器人，可以准确回答基本问题并在用户尝试编写量子程序时指导用户。与其他方法相反，C4Q 使用预先训练的大型语言模型仅用于发现和分类用户请求。然后它使用自己的引擎生成准确的答案。得益于这种架构设计，C4Q 的答案始终是正确的，因此 C4Q 可以成为一种支持工具，使量子计算更适合非专家使用。</li>
</ul>

<h3>Title: OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01739">https://arxiv.org/abs/2402.01739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01739">https://arxiv.org/pdf/2402.01739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01739]] OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models(https://arxiv.org/abs/2402.01739)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development. One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.</li>
<li><strong>摘要：</strong>为了帮助开源社区更好地理解基于专家混合 (MoE) 的大型语言模型 (LLM)，我们训练并发布了 OpenMoE，这是一系列完全开源且可复制的仅限解码器的 MoE LLM，范围包括从 650M 到 34B 参数，并在多达超过 1T 的令牌上进行训练。我们的调查证实，基于教育部的法学硕士可以提供比密集型法学硕士更有利的成本效益权衡，凸显了未来法学硕士发展的潜在有效性。这项研究的另一个重要贡献是对 OpenMoE 模型中的路由机制进行了深入分析，得出了三个重要发现：上下文无关的专业化、早期路由学习和 Drop-towards-the-End。我们发现 MoE 模型中的路由决策主要基于令牌 ID，上下文相关性极小。令牌到专家的分配是在预训练阶段的早期确定的，并且基本上保持不变。这种不完美的路由可能会导致性能下降，特别是在多轮对话等顺序任务中，其中序列中较晚出现的令牌更有可能被丢弃。最后，我们根据上述观察和分析重新思考我们的设计。为了促进未来教育部法学硕士的发展，我们提出了缓解我们发现的问题并进一步改进现成的教育部法学硕士设计的潜在策略。</li>
</ul>

<h3>Title: Compensatory Biases Under Cognitive Load: Reducing Selection Bias in  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>J. E. Eicher, R. F. Irgolič</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01740">https://arxiv.org/abs/2402.01740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01740">https://arxiv.org/pdf/2402.01740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01740]] Compensatory Biases Under Cognitive Load: Reducing Selection Bias in  Large Language Models(https://arxiv.org/abs/2402.01740)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have become instrumental in interpreting and executing semantic-based tasks. Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance. Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we conducted a series of controlled experiments, manipulating temperature, list length, object identity, object type, prompt complexity, and model. This enabled us to isolate and measure the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproprotionately represented in outputs. Furthermore the usage of guard rails, a prompt engineering method of ensuring a response structure, can increase bias and decrease instruction adherence when combined with a selection task. The bias is ablated when the guard rail step is separated from the list sampling step, lowering the complexity of each individual task. The implications of this research are two-fold, practically providing a guide for designing unbiased LLM applications and theoretically suggesting that LLMs experience a form of cognitive load compensated for by increasing bias.</li>
<li><strong>摘要：</strong>像 gpt-3.5-turbo 和 claude-instant-1.2 这样的大型语言模型 (LLM) 已经成为解释和执行基于语义的任务的工具。不幸的是，这些模型的固有偏差（类似于人类认知偏差）会对它们的性能产生不利影响。特别受影响的是从列表中选择对象；数字导航和决策的基本操作。这项研究批判性地研究了这些偏差，并量化了对代表性列表选择任务的影响。为了探索这些偏差，我们进行了一系列受控实验，操纵温度、列表长度、对象标识、对象类型、提示复杂性和模型。这使我们能够隔离和衡量偏差对选择行为的影响。我们的研究结果表明，偏差结构强烈依赖于模型，对象类型调节效果的大小。具有很强的首因效应，导致列表中的第一个对象在输出中不成比例地表示。此外，使用护栏（一种确保响应结构的快速工程方法）在与选择任务结合时可能会增加偏差并降低指令依从性。当护栏步骤与列表采样步骤分离时，偏差就会消除，从而降低每个单独任务的复杂性。这项研究的意义是双重的，实际上为设计无偏见的法学硕士申请提供了指导，并从理论上表明法学硕士经历了一种通过增加偏见来补偿的认知负荷。</li>
</ul>

<h3>Title: Development and Testing of a Novel Large Language Model-Based Clinical  Decision Support Systems for Medication Safety in 12 Clinical Specialties</h3>
<ul>
<li><strong>Authors: </strong>Jasmine Chiat Ling Ong, Liyuan Jin, Kabilan Elangovan, Gilbert Yong San Lim, Daniel Yan Zheng Lim, Gerald Gui Ren Sng, Yuhe Ke, Joshua Yi Min Tung, Ryan Jian Zhong, Christopher Ming Yao Koh, Keane Zhi Hao Lee, Xiang Chen, Jack Kian Chng, Aung Than, Ken Junyang Goh, Daniel Shu Wei Ting</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01741">https://arxiv.org/abs/2402.01741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01741">https://arxiv.org/pdf/2402.01741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01741]] Development and Testing of a Novel Large Language Model-Based Clinical  Decision Support Systems for Medication Safety in 12 Clinical Specialties(https://arxiv.org/abs/2402.01741)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe medication prescription. This model addresses the limitations of traditional rule-based CDSS by providing relevant prescribing error alerts tailored to patient context and institutional guidelines. Objective: The study evaluates the efficacy of an LLM-based CDSS in identifying medication errors across various medical and surgical case vignettes, compared to a human expert panel. It also examines clinician preferences among different CDSS integration modalities: junior pharmacist, LLM-based CDSS alone, and a combination of both. Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the study involved 61 prescribing error scenarios within 23 clinical vignettes across 12 specialties. An expert panel assessed these cases using the PCNE classification and NCC MERP index. Three junior pharmacists independently reviewed each vignette under simulated conditions. Main Outcomes and Measures: The study assesses the LLM-based CDSS's accuracy, precision, recall, and F1 scores in identifying Drug-Related Problems (DRPs), compared to junior pharmacists alone or in an assistive mode with the CDSS. Results: The co-pilot mode of RAG-LLM significantly improved DRP identification accuracy by 22% over solo pharmacists. It showed higher recall and F1 scores, indicating better detection of severe DRPs, despite a slight decrease in precision. Accuracy varied across categories when pharmacists had access to RAG-LLM responses. Conclusions: The RAG-LLM based CDSS enhances medication error identification accuracy when used with junior pharmacists, especially in detecting severe DRPs.</li>
<li><strong>摘要：</strong>重要性：我们引入了一种新颖的检索增强生成（RAG）-大语言模型（LLM）作为安全用药处方的临床决策支持系统（CDSS）。该模型通过提供根据患者情况和机构指南量身定制的相关处方错误警报，解决了传统基于规则的 CDSS 的局限性。目的：该研究评估了基于法学硕士的 CDSS 与人类专家小组相比，在识别各种医疗和外科病例片段中的用药错误方面的功效。它还检查了临床医生对不同 CDSS 整合模式的偏好：初级药剂师、仅基于法学硕士的 CDSS 以及两者的组合。设计、设置和参与者：该研究利用 GPT-4.0 的 RAG 模型，涉及 12 个专业的 23 个临床案例中的 61 个处方错误场景。专家小组使用 PCNE 分类和 NCC MERP 指数评估了这些病例。三名初级药剂师在模拟条件下独立审查每个小插图。主要成果和措施：该研究评估了基于法学硕士的 CDSS 在识别药物相关问题 (DRP) 方面的准确性、精确度、召回率和 F1 分数，与初级药剂师单独或在 CDSS 的辅助模式下进行比较。结果：RAG-LLM的副驾驶模式比单独药剂师显着提高了DRP识别准确率22%。它显示出更高的召回率和 F1 分数，表明尽管精确度略有下降，但可以更好地检测严重的 DRP。当药剂师获得 RAG-LLM 答复时，不同类别的准确性有所不同。结论：当与初级药剂师一起使用时，基于 RAG-LLM 的 CDSS 提高了用药错误识别的准确性，特别是在检测严重的 DRP 时。</li>
</ul>

<h3>Title: Towards Optimizing the Costs of LLM Usage</h3>
<ul>
<li><strong>Authors: </strong>Shivanshu Shekhar, Tanishq Dubey, Koyel Mukherjee, Apoorv Saxena, Atharv Tyagi, Nishanth Kotla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01742">https://arxiv.org/abs/2402.01742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01742">https://arxiv.org/pdf/2402.01742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01742]] Towards Optimizing the Costs of LLM Usage(https://arxiv.org/abs/2402.01742)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, lora</a></li>
<li><strong>Abstract: </strong>Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases. In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sentence simplification model for reducing the number of tokens in a controlled manner. Additionally, we propose several deterministic heuristics for reducing tokens in a quality aware manner, and study the related optimization problem of applying the heuristics optimizing the quality and cost trade-off. We perform extensive empirical validation of our methods on not only enterprise datasets but also on open-source datasets, annotated by us, and show that we perform much better compared to closest baselines. Our methods reduce costs by 40%- 90% while improving quality by 4%-7%. We will release the annotated open source datasets to the community for further research and exploration.</li>
<li><strong>摘要：</strong>如今，生成式人工智能和法学硕士尤其广泛用于各种文档处理任务，例如问答和摘要。然而，不同的法学硕士对于不同的任务具有不同的能力，并且具有不同的成本、标记化和延迟。事实上，企业已经为各自的用例运营或使用法学硕士付出了巨大的成本。在这项工作中，我们建议通过估计 LLM 的输出质量（无需实际调用 LLM）来优化 LLM 的使用成本，然后解决 LLM 选择的优化例程，以将成本控制在预算范围内，或将成本最小化。质量和延迟感知方式。我们提出了一个模型来预测法学硕士在摘要等文档处理任务上的输出质量，然后采用 LP 舍入算法来优化法学硕士的选择。我们从理论上和实证上研究权衡质量和成本的优化问题。我们进一步提出了一种句子简化模型，用于以受控方式减少标记数量。此外，我们提出了几种确定性启发式方法，用于以质量感知的方式减少令牌，并研究应用启发式优化质量和成本权衡的相关优化问题。我们不仅在企业数据集上，而且在我们注释的开源数据集上对我们的方法进行了广泛的实证验证，并表明与最接近的基线相比，我们的表现要好得多。我们的方法可降低成本 40%- 90%，同时提高质量 4%-7%。我们将向社区发布带注释的开源数据集，以供进一步的研究和探索。</li>
</ul>

<h3>Title: PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxuan Li, Minxi Yang, Dahua Gao, Wenlong Xu, Guangming Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01750">https://arxiv.org/abs/2402.01750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01750">https://arxiv.org/pdf/2402.01750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01750]] PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using  Large Language Models(https://arxiv.org/abs/2402.01750)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag, agent</a></li>
<li><strong>Abstract: </strong>Current communication technologies face limitations in terms of theoretical capacity, spectrum availability, and power resources. Pragmatic communication, leveraging terminal intelligence for selective data transmission, offers resource conservation. Existing research lacks universal intention resolution tools, limiting applicability to specific tasks. This paper proposes an image pragmatic communication framework based on a Pragmatic Agent for Communication Efficiency (PACE) using Large Language Models (LLM). In this framework, PACE sequentially performs semantic perception, intention resolution, and intention-oriented coding. To ensure the effective utilization of LLM in communication, a knowledge base is designed to supplement the necessary knowledge, dedicated prompts are introduced to facilitate understanding of pragmatic communication scenarios and task requirements, and a chain of thought is designed to assist in making reasonable trade-offs between transmission efficiency and cost. For experimental validation, this paper constructs an image pragmatic communication dataset along with corresponding evaluation standards. Simulation results indicate that the proposed method outperforms traditional and non-LLM-based pragmatic communication in terms of transmission efficiency.</li>
<li><strong>摘要：</strong>当前的通信技术在理论容量、频谱可用性和电力资源方面面临限制。务实的通信，利用终端智能进行选择性数据传输，可以节省资源。现有研究缺乏通用的意图解析工具，限制了对特定任务的适用性。本文提出了一种基于使用大型语言模型（LLM）的语用代理通信效率（PACE）的图像语用通信框架。在此框架中，PACE依次执行语义感知、意图解析和面向意图的编码。为了保证LLM在沟通中的有效运用，设计了知识库来补充必要的知识，引入了专门的提示以方便理解务实的沟通场景和任务要求，并设计了一条思路来帮助进行合理的交易。传输效率和成本之间的差异。为了进行实验验证，本文构建了图像语用交际数据集以及相应的评估标准。仿真结果表明，该方法在传输效率方面优于传统和非基于LLM的语用通信。</li>
</ul>

<h3>Title: Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's  Dementia</h3>
<ul>
<li><strong>Authors: </strong>Balamurali B T, Jer-Ming Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01751">https://arxiv.org/abs/2402.01751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01751">https://arxiv.org/pdf/2402.01751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01751]] Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's  Dementia(https://arxiv.org/abs/2402.01751)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) find increasing applications in many fields. Here, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in their current form, as publicly available - for their ability to recognize Alzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual input derived from spontaneous speech recordings. Zero-shot learning approach is used at two levels of independent queries, with the second query (chain-of-thought prompting) eliciting more detailed than the first. Each LLM chatbot's performance is evaluated on the prediction generated in terms of accuracy, sensitivity, specificity, precision and F1 score. LLM chatbots generated three-class outcome ("AD", "CN", or "Unsure"). When positively identifying AD, Bard produced highest true-positives (89% recall) and highest F1 score (71%), but tended to misidentify CN as AD, with high confidence (low "Unsure" rates); for positively identifying CN, GPT-4 resulted in the highest true-negatives at 56% and highest F1 score (62%), adopting a diplomatic stance (moderate "Unsure" rates). Overall, three LLM chatbots identify AD vs CN surpassing chance-levels but do not currently satisfy clinical application.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在许多领域都有越来越多的应用。在这里，对三个 LLM 聊天机器人（ChatGPT-3.5、ChatGPT-4 和 Bard）进行了评估（以目前公开的形式），评估它们使用源自以下文本输入的文本输入来识别阿尔茨海默氏痴呆症 (AD) 和认知正常 (CN) 个体的能力：自发语音录音。零样本学习方法用于两个级别的独立查询，第二个查询（思维链提示）比第一个查询更详细。每个 LLM 聊天机器人的性能都是根据准确度、灵敏度、特异性、精确度和 F1 分数生成的预测进行评估的。 LLM 聊天机器人生成三级结果（“AD”、“CN”或“不确定”）。在积极识别 AD 时，Bard 产生了最高的真阳性（89% 召回率）和最高的 F1 分数（71%），但倾向于将 CN 误认为 AD，置信度较高（“不确定”率较低）；对于积极识别 CN，GPT-4 的真阴性率最高，为 56%，F1 得分最高 (62%)，采取外交立场（中等“不确定”率）。总体而言，三个 LLM 聊天机器人对 AD 与 CN 的识别超过了机会水平，但目前尚不能满足临床应用。</li>
</ul>

<h3>Title: Rethinking Interpretability in the Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, Jianfeng Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01761">https://arxiv.org/abs/2402.01761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01761">https://arxiv.org/pdf/2402.01761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01761]] Rethinking Interpretability in the Era of Large Language Models(https://arxiv.org/abs/2402.01761)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.</li>
<li><strong>摘要：</strong>在过去十年中，由于日益庞大的数据集和深度神经网络的兴起，可解释的机器学习已成为一个令人感兴趣的领域。同时，大型语言模型（LLM）在广泛的任务中表现出了卓越的能力，为重新思考可解释机器学习的机会提供了机会。值得注意的是，用自然语言解释的能力使法学硕士能够扩大可以提供给人类的模式的规模和复杂性。然而，这些新功能带来了新的挑战，例如幻觉的解释和巨大的计算成本。在这篇立场文件中，我们首先回顾评估法学硕士解释这一新兴领域的现有方法（包括解释法学硕士和使用法学硕士进行解释）。我们认为，尽管有其局限性，法学硕士仍有机会在许多应用程序中以更雄心勃勃的范围重新定义可解释性，包括审计法学硕士本身。我们强调了法学硕士解释的两个新兴研究重点：使用法学硕士直接分析新数据集并生成交互式解释。</li>
</ul>

<h3>Title: LLMs Simulate Big Five Personality Traits: Further Evidence</h3>
<ul>
<li><strong>Authors: </strong>Aleksandra Sorokovikova, Natalia Fedorova, Sharwin Rezagholi, Ivan P. Yamshchikov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01765">https://arxiv.org/abs/2402.01765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01765">https://arxiv.org/pdf/2402.01765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01765]] LLMs Simulate Big Five Personality Traits: Further Evidence(https://arxiv.org/abs/2402.01765)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>An empirical investigation into the simulation of the Big Five personality traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is presented. We analyze the personality traits simulated by these models and their stability. This contributes to the broader understanding of the capabilities of LLMs to simulate personality traits and the respective implications for personalized human-computer interaction.</li>
<li><strong>摘要：</strong>提出了通过大型语言模型 (LLM)（即 Llama2、GPT4 和 Mixtral）模拟大五人格特征的实证研究。我们分析这些模型模拟的人格特质及其稳定性。这有助于更广泛地了解法学硕士模拟人格特质的能力以及对个性化人机交互的各自影响。</li>
</ul>

<h3>Title: LLM Voting: Human Choices and AI Collective Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Joshua C. Yang, Marcin Korecki, Damian Dailisan, Carina I. Hausladen, Dirk Helbing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01766">https://arxiv.org/abs/2402.01766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01766">https://arxiv.org/pdf/2402.01766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01766]] LLM Voting: Human Choices and AI Collective Decision Making(https://arxiv.org/abs/2402.01766)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.</li>
<li><strong>摘要：</strong>本文研究了大型语言模型 (LLM) 的投票行为，特别是 OpenAI 的 GPT4 和 LLaMA2，以及它们与人类投票模式的一致性。我们的方法包括人类投票实验，以建立人类偏好的基线，以及与法学硕士代理人的平行实验。该研究关注集体结果和个人偏好，揭示了人类和法学硕士之间决策的差异和固有偏见。我们观察到法学硕士的偏好多样性和一致性之间存在权衡，与人类选民的多样化偏好相比，倾向于更统一的选择。这一发现表明，法学硕士在用于投票援助时可能会导致更加同质化的集体结果，这强调了将法学硕士谨慎纳入民主进程的必要性。</li>
</ul>

<h3>Title: HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents  QA</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Chen, Pengyu Gao, Jiangjiang Song, Xiaoyang Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01767">https://arxiv.org/abs/2402.01767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01767">https://arxiv.org/pdf/2402.01767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01767]] HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents  QA(https://arxiv.org/abs/2402.01767)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination, retrieval-augmented generation, rag, agent</a></li>
<li><strong>Abstract: </strong>As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.</li>
<li><strong>摘要：</strong>随着利用外部工具的语言模型代理的快速发展，利用补充文档和检索增强生成（RAG）方法的问答（QA）方法已经取得了重大进展。这一进步提高了语言模型的响应质量并减轻了幻觉的出现。然而，这些方法在面对大量难以区分的文档时表现出有限的检索精度，在实际应用中提出了显着的挑战。为了应对这些新出现的挑战，我们提出了 HiQA，这是一种先进的多文档问答 (MDQA) 框架，它将级联元数据集成到内容中以及多路由检索机制。我们还发布了一个名为 MasQA 的基准来评估和研究 MDQA。最后，HiQA 展示了多文档环境中最先进的性能。</li>
</ul>

<h3>Title: Redefining "Hallucination" in LLMs: Towards a psychology-informed  framework for mitigating misinformation</h3>
<ul>
<li><strong>Authors: </strong>Elijah Berberette, Jack Hutchins, Amir Sadovnik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01769">https://arxiv.org/abs/2402.01769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01769">https://arxiv.org/pdf/2402.01769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01769]] Redefining "Hallucination" in LLMs: Towards a psychology-informed  framework for mitigating misinformation(https://arxiv.org/abs/2402.01769)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat, rag</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) have become incredibly popular, with ChatGPT for example being used by over a billion users. While these models exhibit remarkable language understanding and logical prowess, a notable challenge surfaces in the form of "hallucinations." This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base. However, we question the appropriateness of the term "hallucination" in LLMs, proposing a psychological taxonomy based on cognitive biases and other psychological phenomena. Our approach offers a more fine-grained understanding of this phenomenon, allowing for targeted solutions. By leveraging insights from how humans internally resolve similar challenges, we aim to develop strategies to mitigate LLM hallucinations. This interdisciplinary approach seeks to move beyond conventional terminology, providing a nuanced understanding and actionable pathways for improvement in LLM reliability.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 变得异常流行，例如 ChatGPT 已被超过 10 亿用户使用。虽然这些模型表现出卓越的语言理解能力和逻辑能力，但一个显着的挑战以“幻觉”的形式出现。这种现象导致法学硕士以自信的方式输出错误信息，这可能会导致如此庞大的用户群带来毁灭性后果。然而，我们质疑法学硕士中“幻觉”一词的适当性，并提出了基于认知偏差和其他心理现象的心理分类法。我们的方法可以更细致地理解这种现象，从而提供有针对性的解决方案。通过利用人类内部如何解决类似挑战的见解，我们的目标是制定策略来减轻法学硕士的幻觉。这种跨学科方法旨在超越传统术语，为提高法学硕士的可靠性提供细致入微的理解和可行的途径。</li>
</ul>

<h3>Title: BlackMamba: Mixture of Experts for State-Space Models</h3>
<ul>
<li><strong>Authors: </strong>Quentin Anthony, Yury Tokpanov, Paolo Glorioso, Beren Millidge</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01771">https://arxiv.org/abs/2402.01771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01771">https://arxiv.org/pdf/2402.01771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01771]] BlackMamba: Mixture of Experts for State-Space Models(https://arxiv.org/abs/2402.01771)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits of SSM and MoE architectures, combining linear-complexity generation from SSM with cheap and fast inference from MoE. We release all weights, checkpoints, and inference code open-source. Inference code at: https://github.com/Zyphra/BlackMamba</li>
<li><strong>摘要：</strong>状态空间模型 (SSM) 最近在大规模语言建模基准测试中向 Transformer 展示了具有竞争力的性能，同时实现了作为序列长度函数的线性时间和内存复杂性。最近发布的 SSM 模型 Mamba 在语言建模和长序列处理任务中都表现出了令人印象深刻的性能。同时，混合专家 (MoE) 模型表现出了卓越的性能，同时显着降低了推理的计算和延迟成本，但代价是占用了更大的内存占用。在本文中，我们提出了 BlackMamba，这是一种将 Mamba SSM 与 MoE 结合起来以获得两者优点的新颖架构。我们证明 BlackMamba 的性能与 Mamba 和 Transformer 基线相比具有竞争力，并且在推理和训练失败方面表现出色。我们在自定义数据集的 300B 代币上全面训练并开源 340M/1.5B 和 630M/2.8B BlackMamba 模型。我们证明 BlackMamba 继承并结合了 SSM 和 MoE 架构的优点，将 SSM 的线性复杂度生成与 MoE 的廉价且快速的推理相结合。我们开源了所有权重、检查点和推理代码。推理代码位于：https://github.com/Zyphra/BlackMamba</li>
</ul>

<h3>Title: On the Psychology of GPT-4: Moderately anxious, slightly masculine,  honest, and humble</h3>
<ul>
<li><strong>Authors: </strong>Adrita Barua, Gary Brase, Ke Dong, Pascal Hitzler, Eugene Vasserman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01777">https://arxiv.org/abs/2402.01777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01777">https://arxiv.org/pdf/2402.01777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01777]] On the Psychology of GPT-4: Moderately anxious, slightly masculine,  honest, and humble(https://arxiv.org/abs/2402.01777)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, rag</a></li>
<li><strong>Abstract: </strong>We subject GPT-4 to a number of rigorous psychometric tests and analyze the results. We find that, compared to the average human, GPT-4 tends to show more honesty and humility, and less machiavellianism and narcissism. It sometimes exhibits ambivalent sexism, leans slightly toward masculinity, is moderately anxious but mostly not depressive (but not always). It shows human-average numerical literacy and has cognitive reflection abilities that are above human average for verbal tasks.</li>
<li><strong>摘要：</strong>我们对 GPT-4 进行了一系列严格的心理测量测试并分析结果。我们发现，与普通人相比，GPT-4 往往表现出更多的诚实和谦逊，而不是马基雅维利主义和自恋。它有时表现出矛盾的性别歧视，稍微倾向于男性气质，适度焦虑，但大多不抑郁（但并非总是如此）。它显示出人类平均的数字读写能力，并且在语言任务方面具有高于人类平均水平的认知反映能力。</li>
</ul>

<h3>Title: When Benchmarks are Targets: Revealing the Sensitivity of Large Language  Model Leaderboards</h3>
<ul>
<li><strong>Authors: </strong>Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi, Nora Altwairesh, Areeb Alowisheq, M Saiful Bari, Haidar Khan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01781">https://arxiv.org/abs/2402.01781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01781">https://arxiv.org/pdf/2402.01781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01781]] When Benchmarks are Targets: Revealing the Sensitivity of Large Language  Model Leaderboards(https://arxiv.org/abs/2402.01781)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks.</li>
<li><strong>摘要：</strong>基于基准排名的大型语言模型（LLM）排行榜经常用于指导从业者进行模型选择。通常，公布的排行榜排名都是按表面价值计算的——我们表明这是一个（可能代价高昂的）错误。在现有的排行榜下，法学硕士的相对表现对（通常是微小的）细节高度敏感。我们表明，对于流行的多项选择题基准（例如 MMLU），对基准的微小干扰（例如改变选择顺序或答案选择方法）会导致排名变化最多 8 个位置。我们通过对三大类基准扰动进行系统实验并确定这种行为的来源来解释这种现象。我们的分析得出了一些最佳实践建议，包括用于答案选择的混合评分方法的优势。我们的研究强调了依赖简单基准评估的危险，并为基于现有基准的更稳健的评估方案绘制了路径。</li>
</ul>

<h3>Title: Hierarchical Multi-Label Classification of Online Vaccine Concerns</h3>
<ul>
<li><strong>Authors: </strong>Chloe Qinyu Zhu, Rickard Stureborg, Bhuwan Dhingra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01783">https://arxiv.org/abs/2402.01783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01783">https://arxiv.org/pdf/2402.01783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01783]] Hierarchical Multi-Label Classification of Online Vaccine Concerns(https://arxiv.org/abs/2402.01783)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Vaccine concerns are an ever-evolving target, and can shift quickly as seen during the COVID-19 pandemic. Identifying longitudinal trends in vaccine concerns and misinformation might inform the healthcare space by helping public health efforts strategically allocate resources or information campaigns. We explore the task of detecting vaccine concerns in online discourse using large language models (LLMs) in a zero-shot setting without the need for expensive training datasets. Since real-time monitoring of online sources requires large-scale inference, we explore cost-accuracy trade-offs of different prompting strategies and offer concrete takeaways that may inform choices in system designs for current applications. An analysis of different prompting strategies reveals that classifying the concerns over multiple passes through the LLM, each consisting a boolean question whether the text mentions a vaccine concern or not, works the best. Our results indicate that GPT-4 can strongly outperform crowdworker accuracy when compared to ground truth annotations provided by experts on the recently introduced VaxConcerns dataset, achieving an overall F1 score of 78.7%.</li>
<li><strong>摘要：</strong>对疫苗的担忧是一个不断变化的目标，并且可能会像 COVID-19 大流行期间那样迅速发生变化。识别疫苗问题和错误信息的纵向趋势可能会帮助公共卫生工作战略性地分配资源或信息活动，从而为医疗保健领域提供信息。我们探索在零样本环境中使用大型语言模型（LLM）检测在线话语中的疫苗问题的任务，而不需要昂贵的训练数据集。由于在线资源的实时监控需要大规模推理，因此我们探索了不同提示策略的成本准确性权衡，并提供了具体的结论，可以为当前应用程序的系统设计选择提供信息。对不同提示策略的分析表明，对法学硕士多次通过的问题进行分类（每次都包含一个布尔问题，文本是否提及疫苗问题）效果最好。我们的结果表明，与专家在最近推出的 VaxConcerns 数据集上提供的地面真实注释相比，GPT-4 的准确率明显优于众包工作人员的准确性，总体 F1 得分为 78.7%。</li>
</ul>

<h3>Title: COA-GPT: Generative Pre-trained Transformers for Accelerated Course of  Action Development in Military Operations</h3>
<ul>
<li><strong>Authors: </strong>Vinicius G. Goecks, Nicholas Waytowich</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01786">https://arxiv.org/abs/2402.01786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01786">https://arxiv.org/pdf/2402.01786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01786]] COA-GPT: Generative Pre-trained Transformers for Accelerated Course of  Action Development in Military Operations(https://arxiv.org/abs/2402.01786)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, with added benefits of enhanced adaptability and alignment with commander intentions. COA-GPT's capability to rapidly adapt and update COAs during missions presents a transformative potential for military planning, particularly in addressing planning discrepancies and capitalizing on emergent windows of opportunities.</li>
<li><strong>摘要：</strong>军事行动中行动方针（COA）的制定传统上是一个耗时且复杂的过程。为了解决这一挑战，本研究引入了 COA-GPT，这是一种采用大型语言模型 (LLM) 快速高效生成有效 COA 的新颖算法。 COA-GPT 通过情境学习将军事学说和领域专业知识融入法学硕士，允许指挥官以文本和图像格式输入任务信息，并接收战略一致的 COA 进行审查和批准。独特的是，COA-GPT 不仅加速 COA 开发，在几秒钟内生成初始 COA，而且还有助于根据指挥官反馈进行实时改进。这项工作在军事版《星际争霸 II》游戏的军事相关场景中评估 COA-GPT，将其性能与最先进的强化学习算法进行比较。我们的结果表明，COA-GPT 在更迅速地生成战略上合理的 COA 方面具有优势，并具有增强适应性和与指挥官意图保持一致的额外优势。 COA-GPT 在任务期间快速调整和更新 COA 的能力为军事规划带来了变革潜力，特别是在解决规划差异和利用紧急机会窗口方面。</li>
</ul>

<h3>Title: LitLLM: A Toolkit for Scientific Literature Review</h3>
<ul>
<li><strong>Authors: </strong>Shubham Agarwal, Issam H. Laradji, Laurent Charlin, Christopher Pal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01788">https://arxiv.org/abs/2402.01788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01788">https://arxiv.org/pdf/2402.01788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01788]] LitLLM: A Toolkit for Scientific Literature Review(https://arxiv.org/abs/2402.01788)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on the user-provided abstract. Finally, the related work section is generated based on the re-ranked results and the abstract. There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative. Our open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM) with the video demo at https://youtu.be/E2ggOZBAFw0.</li>
<li><strong>摘要：</strong>对科学论文进行文献综述对于理解研究及其局限性以及在现有工作的基础上发展至关重要。这是一项乏味的任务，但自动文献综述生成器却很有吸引力。不幸的是，许多使用大型语言模型（LLM）生成此类评论的现有作品都有很大的局限性。他们倾向于产生幻觉——产生非实际信息——并忽视他们未经训练的最新研究。为了解决这些限制，我们提出了一个工具包，该工具包在法学硕士的帮助下按照检索增强生成（RAG）原则、专门的提示和指导技术进行操作。我们的系统首先启动网络搜索，通过使用现成的法学硕士将用户提供的摘要总结为关键词来检索相关论文。作者可以通过补充相关论文或关键词来增强搜索，从而有助于定制检索过程。其次，系统根据用户提供的摘要对检索到的论文重新排序。最后，根据重新排序的结果和摘要生成相关工作部分。与传统方法相比，文献综述的时间和精力大大减少，使我们的工具包成为一种有效的替代方法。我们的开源工具包可通过 https://github.com/shubhamagarwal92/LitLLM 和 Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM) 访问，视频演示位于 https://youtu.be/ E2ggOZBAFw0。</li>
</ul>

<h3>Title: An introduction to graphical tensor notation for mechanistic  interpretability</h3>
<ul>
<li><strong>Authors: </strong>Jordan K. Taylor</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01790">https://arxiv.org/abs/2402.01790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01790">https://arxiv.org/pdf/2402.01790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01790]] An introduction to graphical tensor notation for mechanistic  interpretability(https://arxiv.org/abs/2402.01790)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Graphical tensor notation is a simple way of denoting linear operations on tensors, originating from physics. Modern deep learning consists almost entirely of operations on or between tensors, so easily understanding tensor operations is quite important for understanding these systems. This is especially true when attempting to reverse-engineer the algorithms learned by a neural network in order to understand its behavior: a field known as mechanistic interpretability. It's often easy to get confused about which operations are happening between tensors and lose sight of the overall structure, but graphical tensor notation makes it easier to parse things at a glance and see interesting equivalences. The first half of this document introduces the notation and applies it to some decompositions (SVD, CP, Tucker, and tensor network decompositions), while the second half applies it to some existing some foundational approaches for mechanistically understanding language models, loosely following ``A Mathematical Framework for Transformer Circuits'', then constructing an example ``induction head'' circuit in graphical tensor notation.</li>
<li><strong>摘要：</strong>图形张量表示法是一种表示张量线性运算的简单方法，源自物理学。现代深度学习几乎完全由张量上或张量之间的运算组成，因此轻松理解张量运算对于理解这些系统非常重要。当试图对神经网络学习的算法进行逆向工程以理解其行为时尤其如此：这个领域被称为机械可解释性。通常很容易对张量之间发生的操作感到困惑并忽略整体结构，但图形张量表示法可以更轻松地一目了然地解析事物并查看有趣的等价物。本文档的前半部分介绍了该符号并将其应用于一些分解（SVD、CP、Tucker 和张量网络分解），而后半部分则将其应用于一些现有的一些用于机械地理解语言模型的基础方法，大致遵循``变压器电路的数学框架”，然后用图形张量符号构建一个示例“感应头”电路。</li>
</ul>

<h3>Title: Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward</h3>
<ul>
<li><strong>Authors: </strong>Arnav Chavan, Raghav Magazine, Shubham Kushwaha, Mérouane Debbah, Deepak Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01799">https://arxiv.org/abs/2402.01799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01799">https://arxiv.org/pdf/2402.01799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01799]] Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward(https://arxiv.org/abs/2402.01799)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, code</a></li>
<li><strong>Abstract: </strong>Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey</li>
<li><strong>摘要：</strong>尽管法学硕士的性能令人印象深刻，但由于推理过程中大量的计算和内存需求，它们的广泛采用面临着挑战。模型压缩和系统级优化方法的最新进展旨在增强 LLM 推理。本调查概述了这些方法，强调了最新的发展。通过 LLaMA(/2)-7B 上的实验，我们评估了各种压缩技术，为在统一环境中高效部署 LLM 提供了实用见解。对LLaMA(/2)-7B的实证分析凸显了这些方法的有效性。根据调查见解，我们确定了当前的局限性并讨论了提高法学硕士推理效率的潜在未来方向。我们在 https://github.com/nyunAI/Faster-LLM-Survey 发布了代码库以重现本文中提出的结果</li>
</ul>

<h3>Title: Large Language Models for Time Series: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01801">https://arxiv.org/abs/2402.01801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01801">https://arxiv.org/pdf/2402.01801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01801]] Large Language Models for Time Series: A Survey(https://arxiv.org/abs/2402.01801)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discussed in the survey.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在自然语言处理和计算机视觉等领域得到了广泛应用。除了文本、图像和图形之外，法学硕士还具有分析时间序列数据的巨大潜力，使气候、物联网、医疗保健、交通、音频和金融等领域受益。这篇调查论文对利用法学硕士进行时间序列分析的各种方法进行了深入的探索和详细的分类。我们解决了弥合法学硕士原始文本数据训练与时间序列数据的数值性质之间差距的固有挑战，并探索将法学硕士知识转移和提炼到数值时间序列分析的策略。我们详细介绍了各种方法，包括（1）法学硕士的直接提示，（2）时间序列量化，（3）对齐技术，（4）利用视觉模态作为桥接机制，以及（5）法学硕士与工具的结合。此外，这项调查还全面概述了现有的多模式时间序列和文本数据集，并深入探讨了这一新兴领域的挑战和未来机遇。我们维护一个最新的 Github 存储库，其中包含调查中讨论的所有论文和数据集。</li>
</ul>

<h3>Title: An Auction-based Marketplace for Model Trading in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Yue Cui, Liuyi Yao, Yaliang Li, Ziqian Chen, Bolin Ding, Xiaofang Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01802">https://arxiv.org/abs/2402.01802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01802">https://arxiv.org/pdf/2402.01802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01802]] An Auction-based Marketplace for Model Trading in Federated Learning(https://arxiv.org/abs/2402.01802)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is increasingly recognized for its efficacy in training models using locally distributed data. However, the proper valuation of shared data in this collaborative process remains insufficiently addressed. In this work, we frame FL as a marketplace of models, where clients act as both buyers and sellers, engaging in model trading. This FL market allows clients to gain monetary reward by selling their own models and improve local model performance through the purchase of others' models. We propose an auction-based solution to ensure proper pricing based on performance gain. Incentive mechanisms are designed to encourage clients to truthfully reveal their model valuations. Furthermore, we introduce a reinforcement learning (RL) framework for marketing operations, aiming to achieve maximum trading volumes under the dynamic and evolving market status. Experimental results on four datasets demonstrate that the proposed FL market can achieve high trading revenue and fair downstream task accuracy.</li>
<li><strong>摘要：</strong>联邦学习 (FL) 因其在使用本地分布式数据训练模型方面的功效而日益得到认可。然而，在这个协作过程中共享数据的正确评估仍然没有得到充分解决。在这项工作中，我们将 FL 构建为一个模型市场，客户既充当买家又充当卖家，参与模型交易。这个 FL 市场允许客户通过销售自己的模型来获得金钱奖励，并通过购买其他模型来提高本地模型的性能。我们提出了一种基于拍卖的解决方案，以确保根据性能增益进行适当的定价。激励机制旨在鼓励客户如实披露其模型估值。此外，我们为营销运营引入了强化学习（RL）框架，旨在在动态和不断变化的市场状态下实现最大交易量。四个数据集的实验结果表明，所提出的 FL 市场可以实现高交易收入和公平的下游任务准确性。</li>
</ul>

<h3>Title: Exploring the Limitations of Graph Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Palaash Agrawal, Shavak Vasania, Cheston Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01805">https://arxiv.org/abs/2402.01805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01805">https://arxiv.org/pdf/2402.01805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01805]] Exploring the Limitations of Graph Reasoning in Large Language Models(https://arxiv.org/abs/2402.01805)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed for graph traversal tasks, known as PathCompare, which shows a notable increase in the performance of LLMs in comparison to standard prompting and CoT.</li>
<li><strong>摘要：</strong>预训练的大型语言模型仅通过基于语言的提示就展示了各种类型的推理能力。然而，在本文中，我们通过图推理问题测试了 5 个不同的 LLM（GPT-4、GPT-3.5、Claude-2、Llama-2 和 Palm-2）的图推理深度。特别是，我们设计了 10 个不同的图遍历问题，每个问题都代表了不断增加的复杂程度。此外，我们分析了模型在各种设置下的性能，例如不同大小的图表以及不同形式的 k-shot 提示。我们通过这个基准测试过程强调了 LLM 的各种局限性、偏差和属性，例如与图中每个节点遍历的平均自由度的反比关系、k-shot 提示对图推理任务的总体负面影响，以及积极的反应偏差会阻止法学硕士识别有效解决方案的缺乏。最后，我们提出了一种专门为图遍历任务设计的新提示技术，称为 PathCompare，与标准提示和 CoT 相比，LLM 的性能显着提高。</li>
</ul>

<h3>Title: HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack  on Text</h3>
<ul>
<li><strong>Authors: </strong>Han Liu, Zhi Xu, Xiaotong Zhang, Feng Zhang, Fenglong Ma, Hongyang Chen, Hong Yu, Xianchao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01806">https://arxiv.org/abs/2402.01806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01806">https://arxiv.org/pdf/2402.01806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01806]] HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack  on Text(https://arxiv.org/abs/2402.01806)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Black-box hard-label adversarial attack on text is a practical and challenging task, as the text data space is inherently discrete and non-differentiable, and only the predicted label is accessible. Research on this problem is still in the embryonic stage and only a few methods are available. Nevertheless, existing methods rely on the complex heuristic algorithm or unreliable gradient estimation strategy, which probably fall into the local optimum and inevitably consume numerous queries, thus are difficult to craft satisfactory adversarial examples with high semantic similarity and low perturbation rate in a limited query budget. To alleviate above issues, we propose a simple yet effective framework to generate high quality textual adversarial examples under the black-box hard-label attack scenarios, named HQA-Attack. Specifically, after initializing an adversarial example randomly, HQA-attack first constantly substitutes original words back as many as possible, thus shrinking the perturbation rate. Then it leverages the synonym set of the remaining changed words to further optimize the adversarial example with the direction which can improve the semantic similarity and satisfy the adversarial condition simultaneously. In addition, during the optimizing procedure, it searches a transition synonym word for each changed word, thus avoiding traversing the whole synonym set and reducing the query number to some extent. Extensive experimental results on five text classification datasets, three natural language inference datasets and two real-world APIs have shown that the proposed HQA-Attack method outperforms other strong baselines significantly.</li>
<li><strong>摘要：</strong>对文本的黑盒硬标签对抗性攻击是一项实用且具有挑战性的任务，因为文本数据空间​​本质上是离散且不可微的，并且只有预测的标签是可访问的。对这个问题的研究仍处于萌芽阶段，可用的方法也很少。然而，现有方法依赖于复杂的启发式算法或不可靠的梯度估计策略，可能陷入局部最优并不可避免地消耗大量查询，因此很难在有限的查询预算内制作出具有高语义相似性和低扰动率的令人满意的对抗性示例。为了缓解上述问题，我们提出了一个简单而有效的框架，用于在黑盒硬标签攻击场景下生成高质量的文本对抗示例，称为 HQA-Attack。具体来说，在随机初始化一个对抗样本后，HQA 攻击首先不断地尽可能多地替换回原始单词，从而缩小扰动率。然后利用剩余变化词的同义词集进一步优化对抗样本，以提高语义相似度并同时满足对抗条件。另外，在优化过程中，针对每个变化词都查找一个过渡同义词词，从而避免了遍历整个同义词集，一定程度上减少了查询次数。对五个文本分类数据集、三个自然语言推理数据集和两个真实世界 API 的广泛实验结果表明，所提出的 HQA-Attack 方法显着优于其他强基线。</li>
</ul>

<h3>Title: Distilling LLMs' Decomposition Abilities into Compact Language Models</h3>
<ul>
<li><strong>Authors: </strong>Denis Tarasov, Kumar Shridhar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01812">https://arxiv.org/abs/2402.01812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01812">https://arxiv.org/pdf/2402.01812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01812]] Distilling LLMs' Decomposition Abilities into Compact Language Models(https://arxiv.org/abs/2402.01812)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已经证明了其推理能力的熟练程度，但其庞大的规模带来了可扩展性挑战并限制了任何进一步的定制。相比之下，紧凑模型提供定制训练，但通常无法解决复杂的推理任务。本研究的重点是使用离线强化学习将法学硕士的分解技能提炼成紧凑的模型。我们利用法学硕士能力的进步来提供反馈并生成专门的特定任务数据集来训练紧凑模型。人工智能生成的数据集的开发和基线的建立构成了我们工作的主要贡献，强调了紧凑模型在复制复杂问题解决技能方面的潜力。</li>
</ul>

<h3>Title: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01817">https://arxiv.org/abs/2402.01817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01817">https://arxiv.org/pdf/2402.01817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01817]] LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks(https://arxiv.org/abs/2402.01817)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of {\bf LLM-Modulo Frameworks} that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.</li>
<li><strong>摘要：</strong>人们对大型语言模型 (LLM) 在规划和推理任务中的作用存在相当大的困惑。一方面是过于乐观的说法，即法学硕士确实可以通过正确的提示或自我验证策略来完成这些任务。另一方面，也许有些过于悲观的说法，即法学硕士在计划/推理任务中的所有优势都只是将问题规范从一种句法格式转换为另一种句法格式，并将问题传递给外部符号求解器。在本立场文件中，我们认为这两种极端观点都是错误的。我们认为，自回归法学硕士本身无法进行规划或自我验证（这毕竟是推理的一种形式），并阐明了文献中误解的原因。我们还将认为，法学硕士应该被视为通用的近似知识源，除了简单的前端/后端格式翻译器之外，它们在规划/推理任务中可以发挥更有意义的作用。我们提出了 {\bf LLM-Modulo Frameworks} 的愿景，它将 LLM 的优势与基于外部模型的验证器结合在更紧密的双向交互机制中。我们将展示如何在法学硕士的帮助下获得驱动外部验证器本身的模型。我们还将认为，这种 LLM-Modulo 框架不是简单地流水线化 LLM 和符号组件，而是提供了一种更好的神经符号方法，可以在 LLM 和符号组件之间提供更紧密的集成，并允许将基于模型的规划/推理机制的范围扩展到更灵活的知识、问题和偏好规范。</li>
</ul>

<h3>Title: Ecologically rational meta-learned inference explains human category  learning</h3>
<ul>
<li><strong>Authors: </strong>Akshay K. Jagadish, Julian Coda-Forno, Mirko Thalmann, Eric Schulz, Marcel Binz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01821">https://arxiv.org/abs/2402.01821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01821">https://arxiv.org/pdf/2402.01821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01821]] Ecologically rational meta-learned inference explains human category  learning(https://arxiv.org/abs/2402.01821)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy for assigning categories with learning, and (3) it generalizes to unseen stimuli in a human-like way. Furthermore, we show that ERMI's ecologically valid priors allow it to achieve state-of-the-art performance on the OpenML-CC18 classification benchmark.</li>
<li><strong>摘要：</strong>生态理性是指人类是适应环境的理性主体的观念。然而，由于两个原因，测试这一理论仍然具有挑战性：定义哪些任务在生态上有效以及为这些任务建立合理的模型存在困难。在这项工作中，我们证明大型语言模型可以生成与现实世界任务的统计数据相匹配的认知任务，特别是类别学习任务，从而解决第一个挑战。我们通过使用元学习框架派生适应这些任务的理性代理来解决第二个挑战，从而产生一类称为生态理性元学习推理（ERMI）的模型。在两个不同的实验中，ERMI 比其他七种认知模型更好地定量解释了人类数据。它还在定性水平上匹配人类行为：（1）它发现人类认为困难的相同任务很困难，（2）它变得更加依赖基于示例的策略来通过学习分配类别，以及（3）它概括为以类似人类的方式接受看不见的刺激。此外，我们还表明 ERMI 的生态有效先验使其能够在 OpenML-CC18 分类基准上实现最先进的性能。</li>
</ul>

<h3>Title: Building Guardrails for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao, Jie Meng, Wenjie Ruan, Xiaowei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01822">https://arxiv.org/abs/2402.01822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01822">https://arxiv.org/pdf/2402.01822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01822]] Building Guardrails for Large Language Models(https://arxiv.org/abs/2402.01822)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and testing to ensure the utmost quality of the final product.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 越来越融入我们的日常生活，识别和减轻其风险至关重要，特别是当这些风险可能对人类用户和社会产生深远影响时。护栏可以过滤法学硕士的输入或输出，已成为一项核心保护技术。本立场文件深入研究了当前的开源解决方案（Llama Guard、Nvidia NeMo、Guardrails AI），并讨论了挑战和构建更完整解决方案的道路。借鉴以往研究中的有力证据，我们主张在综合考虑各种法学硕士申请的不同背景的基础上，采用系统的方法为法学硕士构建护栏。我们建议通过与多学科团队合作，采用社会技术方法来确定精确的技术要求，探索先进的神经符号实现以适应要求的复杂性，并开发验证和测试以确保最终产品的最高质量。</li>
</ul>

<h3>Title: Fractal Patterns May Unravel the Intelligence in Next-Token Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Alabdulmohsin, Vinh Q. Tran, Mostafa Dehghani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01825">https://arxiv.org/abs/2402.01825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01825">https://arxiv.org/pdf/2402.01825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01825]] Fractal Patterns May Unravel the Intelligence in Next-Token Prediction(https://arxiv.org/abs/2402.01825)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, rag</a></li>
<li><strong>Abstract: </strong>We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.70. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. We also demonstrate that fractal parameters improve upon perplexity-based bits-per-byte (BPB) in predicting downstream performance. We hope these findings offer a fresh perspective on language and the mechanisms underlying the success of LLMs.</li>
<li><strong>摘要：</strong>我们研究语言的分形结构，旨在提供一种精确的形式来量化以前可能被怀疑但尚未正式显示的属性。我们确定该语言是：（1）自相似，在所有粒度级别上表现出复杂性，没有特定的特征上下文长度，以及（2）长程依赖（LRD），赫斯特参数约为 H=0.70。基于这些发现，我们认为语言中的短期模式/依赖性（例如段落）反映了更大范围（例如整个文档）的模式/依赖性。这可能会揭示下一个标记预测如何导致对多个粒度级别的文本结构的理解，从单词和子句到更广泛的上下文和意图。我们还证明了分形参数在预测下游性能方面优于基于困惑度的每字节位数（BPB）。我们希望这些发现能够为语言和法学硕士成功背后的机制提供新的视角。</li>
</ul>

<h3>Title: Leveraging Large Language Models for Analyzing Blood Pressure Variations  Across Biological Sex from Scientific Literature</h3>
<ul>
<li><strong>Authors: </strong>Yuting Guo, Seyedeh Somayyeh Mousavi, Reza Sameni, Abeed Sarker</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01826">https://arxiv.org/abs/2402.01826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01826">https://arxiv.org/pdf/2402.01826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01826]] Leveraging Large Language Models for Analyzing Blood Pressure Variations  Across Biological Sex from Scientific Literature(https://arxiv.org/abs/2402.01826)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag</a></li>
<li><strong>Abstract: </strong>Hypertension, defined as blood pressure (BP) that is above normal, holds paramount significance in the realm of public health, as it serves as a critical precursor to various cardiovascular diseases (CVDs) and significantly contributes to elevated mortality rates worldwide. However, many existing BP measurement technologies and standards might be biased because they do not consider clinical outcomes, comorbidities, or demographic factors, making them inconclusive for diagnostic purposes. There is limited data-driven research focused on studying the variance in BP measurements across these variables. In this work, we employed GPT-35-turbo, a large language model (LLM), to automatically extract the mean and standard deviation values of BP for both males and females from a dataset comprising 25 million abstracts sourced from PubMed. 993 article abstracts met our predefined inclusion criteria (i.e., presence of references to blood pressure, units of blood pressure such as mmHg, and mention of biological sex). Based on the automatically-extracted information from these articles, we conducted an analysis of the variations of BP values across biological sex. Our results showed the viability of utilizing LLMs to study the BP variations across different demographic factors.</li>
<li><strong>摘要：</strong>高血压被定义为血压（BP）高于正常水平，在公共卫生领域具有至关重要的意义，因为它是各种心血管疾病（CVD）的重要先兆，并导致全球死亡率升高。然而，许多现有的血压测量技术和标准可能存在偏差，因为它们没有考虑临床结果、合并症或人口因素，这使得它们对于诊断目的缺乏结论。专注于研究这些变量的血压测量差异的数据驱动研究有限。在这项工作中，我们采用大型语言模型 (LLM) GPT-35-turbo 从包含来自 PubMed 的 2500 万条摘要的数据集中自动提取男性和女性的血压平均值和标准差值。 993 篇文章摘要符合我们预先定义的纳入标准（即，提及血压、血压单位（例如 mmHg）以及提及生物性别）。根据从这些文章中自动提取的信息，我们对不同生物性别的血压值的变化进行了分析。我们的结果表明，利用法学硕士研究不同人口因素的血压变化是可行的。</li>
</ul>

<h3>Title: Retrieval Augmented End-to-End Spoken Dialog Models</h3>
<ul>
<li><strong>Authors: </strong>Mingqiu Wang, Izhak Shafran, Hagen Soltau, Wei Han, Yuan Cao, Dian Yu, Laurent El Shafey</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01828">https://arxiv.org/abs/2402.01828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01828">https://arxiv.org/pdf/2402.01828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01828]] Retrieval Augmented End-to-End Spoken Dialog Models(https://arxiv.org/abs/2402.01828)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>We recently developed SLM, a joint speech and language model, which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to speech dialog applications where the dialog states are inferred directly from the audio signal. Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to retrieve text entities mentioned in the audio. The retrieved entities are then added as text inputs to the underlying SLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that this retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to other speech tasks requiring contextual information or domain-specific entities, such as contextual ASR with biasing capability.</li>
<li><strong>摘要：</strong>我们最近开发了 SLM，一种联合语音和语言模型，它融合了预训练的基础语音模型和大型语言模型 (LLM)，同时保留了预训练的 LLM 固有的上下文学习能力。在本文中，我们将 SLM 应用于语音对话应用，其中对话状态是直接从音频信号推断出来的。面向任务的对话框通常包含特定于域的实体，即餐馆、酒店、火车站和城市名称，这些实体很难识别，但对下游应用程序至关重要。受 RAG（检索增强生成）范式的启发，我们提出了一种检索增强 SLM（ReSLM）来克服这一弱点。我们首先训练语音检索器来检索音频中提到的文本实体。然后，检索到的实体作为文本输入添加到底层 SLM，以偏置模型预测。我们在语音 MultiWoz 任务（DSTC-11 挑战）上评估了 ReSLM，发现这种检索增强提高了模型性能，实现了联合目标准确率（38.6% vs 32.7%）、槽错误率（20.6% vs 24.8%）和 ASR 单词错误率率（5.5% vs 6.7%）。虽然在对话状态跟踪上得到了证明，但我们的方法广泛适用于需要上下文信息或特定领域实体的其他语音任务，例如具有偏置能力的上下文 ASR。</li>
</ul>

<h3>Title: Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in  Open-environment</h3>
<ul>
<li><strong>Authors: </strong>Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui Liu, Yu Wang, Ming Pang, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01830">https://arxiv.org/abs/2402.01830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01830">https://arxiv.org/pdf/2402.01830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01830]] Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in  Open-environment(https://arxiv.org/abs/2402.01830)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning human rankings. We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.</li>
<li><strong>摘要：</strong>现有的大语言模型（LLM）评估方法通常侧重于使用人工注释来测试某些封闭环境和特定领域基准的性能。在本文中，我们探索了一种新颖的无监督评估方向，利用同行评审机制自动衡量法学硕士。在这种情况下，开源法学硕士和闭源法学硕士都处于相同的环境中，能够回答未标记的问题并相互评估，其中每个法学硕士的回答分数由其他匿名法学硕士共同确定。为了获得这些模型之间的能力层次结构，我们为每个 LLM 分配一个可学习的能力参数来调整最终排名。我们将其形式化为一个约束优化问题，旨在最大限度地提高每个法学硕士能力和分数的一致性。背后的关键假设是，高水平的法学硕士可以比低水平的法学硕士更准确地评估他人的答案，而高水平的法学硕士也可以获得更高的回答分数。此外，我们提出了 PEN、CIN 和 LIS 三个指标来评估调整人类排名方面的差距。我们使用这些指标对多个数据集进行实验，验证所提出方法的有效性。</li>
</ul>

<h3>Title: What Will My Model Forget? Forecasting Forgotten Examples in Language  Model Refinement</h3>
<ul>
<li><strong>Authors: </strong>Xisen Jin, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01865">https://arxiv.org/abs/2402.01865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01865">https://arxiv.org/pdf/2402.01865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01865]] What Will My Model Forget? Forecasting Forgotten Examples in Language  Model Refinement(https://arxiv.org/abs/2402.01865)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products of example representations achieves better forecasting performance over a series of setups. Finally, we show that we reduce forgetting of upstream pretraining examples by replaying examples that are forecasted to be forgotten, demonstrating the practical utility of forecasting example forgetting.</li>
<li><strong>摘要：</strong>野外部署的语言模型会出错。然而，简单地用纠正后的错误实例更新模型会导致灾难性的遗忘——更新后的模型在指令调整或上游训练阶段学习到的实例上会出现错误。随机重放上游数据会产生不令人满意的性能，并且通常会带来高方差和差的可控性。为此，我们尝试预测由于模型更新而被遗忘的上游示例，以提高重放过程的可控性和可解释性。我们在给出一系列在线学习示例和相应的被遗忘的上游预训练示例的情况下训练预测模型。我们基于以下观察提出了一种部分可解释的预测模型：预训练示例的 pre-softmax Logit 分数的变化类似于在线学习示例的变化，该模型在 BART 上表现良好，但在 T5 模型上表现不佳。我们进一步展示了基于示例表示的内积的黑盒分类器在一系列设置中实现了更好的预测性能。最后，我们表明，通过重放预测会被遗忘的示例，我们可以减少上游预训练示例的遗忘，从而证明了预测示例遗忘的实用性。</li>
</ul>

<h3>Title: Leveraging Large Language Models for Structure Learning in Prompted Weak  Supervision</h3>
<ul>
<li><strong>Authors: </strong>Jinyan Su, Peilin Yu, Jieyu Zhang, Stephen H. Bach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01867">https://arxiv.org/abs/2402.01867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01867">https://arxiv.org/pdf/2402.01867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01867]] Leveraging Large Language Models for Structure Learning in Prompted Weak  Supervision(https://arxiv.org/abs/2402.01867)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code, rag</a></li>
<li><strong>Abstract: </strong>Prompted weak supervision (PromptedWS) applies pre-trained large language models (LLMs) as the basis for labeling functions (LFs) in a weak supervision framework to obtain large labeled datasets. We further extend the use of LLMs in the loop to address one of the key challenges in weak supervision: learning the statistical dependency structure among supervision sources. In this work, we ask the LLM how similar are these prompted LFs. We propose a Structure Refining Module, a simple yet effective first approach based on the similarities of the prompts by taking advantage of the intrinsic structure in the embedding space. At the core of Structure Refining Module are Labeling Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared to previous methods that learn the dependencies from weak labels, our method finds the dependencies which are intrinsic to the LFs and less dependent on the data. We show that our Structure Refining Module improves the PromptedWS pipeline by up to 12.7 points on the benchmark tasks. We also explore the trade-offs between efficiency and performance with comprehensive ablation experiments and analysis. Code for this project can be found in https://github.com/BatsResearch/su-bigdata23-code.</li>
<li><strong>摘要：</strong>提示性弱监督（PromptedWS）应用预训练的大型语言模型（LLM）作为弱监督框架中标记函数（LF）的基础，以获得大型标记数据集。我们进一步扩展了 LLM 在循环中的使用，以解决弱监督的关键挑战之一：学习监督源之间的统计依赖结构。在这项工作中，我们询问法学硕士这些提示的 LF 有多相似。我们提出了一个结构细化模块，这是一种简单而有效的第一种方法，它基于提示的相似性，利用嵌入空间中的内在结构。结构细化模块的核心是标签函数去除（LaRe）和相关结构生成（CosGen）。与以前从弱标签学习依赖关系的方法相比，我们的方法找到了 LF 固有的依赖关系，并且较少依赖于数据。我们表明，我们的 Structure Refining 模块在基准任务中将 PromptedWS 管道提高了 12.7 个点。我们还通过全面的消融实验和分析探索效率和性能之间的权衡。该项目的代码可以在 https://github.com/BatsResearch/su-bigdata23-code 中找到。</li>
</ul>

<h3>Title: APIServe: Efficient API Support for Large-Language Model Inferencing</h3>
<ul>
<li><strong>Authors: </strong>Reyna Abhyankar, Zijian He, Vikranth Srivatsa, Hao Zhang, Yiying Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01869">https://arxiv.org/abs/2402.01869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01869">https://arxiv.org/pdf/2402.01869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01869]] APIServe: Efficient API Support for Large-Language Model Inferencing(https://arxiv.org/abs/2402.01869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly integrated with external tools and APIs like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat API calls as new requests, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents APIServe, the first LLM inference framework targeting API-augmented LLMs. APISERVE minimizes the GPU resource waste caused by API calls and dedicates saved memory for serving more requests. APISERVE improves the overall serving throughput by 1.6x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.</li>
<li><strong>摘要：</strong>大型语言模型越来越多地与外部工具和 API（例如 ChatGPT 插件）集成，以将其功能扩展到以语言为中心的任务之外。然而，当今的法学硕士推理系统是为独立的法学硕士而设计的。他们将 API 调用视为新请求，导致对已计算的上下文进行不必要的重新计算，这占总模型转发时间的 37-40%。本文介绍了 APIServe，这是第一个针对 API 增强的 LLM 的 LLM 推理框架。 APISERVE 最大限度地减少了 API 调用造成的 GPU 资源浪费，并将节省的内存用于服务更多请求。与最先进的 LLM 推理系统相比，APISERVE 将整体服务吞吐量提高了 1.6 倍，每秒完成的请求数增加了 2 倍。</li>
</ul>

<h3>Title: The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement  Learning and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Moschoula Pternea, Prerna Singh, Abir Chakraborty, Yagna Oruganti, Mirco Milletari, Sayli Bapat, Kebei Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01874">https://arxiv.org/abs/2402.01874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01874">https://arxiv.org/pdf/2402.01874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01874]] The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement  Learning and Large Language Models(https://arxiv.org/abs/2402.01874)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag, agent</a></li>
<li><strong>Abstract: </strong>In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a common planning framework without either of them contributing to training or fine-tuning of the other. We further branch this class to distinguish between studies with and without natural language feedback. We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.</li>
<li><strong>摘要：</strong>在这项工作中，我们回顾了结合强化学习（RL）和大型语言模型（LLM）的研究，这两个领域的发展动力归功于深度神经网络的发展。我们根据两种模型类型相互作用的方式提出了三个主要类别的新颖分类法。第一类是 RL4LLM，包括利用 RL 来提高法学硕士在自然语言处理相关任务上的表现的研究。 L4LLM根据RL是用于直接微调现有的LLM还是用于提高LLM的提示而分为两个子类别。在第二类LLM4RL中，LLM协助训练RL模型，该模型执行与自然语言本质上不相关的任务。我们根据LLM辅助或取代的RL训练框架的组成部分，即奖励塑造、目标生成和政策功能，进一步分解LLM4RL。最后，在第三类中，RL+LLM，LLM 和 RL 代理嵌入到一个通用的规划框架中，其中任何一个都不会为另一个的训练或微调做出贡献。我们进一步对此类进行分支，以区分有自然语言反馈和没有自然语言反馈的研究。我们使用这种分类法来探索法学硕士和强化学习协同作用背后的动机，并解释其成功的原因，同时指出潜在的缺点和需要进一步研究的领域，以及服务于同一目标的替代方法。</li>
</ul>

<h3>Title: LiPO: Listwise Preference Optimization through Learning-to-Rank</h3>
<ul>
<li><strong>Authors: </strong>Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, Peter J. Liu, Xuanhui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01878">https://arxiv.org/abs/2402.01878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01878">https://arxiv.org/pdf/2402.01878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01878]] LiPO: Listwise Preference Optimization through Learning-to-Rank(https://arxiv.org/abs/2402.01878)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, rag</a></li>
<li><strong>Abstract: </strong>Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, especially pairwise ones. Following this connection, we provide an examination of ranking objectives that are not well studied for LM alignment withDPO and SLiC as special cases when list size is two. In particular, we highlight a specific method, LiPO-{\lambda}, which leverages a state-of-the-art listwise ranking objective and weights each preference pair in a more advanced manner. We show that LiPO-{\lambda} can outperform DPO and SLiC by a clear margin on two preference alignment tasks.</li>
<li><strong>摘要：</strong>将语言模型 (LM) 与精心策划的人类反馈保持一致对于控制其在实际应用中的行为至关重要。最近的几种策略优化方法，例如 DPO 和 SLiC，可以作为传统的人类反馈强化学习 (RLHF) 方法的有希望的替代方案。在实践中，人类反馈通常以多个响应的排名列表的形式出现，以分摊阅读提示的成本。多个响应也可以通过奖励模型或人工智能反馈进行排名。缺乏直接拟合响应列表的研究。在这项工作中，我们将 LM 对齐公式化为列表排序问题，并描述了列表偏好优化 (LiPO) 框架，其中策略可以根据提示从合理响应的排序列表中更有效地学习。这种观点与学习排名（LTR）建立了明确的联系，其中大多数现有的偏好优化工作可以映射到现有的排名目标，尤其是成对的排名目标。在此连接之后，我们对排序目标进行了检查，这些排序目标尚未充分研究用于与 DPO 和 SLiC 进行 LM 对齐（作为列表大小为 2 时的特殊情况）。我们特别强调一种特定的方法，LiPO-{\lambda}，它利用最先进的列表排名目标，并以更高级的方式对每个偏好对进行加权。我们证明 LiPO-{\lambda} 在两项偏好对齐任务上明显优于 DPO 和 SLiC。</li>
</ul>

<h3>Title: $σ$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial  Examples</h3>
<ul>
<li><strong>Authors: </strong>Antonio Emanuele Cinà, Francesco Villani, Maura Pintor, Lea Schönherr, Battista Biggio, Marcello Pelillo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01879">https://arxiv.org/abs/2402.01879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01879">https://arxiv.org/pdf/2402.01879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01879]] $σ$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial  Examples(https://arxiv.org/abs/2402.01879)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging. While most attacks consider $\ell_2$- and $\ell_\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular, $\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint. However, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm attacks. In this work, we propose a novel $\ell_0$-norm attack, called $\sigma$-zero, which leverages an ad hoc differentiable approximation of the $\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving robust and non-robust models, show that $\sigma$-zero finds minimum $\ell_0$-norm adversarial examples without requiring any time-consuming hyperparameter tuning, and that it outperforms all competing sparse attacks in terms of success rate, perturbation size, and scalability.</li>
<li><strong>摘要：</strong>评估深度网络对基于梯度的攻击的对抗鲁棒性具有挑战性。虽然大多数攻击考虑 $\ell_2$- 和 $\ell_\infty$-norm 约束来制作输入扰动，但只有少数攻击研究稀疏 $\ell_1$- 和 $\ell_0$-norm 攻击。特别是，由于非凸和不可微约束优化的固有复杂性，$\ell_0$-范数攻击仍然是研究最少的。然而，评估这些攻击下的对抗鲁棒性可能会揭示更传统的 $\ell_2$- 和 $\ell_\infty$-norm 攻击未测试的弱点。在这项工作中，我们提出了一种新颖的 $\ell_0$-norm 攻击，称为 $\sigma$-zero，它利用 $\ell_0$ 范数的临时可微近似来促进基于梯度的优化，以及自适应投影算子动态调整损失最小化和扰动稀疏性之间的权衡。使用 MNIST、CIFAR10 和 ImageNet 数据集（涉及鲁棒和非鲁棒模型）进行的广泛评估表明，$\sigma$-zero 可以找到最小的 $\ell_0$-norm 对抗示例，而不需要任何耗时的超参数调整，并且它的性能优于在成功率、扰动大小和可扩展性方面所有竞争的稀疏攻击。</li>
</ul>

<h3>Title: Large Language Model Agent for Hyper-Parameter Optimization</h3>
<ul>
<li><strong>Authors: </strong>Siyi Liu, Chen Gao, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01881">https://arxiv.org/abs/2402.01881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01881">https://arxiv.org/pdf/2402.01881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01881]] Large Language Model Agent for Hyper-Parameter Optimization(https://arxiv.org/abs/2402.01881)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted on 12 representative machine-learning tasks indicate that AgentHPO not only matches but also often surpasses the best human trials in terms of performance while simultaneously providing explainable results. Further analysis sheds light on the strategies employed by the LLM in optimizing these tasks, highlighting its effectiveness and adaptability in various scenarios.</li>
<li><strong>摘要：</strong>超参数优化在现代机器学习中至关重要，需要专业知识、大量试验以及大量计算资源和人力资源。尽管自动化机器学习 (AutoML) 取得了进步，但试验效率、设置复杂性和互操作性方面的挑战仍然存在。为了解决这些问题，我们引入了一种利用大型语言模型 (LLM) 来跨不同机器学习任务自动执行超参数优化的新颖范例，该范例被命名为 AgentHPO（LLM 基于代理的超参数优化的缩写）。具体来说，AgentHPO自主处理任务信息，使用特定的超参数（HP）进行实验，并根据历史试验迭代优化它们。与传统的 AutoML 方法相比，这种类人的优化过程大大减少了所需的试验次数，简化了设置过程，并增强了可解释性和用户信任度。对 12 个代表性机器学习任务进行的广泛实证实验表明，AgentHPO 不仅在性能方面匹配甚至经常超越最好的人类试验，同时提供可解释的结果。进一步的分析揭示了法学硕士在优化这些任务时所采用的策略，强调了其在各种情况下的有效性和适应性。</li>
</ul>

<h3>Title: The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Daniel Cunnington, Mark Law, Jorge Lobo, Alessandra Russo</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01889">https://arxiv.org/abs/2402.01889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01889">https://arxiv.org/pdf/2402.01889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01889]] The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning(https://arxiv.org/abs/2402.01889)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, rag</a></li>
<li><strong>Abstract: </strong>Neuro-Symbolic AI (NeSy) holds promise to ensure the safe deployment of AI systems, as interpretable symbolic techniques provide formal behaviour guarantees. The challenge is how to effectively integrate neural and symbolic computation, to enable learning and reasoning from raw data. Existing pipelines that train the neural and symbolic components sequentially require extensive labelling, whereas end-to-end approaches are limited in terms of scalability, due to the combinatorial explosion in the symbol grounding problem. In this paper, we leverage the implicit knowledge within foundation models to enhance the performance in NeSy tasks, whilst reducing the amount of data labelling and manual engineering. We introduce a new architecture, called NeSyGPT, which fine-tunes a vision-language foundation model to extract symbolic features from raw data, before learning a highly expressive answer set program to solve a downstream task. Our comprehensive evaluation demonstrates that NeSyGPT has superior accuracy over various baselines, and can scale to complex NeSy tasks. Finally, we highlight the effective use of a large language model to generate the programmatic interface between the neural and symbolic components, significantly reducing the amount of manual engineering required.</li>
<li><strong>摘要：</strong>神经符号人工智能（NeSy）有望确保人工智能系统的安全部署，因为可解释的符号技术提供了正式的行为保证。挑战在于如何有效地集成神经计算和符号计算，以便能够从原始数据中进行学习和推理。按顺序训练神经和符号组件的现有管道需要大量标签，而由于符号接地问题中的组合爆炸，端到端方法在可扩展性方面受到限制。在本文中，我们利用基础模型中的隐式知识来提高 NeSy 任务的性能，同时减少数据标记和手动工程的数量。我们引入了一种名为 NeSyGPT 的新架构，它对视觉语言基础模型进行微调，以从原始数据中提取符号特征，然后再学习高度表达的答案集程序来解决下游任务。我们的综合评估表明，NeSyGPT 比各种基线都具有更高的准确性，并且可以扩展到复杂的 NeSy 任务。最后，我们强调有效使用大型语言模型来生成神经组件和符号组件之间的编程接口，从而显着减少所需的手动工程量。</li>
</ul>

<h3>Title: From PEFT to DEFT: Parameter Efficient Finetuning for Reducing  Activation Density in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Bharat Runwal, Tejaswini Pedapati, Pin-Yu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01911">https://arxiv.org/abs/2402.01911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01911">https://arxiv.org/pdf/2402.01911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01911]] From PEFT to DEFT: Parameter Efficient Finetuning for Reducing  Activation Density in Transformers(https://arxiv.org/abs/2402.01911)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora, prompt, code, rag</a></li>
<li><strong>Abstract: </strong>Pretrained Language Models (PLMs) have become the de facto starting point for fine-tuning on downstream tasks. However, as model sizes continue to increase, traditional fine-tuning of all parameters becomes challenging. To address this, parameter-efficient fine-tuning (PEFT) methods have gained popularity as a means to adapt PLMs effectively. In parallel, recent studies have revealed the presence of activation sparsity within the intermediate outputs of the multilayer perception (MLP) blocks in transformers. Low activation density enables efficient model inference on sparsity-aware hardware. Building upon this insight, in this work, we propose a novel density loss that encourages higher activation sparsity (equivalently, lower activation density) in the pre-trained models. We demonstrate the effectiveness of our approach by utilizing mainstream PEFT techniques including QLoRA, LoRA, Adapter, Prompt/Prefix Tuning to facilitate efficient model adaptation across diverse downstream tasks. Experiments show that our proposed method DEFT, Density-Efficient Fine-Tuning, can reduce the activation density consistently and up to $\boldsymbol{50.72\%}$ on RoBERTa$_\mathrm{Large}$, and $\boldsymbol {53.19\%}$ (encoder density) and $\boldsymbol{90.60\%}$ (decoder density) on Flan-T5$_\mathrm{XXL}$ ($\boldsymbol{11B}$) compared to PEFT using GLUE and QA (SQuAD) benchmarks respectively while maintaining competitive performance on downstream tasks. We also showcase that DEFT works complementary with quantized and pruned models</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 已成为下游任务微调的事实上的起点。然而，随着模型尺寸的不断增加，所有参数的传统微调变得具有挑战性。为了解决这个问题，参数高效微调 (PEFT) 方法作为有效适应 PLM 的一种手段而受到欢迎。与此同时，最近的研究揭示了变压器中多层感知（MLP）块的中间输出中存在激活稀疏性。低激活密度可以在稀疏感知硬件上实现高效的模型推理。基于这一见解，在这项工作中，我们提出了一种新颖的密度损失，它鼓励预训练模型中更高的激活稀疏性（相当于更低的激活密度）。我们通过利用主流 PEFT 技术（包括 QLoRA、LoRA、Adapter、Prompt/Prefix Tuning）来证明我们方法的有效性，以促进跨不同下游任务的高效模型适应。实验表明，我们提出的方法 DEFT（密度高效微调）可以一致地降低激活密度，在 RoBERTa$_\mathrm{Large}$ 上达到 $\boldsymbol{50.72\%}$ 和 $\boldsymbol {53.19 Flan-T5$_\mathrm{XXL}$ ($\boldsymbol{11B}$) 上的 \%}$ （编码器密度）和 $\boldsymbol{90.60\%}$ （解码器密度）与使用 GLUE 和 QA 的 PEFT 相比(SQuAD) 分别进行基准测试，同时保持下游任务的竞争性能。我们还展示了 DEFT 与量化和剪枝模型的互补作用</li>
</ul>

<h3>Title: Preference Poisoning Attacks on Reward Model Learning</h3>
<ul>
<li><strong>Authors: </strong>Junlin Wu, Jiongxiao Wang, Chaowei Xiao, Chenguang Wang, Ning Zhang, Yevgeniy Vorobeychik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01920">https://arxiv.org/abs/2402.01920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01920">https://arxiv.org/pdf/2402.01920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01920]] Preference Poisoning Attacks on Reward Model Learning(https://arxiv.org/abs/2402.01920)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Learning utility, or reward, models from pairwise comparisons is a fundamental component in a number of application domains. These approaches inherently entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability systematically by considering a threat model in which an attacker can flip a small subset of preference comparisons with the goal of either promoting or demoting a target outcome. First, we propose two classes of algorithmic approaches for these attacks: a principled gradient-based framework, and several variants of rank-by-distance methods. Next, we demonstrate the efficacy of best attacks in both these classes in successfully achieving malicious goals on datasets from three diverse domains: autonomous control, recommendation system, and textual prompt-response preference learning. We find that the best attacks are often highly successful, achieving in the most extreme case 100% success rate with only 0.3% of the data poisoned. However, which attack is best can vary significantly across domains, demonstrating the value of our comprehensive vulnerability analysis that involves several classes of attack algorithms. In addition, we observe that the simpler and more scalable rank-by-distance approaches are often competitive with the best, and on occasion significantly outperform gradient-based methods. Finally, we show that several state-of-the-art defenses against other classes of poisoning attacks exhibit, at best, limited efficacy in our setting.</li>
<li><strong>摘要：</strong>从成对比较中学习效用或奖励模型是许多应用领域的基本组成部分。这些方法本质上需要收集人们的偏好信息，并且通常以匿名方式提供反馈。由于偏好是主观的，因此没有可比较的黄金标准；然而，高影响力系统对偏好学习的依赖为恶意行为者提供了强烈的动机，使他们能够以这种方式收集的数据达到他们的目的。我们通过考虑威胁模型来系统地调查此漏洞的性质和程度，在该模型中，攻击者可以翻转一小部分偏好比较，以达到提升或降低目标结果的目的。首先，我们针对这些攻击提出了两类算法方法：基于原则的梯度框架和按距离排名方法的几种变体。接下来，我们展示了这两类最佳攻击在对来自三个不同领域的数据集成功实现恶意目标方面的功效：自主控制、推荐系统和文本提示响应偏好学习。我们发现最好的攻击通常非常成功，在最极端的情况下达到 100% 的成功率，而只有 0.3% 的数据中毒。然而，哪种攻击最好在不同领域之间可能存在很大差异，这证明了我们涉及几类攻击算法的全面漏洞分析的价值。此外，我们观察到，更简单、更可扩展的按距离排名方法通常可以与最好的方法竞争，有时甚至显着优于基于梯度的方法。最后，我们表明，针对其他类别的中毒攻击的几种最先进的防御措施在我们的环境中最多只能发挥有限的功效。</li>
</ul>

<h3>Title: Code Representation Learning At Scale</h3>
<ul>
<li><strong>Authors: </strong>Dejiao Zhang, Wasi Ahmad, Ming Tan, Hantian Ding, Ramesh Nallapati, Dan Roth, Xiaofei Ma, Bing Xiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01935">https://arxiv.org/abs/2402.01935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01935">https://arxiv.org/pdf/2402.01935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01935]] Code Representation Learning At Scale(https://arxiv.org/abs/2402.01935)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, rag</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that code language models at scale demonstrate significant performance gains on downstream tasks, i.e., code generation. However, most of the existing works on code representation learning train models at a hundred million parameter scale using very limited pretraining corpora. In this work, we fuel code representation learning with a vast amount of code data via a two-stage pretraining scheme. We first train the encoders via a mix that leverages both randomness in masking language modeling and the structure aspect of programming language. We then enhance the representations via contrastive learning with hard negative and hard positive constructed in an unsupervised manner. We establish an off-the-shelf encoder model that persistently outperforms the existing models on a wide variety of downstream tasks by large margins. To comprehend the factors contributing to successful code representation learning, we conduct detailed ablations and share our findings on (i) a customized and effective token-level denoising scheme for source code; (ii) the importance of hard negatives and hard positives; (iii) how the proposed bimodal contrastive learning boost the cross-lingual semantic search performance; and (iv) how the pretraining schemes decide the downstream task performance scales with the model size.</li>
<li><strong>摘要：</strong>最近的研究表明，大规模的代码语言模型在下游任务（即代码生成）上表现出显着的性能提升。然而，大多数现有的代码表示学习工作都使用非常有限的预训练语料库来训练一亿个参数规模的模型。在这项工作中，我们通过两阶段预训练方案使用大量代码数据来促进代码表示学习。我们首先通过利用掩码语言建模中的随机性和编程语言的结构方面的混合来训练编码器。然后，我们通过对比学习与以无监督方式构建的硬阴性和硬阳性来增强表征。我们建立了一个现成的编码器模型，该模型在各种下游任务上持续大幅优于现有模型。为了理解有助于成功代码表示学习的因素，我们进行了详细的消融并分享了我们的发现：（i）针对源代码定制且有效的令牌级去噪方案； (ii) 硬否定和硬肯定的重要性； （iii）所提出的双模态对比学习如何提高跨语言语义搜索性能； (iv) 预训练方案如何决定下游任务性能随模型大小的变化。</li>
</ul>

<h3>Title: A Case Study on Filtering for End-to-End Speech Translation</h3>
<ul>
<li><strong>Authors: </strong>Md Mahfuz Ibn Alam, Antonios Anastasopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01945">https://arxiv.org/abs/2402.01945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01945">https://arxiv.org/pdf/2402.01945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01945]] A Case Study on Filtering for End-to-End Speech Translation(https://arxiv.org/abs/2402.01945)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>It is relatively easy to mine a large parallel corpus for any machine learning task, such as speech-to-text or speech-to-speech translation. Although these mined corpora are large in volume, their quality is questionable. This work shows that the simplest filtering technique can trim down these big, noisy datasets to a more manageable, clean dataset. We also show that using this clean dataset can improve the model's performance, as in the case of the multilingual-to-English Speech Translation (ST) model, where, on average, we obtain a 4.65 BLEU score improvement.</li>
<li><strong>摘要：</strong>为任何机器学习任务（例如语音到文本或语音到语音翻译）挖掘大型并行语料库都相对容易。尽管这些挖掘的语料库数量庞大，但其质量却值得怀疑。这项工作表明，最简单的过滤技术可以将这些大的、嘈杂的数据集修剪成更易于管理、更干净的数据集。我们还表明，使用这个干净的数据集可以提高模型的性能，就像多语言到英语语音翻译 (ST) 模型的情况一样，我们平均获得了 4.65 BLEU 分数的提高。</li>
</ul>

<h3>Title: Calibrated Uncertainty Quantification for Operator Learning via  Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Ma, Kamyar Azizzadenesheli, Anima Anandkumar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01960">https://arxiv.org/abs/2402.01960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01960">https://arxiv.org/pdf/2402.01960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01960]] Calibrated Uncertainty Quantification for Operator Learning via  Conformal Prediction(https://arxiv.org/abs/2402.01960)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Operator learning has been increasingly adopted in scientific and engineering applications, many of which require calibrated uncertainty quantification. Since the output of operator learning is a continuous function, quantifying uncertainty simultaneously at all points in the domain is challenging. Current methods consider calibration at a single point or over one scalar function or make strong assumptions such as Gaussianity. We propose a risk-controlling quantile neural operator, a distribution-free, finite-sample functional calibration conformal prediction method. We provide a theoretical calibration guarantee on the coverage rate, defined as the expected percentage of points on the function domain whose true value lies within the predicted uncertainty ball. Empirical results on a 2D Darcy flow and a 3D car surface pressure prediction tasks validate our theoretical results, demonstrating calibrated coverage and efficient uncertainty bands outperforming baseline methods. In particular, on the 3D problem, our method is the only one that meets the target calibration percentage (percentage of test samples for which the uncertainty estimates are calibrated) of 98\%.</li>
<li><strong>摘要：</strong>算子学习在科学和工程应用中得到越来越多的采用，其中许多应用需要校准的不确定性量化。由于算子学习的输出是连续函数，因此同时量化域中所有点的不确定性具有挑战性。当前的方法考虑在单点或在一个标量函数上进行校准，或者做出强假设，例如高斯性。我们提出了一种风险控制分位数神经算子，一种无分布、有限样本函数校准保角预测方法。我们为覆盖率提供了理论校准保证，覆盖率定义为函数域上真实值位于预测不确定性球内的点的预期百分比。 2D 达西流和 3D 汽车表面压力预测任务的实证结果验证了我们的理论结果，证明了校准覆盖范围和有效的不确定性带优于基线方法。特别是，在 3D 问题上，我们的方法是唯一满足 98% 目标校准百分比（校准不确定性估计的测试样本的百分比）的方法。</li>
</ul>

<h3>Title: Improving Large-Scale k-Nearest Neighbor Text Categorization with Label  Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Francisco J. Ribadas-Pena, Shuyuan Cao, Víctor M. Darriba Bilbao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01963">https://arxiv.org/abs/2402.01963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01963">https://arxiv.org/pdf/2402.01963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01963]] Improving Large-Scale k-Nearest Neighbor Text Categorization with Label  Autoencoders(https://arxiv.org/abs/2402.01963)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a multi-label lazy learning approach to deal with automatic semantic indexing in large document collections in the presence of complex and structured label vocabularies with high inter-label correlation. The proposed method is an evolution of the traditional k-Nearest Neighbors algorithm which uses a large autoencoder trained to map the large label space to a reduced size latent space and to regenerate the predicted labels from this latent space. We have evaluated our proposal in a large portion of the MEDLINE biomedical document collection which uses the Medical Subject Headings (MeSH) thesaurus as a controlled vocabulary. In our experiments we propose and evaluate several document representation approaches and different label autoencoder configurations.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了一种多标签惰性学习方法，用于在存在具有高标签间相关性的复杂和结构化标签词汇的情况下处理大型文档集合中的自动语义索引。所提出的方法是传统 k 最近邻算法的演变，该算法使用经过训练的大型自动编码器将大型标签空间映射到尺寸减小的潜在空间，并从该潜在空间重新生成预测标签。我们在 MEDLINE 生物医学文档集中的大部分内容中评估了我们的提案，这些文档集合使用医学主题词 (MeSH) 同义词库作为受控词汇。在我们的实验中，我们提出并评估了几种文档表示方法和不同的标签自动编码器配置。</li>
</ul>

<h3>Title: No Need to Look Back: An Efficient and Scalable Approach for Temporal  Network Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuhong Luo, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01964">https://arxiv.org/abs/2402.01964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01964">https://arxiv.org/pdf/2402.01964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01964]] No Need to Look Back: An Efficient and Scalable Approach for Temporal  Network Representation Learning(https://arxiv.org/abs/2402.01964)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Temporal graph representation learning (TGRL) is crucial for modeling complex, dynamic systems in real-world networks. Traditional TGRL methods, though effective, suffer from high computational demands and inference latency. This is mainly induced by their inefficient sampling of temporal neighbors by backtracking the interaction history of each node when making model inference. This paper introduces a novel efficient TGRL framework, No-Looking-Back (NLB). NLB employs a "forward recent sampling" strategy, which bypasses the need for backtracking historical interactions. This strategy is implemented using a GPU-executable size-constrained hash table for each node, recording down-sampled recent interactions, which enables rapid response to queries with minimal inference latency. The maintenance of this hash table is highly efficient, with $O(1)$ complexity. NLB is fully compatible with GPU processing, maximizing programmability, parallelism, and power efficiency. Empirical evaluations demonstrate that NLB matches or surpasses state-of-the-art methods in accuracy for link prediction and node classification across six real-world datasets. Significantly, it is 1.32-4.40 $\times$ faster in training, 1.2-7.94 $\times$ more energy efficient, and 1.97-5.02 $\times$ more effective in reducing inference latency compared to the most competitive baselines. The link to the code: https://github.com/Graph-COM/NLB.</li>
<li><strong>摘要：</strong>时态图表示学习 (TGRL) 对于现实网络中复杂动态系统的建模至关重要。传统的 TGRL 方法虽然有效，但存在高计算需求和推理延迟的问题。这主要是由于在进行模型推理时回溯每个节点的交互历史来对时间邻居进行低效采样所致。本文介绍了一种新颖高效的 TGRL 框架——No-Looking-Back (NLB)。 NLB 采用“前向最近采样”策略，无需回溯历史交互。该策略是使用每个节点的 GPU 可执行大小受限的哈希表来实现的，记录下采样的最近交互，从而能够以最小的推理延迟快速响应查询。该哈希表的维护效率很高，复杂度为 $O(1)$。 NLB 与 GPU 处理完全兼容，最大限度地提高了可编程性、并行性和功效。实证评估表明，NLB 在六个真实数据集的链路预测和节点分类的准确性方面匹配或超越了最先进的方法。值得注意的是，与最具竞争力的基准相比，它的训练速度提高了 1.32-4.40 $\times$，能源效率提高了 1.2-7.94 $\times$，在减少推理延迟方面更加有效 1.97-5.02 $\times$。代码链接：https://github.com/Graph-COM/NLB。</li>
</ul>

<h3>Title: Simulation-Enhanced Data Augmentation for Machine Learning Pathloss  Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ahmed P. Mohamed, Byunghyun Lee, Yaguang Zhang, Max Hollingsworth, C. Robert Anderson, James V. Krogmeier, David J. Love</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01969">https://arxiv.org/abs/2402.01969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01969">https://arxiv.org/pdf/2402.01969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01969]] Simulation-Enhanced Data Augmentation for Machine Learning Pathloss  Prediction(https://arxiv.org/abs/2402.01969)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) offers a promising solution to pathloss prediction. However, its effectiveness can be degraded by the limited availability of data. To alleviate these challenges, this paper introduces a novel simulation-enhanced data augmentation method for ML pathloss prediction. Our method integrates synthetic data generated from a cellular coverage simulator and independently collected real-world datasets. These datasets were collected through an extensive measurement campaign in different environments, including farms, hilly terrains, and residential areas. This comprehensive data collection provides vital ground truth for model training. A set of channel features was engineered, including geographical attributes derived from LiDAR datasets. These features were then used to train our prediction model, incorporating the highly efficient and robust gradient boosting ML algorithm, CatBoost. The integration of synthetic data, as demonstrated in our study, significantly improves the generalizability of the model in different environments, achieving a remarkable improvement of approximately 12dB in terms of mean absolute error for the best-case scenario. Moreover, our analysis reveals that even a small fraction of measurements added to the simulation training set, with proper data balance, can significantly enhance the model's performance.</li>
<li><strong>摘要：</strong>机器学习 (ML) 为路径损耗预测提供了一种有前景的解决方案。然而，其有效性可能会因数据的有限可用性而降低。为了缓解这些挑战，本文引入了一种用于机器学习路径损耗预测的新型模拟增强数据增强方法。我们的方法集成了从蜂窝覆盖模拟器生成的合成数据和独立收集的真实世界数据集。这些数据集是通过在不同环境（包括农场、丘陵地形和住宅区）进行的广泛测量活动收集的。这种全面的数据收集为模型训练提供了重要的基础事实。设计了一组通道特征，包括从 LiDAR 数据集导出的地理属性。然后，这些特征被用来训练我们的预测模型，并结合高效且稳健的梯度增强 ML 算法 CatBoost。正如我们的研究所证明的，合成数据的集成显着提高了模型在不同环境中的通用性，在最佳情况下的平均绝对误差方面实现了约 12dB 的显着改进。此外，我们的分析表明，即使在模拟训练集中添加一小部分测量数据，只要保持适当的数据平衡，也可以显着提高模型的性能。</li>
</ul>

<h3>Title: SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks</h3>
<ul>
<li><strong>Authors: </strong>Gourab Dey, Adithya V Ganesan, Yash Kumar Lal, Manal Shah, Shreyashee Sinha, Matthew Matero, Salvatore Giorgi, Vivek Kulkarni, H. Andrew Schwartz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01980">https://arxiv.org/abs/2402.01980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01980">https://arxiv.org/pdf/2402.01980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01980]] SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks(https://arxiv.org/abs/2402.01980)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag</a></li>
<li><strong>Abstract: </strong>Social science NLP tasks, such as emotion or humor detection, are required to capture the semantics along with the implicit pragmatics from text, often with limited amounts of training data. Instruction tuning has been shown to improve the many capabilities of large language models (LLMs) such as commonsense reasoning, reading comprehension, and computer programming. However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured. We explore the use of instruction tuning for social science NLP tasks and introduce Socialite-Llama -- an open-source, instruction-tuned Llama. On a suite of 20 social science tasks, Socialite-Llama improves upon the performance of Llama as well as matches or improves upon the performance of a state-of-the-art, multi-task finetuned model on a majority of them. Further, Socialite-Llama also leads to improvement on 5 out of 6 related social tasks as compared to Llama, suggesting instruction tuning can lead to generalized social understanding. All resources including our code, model and dataset can be found through bit.ly/socialitellama.</li>
<li><strong>摘要：</strong>社会科学 NLP 任务（例如情绪或幽默检测）需要从文本中捕获语义以及隐含的语用，通常需要有限数量的训练数据。指令调优已被证明可以提高大型语言模型 (LLM) 的许多功能，例如常识推理、阅读理解和计算机编程。然而，人们对社交领域指令调整的有效性知之甚少，因为社交领域通常需要捕捉隐含的语用线索。我们探索了指令调优在社会科学 NLP 任务中的使用，并介绍了 Socialite-Llama——一种开源的、指令调优的 Llama。在一系列 20 项社会科学任务中，Socialite-Llama 改进了 Llama 的性能，并且在大多数任务上匹配或改进了最先进的多任务微调模型的性能。此外，与 Llama 相比，Socialite-Llama 还导致 6 项相关社交任务中的 5 项得到改善，这表明指令调整可以带来普遍的社会理解。所有资源，包括我们的代码、模型和数据集，都可以通过 bit.ly/socialitellama 找到。</li>
</ul>

<h3>Title: Self-Debiasing Large Language Models: Zero-Shot Recognition and  Reduction of Stereotypes</h3>
<ul>
<li><strong>Authors: </strong>Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Tong Yu, Hanieh Deilamsalehy, Ruiyi Zhang, Sungchul Kim, Franck Dernoncourt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01981">https://arxiv.org/abs/2402.01981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01981">https://arxiv.org/pdf/2402.01981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01981]] Self-Debiasing Large Language Models: Zero-Shot Recognition and  Reduction of Stereotypes(https://arxiv.org/abs/2402.01981)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在语言生成和理解方面显示出显着的进步，但也容易表现出有害的社会偏见。虽然对这些行为的识别已经产生了大量的偏差缓解技术，但大多数都需要修改训练数据、模型参数或解码策略，如果没有可训练模型，这可能是不可行的。在这项工作中，我们利用法学硕士的零样本功能来减少我们引入的零样本自偏置技术中的刻板印象。通过两种方法，即通过解释进行自我消除偏见和通过重新提示进行自我消除偏见，我们表明，自我消除偏见可以显着降低九个不同社会群体的刻板印象程度，同时仅依靠法学硕士本身和一个简单的提示，并通过解释正确识别无效的假设并重新提示以最大程度地减少偏差。我们希望这项工作能够开启对其他用于缓解偏差的零样本技术的探究。</li>
</ul>

<h3>Title: A Novel Hyperdimensional Computing Framework for Online Time Series  Forecasting on the Edge</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Mejri, Chandramouli Amarnath, Abhijit Chatterjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01999">https://arxiv.org/abs/2402.01999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01999">https://arxiv.org/pdf/2402.01999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01999]] A Novel Hyperdimensional Computing Framework for Online Time Series  Forecasting on the Edge(https://arxiv.org/abs/2402.01999)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>In recent years, both online and offline deep learning models have been developed for time series forecasting. However, offline deep forecasting models fail to adapt effectively to changes in time-series data, while online deep forecasting models are often expensive and have complex training procedures. In this paper, we reframe the online nonlinear time-series forecasting problem as one of linear hyperdimensional time-series forecasting. Nonlinear low-dimensional time-series data is mapped to high-dimensional (hyperdimensional) spaces for linear hyperdimensional prediction, allowing fast, efficient and lightweight online time-series forecasting. Our framework, TSF-HD, adapts to time-series distribution shifts using a novel co-training framework for its hyperdimensional mapping and its linear hyperdimensional predictor. TSF-HD is shown to outperform the state of the art, while having reduced inference latency, for both short-term and long-term time series forecasting. Our code is publicly available at this http URL</li>
<li><strong>摘要：</strong>近年来，在线和离线深度学习模型都被开发用于时间序列预测。然而，离线深度预测模型无法有效适应时间序列数据的变化，而在线深度预测模型通常成本高昂且训练过程复杂。在本文中，我们将在线非线性时间序列预测问题重新定义为线性超维时间序列预测问题之一。将非线性低维时间序列数据映射到高维（超维）空间进行线性超维预测，实现快速、高效、轻量级的在线时间序列预测。我们的框架 TSF-HD 使用一种新颖的协同训练框架来适应时间序列分布的变化，用于其超维映射和线性超维预测器。对于短期和长期时间序列预测，TSF-HD 的性能优于最先进的技术，同时减少了推理延迟。我们的代码可通过此 http URL 公开获取</li>
</ul>

<h3>Title: Topology-Informed Graph Transformer</h3>
<ul>
<li><strong>Authors: </strong>Yun Young Choi, Sun Woo Park, Minho Lee, Youngho Woo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02005">https://arxiv.org/abs/2402.02005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02005">https://arxiv.org/pdf/2402.02005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02005]] Topology-Informed Graph Transformer(https://arxiv.org/abs/2402.02005)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Transformers have revolutionized performance in Natural Language Processing and Vision, paving the way for their integration with Graph Neural Networks (GNNs). One key challenge in enhancing graph transformers is strengthening the discriminative power of distinguishing isomorphisms of graphs, which plays a crucial role in boosting their predictive performances. To address this challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel transformer enhancing both discriminative power in detecting graph isomorphisms and the overall performance of Graph Transformers. TIGT consists of four components: A topological positional embedding layer using non-isomorphic universal covers based on cyclic subgraphs of graphs to ensure unique graph representation: A dual-path message-passing layer to explicitly encode topological characteristics throughout the encoder layers: A global attention mechanism: And a graph information layer to recalibrate channel-wise graph features for better feature representation. TIGT outperforms previous Graph Transformers in classifying synthetic dataset aimed at distinguishing isomorphism classes of graphs. Additionally, mathematical analysis and empirical evaluations highlight our model's competitive edge over state-of-the-art Graph Transformers across various benchmark datasets.</li>
<li><strong>摘要：</strong>Transformer 彻底改变了自然语言处理和视觉的性能，为它们与图神经网络 (GNN) 的集成铺平了道路。增强图转换器的一个关键挑战是增强区分图同构的判别能力，这对于提高其预测性能起着至关重要的作用。为了应对这一挑战，我们引入了“拓扑信息图变换器（TIGT）”，这是一种新颖的变换器，增强了检测图同构的判别能力和图变换器的整体性能。 TIGT 由四个组件组成： 拓扑位置嵌入层，使用基于图的循环子图的非同构通用覆盖来确保唯一的图表示： 双路径消息传递层，用于在整个编码器层中显式编码拓扑特征： 全局注意力机制：以及一个图形信息层，用于重新校准通道方式的图形特征，以获得更好的特征表示。 TIGT 在对旨在区分图的同构类的合成数据集进行分类方面优于以前的图转换器。此外，数学分析和实证评估凸显了我们的模型在各种基准数据集上相对于最先进的图形转换器的竞争优势。</li>
</ul>

<h3>Title: PresAIse, An Enterprises Prescriptive AI Solution</h3>
<ul>
<li><strong>Authors: </strong>Wei Sun, Scott McFaddin, Linh Ha Tran, Shivaram Subramanian, Kristjan Greenewald, Yeshi Tenzin, Zack Xue, Youssef Drissi, Markus Ettl</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02006">https://arxiv.org/abs/2402.02006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02006">https://arxiv.org/pdf/2402.02006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02006]] PresAIse, An Enterprises Prescriptive AI Solution(https://arxiv.org/abs/2402.02006)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>Prescriptive AI represents a transformative shift in decision-making, offering causal insights and actionable recommendations. Despite its huge potential, enterprise adoption often faces several challenges. The first challenge is caused by the limitations of observational data for accurate causal inference which is typically a prerequisite for good decision-making. The second pertains to the interpretability of recommendations, which is crucial for enterprise decision-making settings. The third challenge is the silos between data scientists and business users, hindering effective collaboration. This paper outlines an initiative from IBM Research, aiming to address some of these challenges by offering a suite of prescriptive AI solutions. Leveraging insights from various research papers, the solution suite includes scalable causal inference methods, interpretable decision-making approaches, and the integration of large language models (LLMs) to bridge communication gaps via a conversation agent. A proof-of-concept, PresAIse, demonstrates the solutions' potential by enabling non-ML experts to interact with prescriptive AI models via a natural language interface, democratizing advanced analytics for strategic decision-making.</li>
<li><strong>摘要：</strong>规范性人工智能代表了决策的变革性转变，提供因果洞察和可行的建议。尽管潜力巨大，但企业采用往往面临一些挑战。第一个挑战是由于准确因果推断的观测数据的局限性造成的，这通常是良好决策的先决条件。第二个涉及建议的可解释性，这对于企业决策设置至关重要。第三个挑战是数据科学家和业务用户之间的孤岛，阻碍了有效的协作。本文概述了 IBM Research 的一项计划，旨在通过提供一套规范的 AI 解决方案来应对其中一些挑战。该解决方案套件利用各种研究论文的见解，包括可扩展的因果推理方法、可解释的决策方法以及大型语言模型 (LLM) 的集成，以通过对话代理弥合沟通差距。 PresAIse 是一项概念验证，它使非机器学习专家能够通过自然语言界面与规定的人工智能模型进行交互，从而使战略决策的高级分析民主化，从而展示了该解决方案的潜力。</li>
</ul>

<h3>Title: How well do LLMs cite relevant medical references? An evaluation  framework and analyses</h3>
<ul>
<li><strong>Authors: </strong>Kevin Wu, Eric Wu, Ally Cassasola, Angela Zhang, Kevin Wei, Teresa Nguyen, Sith Riantawan, Patricia Shi Riantawan, Daniel E. Ho, James Zou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02008">https://arxiv.org/abs/2402.02008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02008">https://arxiv.org/pdf/2402.02008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02008]] How well do LLMs cite relevant medical references? An evaluation  framework and analyses(https://arxiv.org/abs/2402.02008)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are currently being used to answer medical questions across a variety of clinical domains. Recent top-performing commercial LLMs, in particular, are also capable of citing sources to support their responses. In this paper, we ask: do the sources that LLMs generate actually support the claims that they make? To answer this, we propose three contributions. First, as expert medical annotations are an expensive and time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is highly accurate in validating source relevance, agreeing 88% of the time with a panel of medical doctors. Second, we develop an end-to-end, automated pipeline called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs on a dataset of 1200 generated questions, totaling over 40K pairs of statements and sources. Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with retrieval augmented generation (RAG) and find that, even still, around 30\% of individual statements are unsupported, while nearly half of its responses are not fully supported. Third, we open-source our curated dataset of medical questions and expert annotations for future evaluations. Given the rapid pace of LLM development and the potential harms of incorrect or outdated medical information, it is crucial to also understand and quantify their capability to produce relevant, trustworthy medical references.</li>
<li><strong>摘要：</strong>大语言模型（LLM）目前被用来回答各个临床领域的医学问题。特别是最近表现最好的商业法学硕士也能够引用消息来源来支持他们的回应。在本文中，我们要问：法学硕士生成的来源实际上支持他们所做的主张吗？为了回答这个问题，我们提出了三项贡献。首先，由于专家医学注释是可扩展评估的昂贵且耗时的瓶颈，因此我们证明 GPT-4 在验证源相关性方面非常准确，88% 的时间与医生小组达成一致。其次，我们开发了一个名为 \textit{SourceCheckup} 的端到端自动化管道，并使用它在包含 1200 个生成问题的数据集（总计超过 40K 对语句和来源）上评估五个表现最佳的 LLM。有趣的是，我们发现大约 50% 到 90% 的 LLM 回复没有得到他们提供的来源的完全支持。我们还使用检索增强生成 (RAG) 来评估 GPT-4，发现即使如此，仍有约 30% 的个人陈述不受支持，而其近一半的响应并未得到完全支持。第三，我们开源了我们精心策划的医学问题数据集和专家注释，以供未来评估。鉴于法学硕士发展的快速步伐以及不正确或过时的医疗信息的潜在危害，了解和量化其提供相关的、值得信赖的医疗参考的能力至关重要。</li>
</ul>

<h3>Title: Position Paper: The Landscape and Challenges of HPC Research and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Le Chen, Nesreen K. Ahmed, Akash Dutta, Arijit Bhattacharjee, Sixing Yu, Quazi Ishtiaque Mahmud, Waqwoya Abebe, Hung Phan, Aishwarya Sarkar, Branden Butler, Niranjan Hasabnis, Gal Oren, Vy A. Vo, Juan Pablo Munoz, Theodore L. Willke, Tim Mattson, Ali Jannesari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02018">https://arxiv.org/abs/2402.02018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02018">https://arxiv.org/pdf/2402.02018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02018]] Position Paper: The Landscape and Challenges of HPC Research and LLMs(https://arxiv.org/abs/2402.02018)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>Recently, language models (LMs), especially large language models (LLMs), have revolutionized the field of deep learning. Both encoder-decoder models and prompt-based techniques have shown immense potential for natural language processing and code-based tasks. Over the past several years, many research labs and institutions have invested heavily in high-performance computing, approaching or breaching exascale performance levels. In this paper, we posit that adapting and utilizing such language model-based techniques for tasks in high-performance computing (HPC) would be very beneficial. This study presents our reasoning behind the aforementioned position and highlights how existing ideas can be improved and adapted for HPC tasks.</li>
<li><strong>摘要：</strong>最近，语言模型（LM），特别是大型语言模型（LLM），已经彻底改变了深度学习领域。编码器-解码器模型和基于提示的技术都在自然语言处理和基于代码的任务方面显示出了巨大的潜力。在过去的几年中，许多研究实验室和机构在高性能计算方面投入了大量资金，接近或突破了百亿亿次性能水平。在本文中，我们认为针对高性能计算（HPC）中的任务调整和利用这种基于语言模型的技术将非常有益。这项研究提出了我们上述立场背后的推理，并强调了如何改进现有想法并使其适应 HPC 任务。</li>
</ul>

<h3>Title: Transfer Learning in ECG Diagnosis: Is It Effective?</h3>
<ul>
<li><strong>Authors: </strong>Cuong V. Nguyen, Cuong D.Do</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02021">https://arxiv.org/abs/2402.02021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02021">https://arxiv.org/pdf/2402.02021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02021]] Transfer Learning in ECG Diagnosis: Is It Effective?(https://arxiv.org/abs/2402.02021)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The adoption of deep learning in ECG diagnosis is often hindered by the scarcity of large, well-labeled datasets in real-world scenarios, leading to the use of transfer learning to leverage features learned from larger datasets. Yet the prevailing assumption that transfer learning consistently outperforms training from scratch has never been systematically validated. In this study, we conduct the first extensive empirical study on the effectiveness of transfer learning in multi-label ECG classification, by investigating comparing the fine-tuning performance with that of training from scratch, covering a variety of ECG datasets and deep neural networks. We confirm that fine-tuning is the preferable choice for small downstream datasets; however, when the dataset is sufficiently large, training from scratch can achieve comparable performance, albeit requiring a longer training time to catch up. Furthermore, we find that transfer learning exhibits better compatibility with convolutional neural networks than with recurrent neural networks, which are the two most prevalent architectures for time-series ECG applications. Our results underscore the importance of transfer learning in ECG diagnosis, yet depending on the amount of available data, researchers may opt not to use it, considering the non-negligible cost associated with pre-training.</li>
<li><strong>摘要：</strong>在心电图诊断中采用深度学习通常会受到现实场景中缺乏大型、标记良好的数据集的阻碍，从而导致使用迁移学习来利用从较大数据集中学到的特征。然而，迁移学习始终优于从头开始训练的普遍假设从未得到系统验证。在本研究中，我们通过研究比较微调性能与从头开始训练的性能，涵盖各种心电图数据集和深度神经网络，对多标签心电图分类中迁移学习的有效性进行了首次广泛的实证研究。我们确认微调是小型下游数据集的首选；然而，当数据集足够大时，从头开始训练可以达到相当的性能，尽管需要更长的训练时间才能赶上。此外，我们发现迁移学习与卷积神经网络比循环神经网络具有更好的兼容性，循环神经网络是时间序列心电图应用的两种最流行的架构。我们的结果强调了迁移学习在心电图诊断中的重要性，但根据可用数据的数量，考虑到与预训练相关的不可忽略的成本，研究人员可能选择不使用它。</li>
</ul>

<h3>Title: Self-Supervised Contrastive Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Junwoo Park, Daehoon Gwak, Jaegul Choo, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02023">https://arxiv.org/abs/2402.02023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02023">https://arxiv.org/pdf/2402.02023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02023]] Self-Supervised Contrastive Forecasting(https://arxiv.org/abs/2402.02023)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, especially in challenging scenarios that require a significantly long output for forecasting. Source code is available at https://github.com/junwoopark92/Self-Supervised-Contrastive-Forecsating.</li>
<li><strong>摘要：</strong>由于处理长序列的时间和内存复杂性，长期预测提出了独特的挑战。现有的方法依靠滑动窗口来处理长序列，很难有效地捕获部分捕获在短窗口内的长期变化（即外窗口变化）。在本文中，我们介绍了一种新颖的方法，通过采用对比学习和增强的分解架构来克服这一限制，该方法专门针对长期变化而设计。为此，我们的对比损失结合了整个时间序列中的全局自相关性，这有助于以自我监督的方式构建正负对。当与我们的分解网络相结合时，我们的对比学习显着提高了长期预测性能。大量实验表明，我们的方法在超过 9 个长期基准的多个实验中优于 14 个基线模型，特别是在需要长时间输出进行预测的具有挑战性的场景中。源代码可在 https://github.com/junwoopark92/Self-Supervised-Contrastive-Forecsating 获取。</li>
</ul>

<h3>Title: A Survey of Constraint Formulations in Safe Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Akifumi Wachi, Xun Shen, Yanan Sui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02025">https://arxiv.org/abs/2402.02025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02025">https://arxiv.org/pdf/2402.02025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02025]] A Survey of Constraint Formulations in Safe Reinforcement Learning(https://arxiv.org/abs/2402.02025)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Ensuring safety is critical when applying reinforcement learning (RL) to real-world problems. Consequently, safe RL emerges as a fundamental and powerful paradigm for safely optimizing an agent's policy from experimental data. A popular safe RL approach is based on a constrained criterion, which solves the problem of maximizing expected cumulative reward under safety constraints. Though there has been recently a surge of such attempts to achieve safety in RL, a systematic understanding of the field is difficult due to 1) the diversity of constraint representations and 2) little discussion of their interrelations. To address this knowledge gap, we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms specifically designed for each formulation. Furthermore, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current state and future directions of safe reinforcement learning research.</li>
<li><strong>摘要：</strong>将强化学习 (RL) 应用于现实问题时，确保安全至关重要。因此，安全强化学习成为根据实验数据安全优化代理策略的基本且强大的范例。流行的安全强化学习方法基于约束准则，它解决了在安全约束下最大化预期累积奖励的问题。尽管最近出现了大量在强化学习中实现安全性的尝试，但由于 1）约束表示的多样性以及 2）对其相互关系的讨论很少，因此很难系统地理解该领域。为了解决这一知识差距，我们对代表性约束公式进行了全面审查，并精选了专门为每个公式设计的算法。此外，我们阐明了揭示常见问题表述之间数学相互关系的理论基础。最后我们讨论了安全强化学习研究的现状和未来方向。</li>
</ul>

<h3>Title: Panacea: Pareto Alignment via Preference Adaptation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhong, Chengdong Ma, Xiaoyuan Zhang, Ziran Yang, Qingfu Zhang, Siyuan Qi, Yaodong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02030">https://arxiv.org/abs/2402.02030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02030">https://arxiv.org/pdf/2402.02030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02030]] Panacea: Pareto Alignment via Preference Adaptation for LLMs(https://arxiv.org/abs/2402.02030)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss aggregation methods under mild conditions. Moreover, our experiments demonstrate, for the first time, the feasibility of aligning a single LLM to represent a spectrum of human preferences through various optimization methods. Our work marks a step forward in effectively and efficiently aligning models to diverse and intricate human preferences in a controllable and Pareto-optimal manner.</li>
<li><strong>摘要：</strong>当前用于大语言模型对齐的方法通常使用标量人类偏好标签。然而，这种惯例往往过于简单化人类偏好的多维性和异质性，导致表达性降低甚至失调。本文提出了 Panacea，这是一种创新方法，将对齐重新构建为多维偏好优化问题。 Panacea 训练一个能够在线和帕累托最优地适应不同偏好的单一模型，而无需进一步调整。这里的一个主要挑战是使用低维偏好向量来指导模型的行为，尽管它受到绝大多数参数的控制。为了解决这个问题，Panacea 被设计为使用基于奇异值分解（SVD）的低秩自适应，它允许将偏好向量简单地作为奇异值在线注入。理论上，我们证明了 Panacea 在温和条件下可以通过常见的损失聚合方法恢复整个 Pareto 前沿。此外，我们的实验首次证明了通过各种优化方法调整单个法学硕士来代表一系列人类偏好的可行性。我们的工作标志着在以可控和帕累托最优的方式有效且高效地将模型与多样化和复杂的人类偏好相结合方面向前迈出了一步。</li>
</ul>

<h3>Title: Multi-fidelity physics constrained neural networks for dynamical systems</h3>
<ul>
<li><strong>Authors: </strong>Hao Zhou, Sibo Cheng, Rossella Arcucci</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02031">https://arxiv.org/abs/2402.02031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02031">https://arxiv.org/pdf/2402.02031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02031]] Multi-fidelity physics constrained neural networks for dynamical systems(https://arxiv.org/abs/2402.02031)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Physics-constrained neural networks are commonly employed to enhance prediction robustness compared to purely data-driven models, achieved through the inclusion of physical constraint losses during the model training process. However, one of the major challenges of physics-constrained neural networks consists of the training complexity especially for high-dimensional systems. In fact, conventional physics-constrained models rely on singular-fidelity data necessitating the assessment of physical constraints within high-dimensional fields, which introduces computational difficulties. Furthermore, due to the fixed input size of the neural networks, employing multi-fidelity training data can also be cumbersome. In this paper, we propose the Multi-Scale Physics-Constrained Neural Network (MSPCNN), which offers a novel methodology for incorporating data with different levels of fidelity into a unified latent space through a customised multi-fidelity autoencoder. Additionally, multiple decoders are concurrently trained to map latent representations of inputs into various fidelity physical spaces. As a result, during the training of predictive models, physical constraints can be evaluated within low-fidelity spaces, yielding a trade-off between training efficiency and accuracy. In addition, unlike conventional methods, MSPCNN also manages to employ multi-fidelity data to train the predictive model. We assess the performance of MSPCNN in two fluid dynamics problems, namely a two-dimensional Burgers' system and a shallow water system. Numerical results clearly demonstrate the enhancement of prediction accuracy and noise robustness when introducing physical constraints in low-fidelity fields. On the other hand, as expected, the training complexity can be significantly reduced by computing physical constraint loss in the low-fidelity field rather than the high-fidelity one.</li>
<li><strong>摘要：</strong>与纯数据驱动模型相比，物理约束神经网络通常用于增强预测鲁棒性，这是通过在模型训练过程中包含物理约束损失来实现的。然而，物理约束神经网络的主要挑战之一是训练复杂性，特别是对于高维系统。事实上，传统的物理约束模型依赖于奇异保真度数据，需要评估高维场内的物理约束，这带来了计算困难。此外，由于神经网络的输入大小固定，采用多保真度训练数据也可能很麻烦。在本文中，我们提出了多尺度物理约束神经网络（MSPCNN），它提供了一种新颖的方法，通过定制的多保真度自动编码器将不同保真度级别的数据合并到统一的潜在空间中。此外，同时训练多个解码器将输入的潜在表示映射到各种保真度物理空间中。因此，在预测模型的训练过程中，可以在低保真度空间内评估物理约束，从而在训练效率和准确性之间进行权衡。此外，与传统方法不同，MSPCNN 还设法采用多保真度数据来训练预测模型。我们评估了 MSPCNN 在两个流体动力学问题（即二维 Burgers 系统和浅水系统）中的性能。数值结果清楚地证明了在低保真场中引入物理约束时预测精度和噪声鲁棒性的增强。另一方面，正如预期的那样，通过计算低保真场而不是高保真场中的物理约束损失可以显着降低训练复杂度。</li>
</ul>

<h3>Title: RobustTSF: Towards Theory and Design of Robust Time Series Forecasting  with Anomalies</h3>
<ul>
<li><strong>Authors: </strong>Hao Cheng, Qingsong Wen, Yang Liu, Liang Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02032">https://arxiv.org/abs/2402.02032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02032">https://arxiv.org/pdf/2402.02032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02032]] RobustTSF: Towards Theory and Design of Robust Time Series Forecasting  with Anomalies(https://arxiv.org/abs/2402.02032)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Time series forecasting is an important and forefront task in many real-world applications. However, most of time series forecasting techniques assume that the training data is clean without anomalies. This assumption is unrealistic since the collected time series data can be contaminated in practice. The forecasting model will be inferior if it is directly trained by time series with anomalies. Thus it is essential to develop methods to automatically learn a robust forecasting model from the contaminated data. In this paper, we first statistically define three types of anomalies, then theoretically and experimentally analyze the loss robustness and sample robustness when these anomalies exist. Based on our analyses, we propose a simple and efficient algorithm to learn a robust forecasting model. Extensive experiments show that our method is highly robust and outperforms all existing approaches. The code is available at https://github.com/haochenglouis/RobustTSF.</li>
<li><strong>摘要：</strong>时间序列预测是许多现实应用中的一项重要且前沿的任务。然而，大多数时间序列预测技术都假设训练数据是干净的，没有异常。这种假设是不现实的，因为收集的时间序列数据在实践中可能会受到污染。如果直接用有异常的时间序列来训练，预测模型的效果会较差。因此，有必要开发从污染数据中自动学习稳健预测模型的方法。在本文中，我们首先统计定义了三类异常，然后从理论上和实验上分析了这些异常存在时的损失鲁棒性和样本鲁棒性。根据我们的分析，我们提出了一种简单而有效的算法来学习稳健的预测模型。大量的实验表明，我们的方法非常稳健，并且优于所有现有方法。代码可在 https://github.com/hao Chenglouis/RobustTSF 获取。</li>
</ul>

<h3>Title: Benchmark for CEC 2024 Competition on Multiparty Multiobjective  Optimization</h3>
<ul>
<li><strong>Authors: </strong>Wenjian Luo, Peilan Xu, Shengxiang Yang, Yuhui Shi</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02033">https://arxiv.org/abs/2402.02033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02033">https://arxiv.org/pdf/2402.02033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02033]] Benchmark for CEC 2024 Competition on Multiparty Multiobjective  Optimization(https://arxiv.org/abs/2402.02033)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The competition focuses on Multiparty Multiobjective Optimization Problems (MPMOPs), where multiple decision makers have conflicting objectives, as seen in applications like UAV path planning. Despite their importance, MPMOPs remain understudied in comparison to conventional multiobjective optimization. The competition aims to address this gap by encouraging researchers to explore tailored modeling approaches. The test suite comprises two parts: problems with common Pareto optimal solutions and Biparty Multiobjective UAV Path Planning (BPMO-UAVPP) problems with unknown solutions. Optimization algorithms for the first part are evaluated using Multiparty Inverted Generational Distance (MPIGD), and the second part is evaluated using Multiparty Hypervolume (MPHV) metrics. The average algorithm ranking across all problems serves as a performance benchmark.</li>
<li><strong>摘要：</strong>竞赛的重点是多方多目标优化问题 (MPMOP)，其中多个决策者的目标相互冲突，如无人机路径规划等应用中所示。尽管 MPMOP 很重要，但与传统的多目标优化相比，其研究仍然不足。该竞赛旨在通过鼓励研究人员探索量身定制的建模方法来弥补这一差距。该测试套件包括两部分：常见帕累托最优解的问题和未知解的双方多目标无人机路径规划（BPMO-UAVPP）问题。第一部分的优化算法使用多方倒代距离 (MPIGD) 进行评估，第二部分的优化算法使用多方超卷 (MPHV) 指标进行评估。所有问题的平均算法排名作为性能基准。</li>
</ul>

<h3>Title: Learning General Parameterized Policies for Infinite Horizon Average  Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Qinbo Bai, Washim Uddin Mondal, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02042">https://arxiv.org/abs/2402.02042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02042">https://arxiv.org/pdf/2402.02042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02042]] Learning General Parameterized Policies for Infinite Horizon Average  Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm(https://arxiv.org/abs/2402.02042)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demonstrate that our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and $\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.</li>
<li><strong>摘要：</strong>本文探讨了无限水平平均奖励约束马尔可夫决策过程（CMDP）的领域。据我们所知，这项工作是第一个深入研究具有一般政策参数化的平均奖励 CMDP 的遗憾和约束违规分析的工作。为了应对这一挑战，我们提出了一种原始的基于对偶的策略梯度算法，该算法能够熟练地管理约束，同时确保实现全局最优策略的低遗憾保证。特别是，我们证明了我们提出的算法实现了 $\tilde{\mathcal{O}}({T}^{3/4})$ 客观遗憾和 $\tilde{\mathcal{O}}({T}^ {3/4})$ 违反约束边界。</li>
</ul>

<h3>Title: A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data  Transmission</h3>
<ul>
<li><strong>Authors: </strong>Wenjun Huang, Arghavan Rezvani, Hanning Chen, Yang Ni, Sanggeon Yun, Sungheon Jeong, Mohsen Imani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02043">https://arxiv.org/abs/2402.02043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02043">https://arxiv.org/pdf/2402.02043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02043]] A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data  Transmission(https://arxiv.org/abs/2402.02043)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, rag</a></li>
<li><strong>Abstract: </strong>Applications in the Internet of Things (IoT) utilize machine learning to analyze sensor-generated data. However, a major challenge lies in the lack of targeted intelligence in current sensing systems, leading to vast data generation and increased computational and communication costs. To address this challenge, we propose a novel sensing module to equip sensing frameworks with intelligent data transmission capabilities by integrating a highly efficient machine learning model placed near the sensor. This model provides prompt feedback for the sensing system to transmit only valuable data while discarding irrelevant information by regulating the frequency of data transmission. The near-sensor model is quantized and optimized for real-time sensor control. To enhance the framework's performance, the training process is customized and a "lazy" sensor deactivation strategy utilizing temporal information is introduced. The suggested method is orthogonal to other IoT frameworks and can be considered as a plugin for selective data transmission. The framework is implemented, encompassing both software and hardware components. The experiments demonstrate that the framework utilizing the suggested module achieves over 85% system efficiency in terms of energy consumption and storage, with negligible impact on performance. This methodology has the potential to significantly reduce data output from sensors, benefiting a wide range of IoT applications.</li>
<li><strong>摘要：</strong>物联网 (IoT) 中的应用利用机器学习来分析传感器生成的数据。然而，一个主要挑战在于当前传感系统缺乏有针对性的智能，导致大量数据生成并增加计算和通信成本。为了应对这一挑战，我们提出了一种新颖的传感模块，通过集成放置在传感器附近的高效机器学习模型，为传感框架配备智能数据传输功能。该模型通过调节数据传输的频率，为传感系统提供及时反馈，仅传输有价值的数据，同时丢弃不相关的信息。近传感器模型经过量化和优化，可实现实时传感器控制。为了提高框架的性能，定制了训练过程，并引入了利用时间信息的“惰性”传感器停用策略。建议的方法与其他物联网框架正交，可以被视为选择性数据传输的插件。该框架已实施，包括软件和硬件组件。实验表明，使用建议模块的框架在能耗和存储方面实现了 85% 以上的系统效率，对性能的影响可以忽略不计。这种方法有可能显着减少传感器的数据输出，从而使广泛的物联网应用受益。</li>
</ul>

<h3>Title: Locally-Adaptive Quantization for Streaming Vector Search</h3>
<ul>
<li><strong>Authors: </strong>Cecilia Aguerrebere, Mark Hildebrand, Ishwar Singh Bhati, Theodore Willke, Mariano Tepper</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02044">https://arxiv.org/abs/2402.02044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02044">https://arxiv.org/pdf/2402.02044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02044]] Locally-Adaptive Quantization for Streaming Vector Search(https://arxiv.org/abs/2402.02044)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieving the most similar vector embeddings to a given query among a massive collection of vectors has long been a key component of countless real-world applications. The recently introduced Retrieval-Augmented Generation is one of the most prominent examples. For many of these applications, the database evolves over time by inserting new data and removing outdated data. In these cases, the retrieval problem is known as streaming similarity search. While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector compression method, yields state-of-the-art search performance for non-evolving databases, its usefulness in the streaming setting has not been yet established. In this work, we study LVQ in streaming similarity search. In support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and multi-means LVQ that boost its search performance by up to 28% and 27%, respectively. Our studies show that LVQ and its new variants enable blazing fast vector search, outperforming its closest competitor by up to 9.4x for identically distributed data and by up to 8.8x under the challenging scenario of data distribution shifts (i.e., where the statistical distribution of the data changes over time). We release our contributions as part of Scalable Vector Search, an open-source library for high-performance similarity search.</li>
<li><strong>摘要：</strong>在大量向量集合中检索与给定查询最相似的向量嵌入长期以来一直是无数现实世界应用程序的关键组成部分。最近推出的检索增强生成就是最突出的例子之一。对于许多这样的应用程序，数据库随着时间的推移通过插入新数据和删除过时的数据而不断发展。在这些情况下，检索问题称为流相似性搜索。虽然局部自适应矢量量化 (LVQ) 是一种高效矢量压缩方法，可为非演进数据库提供最先进的搜索性能，但其在流设置中的实用性尚未确立。在这项工作中，我们研究了流相似性搜索中的 LVQ。为了支持我们的评估，我们引入了 LVQ 的两项改进：Turbo LVQ 和多手段 LVQ，分别将其搜索性能提高了 28% 和 27%。我们的研究表明，LVQ 及其新变体能够实现极快的矢量搜索，对于相同分布的数据，其性能比最接近的竞争对手高出 9.4 倍，在数据分布变化的挑战性场景下（即，统计分布数据随时间变化）。我们将我们的贡献作为可扩展向量搜索的一部分发布，这是一个用于高性能相似性搜索的开源库。</li>
</ul>

<h3>Title: Nonlinear subspace clustering by functional link neural networks</h3>
<ul>
<li><strong>Authors: </strong>Long Shi, Lei Cao, Zhongpu Chen, Badong Chen, Yu Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02051">https://arxiv.org/abs/2402.02051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02051">https://arxiv.org/pdf/2402.02051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02051]] Nonlinear subspace clustering by functional link neural networks(https://arxiv.org/abs/2402.02051)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Nonlinear subspace clustering based on a feed-forward neural network has been demonstrated to provide better clustering accuracy than some advanced subspace clustering algorithms. While this approach demonstrates impressive outcomes, it involves a balance between effectiveness and computational cost. In this study, we employ a functional link neural network to transform data samples into a nonlinear domain. Subsequently, we acquire a self-representation matrix through a learning mechanism that builds upon the mapped samples. As the functional link neural network is a single-layer neural network, our proposed method achieves high computational efficiency while ensuring desirable clustering performance. By incorporating the local similarity regularization to enhance the grouping effect, our proposed method further improves the quality of the clustering results. Additionally, we introduce a convex combination subspace clustering scheme, which combining a linear subspace clustering method with the functional link neural network subspace clustering approach. This combination approach allows for a dynamic balance between linear and nonlinear representations. Extensive experiments confirm the advancement of our methods. The source code will be released on https://lshi91.github.io/ soon.</li>
<li><strong>摘要：</strong>基于前馈神经网络的非线性子空间聚类已被证明可以提供比某些高级子空间聚类算法更好的聚类精度。虽然这种方法展示了令人印象深刻的结果，但它涉及有效性和计算成本之间的平衡。在本研究中，我们采用功能链接神经网络将数据样本转换为非线性域。随后，我们通过基于映射样本的学习机制获得自我表示矩阵。由于功能链接神经网络是单层神经网络，我们提出的方法在保证理想的聚类性能的同时实现了高计算效率。通过结合局部相似性正则化来增强分组效果，我们提出的方法进一步提高了聚类结果的质量。此外，我们引入了凸组合子空间聚类方案，该方案将线性子空间聚类方法与函数链接神经网络子空间聚类方法相结合。这种组合方法允许线性和非线性表示之间的动态平衡。大量的实验证实了我们方法的进步。源代码即将在 https://lshi91.github.io/ 上发布。</li>
</ul>

<h3>Title: Affordable Generative Agents</h3>
<ul>
<li><strong>Authors: </strong>Yangbin Yu, Qin Zhang, Junyou Li, Qiang Fu, Deheng Ye</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02053">https://arxiv.org/abs/2402.02053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02053">https://arxiv.org/pdf/2402.02053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02053]] Affordable Generative Agents(https://arxiv.org/abs/2402.02053)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, agent</a></li>
<li><strong>Abstract: </strong>The emergence of large language models (LLMs) has significantly advanced the simulation of believable interactive agents. However, the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents. Therefore, in this paper, we develop Affordable Generative Agents (AGA), a framework for enabling the generation of believable and low-cost interactions on both agent-environment and inter-agents levels. Specifically, for agent-environment interactions, we substitute repetitive LLM inferences with learned policies; while for inter-agent interactions, we model the social relationships between agents and compress auxiliary dialogue information. Extensive experiments on multiple environments show the effectiveness and efficiency of our proposed framework. Also, we delve into the mechanisms of emergent believable behaviors lying in LLM agents, demonstrating that agents can only generate finite behaviors in fixed environments, based upon which, we understand ways to facilitate emergent interaction behaviors. Our code is publicly available at: \url{https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents}.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的出现极大地推进了可信交互代理的模拟。然而，维持长时间代理交互的巨大成本对部署可信的基于 LLM 的代理提出了挑战。因此，在本文中，我们开发了可负担的生成代理（AGA），这是一个框架，用于在代理环境和代理间级别上生成可信且低成本的交互。具体来说，对于主体与环境的交互，我们用学习的策略替代重复的 LLM 推论；而对于代理间交互，我们对代理之间的社会关系进行建模并压缩辅助对话信息。对多种环境的大量实验表明了我们提出的框架的有效性和效率。此外，我们深入研究了 LLM 智能体中紧急可信行为的机制，证明智能体只能在固定环境中生成有限行为，基于此，我们了解促进紧急交互行为的方法。我们的代码公开于：\url{https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents}。</li>
</ul>

<h3>Title: Neural Scaling Laws on Graphs</h3>
<ul>
<li><strong>Authors: </strong>Jingzhe Liu, Haitao Mao, Zhikai Chen, Tong Zhao, Neil Shah, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02054">https://arxiv.org/abs/2402.02054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02054">https://arxiv.org/pdf/2402.02054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02054]] Neural Scaling Laws on Graphs(https://arxiv.org/abs/2402.02054)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Deep graph models (e.g., graph neural networks and graph transformers) have become important techniques for leveraging knowledge across various types of graphs. Yet, the scaling properties of deep graph models have not been systematically investigated, casting doubt on the feasibility of achieving large graph models through enlarging the model and dataset sizes. In this work, we delve into neural scaling laws on graphs from both model and data perspectives. We first verify the validity of such laws on graphs, establishing formulations to describe the scaling behaviors. For model scaling, we investigate the phenomenon of scaling law collapse and identify overfitting as the potential reason. Moreover, we reveal that the model depth of deep graph models can impact the model scaling behaviors, which differ from observations in other domains such as CV and NLP. For data scaling, we suggest that the number of graphs can not effectively metric the graph data volume in scaling law since the sizes of different graphs are highly irregular. Instead, we reform the data scaling law with the number of edges as the metric to address the irregular graph sizes. We further demonstrate the reformed law offers a unified view of the data scaling behaviors for various fundamental graph tasks including node classification, link prediction, and graph classification. This work provides valuable insights into neural scaling laws on graphs, which can serve as an essential step toward large graph models.</li>
<li><strong>摘要：</strong>深度图模型（例如图神经网络和图转换器）已成为利用各种类型图的知识的重要技术。然而，深度图模型的缩放特性尚未得到系统研究，这使人们对通过扩大模型和数据集大小来实现大型图模型的可行性产生了怀疑。在这项工作中，我们从模型和数据的角度深入研究图上的神经缩放定律。我们首先在图上验证这些定律的有效性，建立公式来描述缩放行为。对于模型缩放，我们研究了缩放定律崩溃的现象，并将过度拟合确定为潜在原因。此外，我们发现深度图模型的模型深度会影响模型的缩放行为，这与 CV 和 NLP 等其他领域的观察结果不同。对于数据缩放，我们认为图的数量不能有效地衡量缩放定律中的图数据量，因为不同图的大小高度不规则。相反，我们以边数作为度量来改革数据缩放定律，以解决不规则的图大小问题。我们进一步证明，改革后的定律为各种基本图任务（包括节点分类、链接预测和图分类）提供了数据缩放行为的统一视图。这项工作为图的神经缩放定律提供了有价值的见解，这可以作为迈向大型图模型的重要一步。</li>
</ul>

<h3>Title: Variance Alignment Score: A Simple But Tough-to-Beat Data Selection  Method for Multimodal Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Yiping Wang, Yifang Chen, Wendan Yan, Kevin Jamieson, Simon Shaolei Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02055">https://arxiv.org/abs/2402.02055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02055">https://arxiv.org/pdf/2402.02055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02055]] Variance Alignment Score: A Simple But Tough-to-Beat Data Selection  Method for Multimodal Contrastive Learning(https://arxiv.org/abs/2402.02055)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>In recent years, data selection has emerged as a core issue for large-scale visual-language model pretraining, especially on noisy web-curated datasets. One widely adopted strategy assigns quality scores such as CLIP similarity for each sample and retains the data pairs with the highest scores. However, these approaches are agnostic of data distribution and always fail to select the most informative samples. To solve this problem, we propose a simple yet theoretically principled metric named Variance Alignment Score (VAS), which has the form $\langle \Sigma_{\text{test}}, \Sigma_i\rangle$. Here, $\Sigma_{\text{test}}$ represents the target (cross-)covariance matrix we aim to align, potentially based on prior knowledge, while $\Sigma_i$ denotes the tensor product of single or multi-modal representations for the $i$-th sample. We further design a new data selection method that maximizes the total VAS. We provide theoretical analysis in a simplified setting to demonstrate the theoretical advantage of VAS over random or other existing data selection. Experimentally, applying VAS and CLIP scores together can outperform baselines by a margin of $1.3\%$ average on 38 evaluation sets for noisy dataset DataComp and $2.5\%$ on VTAB for high-quality dataset CC12M. Additionally, our ablation study also shows visual features are better than text for calculating VAS, and the related classical experimental design methods may fail under this context.</li>
<li><strong>摘要：</strong>近年来，数据选择已成为大规模视觉语言模型预训练的核心问题，特别是在嘈杂的网络数据集上。一种广泛采用的策略为每个样本分配质量分数，例如 CLIP 相似性，并保留分数最高的数据对。然而，这些方法与数据分布无关，并且总是无法选择信息最丰富的样本。为了解决这个问题，我们提出了一个简单但理论上有原则的指标，称为方差对齐分数（VAS），其形式为$\langle \Sigma_{\text{test}}, \Sigma_i\rangle$。这里，$\Sigma_{\text{test}}$表示我们想要对齐的目标（交叉）协方差矩阵，可能基于先验知识，而$\Sigma_i$表示单模态或多模态表示的张量积第 $i$ 个样本。我们进一步设计了一种新的数据选择方法，可以最大化总 VAS。我们在简化的环境中提供理论分析，以证明 VAS 相对于随机或其他现有数据选择的理论优势。实验上，将 VAS 和 CLIP 评分一起应用，在噪声数据集 DataComp 的 38 个评估集上，平均优于基线 1.3\%$，在高质量数据集 CC12M 的 VTAB 上，平均优于基线 $2.5\%$。此外，我们的消融研究还表明，视觉特征比文本更适合计算 VAS，而相关的经典实验设计方法在这种情况下可能会失败。</li>
</ul>

<h3>Title: AnthroScore: A Computational Linguistic Measure of Anthropomorphism</h3>
<ul>
<li><strong>Authors: </strong>Myra Cheng, Kristina Gligoric, Tiziano Piccardi, Dan Jurafsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02056">https://arxiv.org/abs/2402.02056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02056">https://arxiv.org/pdf/2402.02056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02056]] AnthroScore: A Computational Linguistic Measure of Anthropomorphism(https://arxiv.org/abs/2402.02056)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Anthropomorphism, or the attribution of human-like characteristics to non-human entities, has shaped conversations about the impacts and possibilities of technology. We present AnthroScore, an automatic metric of implicit anthropomorphism in language. We use a masked language model to quantify how non-human entities are implicitly framed as human by the surrounding context. We show that AnthroScore corresponds with human judgments of anthropomorphism and dimensions of anthropomorphism described in social science literature. Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles. In research papers, we find that anthropomorphism has steadily increased over time, and that papers related to language models have the most anthropomorphism. Within ACL papers, temporal increases in anthropomorphism are correlated with key neural advancements. Building upon concerns of scientific misinformation in mass media, we identify higher levels of anthropomorphism in news headlines compared to the research papers they cite. Since AnthroScore is lexicon-free, it can be directly applied to a wide range of text sources.</li>
<li><strong>摘要：</strong>拟人化，或者将类人特征归因于非人类实体，已经影响了有关技术的影响和可能性的对话。我们提出了 AnthroScore，一种语言中隐含拟人化的自动度量。我们使用掩码语言模型来量化非人类实体如何被周围环境隐式地框定为人类。我们表明，AnthroScore 与人类对拟人化的判断以及社会科学文献中描述的拟人化维度相对应。出于对计算机科学话语中误导性拟人论的担忧，我们使用 AnthroScore 来分析 15 年的研究论文和下游新闻文章。在研究论文中，我们发现拟人化程度随着时间的推移而稳步增加，其中与语言模型相关的论文拟人化程度最高。在 ACL 论文中，拟人化的时间增加与关键的神经进步相关。基于对大众媒体中科学错误信息的担忧，我们发现新闻标题中的拟人化程度比他们引用的研究论文更高。由于 AnthroScore 不受词典影响，因此可以直接应用于广泛的文本源。</li>
</ul>

<h3>Title: Break the Sequential Dependency of LLM Inference Using Lookahead  Decoding</h3>
<ul>
<li><strong>Authors: </strong>Yichao Fu, Peter Bailis, Ion Stoica, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02057">https://arxiv.org/abs/2402.02057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02057">https://arxiv.org/pdf/2402.02057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02057]] Break the Sequential Dependency of LLM Inference Using Lookahead  Decoding(https://arxiv.org/abs/2402.02057)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的自回归解码受内存带宽限制，导致高延迟并严重浪费现代加速器的并行处理能力。现有的加速 LLM 解码的方法通常需要草稿模型（例如推测解码），而该模型的获取并不简单并且无法泛化。在本文中，我们介绍了 Lookahead 解码，这是一种精确的并行解码算法，可以在不需要辅助模型或数据存储的情况下加速 LLM 解码。它允许交易每步日志（FLOP）以减少总解码步骤数，在单个或多个现代加速器上更可并行化，并且与并发内存高效注意力（例如 FlashAttention）兼容。我们实施的 Lookahead 解码可在 MT-bench 上将自回归解码速度提高高达 1.8 倍，在代码完成任务中在多个 GPU 上可实现 4 倍的强大扩展。我们的代码可在 https://github.com/hao-ai-lab/LookaheadDecoding 获取</li>
</ul>

<h3>Title: Exploring the Robustness of Task-oriented Dialogue Systems for  Colloquial German Varieties</h3>
<ul>
<li><strong>Authors: </strong>Ekaterina Artemova, Verena Blaschke, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02078">https://arxiv.org/abs/2402.02078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02078">https://arxiv.org/pdf/2402.02078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02078]] Exploring the Robustness of Task-oriented Dialogue Systems for  Colloquial German Varieties(https://arxiv.org/abs/2402.02078)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Mainstream cross-lingual task-oriented dialogue (ToD) systems leverage the transfer learning paradigm by training a joint model for intent recognition and slot-filling in English and applying it, zero-shot, to other languages. We address a gap in prior research, which often overlooked the transfer to lower-resource colloquial varieties due to limited test data. Inspired by prior work on English varieties, we craft and manually evaluate perturbation rules that transform German sentences into colloquial forms and use them to synthesize test sets in four ToD datasets. Our perturbation rules cover 18 distinct language phenomena, enabling us to explore the impact of each perturbation on slot and intent performance. Using these new datasets, we conduct an experimental evaluation across six different transformers. Here, we demonstrate that when applied to colloquial varieties, ToD systems maintain their intent recognition performance, losing 6% (4.62 percentage points) in accuracy on average. However, they exhibit a significant drop in slot detection, with a decrease of 31% (21 percentage points) in slot F1 score. Our findings are further supported by a transfer experiment from Standard American English to synthetic Urban African American Vernacular English.</li>
<li><strong>摘要：</strong>主流跨语言面向任务的对话（ToD）系统通过训练英语意图识别和槽位填充的联合模型来利用迁移学习范式，并将其零样本地应用于其他语言。我们解决了先前研究中的一个空白，由于测试数据有限，这些研究经常忽视向资源较低的口语品种的转移。受到之前关于英语变体的研究的启发，我们制定并手动评估扰动规则，将德语句子转换为口语形式，并使用它们来合成四个 ToD 数据集中的测试集。我们的扰动规则涵盖 18 种不同的语言现象，使我们能够探索每种扰动对槽位和意图表现的影响。使用这些新数据集，我们对六个不同的变压器进行了实验评估。在这里，我们证明，当应用于口语变体时，ToD 系统保持其意图识别性能，但准确率平均下降 6%（4.62 个百分点）。然而，它们在槽位检测方面表现出显着下降，槽位 F1 分数下降了 31%（21 个百分点）。我们的研究结果得到了从标准美式英语到合成城市非裔美国白话英语的迁移实验的进一步支持。</li>
</ul>

<h3>Title: Risk-Sensitive Diffusion: Learning the Underlying Distribution from  Noisy Samples</h3>
<ul>
<li><strong>Authors: </strong>Yangming Li, Max Ruiz Luyten, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02081">https://arxiv.org/abs/2402.02081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02081">https://arxiv.org/pdf/2402.02081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02081]] Risk-Sensitive Diffusion: Learning the Underlying Distribution from  Noisy Samples(https://arxiv.org/abs/2402.02081)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>While achieving remarkable performances, we show that diffusion models are fragile to the presence of noisy samples, limiting their potential in the vast amount of settings where, unlike image synthesis, we are not blessed with clean data. Motivated by our finding that such fragility originates from the distribution gaps between noisy and clean samples along the diffusion process, we introduce risk-sensitive SDE, a stochastic differential equation that is parameterized by the risk (i.e., data "dirtiness") to adjust the distributions of noisy samples, reducing misguidance while benefiting from their contained information. The optimal expression for risk-sensitive SDE depends on the specific noise distribution, and we derive its parameterizations that minimize the misguidance of noisy samples for both Gaussian and general non-Gaussian perturbations. We conduct extensive experiments on both synthetic and real-world datasets (e.g., medical time series), showing that our model effectively recovers the clean data distribution from noisy samples, significantly outperforming conditional generation baselines.</li>
<li><strong>摘要：</strong>在实现卓越性能的同时，我们表明扩散模型对于噪声样本的存在很脆弱，限制了它们在大量设置中的潜力，与图像合成不同，我们没有干净的数据。我们发现这种脆弱性源于扩散过程中噪声样本和干净样本之间的分布差距，受此启发，我们引入了风险敏感的 SDE，这是一种随机微分方程，由风险（即数据“脏度”）参数化，以调整噪声样本的分布，减少误导，同时受益于其中包含的信息。风险敏感 SDE 的最佳表达式取决于特定的噪声分布，我们推导出其参数化，以最大限度地减少高斯和一般非高斯扰动的噪声样本的误导。我们对合成数据集和真实数据集（例如医疗时间序列）进行了广泛的实验，表明我们的模型有效地从噪声样本中恢复了干净的数据分布，显着优于条件生成基线。</li>
</ul>

<h3>Title: GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative  Decoding</h3>
<ul>
<li><strong>Authors: </strong>Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li, Shenggui Li, Kai Xu, Liqiang Nie, Zhaopeng Tu, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02082">https://arxiv.org/abs/2402.02082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02082">https://arxiv.org/pdf/2402.02082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02082]] GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative  Decoding(https://arxiv.org/abs/2402.02082)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, code, rag</a></li>
<li><strong>Abstract: </strong>Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models.</li>
<li><strong>摘要：</strong>推测性解码是一种相对较新的解码框架，它利用小型且高效的草稿模型来减少 LLM 的延迟。在这项研究中，我们引入了 GliDe 和 CaPE，这是对普通推测解码的两种简单修改，以进一步提高冻结 LLM 的解码速度。具体来说，GliDe 是一种修改后的草案模型架构，它重用目标 LLM 中的缓存键和值，而 CaPE 是一种提案扩展方法，它使用草案模型的置信度分数来帮助选择其他候选标记进行验证。对不同基准的大量实验表明，我们提出的 GliDe 草稿模型显着降低了预期的解码延迟。使用 walltime 进行的额外评估表明，GliDe 可以将 Vicuna 模型加速高达 2.17 倍，并使用 CaPE 将改进进一步扩大到 2.61 倍。我们将发布我们的代码、数据和经过训练的草稿模型。</li>
</ul>

<h3>Title: Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in  Multilingual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sara Rajaee, Christof Monz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02099">https://arxiv.org/abs/2402.02099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02099">https://arxiv.org/pdf/2402.02099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02099]] Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in  Multilingual Language Models(https://arxiv.org/abs/2402.02099)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks of existing cross-lingual test data and evaluation setups, calling for a more nuanced understanding of the cross-lingual capabilities of multilingual models.</li>
<li><strong>摘要：</strong>在大型数据集上训练多语言语言模型的最新进展似乎在跨语言知识转移方面取得了可喜的成果，并在下游任务上实现了高性能。然而，我们质疑当前的评估基准和设置在多大程度上准确测量零样本跨语言知识转移。在这项工作中，我们通过引入涉及多种语言实例的更具挑战性的设置来挑战目标任务上的高零样本性能反映了高跨语言能力的假设。通过大量的实验和分析，我们表明，所观察到的多语言模型的高性能很大程度上归因于不需要迁移实际语言知识的因素，例如任务级和表面级知识。更具体地说，我们观察到跨语言传输的内容主要是数据工件和偏差，特别是对于资源匮乏的语言。我们的研究结果强调了现有跨语言测试数据和评估设置被忽视的缺点，需要对多语言模型的跨语言能力有更细致的了解。</li>
</ul>

<h3>Title: Are Large Language Models Good Prompt Optimizers?</h3>
<ul>
<li><strong>Authors: </strong>Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02101">https://arxiv.org/abs/2402.02101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02101">https://arxiv.org/pdf/2402.02101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02101]] Are Large Language Models Good Prompt Optimizers?(https://arxiv.org/abs/2402.02101)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as Prompt Optimizers to self-reflect and refine prompts, has shown promising performance in recent studies. Despite the success, the underlying mechanism of this approach remains unexplored, and the true effectiveness of LLMs as Prompt Optimizers requires further validation. In this work, we conducted a comprehensive study to uncover the actual mechanism of LLM-based Prompt Optimization. Our findings reveal that the LLM optimizers struggle to identify the true causes of errors during reflection, tending to be biased by their own prior knowledge rather than genuinely reflecting on the errors. Furthermore, even when the reflection is semantically valid, the LLM optimizers often fail to generate appropriate prompts for the target models with a single prompt refinement step, partly due to the unpredictable behaviors of the target models. Based on the observations, we introduce a new "Automatic Behavior Optimization" paradigm, which directly optimizes the target model's behavior in a more controllable manner. We hope our study can inspire new directions for automatic prompt optimization development.</li>
<li><strong>摘要：</strong>基于LLM的自动提示优化通常利用LLM作为提示优化器来自我反思和完善提示，在最近的研究中表现出了良好的性能。尽管取得了成功，但这种方法的根本机制仍未被探索，法学硕士作为提示优化器的真正有效性需要进一步验证。在这项工作中，我们进行了全面的研究，以揭示基于 LLM 的提示优化的实际机制。我们的研究结果表明，LLM 优化者很难在反思过程中识别错误的真正原因，往往会受到自己先验知识的影响，而不是真正反思错误。此外，即使反射在语义上有效，LLM 优化器通常也无法通过单个提示细化步骤为目标模型生成适当的提示，部分原因是目标模型的行为不可预测。基于观察，我们引入了一种新的“自动行为优化”范式，它以更可控的方式直接优化目标模型的行为。我们希望我们的研究能够为自动提示优化开发提供新的方向。</li>
</ul>

<h3>Title: Learning Structure-Aware Representations of Dependent Types</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Kogkalidis, Orestis Melkonian, Jean-Philippe Bernardy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02104">https://arxiv.org/abs/2402.02104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02104">https://arxiv.org/pdf/2402.02104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02104]] Learning Structure-Aware Representations of Dependent Types(https://arxiv.org/abs/2402.02104)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Agda is a dependently-typed programming language and a proof assistant, pivotal in proof formalization and programming language theory. This paper extends the Agda ecosystem into machine learning territory, and, vice versa, makes Agda-related resources available to machine learning practitioners. We introduce and release a novel dataset of Agda program-proofs that is elaborate and extensive enough to support various machine learning applications -- the first of its kind. Leveraging the dataset's ultra-high resolution, detailing proof states at the sub-type level, we propose a novel neural architecture targeted at faithfully representing dependently-typed programs on the basis of structural rather than nominal principles. We instantiate and evaluate our architecture in a premise selection setup, where it achieves strong initial results.</li>
<li><strong>摘要：</strong>Agda 是一种依赖类型编程语言和证明助手，在证明形式化和编程语言理论中至关重要。本文将 Agda 生态系统扩展到机器学习领域，反之亦然，使 Agda 相关资源可供机器学习从业者使用。我们引入并发布了一个新颖的 Agda 程序证明数据集，该数据集足够详尽且广泛，足以支持各种机器学习应用程序——这在同类中尚属首例。利用数据集的超高分辨率，在子类型级别详细说明证明状态，我们提出了一种新颖的神经架构，旨在根据结构原则而不是名义原则忠实地表示依赖类型的程序。我们在前提选择设置中实例化和评估我们的架构，并取得了良好的初步结果。</li>
</ul>

<h3>Title: Composite Active Learning: Towards Multi-Domain Active Learning with  Theoretical Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Guang-Yuan Hao, Hengguan Huang, Haotian Wang, Jie Gao, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02110">https://arxiv.org/abs/2402.02110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02110">https://arxiv.org/pdf/2402.02110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02110]] Composite Active Learning: Towards Multi-Domain Active Learning with  Theoretical Guarantees(https://arxiv.org/abs/2402.02110)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Active learning (AL) aims to improve model performance within a fixed labeling budget by choosing the most informative data points to label. Existing AL focuses on the single-domain setting, where all data come from the same domain (e.g., the same dataset). However, many real-world tasks often involve multiple domains. For example, in visual recognition, it is often desirable to train an image classifier that works across different environments (e.g., different backgrounds), where images from each environment constitute one domain. Such a multi-domain AL setting is challenging for prior methods because they (1) ignore the similarity among different domains when assigning labeling budget and (2) fail to handle distribution shift of data across different domains. In this paper, we propose the first general method, dubbed composite active learning (CAL), for multi-domain AL. Our approach explicitly considers the domain-level and instance-level information in the problem; CAL first assigns domain-level budgets according to domain-level importance, which is estimated by optimizing an upper error bound that we develop; with the domain-level budgets, CAL then leverages a certain instance-level query strategy to select samples to label from each domain. Our theoretical analysis shows that our method achieves a better error bound compared to current AL methods. Our empirical results demonstrate that our approach significantly outperforms the state-of-the-art AL methods on both synthetic and real-world multi-domain datasets. Code is available at https://github.com/Wang-ML-Lab/multi-domain-active-learning.</li>
<li><strong>摘要：</strong>主动学习 (AL) 旨在通过选择信息最丰富的数据点进行标记，在固定的标记预算内提高模型性能。现有的 AL 侧重于单域设置，其中所有数据都来自同一域（例如同一数据集）。然而，许多现实世界的任务通常涉及多个领域。例如，在视觉识别中，通常需要训练一种在不同环境（例如不同背景）下工作的图像分类器，其中来自每个环境的图像构成一个域。这种多域 AL 设置对于现有方法来说是一个挑战，因为它们（1）在分配标签预算时忽略了不同域之间的相似性，（2）无法处理不同域之间数据的分布变化。在本文中，我们提出了第一个用于多域 AL 的通用方法，称为复合主动学习（CAL）。我们的方法明确考虑了问题中的域级和实例级信息； CAL 首先根据域级重要性分配域级预算，这是通过优化我们开发的误差上限来估计的；有了域级预算，CAL 然后利用特定的实例级查询策略从每个域中选择要标记的样本。我们的理论分析表明，与当前的 AL 方法相比，我们的方法实现了更好的误差范围。我们的实证结果表明，我们的方法在合成和真实多域数据集上都显着优于最先进的 AL 方法。代码可在 https://github.com/Wang-ML-Lab/multi-domain-active-learning 获取。</li>
</ul>

<h3>Title: Zero-shot Sentiment Analysis in Low-Resource Languages Using a  Multilingual Sentiment Lexicon</h3>
<ul>
<li><strong>Authors: </strong>Fajri Koto, Tilman Beck, Zeerak Talat, Iryna Gurevych, Timothy Baldwin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02113">https://arxiv.org/abs/2402.02113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02113">https://arxiv.org/pdf/2402.02113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02113]] Zero-shot Sentiment Analysis in Low-Resource Languages Using a  Multilingual Sentiment Lexicon(https://arxiv.org/abs/2402.02113)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, code</a></li>
<li><strong>Abstract: </strong>Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages. In this paper, we relax the reliance on texts in low-resource languages by using multilingual lexicons in pretraining to enhance multilingual capabilities. Specifically, we focus on zero-shot sentiment analysis tasks across 34 languages, including 6 high/medium-resource languages, 25 low-resource languages, and 3 code-switching datasets. We demonstrate that pretraining using multilingual lexicons, without using any sentence-level sentiment data, achieves superior zero-shot performance compared to models fine-tuned on English sentiment datasets, and large language models like GPT--3.5, BLOOMZ, and XGLM. These findings are observable for unseen low-resource languages to code-mixed scenarios involving high-resource languages.</li>
<li><strong>摘要：</strong>由于资源匮乏的语言缺乏大规模数据，提高这些语言的多语言模型能力通常很困难。在本文中，我们通过在预训练中使用多语言词典来增强多语言能力，从而放松对低资源语言文本的依赖。具体来说，我们专注于 34 种语言的零样本情感分析任务，包括 6 种高/中资源语言、25 种低资源语言和 3 个语码转换数据集。我们证明，与在英语情感数据集上微调的模型以及 GPT--3.5、BLOOMZ 和 XGLM 等大型语言模型相比，在不使用任何句子级情感数据的情况下使用多语言词典进行预训练可以实现卓越的零样本性能。对于看不见的低资源语言到涉及高资源语言的代码混合场景，这些发现是可以观察到的。</li>
</ul>

<h3>Title: Rendering Graphs for Graph Reasoning in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yanbin Wei, Shuai Fu, Weisen Jiang, James T. Kwok, Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02130">https://arxiv.org/abs/2402.02130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02130">https://arxiv.org/pdf/2402.02130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02130]] Rendering Graphs for Graph Reasoning in Multimodal Large Language Models(https://arxiv.org/abs/2402.02130)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on the training set achieve higher accuracy than the closed-source model GPT-4(V). We also study the effects of augmentations in graph reasoning.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）越来越多地用于具有图结构的各种任务，例如机器人规划、知识图补全和常识推理。尽管法学硕士可以理解文本格式的图形信息，但他们忽略了丰富的视觉形态，而视觉形态是人类理解结构信息和进行图形推理的直观方式。将图结构表示为视觉图像（即视觉图）的潜在好处和功能尚未被探索。在本文中，我们迈出了将视觉信息纳入图推理任务的第一步，并提出了一个新的基准 GITQA，其中每个样本都是一个元组（图、图像、文本描述）。我们使用最先进的多模式法学硕士对 GITQA 基准进行了广泛的实验。图形推理任务的结果表明，将文本和视觉信息结合在一起比单独使用一种模式表现得更好。此外，在训练集上进行微调的LLaVA-7B/13B模型比闭源模型GPT-4(V)获得了更高的精度。我们还研究了图形推理中增强的影响。</li>
</ul>

<h3>Title: Do Moral Judgment and Reasoning Capability of LLMs Change with Language?  A Study using the Multilingual Defining Issues Test</h3>
<ul>
<li><strong>Authors: </strong>Aditi Khandelwal, Utkarsh Agarwal, Kumar Tanmay, Monojit Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02135">https://arxiv.org/abs/2402.02135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02135">https://arxiv.org/pdf/2402.02135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02135]] Do Moral Judgment and Reasoning Capability of LLMs Change with Language?  A Study using the Multilingual Defining Issues Test(https://arxiv.org/abs/2402.02135)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>This paper explores the moral judgment and moral reasoning abilities exhibited by Large Language Models (LLMs) across languages through the Defining Issues Test. It is a well known fact that moral judgment depends on the language in which the question is asked. We extend the work of beyond English, to 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe three LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial multilingual text processing and generation abilities. Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages. The moral judgments too vary considerably by the language.</li>
<li><strong>摘要：</strong>本文通过定义问题测试探讨了跨语言的大型语言模型（LLM）所表现出的道德判断和道德推理能力。众所周知，道德判断取决于提出问题时使用的语言。我们将工作范围扩展到英语以外的 5 种新语言（中文、印地语、俄语、西班牙语和斯瓦希里语），并探讨了三个法学硕士——ChatGPT、GPT-4 和 Llama2Chat-70B——它们显示出强大的多语言文本处理和生成能力。我们的研究表明，与西班牙语、俄语、中文和英语相比，所有模型的道德推理能力（如后传统分数所示）印地语和斯瓦希里语的道德推理能力明显较差，而道德推理能力的表现没有明显的趋势。后四种语言。道德判断也因语言的不同而有很大差异。</li>
</ul>

<h3>Title: Probing Critical Learning Dynamics of PLMs for Hate Speech Detection</h3>
<ul>
<li><strong>Authors: </strong>Sarah Masud, Mohammad Aflah Khan, Vikram Goyal, Md Shad Akhtar, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02144">https://arxiv.org/abs/2402.02144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02144">https://arxiv.org/pdf/2402.02144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02144]] Probing Critical Learning Dynamics of PLMs for Hate Speech Detection(https://arxiv.org/abs/2402.02144)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite the widespread adoption, there is a lack of research into how various critical aspects of pretrained language models (PLMs) affect their performance in hate speech detection. Through five research questions, our findings and recommendations lay the groundwork for empirically investigating different aspects of PLMs' use in hate speech detection. We deep dive into comparing different pretrained models, evaluating their seed robustness, finetuning settings, and the impact of pretraining data collection time. Our analysis reveals early peaks for downstream tasks during pretraining, the limited benefit of employing a more recent pretraining corpus, and the significance of specific layers during finetuning. We further call into question the use of domain-specific models and highlight the need for dynamic datasets for benchmarking hate speech detection.</li>
<li><strong>摘要：</strong>尽管得到广泛采用，但仍缺乏关于预训练语言模型 (PLM) 的各个关键方面如何影响其在仇恨言论检测中的性能的研究。通过五个研究问题，我们的发现和建议为实证研究 PLM 在仇恨言论检测中使用的不同方面奠定了基础。我们深入研究了比较不同的预训练模型，评估它们的种子稳健性、微调设置以及预训练数据收集时间的影响。我们的分析揭示了预训练期间下游任务的早期峰值、使用更新的预训练语料库的有限好处以及微调期间特定层的重要性。我们进一步质疑特定领域模型的使用，并强调需要动态数据集来对仇恨言论检测进行基准测试。</li>
</ul>

<h3>Title: Analyzing Sentiment Polarity Reduction in News Presentation through  Contextual Perturbation and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alapan Kuila, Somnath Jena, Sudeshna Sarkar, Partha Pratim Chakrabarti</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02145">https://arxiv.org/abs/2402.02145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02145">https://arxiv.org/pdf/2402.02145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02145]] Analyzing Sentiment Polarity Reduction in News Presentation through  Contextual Perturbation and Large Language Models(https://arxiv.org/abs/2402.02145)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>In today's media landscape, where news outlets play a pivotal role in shaping public opinion, it is imperative to address the issue of sentiment manipulation within news text. News writers often inject their own biases and emotional language, which can distort the objectivity of reporting. This paper introduces a novel approach to tackle this problem by reducing the polarity of latent sentiments in news content. Drawing inspiration from adversarial attack-based sentence perturbation techniques and a prompt based method using ChatGPT, we employ transformation constraints to modify sentences while preserving their core semantics. Using three perturbation methods: replacement, insertion, and deletion coupled with a context-aware masked language model, we aim to maximize the desired sentiment score for targeted news aspects through a beam search algorithm. Our experiments and human evaluations demonstrate the effectiveness of these two models in achieving reduced sentiment polarity with minimal modifications while maintaining textual similarity, fluency, and grammatical correctness. Comparative analysis confirms the competitive performance of the adversarial attack based perturbation methods and prompt-based methods, offering a promising solution to foster more objective news reporting and combat emotional language bias in the media.</li>
<li><strong>摘要：</strong>在当今的媒体格局中，新闻媒体在塑造公众舆论方面发挥着关键作用，因此必须解决新闻文本中的情绪操纵问题。新闻撰稿人经常注入自己的偏见和情感语言，这可能会扭曲报道的客观性。本文介绍了一种通过减少新闻内容中潜在情绪的极性来解决此问题的新方法。受到基于对抗性攻击的句子扰动技术和使用 ChatGPT 的基于提示的方法的启发，我们采用转换约束来修改句子，同时保留其核心语义。使用三种扰动方法：替换、插入和删除，再加上上下文感知的掩码语言模型，我们的目标是通过波束搜索算法最大化目标新闻方面的期望情绪分数。我们的实验和人类评估证明了这两种模型在以最小的修改实现减少情感极性，同时保持文本相似性、流畅性和语法正确性方面的有效性。比较分析证实了基于对抗性攻击的扰动方法和基于提示的方法的竞争性能，为促进更客观的新闻报道和对抗媒体中的情感语言偏见提供了一个有前景的解决方案。</li>
</ul>

<h3>Title: Emergency Computing: An Adaptive Collaborative Inference Method Based on  Hierarchical Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Weiqi Fu, Lianming Xu, Xin Wu, Li Wang, Aiguo Fei</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02146">https://arxiv.org/abs/2402.02146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02146">https://arxiv.org/pdf/2402.02146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02146]] Emergency Computing: An Adaptive Collaborative Inference Method Based on  Hierarchical Reinforcement Learning(https://arxiv.org/abs/2402.02146)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>In achieving effective emergency response, the timely acquisition of environmental information, seamless command data transmission, and prompt decision-making are crucial. This necessitates the establishment of a resilient emergency communication dedicated network, capable of providing communication and sensing services even in the absence of basic infrastructure. In this paper, we propose an Emergency Network with Sensing, Communication, Computation, Caching, and Intelligence (E-SC3I). The framework incorporates mechanisms for emergency computing, caching, integrated communication and sensing, and intelligence empowerment. E-SC3I ensures rapid access to a large user base, reliable data transmission over unstable links, and dynamic network deployment in a changing environment. However, these advantages come at the cost of significant computation overhead. Therefore, we specifically concentrate on emergency computing and propose an adaptive collaborative inference method (ACIM) based on hierarchical reinforcement learning. Experimental results demonstrate our method's ability to achieve rapid inference of AI models with constrained computational and communication resources.</li>
<li><strong>摘要：</strong>为了实现有效的应急响应，环境信息的及时获取、指挥数据的无缝传输和及时决策至关重要。这就需要建立一个有弹性的应急通信专用网络，即使在缺乏基础设施的情况下也能够提供通信和传感服务。在本文中，我们提出了一种具有传感、通信、计算、缓存和智能功能的应急网络（E-SC3I）。该框架融合了应急计算、缓存、集成通信和传感以及情报赋能等机制。 E-SC3I确保快速接入庞大的用户群，在不稳定的链路上可靠地传输数据，并在变化的环境中实现动态的网络部署。然而，这些优势是以大量计算开销为代价的。因此，我们特别关注紧急计算，并提出了一种基于分层强化学习的自适应协作推理方法（ACIM）。实验结果证明我们的方法能够在计算和通信资源有限的情况下实现人工智能模型的快速推理。</li>
</ul>

<h3>Title: TSIS: A Supplementary Algorithm to t-SMILES for Fragment-based Molecular  Representation</h3>
<ul>
<li><strong>Authors: </strong>Juan-Ni Wu, Tong Wang, Li-Juan Tang, Hai-Long Wu, Ru-Qin Yu</a></li>
<li><strong>Subjects: </strong>cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02164">https://arxiv.org/abs/2402.02164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02164">https://arxiv.org/pdf/2402.02164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02164]] TSIS: A Supplementary Algorithm to t-SMILES for Fragment-based Molecular  Representation(https://arxiv.org/abs/2402.02164)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>String-based molecular representations, such as SMILES, are a de facto standard for linearly representing molecular information. However, the must be paired symbols and the parsing algorithm result in long grammatical dependencies, making it difficult for even state-of-the-art deep learning models to accurately comprehend the syntax and semantics. Although DeepSMILES and SELFIES have addressed certain limitations, they still struggle with advanced grammar, which makes some strings difficult to read. This study introduces a supplementary algorithm, TSIS (TSID Simplified), to t-SMILES family. Comparative experiments between TSIS and another fragment-based linear solution, SAFE, indicate that SAFE presents challenges in managing long-term dependencies in grammar. TSIS continues to use the tree defined in t-SMILES as its foundational data structure, which sets it apart from the SAFE model. The performance of TSIS models surpasses that of SAFE models, indicating that the tree structure of the t-SMILES family provides certain advantages.</li>
<li><strong>摘要：</strong>基于字符串的分子表示，例如 SMILES，是线性表示分子信息的事实上的标准。然而，必须配对的符号和解析算法导致了较长的语法依赖性，使得即使是最先进的深度学习模型也难以准确理解语法和语义。尽管 DeepSMILES 和 SELFIES 解决了某些限制，但它们仍然难以应对高级语法，这使得某些字符串难以阅读。本研究向 t-SMILES 系列引入了一种补充算法 TSIS（TSID Simplified）。 TSIS 和另一种基于片段的线性解决方案 SAFE 之间的比较实验表明，SAFE 在管理语法中的长期依赖性方面提出了挑战。 TSIS 继续使用 t-SMILES 中定义的树作为其基础数据结构，这使其与 SAFE 模型不同。 TSIS模型的性能超过了SAFE模型，表明t-SMILES族的树结构提供了一定的优势。</li>
</ul>

<h3>Title: Towards Optimal Adversarial Robust Q-learning with Bellman  Infinity-error</h3>
<ul>
<li><strong>Authors: </strong>Haoran Li, Zicheng Zhang, Wang Luo, Congying Han, Yudong Hu, Tiande Guo, Shichen Liao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02165">https://arxiv.org/abs/2402.02165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02165">https://arxiv.org/pdf/2402.02165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02165]] Towards Optimal Adversarial Robust Q-learning with Bellman  Infinity-error(https://arxiv.org/abs/2402.02165)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CAR-DQN) by minimizing a surrogate of Bellman Infinity-error. The top-tier performance of CAR-DQN across various benchmarks validates its practical effectiveness and reinforces the soundness of our theoretical analysis.</li>
<li><strong>摘要：</strong>建立强有力的策略对于反击影响深度强化学习（DRL）代理的攻击或干扰至关重要。最近的研究探讨了状态对抗的鲁棒性，并表明可能缺乏最优鲁棒政策（ORP），这对设置严格的鲁棒性约束提出了挑战。这项工作进一步研究了 ORP：首先，我们引入了政策的一致性假设（CAP），指出马尔可夫决策过程中的最优行动与微小扰动保持一致，并得到经验和理论证据的支持。在 CAP 的基础上，我们至关重要地证明了与贝尔曼最优策略一致的确定性和固定 ORP 的存在。此外，我们还说明了在最小化贝尔曼误差以获得 ORP 时 $L^{\infty}$-norm 的必要性。这一发现澄清了先前以 $L^{1}$-norm 为目标的贝尔曼最优策略的 DRL 算法的漏洞，并激励我们通过最小化贝尔曼无穷大的替代项来训练一致对抗鲁棒深度 Q 网络 (CAR-DQN) -错误。 CAR-DQN 在各种基准测试中的顶级性能验证了其实际有效性，并增强了我们理论分析的可靠性。</li>
</ul>

<h3>Title: One Graph Model for Cross-domain Dynamic Link Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xuanwen Huang, Wei Chow, Yang Wang, Ziwei Chai, Chunping Wang, Lei Chen, Yang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02168">https://arxiv.org/abs/2402.02168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02168">https://arxiv.org/pdf/2402.02168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02168]] One Graph Model for Cross-domain Dynamic Link Prediction(https://arxiv.org/abs/2402.02168)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>This work proposes DyExpert, a dynamic graph model for cross-domain link prediction. It can explicitly model historical evolving processes to learn the evolution pattern of a specific downstream graph and subsequently make pattern-specific link predictions. DyExpert adopts a decode-only transformer and is capable of efficiently parallel training and inference by \textit{conditioned link generation} that integrates both evolution modeling and link prediction. DyExpert is trained by extensive dynamic graphs across diverse domains, comprising 6M dynamic edges. Extensive experiments on eight untrained graphs demonstrate that DyExpert achieves state-of-the-art performance in cross-domain link prediction. Compared to the advanced baseline under the same setting, DyExpert achieves an average of 11.40% improvement Average Precision across eight graphs. More impressive, it surpasses the fully supervised performance of 8 advanced baselines on 6 untrained graphs.</li>
<li><strong>摘要：</strong>这项工作提出了 DyExpert，一种用于跨域链接预测的动态图模型。它可以显式地对历史演化过程进行建模，以学习特定下游图的演化模式，并随后进行特定于模式的链接预测。 DyExpert 采用仅解码转换器，能够通过集成了进化建模和链接预测的 \textit{条件链接生成} 进行高效的并行训练和推理。 DyExpert 通过跨不同领域的广泛动态图进行训练，包括 6M 动态边。对八个未经训练的图进行的大量实验表明，DyExpert 在跨域链接预测方面实现了最先进的性能。与相同设置下的高级基线相比，DyExpert 在八个图表中的平均精度平均提高了 11.40%。更令人印象深刻的是，它超越了 6 个未经训练的图上的 8 个高级基线的完全监督性能。</li>
</ul>

<h3>Title: Evolution Guided Generative Flow Networks</h3>
<ul>
<li><strong>Authors: </strong>Zarif Ikram, Ling Pan, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02186">https://arxiv.org/abs/2402.02186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02186">https://arxiv.org/pdf/2402.02186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02186]] Evolution Guided Generative Flow Networks(https://arxiv.org/abs/2402.02186)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Generative Flow Networks (GFlowNets) are a family of probabilistic generative models that learn to sample compositional objects proportional to their rewards. One big challenge of GFlowNets is training them effectively when dealing with long time horizons and sparse rewards. To address this, we propose Evolution guided generative flow networks (EGFN), a simple but powerful augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our method can work on top of any GFlowNets training objective, by training a set of agent parameters using EA, storing the resulting trajectories in the prioritized replay buffer, and training the GFlowNets agent using the stored trajectories. We present a thorough investigation over a wide range of toy and real-world benchmark tasks showing the effectiveness of our method in handling long trajectories and sparse rewards.</li>
<li><strong>摘要：</strong>生成流网络 (GFlowNets) 是一系列概率生成模型，可学习按其奖励比例对组合对象进行采样。 GFlowNet 的一大挑战是在处理长期时间范围和稀疏奖励时有效地训练它们。为了解决这个问题，我们提出了进化引导生成流网络（EGFN），这是使用进化算法（EA）对 GFlowNets 训练的简单但强大的增强。我们的方法可以在任何 GFlowNets 训练目标之上工作，通过使用 EA 训练一组代理参数，将生成的轨迹存储在优先重放缓冲区中，并使用存储的轨迹训练 GFlowNets 代理。我们对各种玩具和现实世界的基准任务进行了彻底的调查，显示了我们的方法在处理长轨迹和稀疏奖励方面的有效性。</li>
</ul>

<h3>Title: Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, Timothy Hospedales</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02207">https://arxiv.org/abs/2402.02207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02207">https://arxiv.org/pdf/2402.02207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02207]] Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large  Language Models(https://arxiv.org/abs/2402.02207)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks, which approach zero in many cases. The code and dataset are available at https://github.com/ys-zong/VLGuard.</li>
<li><strong>摘要：</strong>当前的视觉大语言模型（VLLM）表现出卓越的功能，但很容易生成有害内容，甚至很容易受到最简单的越狱攻击。我们的初步分析发现，这是由于视觉语言指令微调期间存在有害数据，并且 VLLM 微调可能会导致忘记基础 LLM 先前学到的安全对齐。为了解决这个问题，我们首先策划一个涵盖各种有害类别的视觉语言安全指令跟踪数据集 VLGuard。我们的实验表明，将该数据集集成到标准视觉语言微调中或利用它进行事后微调可以有效地安全调整 VLLM。这种一致性是在对模型的有用性影响最小、甚至增强的情况下实现的。我们的安全微调数据集的多功能性使其成为安全测试现有 VLLM、训练新模型或保护预先训练的 VLLM 的宝贵资源。实证结果表明，经过微调的 VLLM 可以有效拒绝不安全指令，并大幅降低多种黑盒对抗攻击的成功率，在许多情况下成功率接近于零。代码和数据集可在 https://github.com/ys-zong/VLGuard 获取。</li>
</ul>

<h3>Title: Query-decision Regression between Shortest Path and Minimum Steiner Tree</h3>
<ul>
<li><strong>Authors: </strong>Guangmo Tong, Peng Zhao, Mina Samizadeh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02211">https://arxiv.org/abs/2402.02211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02211">https://arxiv.org/pdf/2402.02211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02211]] Query-decision Regression between Shortest Path and Minimum Steiner Tree(https://arxiv.org/abs/2402.02211)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Considering a graph with unknown weights, can we find the shortest path for a pair of nodes if we know the minimal Steiner trees associated with some subset of nodes? That is, with respect to a fixed latent decision-making system (e.g., a weighted graph), we seek to solve one optimization problem (e.g., the shortest path problem) by leveraging information associated with another optimization problem (e.g., the minimal Steiner tree problem). In this paper, we study such a prototype problem called \textit{query-decision regression with task shifts}, focusing on the shortest path problem and the minimum Steiner tree problem. We provide theoretical insights regarding the design of realizable hypothesis spaces for building scoring models, and present two principled learning frameworks. Our experimental studies show that such problems can be solved to a decent extent with statistical significance.</li>
<li><strong>摘要：</strong>考虑一个权重未知的图，如果我们知道与某些节点子集关联的最小斯坦纳树，我们能否找到一对节点的最短路径？也就是说，对于固定的潜在决策系统（例如，加权图），我们寻求通过利用与另一个优化问题（例如，最小 Steiner 问题）相关的信息来解决一个优化问题（例如，最短路径问题）。树问题）。在本文中，我们研究了一个名为 \textit{带有任务转移的查询决策回归}的原型问题，重点关注最短路径问题和最小斯坦纳树问题。我们提供了有关设计可实现的假设空间以构建评分模型的理论见解，并提出了两个原则性的学习框架。我们的实验研究表明，此类问题可以在一定程度上得到解决，并具有统计学意义。</li>
</ul>

<h3>Title: A Data Generation Perspective to the Mechanism of In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Haitao Mao, Guangliang Liu, Yao Ma, Rongrong Wang, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02212">https://arxiv.org/abs/2402.02212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02212">https://arxiv.org/pdf/2402.02212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02212]] A Data Generation Perspective to the Mechanism of In-Context Learning(https://arxiv.org/abs/2402.02212)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>In-Context Learning (ICL) empowers Large Language Models (LLMs) with the capacity to learn in context, achieving downstream generalization without gradient updates but with a few in-context examples. Despite the encouraging empirical success, the underlying mechanism of ICL remains unclear, and existing research offers various viewpoints of understanding. These studies propose intuition-driven and ad-hoc technical solutions for interpreting ICL, illustrating an ambiguous road map. In this paper, we leverage a data generation perspective to reinterpret recent efforts and demonstrate the potential broader usage of popular technical solutions, approaching a systematic angle. For a conceptual definition, we rigorously adopt the terms of skill learning and skill recognition. The difference between them is skill learning can learn new data generation functions from in-context data. We also provide a comprehensive study on the merits and weaknesses of different solutions, and highlight the uniformity among them given the perspective of data generation, establishing a technical foundation for future research to incorporate the strengths of different lines of research.</li>
<li><strong>摘要：</strong>上下文学习 (ICL) 使大型语言模型 (LLM) 具有在上下文中学习的能力，无需梯度更新但只需一些上下文示例即可实现下游泛化。尽管取得了令人鼓舞的实证成功，但 ICL 的潜在机制仍不清楚，现有研究提供了多种理解观点。这些研究提出了用于解释 ICL 的直觉驱动和临时技术解决方案，阐明了模糊的路线图。在本文中，我们利用数据生成的角度来重新解释最近的努力，并从系统的角度展示流行技术解决方案的潜在更广泛用途。对于概念定义，我们严格采用技能学习和技能识别的术语。它们之间的区别在于技能学习可以从上下文数据中学习新的数据生成功能。我们还对不同解决方案的优缺点进行了全面的研究，并从数据生成的角度强调了它们之间的一致性，为未来研究融合​​不同研究领域的优势奠定了技术基础。</li>
</ul>

<h3>Title: Graph Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Michael Galkin, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02216">https://arxiv.org/abs/2402.02216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02216">https://arxiv.org/pdf/2402.02216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02216]] Graph Foundation Models(https://arxiv.org/abs/2402.02216)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Graph Foundation Model (GFM) is a new trending research topic in the graph domain, aiming to develop a graph model capable of generalizing across different graphs and tasks. However, a versatile GFM has not yet been achieved. The key challenge in building GFM is how to enable positive transfer across graphs with diverse structural patterns. Inspired by the existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, theoretical foundations, and stability. Such a vocabulary perspective can potentially advance the future GFM design following the neural scaling laws.</li>
<li><strong>摘要：</strong>图基础模型（GFM）是图领域的一个新的趋势研究主题，旨在开发一种能够泛化不同图和任务的图模型。然而，通用的 GFM 尚未实现。构建 GFM 的关键挑战是如何实现具有不同结构模式的图之间的正迁移。受到 CV 和 NLP 领域现有基础模型的启发，我们通过倡导“图词汇”，为 GFM 的发展提出了一种新的视角，其中图底层的基本可转移单元编码图上的不变性。我们从网络分析、理论基础和稳定性等基本方面奠定了图词汇构建的基础。这种词汇视角有可能推动未来遵循神经尺度法则的 GFM 设计。</li>
</ul>

<h3>Title: Rethinking the Starting Point: Enhancing Performance and Fairness of  Federated Learning via Collaborative Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Yun-Wei Chu, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02225">https://arxiv.org/abs/2402.02225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02225">https://arxiv.org/pdf/2402.02225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02225]] Rethinking the Starting Point: Enhancing Performance and Fairness of  Federated Learning via Collaborative Pre-Training(https://arxiv.org/abs/2402.02225)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model. Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL. In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task. The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task. CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations. Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average performance and more equitable predictions.</li>
<li><strong>摘要：</strong>大多数现有的联邦学习 (FL) 方法都假设训练是从随机初始化的模型开始的。最近，一些研究凭经验证明，利用预训练模型可以为 FL 提供有利的初始化。在本文中，我们提出了一种协作预训练方法 CoPreFL，该方法战略性地设计了一个预训练模型，为任何下游 FL 任务提供良好的初始化。我们的预训练算法的关键思想是模仿下游分布式场景的元学习过程，使其能够适应任何不可预见的 FL 任务。 CoPreFL 的预训练优化过程还在平均性能和公平性之间取得了平衡，旨在通过智能初始化解决下游 FL 任务中的这些竞争挑战。大量的实验结果验证了我们的预训练方法为任何看不见的下游 FL 任务提供了稳健的初始化，从而提高了平均性能和更公平的预测。</li>
</ul>

<h3>Title: Distributional Reduction: Unifying Dimensionality Reduction and  Clustering with Gromov-Wasserstein Projection</h3>
<ul>
<li><strong>Authors: </strong>Hugues Van Assel, Cédric Vincent-Cuaz, Nicolas Courty, Rémi Flamary, Pascal Frossard, Titouan Vayer</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02239">https://arxiv.org/abs/2402.02239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02239">https://arxiv.org/pdf/2402.02239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02239]] Distributional Reduction: Unifying Dimensionality Reduction and  Clustering with Gromov-Wasserstein Projection(https://arxiv.org/abs/2402.02239)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Unsupervised learning aims to capture the underlying structure of potentially large and high-dimensional datasets. Traditionally, this involves using dimensionality reduction methods to project data onto interpretable spaces or organizing points into meaningful clusters. In practice, these methods are used sequentially, without guaranteeing that the clustering aligns well with the conducted dimensionality reduction. In this work, we offer a fresh perspective: that of distributions. Leveraging tools from optimal transport, particularly the Gromov-Wasserstein distance, we unify clustering and dimensionality reduction into a single framework called distributional reduction. This allows us to jointly address clustering and dimensionality reduction with a single optimization problem. Through comprehensive experiments, we highlight the versatility and interpretability of our method and show that it outperforms existing approaches across a variety of image and genomics datasets.</li>
<li><strong>摘要：</strong>无监督学习旨在捕获潜在的大型和高维数据集的底层结构。传统上，这涉及使用降维方法将数据投影到可解释的空间或将点组织成有意义的簇。在实践中，这些方法是按顺序使用的，但不能保证聚类与所进行的降维很好地对齐。在这项工作中，我们提供了一个新的视角：分布的视角。利用最佳传输工具，特别是 Gromov-Wasserstein 距离，我们将聚类和降维统一到一个称为分布缩减的单一框架中。这使我们能够通过单个优化问题联合解决聚类和降维问题。通过全面的实验，我们强调了我们的方法的多功能性和可解释性，并表明它在各种图像和基因组数据集上优于现有方法。</li>
</ul>

<h3>Title: Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding</h3>
<ul>
<li><strong>Authors: </strong>Stevan Harnad</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02243">https://arxiv.org/abs/2402.02243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02243">https://arxiv.org/pdf/2402.02243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02243]] Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding(https://arxiv.org/abs/2402.02243)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Apart from what (little) OpenAI may be concealing from us, we all know (roughly) how ChatGPT works (its huge text database, its statistics, its vector representations, and their huge number of parameters, its next-word training, and so on). But none of us can say (hand on heart) that we are not surprised by what ChatGPT has proved to be able to do with these resources. This has even driven some of us to conclude that ChatGPT actually understands. It is not true that it understands. But it is also not true that we understand how it can do what it can do. I will suggest some hunches about benign biases: convergent constraints that emerge at LLM scale that may be helping ChatGPT do so much better than we would have expected. These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings. These convergent biases are related to (1) the parasitism of indirect verbal grounding on direct sensorimotor grounding, (2) the circularity of verbal definition, (3) the mirroring of language production and comprehension, (4) iconicity in propositions at LLM scale, (5) computational counterparts of human categorical perception in category learning by neural nets, and perhaps also (6) a conjecture by Chomsky about the laws of thought. The exposition will be in the form of a dialogue with ChatGPT-4.</li>
<li><strong>摘要：</strong>除了（小）OpenAI 可能向我们隐瞒的内容之外，我们（大致）都知道 ChatGPT 是如何工作的（其庞大的文本数据库、统计数据、向量表示及其大量参数、下一个单词训练等等）在）。但我们谁都不能说（真心实意）我们对 ChatGPT 已证明能够利用这些资源做的事情并不感到惊讶。这甚至促使我们中的一些人得出这样的结论：ChatGPT 确实能够理解。它所理解的并不真实。但我们也不了解它如何做到它能做到的事情。我将提出一些关于良性偏差的预感：LLM 规模出现的收敛约束可能会帮助 ChatGPT 做得比我们预期的好得多。在法学硕士规模上，这些偏见是语言本身本质所固有的，它们与 ChatGPT 所缺乏的东西密切相关，即直接的感觉运动基础，将其单词与其所指对象以及其命题与其含义联系起来。这些趋同性偏差与（1）间接言语基础对直接感觉运动基础的寄生有关，（2）言语定义的循环性，（3）语言产生和理解的镜像，（4）法学硕士规模命题的象似性， （5）神经网络类别学习中人类类别感知的计算对应物，也许还有（6）乔姆斯基关于思维定律的猜想。展示将以与ChatGPT-4对话的形式进行。</li>
</ul>

<h3>Title: Beyond the Limits: A Survey of Techniques to Extend the Context Length  in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, Armaghan Eshaghi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02244">https://arxiv.org/abs/2402.02244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02244">https://arxiv.org/pdf/2402.02244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02244]] Beyond the Limits: A Survey of Techniques to Extend the Context Length  in Large Language Models(https://arxiv.org/abs/2402.02244)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.</li>
<li><strong>摘要：</strong>最近，大型语言模型（LLM）表现出了非凡的能力，包括理解上下文、进行逻辑推理和生成响应。然而，这是以严格的计算和内存要求为代价实现的，阻碍了它们有效支持长输入序列的能力。这项调查对旨在延长法学硕士序列长度的最新技术和方法进行了包容性回顾，从而增强了他们的长上下文理解能力。特别是，我们回顾和分类了广泛的技术，包括架构修改，例如修改的位置编码和改变的注意机制，这些技术旨在增强较长序列的处理，同时避免计算要求的成比例增加。本研究中研究的多种方法可以在法学硕士的不同阶段（即训练、微调和推理）中利用。这使得法学硕士能够有效地处理扩展序列。最后一节讨论了当前方法的局限性以及对未来研究方向的建议，强调了序列长度在法学硕士持续发展中的重要性。</li>
</ul>

<h3>Title: Frequency Explains the Inverse Correlation of Large Language Models'  Size, Training Data Amount, and Surprisal's Fit to Reading Times</h3>
<ul>
<li><strong>Authors: </strong>Byung-Doh Oh, Shisen Yue, William Schuler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02255">https://arxiv.org/abs/2402.02255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02255">https://arxiv.org/pdf/2402.02255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02255]] Frequency Explains the Inverse Correlation of Large Language Models'  Size, Training Data Amount, and Surprisal's Fit to Reading Times(https://arxiv.org/abs/2402.02255)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models' surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.</li>
<li><strong>摘要：</strong>最近的研究表明，随着基于 Transformer 的语言模型变得越来越大，并且接受大量数据的训练，它们对自然人类阅读时间的惊人估计的拟合度会降低。当前的工作提出了一系列分析，表明词频是这两种趋势背后的关键解释因素。首先，四个语料库上四个语言模型家族的残差表明，模型大小和阅读时间拟合之间的负相关性在最不频繁单词的子集中最强，这是由较大模型变体的过度准确预测驱动的。此外，训练动态表明，在后续训练步骤中，所有模型变体都会学习预测稀有单词，并且较大的模型变体会更准确地预测稀有单词，这解释了训练数据量和模型大小对阅读时间拟合的不利影响。最后，特征归因分析表明，与较小的模型变体相比，较大的模型变体能够基于有效更长的上下文窗口大小以及更强的局部关联来准确预测稀有词。总而言之，这些结果表明，基于 Transformer 的语言模型的意外估计与人类的预期有所不同，因为它们在预测稀有单词时学习了超人复杂的关联。</li>
</ul>

<h3>Title: MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly  Mixed Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Yatong Bai, Mo Zhou, Vishal M. Patel, Somayeh Sojoudi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02263">https://arxiv.org/abs/2402.02263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02263">https://arxiv.org/pdf/2402.02263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02263]] MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly  Mixed Classifiers(https://arxiv.org/abs/2402.02263)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom strong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and near-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points, sacrificing merely 0.87 points in robust accuracy.</li>
<li><strong>摘要：</strong>对抗性的鲁棒性通常以准确性下降为代价，阻碍了鲁棒分类模型的实际应用。为了更好地权衡而基于训练的解决方案受到与已训练的高性能大型模型的不兼容性的限制，因此需要探索免训练的集成方法。观察到鲁棒模型对干净数据和对抗性数据的正确预测比错误预测更有信心，我们推测放大这种“良性置信属性”可以协调集成环境中的准确性和鲁棒性。为了实现这一目标，我们提出了“MixedNUTS”，这是一种免训练方法，其中鲁棒分类器和标准非鲁棒分类器的输出逻辑通过仅三个参数的非线性变换进行处理，并通过有效的算法进行优化。然后 MixedNUTS 将转换后的 logits 转换为概率并将它们混合作为总体输出。在 CIFAR-10、CIFAR-100 和 ImageNet 数据集上，使用自定义强自适应攻击的实验结果表明，MixedNUTS 大大提高了准确性和接近 SOTA 的鲁棒性——它将 CIFAR-100 的干净精度提高了 7.86 点，而鲁棒精度仅牺牲了 0.87 点。</li>
</ul>

<h3>Title: Federated Learning with New Knowledge: Fundamentals, Advances, and  Futures</h3>
<ul>
<li><strong>Authors: </strong>Lixu Wang, Yang Zhao, Jiahua Dong, Ating Yin, Qinbin Li, Xiao Wang, Dusit Niyato, Qi Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02268">https://arxiv.org/abs/2402.02268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02268">https://arxiv.org/pdf/2402.02268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02268]] Federated Learning with New Knowledge: Fundamentals, Advances, and  Futures(https://arxiv.org/abs/2402.02268)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a privacy-preserving distributed learning approach that is rapidly developing in an era where privacy protection is increasingly valued. It is this rapid development trend, along with the continuous emergence of new demands for FL in the real world, that prompts us to focus on a very important problem: Federated Learning with New Knowledge. The primary challenge here is to effectively incorporate various new knowledge into existing FL systems and evolve these systems to reduce costs, extend their lifespan, and facilitate sustainable development. In this paper, we systematically define the main sources of new knowledge in FL, including new features, tasks, models, and algorithms. For each source, we thoroughly analyze and discuss how to incorporate new knowledge into existing FL systems and examine the impact of the form and timing of new knowledge arrival on the incorporation process. Furthermore, we comprehensively discuss the potential future directions for FL with new knowledge, considering a variety of factors such as scenario setups, efficiency, and security. There is also a continuously updating repository for this topic: https://github.com/conditionWang/FLNK.</li>
<li><strong>摘要：</strong>联邦学习（FL）是一种保护隐私的分布式学习方法，在隐私保护日益受到重视的时代迅速发展。正是这种快速的发展趋势，以及现实世界中对FL新需求的不断出现，促使我们关注一个非常重要的问题：新知识的联邦学习。这里的主要挑战是有效地将各种新知识融入现有的 FL 系统并改进这些系统以降低成本、延长其使用寿命并促进可持续发展。在本文中，我们系统地定义了 FL 新知识的主要来源，包括新特征、任务、模型和算法。对于每个来源，我们深入分析和讨论如何将新知识整合到现有的 FL 系统中，并检查新知识到达的形式和时间对整合过程的影响。此外，我们综合考虑了场景设置、效率和安全性等各种因素，利用新知识全面讨论了 FL 的潜在未来方向。该主题还有一个不断更新的存储库：https://github.com/conditionWang/FLNK。</li>
</ul>

<h3>Title: SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing  Applications using a Generative Approach</h3>
<ul>
<li><strong>Authors: </strong>Tianshi Wang, Jinyang Li, Ruijie Wang, Denizhan Kara, Shengzhong Liu, Davis Wertheimer, Antoni Viros-i-Martin, Raghu Ganti, Mudhakar Srivatsa, Tarek Abdelzaher</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02275">https://arxiv.org/abs/2402.02275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02275">https://arxiv.org/pdf/2402.02275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02275]] SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing  Applications using a Generative Approach(https://arxiv.org/abs/2402.02275)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>This paper introduces SudokuSens, a generative framework for automated generation of training data in machine-learning-based Internet-of-Things (IoT) applications, such that the generated synthetic data mimic experimental configurations not encountered during actual sensor data collection. The framework improves the robustness of resulting deep learning models, and is intended for IoT applications where data collection is expensive. The work is motivated by the fact that IoT time-series data entangle the signatures of observed objects with the confounding intrinsic properties of the surrounding environment and the dynamic environmental disturbances experienced. To incorporate sufficient diversity into the IoT training data, one therefore needs to consider a combinatorial explosion of training cases that are multiplicative in the number of objects considered and the possible environmental conditions in which such objects may be encountered. Our framework substantially reduces these multiplicative training needs. To decouple object signatures from environmental conditions, we employ a Conditional Variational Autoencoder (CVAE) that allows us to reduce data collection needs from multiplicative to (nearly) linear, while synthetically generating (data for) the missing conditions. To obtain robustness with respect to dynamic disturbances, a session-aware temporal contrastive learning approach is taken. Integrating the aforementioned two approaches, SudokuSens significantly improves the robustness of deep learning for IoT applications. We explore the degree to which SudokuSens benefits downstream inference tasks in different data sets and discuss conditions under which the approach is particularly effective.</li>
<li><strong>摘要：</strong>本文介绍了 SudokuSens，这是一种在基于机器学习的物联网 (IoT) 应用中自动生成训练数据的生成框架，使得生成的合成数据模拟实际传感器数据收集期间未遇到的实验配置。该框架提高了深度学习模型的稳健性，适用于数据收集成本高昂的物联网应用。这项工作的动机是物联网时间序列数据将观察到的物体的特征与周围环境的混杂的内在属性和经历的动态环境干扰纠缠在一起。因此，为了将足够的多样性纳入物联网训练数据，需要考虑训练案例的组合爆炸，这些案例的数量与所考虑的对象数量以及可能遇到这些对象的可能环境条件成倍增加。我们的框架大大减少了这些乘法培训需求。为了将对象签名与环境条件解耦，我们采用了条件变分自动编码器（CVAE），它使我们能够将数据收集需求从乘法减少到（接近）线性，同时综合生成缺失条件（的数据）。为了获得针对动态干扰的鲁棒性，采用了会话​​感知的时间对比学习方法。 SudokuSens 集成了上述两种方法，显着提高了物联网应用深度学习的鲁棒性。我们探讨了 SudokuSens 在不同数据集中对下游推理任务的好处程度，并讨论了该方法特别有效的条件。</li>
</ul>

<h3>Title: Causal Bayesian Optimization via Exogenous Distribution Learning</h3>
<ul>
<li><strong>Authors: </strong>Shaogang Ren, Xiaoning Qian</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02277">https://arxiv.org/abs/2402.02277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02277">https://arxiv.org/pdf/2402.02277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02277]] Causal Bayesian Optimization via Exogenous Distribution Learning(https://arxiv.org/abs/2402.02277)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods. Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is developed by leveraging the learned exogenous distribution. Experiments on different datasets and applications show the benefits of our proposed method.</li>
<li><strong>摘要：</strong>在结构化因果模型中最大化目标变量作为操作目标是一个重要问题。现有的因果贝叶斯优化（CBO）方法要么依赖于改变因果结构以最大化奖励的硬干预；要么依赖于改变因果结构的硬干预来最大化奖励；或者向内生变量引入动作节点，从而调整数据生成机制以达到目标。本文提出了一种新方法来学习外生变量的分布，而现有方法通常会通过期望忽略或边缘化外生变量的分布。外生分布学习提高了替代模型中结构化因果模型的近似精度，该替代模型通常使用有限的观测数据进行训练。此外，学习到的外生分布将现有的 CBO 扩展到加性噪声模型 (ANM) 之外的一般因果方案。外生变量的恢复使我们能够对噪声或未观察到的隐藏变量使用更灵活的先验。利用学习到的外生分布开发了一种新的 CBO 方法。对不同数据集和应用程序的实验表明了我们提出的方法的优点。</li>
</ul>

<h3>Title: SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State  Tracking</h3>
<ul>
<li><strong>Authors: </strong>Atharva Kulkarni, Bo-Hsiang Tseng, Joel Ruben Antony Moniz, Dhivya Piraviperumal, Hong Yu, Shruti Bhargava</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02285">https://arxiv.org/abs/2402.02285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02285">https://arxiv.org/pdf/2402.02285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02285]] SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State  Tracking(https://arxiv.org/abs/2402.02285)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\method} results in $4-5%$ improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1 and 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of the performance compared to the few-shot setup using human-annotated training data. Our synthetic data and code can be accessed at https://github.com/apple/ml-synthdst</li>
<li><strong>摘要：</strong>使用大型语言模型 (LLM) 进行上下文学习已成为对话状态跟踪 (DST) 的一个有前途的研究途径。然而，表现最好的上下文学习方法涉及检索类似的示例并将其添加到提示中，需要访问标记的训练数据。为广泛的领域和应用程序获取此类训练数据既耗时又昂贵，有时甚至是不可行的。虽然零样本学习不需要训练数据，但它明显落后于少样本设置。因此，“\textit{我们能否有效地为任何对话模式生成合成数据以实现少量提示？}”为了解决这个问题，我们提出 \method，这是一种利用 LLM 为 DST 量身定制的数据生成框架。我们的方法只需要对话模式和一些手工制作的对话模板即可通过 DST 注释合成自然、连贯且自由流动的对话。在 MultiWOZ 2.1 和 2.4 上，使用来自 {\method} 的数据进行少样本学习，联合目标准确度比零样本基线提高了 4-5%$。值得注意的是，与使用人工注释训练数据的少样本设置相比，我们的少样本学习方法恢复了近 98% 的性能。我们的合成数据和代码可以在 https://github.com/apple/ml-synthdst 访问</li>
</ul>

<h3>Title: SemPool: Simple, robust, and interpretable KG pooling for enhancing  language models</h3>
<ul>
<li><strong>Authors: </strong>Costas Mavromatis, Petros Karypis, George Karypis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02289">https://arxiv.org/abs/2402.02289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02289">https://arxiv.org/pdf/2402.02289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02289]] SemPool: Simple, robust, and interpretable KG pooling for enhancing  language models(https://arxiv.org/abs/2402.02289)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Knowledge Graph (KG) powered question answering (QA) performs complex reasoning over language semantics as well as knowledge facts. Graph Neural Networks (GNNs) learn to aggregate information from the underlying KG, which is combined with Language Models (LMs) for effective reasoning with the given question. However, GNN-based methods for QA rely on the graph information of the candidate answer nodes, which limits their effectiveness in more challenging settings where critical answer information is not included in the KG. We propose a simple graph pooling approach that learns useful semantics of the KG that can aid the LM's reasoning and that its effectiveness is robust under graph perturbations. Our method, termed SemPool, represents KG facts with pre-trained LMs, learns to aggregate their semantic information, and fuses it at different layers of the LM. Our experimental results show that SemPool outperforms state-of-the-art GNN-based methods by 2.27% accuracy points on average when answer information is missing from the KG. In addition, SemPool offers interpretability on what type of graph information is fused at different LM layers.</li>
<li><strong>摘要：</strong>知识图 (KG) 支持的问答 (QA) 对语言语义和知识事实执行复杂的推理。图神经网络 (GNN) 学习从底层知识图谱中聚合信息，并与语言模型 (LM) 相结合，对给定问题进行有效推理。然而，基于 GNN 的 QA 方法依赖于候选答案节点的图形信息，这限制了它们在知识图谱中不包含关键答案信息的更具挑战性的环境中的有效性。我们提出了一种简单的图池化方法，该方法可以学习 KG 的有用语义，从而帮助 LM 的推理，并且其有效性在图扰动下具有鲁棒性。我们的方法称为 SemPool，用预先训练的 LM 表示 KG 事实，学习聚合其语义信息，并将其融合在 LM 的不同层。我们的实验结果表明，当知识图谱中缺少答案信息时，SemPool 的准确率平均比最先进的基于 GNN 的方法高出 2.27%。此外，SemPool 还提供了不同 LM 层融合的图信息类型的可解释性。</li>
</ul>

<h3>Title: Jailbreaking Attack against Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, Rong Jin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02309">https://arxiv.org/abs/2402.02309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02309">https://arxiv.org/pdf/2402.02309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02309]] Jailbreaking Attack against Multimodal Large Language Model(https://arxiv.org/abs/2402.02309)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>This paper focuses on jailbreaking attacks against multi-modal large language models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to harmful user queries. A maximum likelihood-based algorithm is proposed to find an \emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs across multiple unseen prompts and images (i.e., data-universal property). Our approach exhibits strong model-transferability, as the generated imgJP can be transferred to jailbreak various models, including MiniGPT-v2, LLaVA, InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we introduce a construction-based method to harness our approach for LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art methods. The code is available here. \textbf{Warning: some content generated by language models may be offensive to some readers.}</li>
<li><strong>摘要：</strong>本文重点关注针对多模式大语言模型 (MLLM) 的越狱攻击，试图引发 MLLM 对有害用户查询生成令人反感的响应。提出了一种基于最大似然的算法来查找 \emph{图像越狱提示} (imgJP)，从而能够跨多个未见的提示和图像（即数据通用属性）对 MLLM 进行越狱。我们的方法表现出很强的模型可转移性，因为生成的 imgJP 可以以黑盒方式转移到越狱各种模型，包括 MiniGPT-v2、LLaVA、InstructBLIP 和 mPLUG-Owl2。此外，我们还揭示了 MLLM 越狱和 LLM 越狱之间的联系。因此，我们引入了一种基于构造的方法来利用我们的法学硕士越狱方法，证明了比当前最先进的方法更高的效率。该代码可在此处获取。 \textbf{警告：语言模型生成的某些内容可能会冒犯某些读者。}</li>
</ul>

<h3>Title: Selecting Large Language Model to Fine-tune via Rectified Scaling Law</h3>
<ul>
<li><strong>Authors: </strong>Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen, Zihao Wang, Sujian Li, Jianzhu Ma, Xiaojun Wan, James Zou, Yitao Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02314">https://arxiv.org/abs/2402.02314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02314">https://arxiv.org/pdf/2402.02314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02314]] Selecting Large Language Model to Fine-tune via Rectified Scaling Law(https://arxiv.org/abs/2402.02314)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection.</li>
<li><strong>摘要：</strong>法学硕士生态系统的不断发展给如何在众多选择中选择最合适的预训练模型进行微调提出了挑战。由于资源有限，对所有模型进行微调并随后进行选择是不现实的。在这项工作中，我们将这种资源受限的选择任务表述为预测微调性能，并说明其与缩放定律的自然联系。与预训练不同，我们发现微调缩放曲线不仅包括众所周知的“功率阶段”，还包括之前未观察到的“功率前阶段”。我们还解释了为什么现有的比例定律无法从理论上和经验上捕捉这种相变现象。为了解决这个问题，我们将“预学习数据大小”的概念引入到我们的修正缩放定律中，这克服了理论限制并更好地拟合了实验结果。通过利用我们的定律，我们提出了一种新颖的 LLM 选择算法，该算法选择资源消耗少数百倍的接近最优模型，而其他方法可能提供负相关选择。</li>
</ul>

<h3>Title: A Survey of Large Language Models in Finance (FinLLMs)</h3>
<ul>
<li><strong>Authors: </strong>Jean Lee, Nicholas Stevens, Soyeon Caren Han, Minseok Song</a></li>
<li><strong>Subjects: </strong>cs.CL, q-fin.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02315">https://arxiv.org/abs/2402.02315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02315">https://arxiv.org/pdf/2402.02315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02315]] A Survey of Large Language Models in Finance (FinLLMs)(https://arxiv.org/abs/2402.02315)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities across a wide variety of Natural Language Processing (NLP) tasks and have attracted attention from multiple domains, including financial services. Despite the extensive research into general-domain LLMs, and their immense potential in finance, Financial LLM (FinLLM) research remains limited. This survey provides a comprehensive overview of FinLLMs, including their history, techniques, performance, and opportunities and challenges. Firstly, we present a chronological overview of general-domain Pre-trained Language Models (PLMs) through to current FinLLMs, including the GPT-series, selected open-source LLMs, and financial LMs. Secondly, we compare five techniques used across financial PLMs and FinLLMs, including training methods, training data, and fine-tuning methods. Thirdly, we summarize the performance evaluations of six benchmark tasks and datasets. In addition, we provide eight advanced financial NLP tasks and datasets for developing more sophisticated FinLLMs. Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency. To support AI research in finance, we compile a collection of accessible datasets and evaluation benchmarks on GitHub.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种自然语言处理 (NLP) 任务中表现出了卓越的能力，并吸引了包括金融服务在内的多个领域的关注。尽管对一般领域的法学硕士进行了广泛的研究，并且它们在金融领域具有巨大的潜力，但金融法学硕士（FinLLM）的研究仍然有限。本调查全面概述了 FinLLM，包括其历史、技术、绩效以及机遇和挑战。首先，我们按时间顺序概述了通用领域预训练语言模型 (PLM) 到当前的 FinLLM，包括 GPT 系列、选定的开源 LLM 和金融 LM。其次，我们比较了金融 PLM 和 FinLLM 中使用的五种技术，包括训练方法、训练数据和微调方法。第三，我们总结了六个基准任务和数据集的性能评估。此外，我们还提供八个高级金融 NLP 任务和数据集，用于开发更复杂的 FinLLM。最后，我们讨论 FinLLM 面临的机遇和挑战，例如幻觉、隐私和效率。为了支持金融领域的人工智能研究，我们在 GitHub 上编译了一系列可访问的数据集和评估基准。</li>
</ul>

<h3>Title: INViT: A Generalizable Routing Problem Solver with Invariant Nested View  Transformer</h3>
<ul>
<li><strong>Authors: </strong>Han Fang, Zhihao Song, Paul Weng, Yutong Ban</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02317">https://arxiv.org/abs/2402.02317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02317">https://arxiv.org/pdf/2402.02317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02317]] INViT: A Generalizable Routing Problem Solver with Invariant Nested View  Transformer(https://arxiv.org/abs/2402.02317)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Recently, deep reinforcement learning has shown promising results for learning fast heuristics to solve routing problems. Meanwhile, most of the solvers suffer from generalizing to an unseen distribution or distributions with different scales. To address this issue, we propose a novel architecture, called Invariant Nested View Transformer (INViT), which is designed to enforce a nested design together with invariant views inside the encoders to promote the generalizability of the learned solver. It applies a modified policy gradient algorithm enhanced with data augmentations. We demonstrate that the proposed INViT achieves a dominant generalization performance on both TSP and CVRP problems with various distributions and different problem scales.</li>
<li><strong>摘要：</strong>最近，深度强化学习在学习快速启发式方法来解决路由问题方面显示出了可喜的成果。同时，大多数求解器都面临着泛化到看不见的分布或不同尺度的分布的问题。为了解决这个问题，我们提出了一种新颖的架构，称为不变嵌套视图变换器（INViT），它旨在强制执行嵌套设计和编码器内的不变视图，以提高学习求解器的通用性。它应用了经过数据增强增强的修改后的策略梯度算法。我们证明了所提出的 INViT 在具有各种分布和不同问题规模的 TSP 和 CVRP 问题上实现了主导泛化性能。</li>
</ul>

<h3>Title: Diversity Measurement and Subset Selection for Instruction Tuning  Datasets</h3>
<ul>
<li><strong>Authors: </strong>Peiqi Wang, Yikang Shen, Zhen Guo, Matthew Stallone, Yoon Kim, Polina Golland, Rameswar Panda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02318">https://arxiv.org/abs/2402.02318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02318">https://arxiv.org/pdf/2402.02318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02318]] Diversity Measurement and Subset Selection for Instruction Tuning  Datasets(https://arxiv.org/abs/2402.02318)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We aim to select data subsets for the fine-tuning of large language models to more effectively follow instructions. Prior work has emphasized the importance of diversity in dataset curation but relied on heuristics such as the number of tasks. In this paper, we use determinantal point processes to capture the diversity and quality of instruction tuning datasets for subset selection. We propose to measure dataset diversity with log determinant distance that is the distance between the dataset of interest and a maximally diverse reference dataset. Our experiments demonstrate that the proposed diversity measure in the normalized weight gradient space is correlated with downstream instruction-following performance. Consequently, it can be used to inform when data selection is the most helpful and to analyze dataset curation strategies. We demonstrate the utility of our approach on various instruction tuning datasets.</li>
<li><strong>摘要：</strong>我们的目标是选择数据子集来微调大型语言模型，以更有效地遵循指令。之前的工作强调了数据集管理多样性的重要性，但依赖于任务数量等启发式方法。在本文中，我们使用行列式点过程来捕获用于子集选择的指令调整数据集的多样性和质量。我们建议用对数行列式距离来衡量数据集多样性，即感兴趣的数据集与最大多样性参考数据集之间的距离。我们的实验表明，所提出的归一化权重梯度空间中的多样性度量与下游指令跟踪性能相关。因此，它可用于告知数据选择何时最有帮助并分析数据集管理策略。我们展示了我们的方法在各种指令调整数据集上的实用性。</li>
</ul>

<h3>Title: Dynamic Incremental Optimization for Best Subset Selection</h3>
<ul>
<li><strong>Authors: </strong>Shaogang Ren, Xiaoning Qian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02322">https://arxiv.org/abs/2402.02322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02322">https://arxiv.org/pdf/2402.02322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02322]] Dynamic Incremental Optimization for Best Subset Selection(https://arxiv.org/abs/2402.02322)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.</li>
<li><strong>摘要：</strong>最佳子集选择被认为是许多稀疏学习问题的“黄金标准”。人们提出了多种优化技术来解决这种非光滑非凸问题。在本文中，我们研究了一系列 $\ell_0$ 正则化问题的双重形式。基于原问题结构和对偶问题结构，开发了一种高效的原对偶算法。通过利用双范围估计和增量策略，我们的算法潜在地减少了冗余计算并改进了最佳子集选择的解决方案。对合成数据集和现实数据集的理论分析和实验验证了所提出的解决方案的效率和统计特性。</li>
</ul>

<h3>Title: Enhance Reasoning for Large Language Models in the Game Werewolf</h3>
<ul>
<li><strong>Authors: </strong>Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, Haobo Fu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02330">https://arxiv.org/abs/2402.02330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02330">https://arxiv.org/pdf/2402.02330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02330]] Enhance Reasoning for Large Language Models in the Game Werewolf(https://arxiv.org/abs/2402.02330)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework's effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when integrated with the Thinker. This paper also contributes the largest dataset for social deduction games to date.</li>
<li><strong>摘要：</strong>本文提出了一种创新框架，将大型语言模型 (LLM) 与外部 Thinker 模块集成，以增强基于 LLM 的代理的推理能力。与通过即时工程增强法学硕士不同，Thinker 直接利用数据库中的知识并采用各种优化技术。该框架形成了一个推理层次结构，其中 LLM 处理直观的 System-1 任务，例如自然语言处理，而 Thinker 则专注于需要复杂逻辑分析和特定领域知识的认知 System-2 任务。我们的框架是使用需要双系统推理的 9 人狼人游戏来呈现的。我们引入了 LLM 和 Thinker 之间的通信协议，并使用 18800 次人类会话和强化学习的数据来训练 Thinker。实验证明了该框架在演绎推理、语音生成和在线游戏评估方面的有效性。此外，我们对 6B LLM 进行了微调，使其在与 Thinker 集成时超越 GPT4。这篇论文还贡献了迄今为止最大的社交推理游戏数据集。</li>
</ul>

<h3>Title: Minusformer: Improving Time Series Forecasting by Progressively Learning  Residuals</h3>
<ul>
<li><strong>Authors: </strong>Daojun Liang, Haixia Zhang, Dongfeng Yuan, Bingzheng Zhang, Minggao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02332">https://arxiv.org/abs/2402.02332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02332">https://arxiv.org/pdf/2402.02332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02332]] Minusformer: Improving Time Series Forecasting by Progressively Learning  Residuals(https://arxiv.org/abs/2402.02332)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this paper, we find that ubiquitous time series (TS) forecasting models are prone to severe overfitting. To cope with this problem, we embrace a de-redundancy approach to progressively reinstate the intrinsic values of TS for future intervals. Specifically, we renovate the vanilla Transformer by reorienting the information aggregation mechanism from addition to subtraction. Then, we incorporate an auxiliary output branch into each block of the original model to construct a highway leading to the ultimate prediction. The output of subsequent modules in this branch will subtract the previously learned results, enabling the model to learn the residuals of the supervision signal, layer by layer. This designing facilitates the learning-driven implicit progressive decomposition of the input and output streams, empowering the model with heightened versatility, interpretability, and resilience against overfitting. Since all aggregations in the model are minus signs, which is called Minusformer. Extensive experiments demonstrate the proposed method outperform existing state-of-the-art methods, yielding an average performance improvement of 11.9% across various datasets.</li>
<li><strong>摘要：</strong>在本文中，我们发现普遍存在的时间序列（TS）预测模型容易出现严重的过度拟合。为了解决这个问题，我们采用去冗余方法来逐步恢复未来间隔的 TS 的内在值。具体来说，我们通过将信息聚合机制从加法重新定位为减法来革新普通 Transformer。然后，我们将辅助输出分支合并到原始模型的每个块中，以构建一条通往最终预测的高速公路。该分支中后续模块的输出将减去先前学习的结果，使模型能够逐层学习监督信号的残差。这种设计促进了输入和输出流的学习驱动的隐式渐进分解，使模型具有更高的多功能性、可解释性和防止过度拟合的弹性。由于模型中所有聚合都是减号，因此称为 Minusformer。大量实验表明，所提出的方法优于现有最先进的方法，在各种数据集上平均性能提高了 11.9%。</li>
</ul>

<h3>Title: Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning</h3>
<ul>
<li><strong>Authors: </strong>Yi Cheng, Renjun Hu, Haochao Ying, Xing Shi, Jian Wu, Wei Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02334">https://arxiv.org/abs/2402.02334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02334">https://arxiv.org/pdf/2402.02334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02334]] Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning(https://arxiv.org/abs/2402.02334)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, code</a></li>
<li><strong>Abstract: </strong>Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive bias for deep learning on tabular data. Code is available at https://github.com/aigc-apps/AMFormer.</li>
<li><strong>摘要：</strong>直到最近，深层模型对表格数据的有效归纳偏差问题仍未得到解答。本文研究了算术特征交互对于深度表格学习是必要的假设。为了测试这一点，我们创建了一个具有温和特征交互假设的合成表格数据集，并检查了支持算术特征交互的修改后的转换器架构，称为 AMFormer。结果表明，AMFormer 在细粒度表格数据建模、训练数据效率和泛化方面优于强大的同类产品。这归因于其并行的加法和乘法注意算子以及基于提示的优化，这有助于在具有算术工程特征的扩展空间中分离表格样本。我们对真实世界数据进行的广泛实验也验证了 AMFormer 的一致有效性、效率和基本原理，表明它为表格数据的深度学习建立了强大的归纳偏差。代码可在 https://github.com/aigc-apps/AMFormer 获取。</li>
</ul>

<h3>Title: Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Fangzhao Zhang, Mert Pilanci</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02347">https://arxiv.org/abs/2402.02347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02347">https://arxiv.org/pdf/2402.02347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02347]] Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models(https://arxiv.org/abs/2402.02347)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora, code, rag</a></li>
<li><strong>Abstract: </strong>In this work we study the enhancement of Low Rank Adaptation (LoRA) fine-tuning procedure by introducing a Riemannian preconditioner in its optimization step. Specifically, we introduce an $r\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. This preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with our preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. Theoretically, we show that fine-tuning a two-layer ReLU network in the convex paramaterization with our preconditioner has convergence rate independent of condition number of the data matrix. This new Riemannian preconditioner, previously explored in classic low-rank matrix recovery, is introduced to deep learning tasks for the first time in our work. We release our code at https://github.com/pilancilab/Riemannian_Preconditioned_LoRA.</li>
<li><strong>摘要：</strong>在这项工作中，我们通过在优化步骤中引入黎曼预条件子来研究低秩适应（LoRA）微调过程的增强。具体来说，我们在每个梯度步骤中引入一个 $r\times r$ 预处理器，其中 $r$ 是 LoRA 等级。此预处理器需要对现有优化器代码进行少量更改，并产生几乎极小的存储和运行时开销。我们对大型语言模型和文本到图像扩散模型的实验结果表明，使用我们的预处理器，SGD 和 AdamW 的收敛性和可靠性可以显着增强。此外，训练过程对于学习率等超参数选择变得更加稳健。理论上，我们表明，使用我们的预处理器对凸参数化中的两层 ReLU 网络进行微调，其收敛速度与数据矩阵的条件数无关。这种新的黎曼预处理器之前在经典的低秩矩阵恢复中进行过探索，在我们的工作中首次引入深度学习任务。我们在 https://github.com/pilancilab/Riemannian_Preconditioned_LoRA 发布了我们的代码。</li>
</ul>

<h3>Title: A Paradigm for Potential Model Performance Improvement in Classification  and Regression Problems. A Proof of Concept</h3>
<ul>
<li><strong>Authors: </strong>Francisco Javier Lobo-Cabrera</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02354">https://arxiv.org/abs/2402.02354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02354">https://arxiv.org/pdf/2402.02354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02354]] A Paradigm for Potential Model Performance Improvement in Classification  and Regression Problems. A Proof of Concept(https://arxiv.org/abs/2402.02354)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>A methodology that seeks to enhance model prediction performance is presented. The method involves generating multiple auxiliary models that capture relationships between attributes as a function of each other. Such information serves to generate additional informative columns in the dataset that can potentially enhance target prediction. A proof of case and related code is provided.</li>
<li><strong>摘要：</strong>提出了一种旨在增强模型预测性能的方法。该方法涉及生成多个辅助模型，这些模型捕获属性之间的关系作为彼此的函数。此类信息用于在数据集中生成额外的信息列，从而有可能增强目标预测。提供案件证明和相关代码。</li>
</ul>

<h3>Title: Multi-modal Causal Structure Learning and Root Cause Analysis</h3>
<ul>
<li><strong>Authors: </strong>Lecheng Zheng, Zhengzhang Chen, Jingrui He, Haifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02357">https://arxiv.org/abs/2402.02357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02357">https://arxiv.org/pdf/2402.02357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02357]] Multi-modal Causal Structure Learning and Root Cause Analysis(https://arxiv.org/abs/2402.02357)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Effective root cause analysis (RCA) is vital for swiftly restoring services, minimizing losses, and ensuring the smooth operation and management of complex systems. Previous data-driven RCA methods, particularly those employing causal discovery techniques, have primarily focused on constructing dependency or causal graphs for backtracking the root causes. However, these methods often fall short as they rely solely on data from a single modality, thereby resulting in suboptimal solutions. In this work, we propose Mulan, a unified multi-modal causal structure learning method for root cause localization. We leverage a log-tailored language model to facilitate log representation learning, converting log sequences into time-series data. To explore intricate relationships across different modalities, we propose a contrastive learning-based approach to extract modality-invariant and modality-specific representations within a shared latent space. Additionally, we introduce a novel key performance indicator-aware attention mechanism for assessing modality reliability and co-learning a final causal graph. Finally, we employ random walk with restart to simulate system fault propagation and identify potential root causes. Extensive experiments on three real-world datasets validate the effectiveness of our proposed framework.</li>
<li><strong>摘要：</strong>有效的根本原因分析 (RCA) 对于快速恢复服务、最大程度地减少损失以及确保复杂系统的顺利运行和管理至关重要。以前的数据驱动 RCA 方法，特别是那些采用因果发现技术的方法，主要侧重于构建依赖性或因果图以回溯根本原因。然而，这些方法往往存在不足，因为它们仅依赖于单一模态的数据，从而导致解决方案不理想。在这项工作中，我们提出了 Mulan，一种用于根本原因定位的统一多模态因果结构学习方法。我们利用日志定制的语言模型来促进日志表示学习，将日志序列转换为时间序列数据。为了探索不同模态之间的复杂关系，我们提出了一种基于对比学习的方法，在共享潜在空间中提取模态不变和模态特定的表示。此外，我们引入了一种新颖的关键绩效指标感知注意力机制，用于评估模态可靠性和共同学习最终因果图。最后，我们采用重新启动的随机游走来模拟系统故障传播并识别潜在的根本原因。对三个现实世界数据集的广泛实验验证了我们提出的框架的有效性。</li>
</ul>

<h3>Title: Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness</h3>
<ul>
<li><strong>Authors: </strong>Liang Qiao, Jun Shi, Xiaoyu Hao, Xi Fang, Minfan Zhao, Ziqi Zhu, Junshi Chen, Hong An, Bing Li, Honghui Yuan, Xinyang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02361">https://arxiv.org/abs/2402.02361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02361">https://arxiv.org/pdf/2402.02361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02361]] Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness(https://arxiv.org/abs/2402.02361)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Tensor program optimization on Deep Learning Accelerators (DLAs) is critical for efficient model deployment. Although search-based Deep Learning Compilers (DLCs) have achieved significant performance gains compared to manual methods, they still suffer from the persistent challenges of low search efficiency and poor cross-platform adaptability. In this paper, we propose $\textbf{Pruner}$, following hardware/software co-design principles to hierarchically boost tensor program optimization. Pruner comprises two primary components: a Parameterized Static Analyzer ($\textbf{PSA}$) and a Pattern-aware Cost Model ($\textbf{PaCM}$). The former serves as a hardware-aware and formulaic performance analysis tool, guiding the pruning of the search space, while the latter enables the performance prediction of tensor programs according to the critical data-flow patterns. Furthermore, to ensure effective cross-platform adaptation, we design a Momentum Transfer Learning ($\textbf{MTL}$) strategy using a Siamese network, which establishes a bidirectional feedback mechanism to improve the robustness of the pre-trained cost model. The extensive experimental results demonstrate the effectiveness and advancement of the proposed Pruner in various tensor program tuning tasks across both online and offline scenarios, with low resource overhead. The code is available at https://github.com/qiaolian9/Pruner.</li>
<li><strong>摘要：</strong>深度学习加速器 (DLA) 上的张量程序优化对于高效模型部署至关重要。尽管与手动方法相比，基于搜索的深度学习编译器（DLC）取得了显着的性能提升，但它们仍然面临搜索效率低和跨平台适应性差的持续挑战。在本文中，我们提出$\textbf{Pruner}$，遵循硬件/软件协同设计原则来分层提升张量程序优化。 Pruner 包含两个主要组件：参数化静态分析器 ($\textbf{PSA}$) 和模式感知成本模型 ($\textbf{PaCM}$)。前者作为硬件感知和公式化的性能分析工具，指导搜索空间的修剪，而后者可以根据关键数据流模式预测张量程序的性能。此外，为了确保有效的跨平台适应，我们使用Siamese网络设计了动量迁移学习（$\textbf{MTL}$）策略，该策略建立了双向反馈机制以提高预训练成本模型的鲁棒性。广泛的实验结果证明了所提出的 Pruner 在在线和离线场景的各种张量程序调整任务中的有效性和先进性，并且资源开销较低。代码可在 https://github.com/qiaolian9/Pruner 获取。</li>
</ul>

<h3>Title: The Developmental Landscape of In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll, Susan Wei, Daniel Murfet</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02364">https://arxiv.org/abs/2402.02364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02364">https://arxiv.org/pdf/2402.02364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02364]] The Developmental Landscape of In-Context Learning(https://arxiv.org/abs/2402.02364)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.</li>
<li><strong>摘要：</strong>我们表明，当变压器接受语言建模或线性回归任务的训练时，上下文学习出现在离散发展阶段的变压器中。我们引入了两种方法来检测分隔这些阶段的里程碑，通过探测参数空间和函数空间中总体损失的几何形状。我们使用一系列行为和结构指标来研究这些新方法揭示的阶段，以确定其有效性。</li>
</ul>

<h3>Title: Transolver: A Fast Transformer Solver for PDEs on General Geometries</h3>
<ul>
<li><strong>Authors: </strong>Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02366">https://arxiv.org/abs/2402.02366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02366">https://arxiv.org/pdf/2402.02366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02366]] Transolver: A Fast Transformer Solver for PDEs on General Geometries(https://arxiv.org/abs/2402.02366)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Transformers have empowered many milestones across various fields and have recently been applied to solve partial differential equations (PDEs). However, since PDEs are typically discretized into large-scale meshes with complex geometries, it is challenging for Transformers to capture intricate physical correlations directly from massive individual points. Going beyond superficial and unwieldy meshes, we present Transolver based on a more foundational idea, which is learning intrinsic physical states hidden behind discretized geometries. Specifically, we propose a new Physics-Attention to adaptively split the discretized domain into a series of learnable slices of flexible shapes, where mesh points under similar physical states will be ascribed to the same slice. By calculating attention to physics-aware tokens encoded from slices, Transovler can effectively capture intricate physical correlations under complex geometrics, which also empowers the solver with endogenetic geometry-general modeling capacity and can be efficiently computed in linear complexity. Transolver achieves consistent state-of-the-art with 22\% relative gain across six standard benchmarks and also excels in large-scale industrial simulations, including car and airfoil designs.</li>
<li><strong>摘要：</strong>Transformer 已经在各个领域实现了许多里程碑，并且最近被应用于求解偏微分方程 (PDE)。然而，由于偏微分方程通常被离散成具有复杂几何形状的大规模网格，因此 Transformers 直接从大量单个点捕获复杂的物理相关性具有挑战性。超越肤浅和笨拙的网格，我们基于一个更基本的想法提出 Transolver，即学习隐藏在离散几何图形背后的内在物理状态。具体来说，我们提出了一种新的物理注意力机制，将离散域自适应地分割为一系列可学习的灵活形状切片，其中相似物理状态下的网格点将归属于同一切片。通过计算对切片编码的物理感知标记的注意力，Transovler 可以有效捕获复杂几何下复杂的物理相关性，这也赋予求解器内生几何通用建模能力，并且可以以线性复杂度进行高效计算。 Transolver 在六个标准基准中实现了一致的最先进水平，相对增益为 22%，并且在大规模工业模拟（包括汽车和翼型设计）方面也表现出色。</li>
</ul>

<h3>Title: Timer: Transformers for Time Series Analysis at Scale</h3>
<ul>
<li><strong>Authors: </strong>Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02368">https://arxiv.org/abs/2402.02368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02368">https://arxiv.org/pdf/2402.02368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02368]] Timer: Transformers for Time Series Analysis at Scale(https://arxiv.org/abs/2402.02368)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world small-sample scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progresses have been achieved as the emergence of large language models, exhibiting unprecedented ability in few-shot generalization, scalability, and task generality, which is however absent in time series models. To change the current practices of training small models on specific datasets from scratch, this paper aims at an early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet diverse application needs, we convert forecasting, imputation, and anomaly detection of time series into a unified generative task. The outcome of this study is a Time Series Transformer (Timer), that is pre-trained by autoregressive next token prediction on large multi-domain datasets, and is fine-tuned to downstream scenarios with promising abilities as an LTSM.</li>
<li><strong>摘要：</strong>深度学习为时间序列分析的进步做出了巨大贡献。尽管如此，深度模型在现实世界的小样本场景中仍可能遇到性能瓶颈，而这种瓶颈可能由于当前基准测试中小模型的性能饱和而被掩盖。同时，大型模型通过大规模预训练在这些场景中展现了强大的威力。随着大型语言模型的出现，取得了持续的进展，在少样本泛化、可扩展性和任务通用性方面展现出前所未有的能力，而这是时间序列模型所不具备的。为了改变当前在特定数据集上从头开始训练小型模型的做法，本文旨在早期开发大型时间序列模型（LTSM）。在预训练过程中，我们整理了多达 10 亿个时间点的大规模数据集，将异构时间序列统一为单序列序列 (S3) 格式，并开发面向 LTSM 的 GPT 风格架构。为了满足不同的应用需求，我们将时间序列的预测、插补和异常检测转换为统一的生成任务。这项研究的成果是一个时间序列转换器（Timer），它通过对大型多域数据集的自回归下一个标记预测进行预训练，并针对下游场景进行微调，具有作为 LTSM 的有前途的能力。</li>
</ul>

<h3>Title: AutoTimes: Autoregressive Time Series Forecasters via Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02370">https://arxiv.org/abs/2402.02370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02370">https://arxiv.org/pdf/2402.02370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02370]] AutoTimes: Autoregressive Time Series Forecasters via Large Language  Models(https://arxiv.org/abs/2402.02370)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, prompt, rag</a></li>
<li><strong>Abstract: </strong>Foundation models of time series have not been fully developed due to the limited availability of large-scale time series and the underexploration of scalable pre-training. Based on the similar sequential structure of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, prior methods may overlook the consistency in aligning time series and natural language, resulting in insufficient utilization of the LLM potentials. To fully exploit the general-purpose token transitions learned from language modeling, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters. The consequent forecasters can handle flexible series lengths and achieve competitive performance as prevalent models. Further, we present token-wise prompting that utilizes corresponding timestamps to make our method applicable to multimodal scenarios. Analysis demonstrates our forecasters inherit zero-shot and in-context learning capabilities of LLMs. Empirically, AutoTimes exhibits notable method generality and achieves enhanced performance by basing on larger LLMs, additional texts, or time series as instructions.</li>
<li><strong>摘要：</strong>由于大规模时间序列的可用性有限以及可扩展预训练的探索不足，时间序列的基础模型尚未完全开发。基于时间序列和自然语言相似的顺序结构，越来越多的研究证明了利用大型语言模型（LLM）处理时间序列的可行性。然而，先前的方法可能忽略了时间序列和自然语言对齐的一致性，导致法学硕士潜力的利用不足。为了充分利用从语言建模中学到的通用标记转换，我们建议 AutoTimes 将 LLM 重新用作自回归时间序列预测器，这与在不更新参数的情况下获取和利用 LLM 是一致的。随后的预测器可以处理灵活的序列长度并实现与流行模型一样的竞争性能。此外，我们提出了令牌明智的提示，利用相应的时间戳使我们的方法适用于多模式场景。分析表明，我们的预测者继承了法学硕士的零样本和情境学习能力。根据经验，AutoTimes 表现出显着的方法通用性，并通过基于更大的 LLM、附加文本或时间序列作为指令来实现增强的性能。</li>
</ul>

<h3>Title: Evaluating Large Language Models in Analysing Classroom Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Yun Long, Haifeng Luo, Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02380">https://arxiv.org/abs/2402.02380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02380">https://arxiv.org/pdf/2402.02380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02380]] Evaluating Large Language Models in Analysing Classroom Dialogue(https://arxiv.org/abs/2402.02380)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code</a></li>
<li><strong>Abstract: </strong>This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in coding between the model and human coders, with some discrepancies in specific codes. These findings highlight the strong potential of LLM in teaching evaluation and facilitation.</li>
<li><strong>摘要：</strong>本研究探讨了大型语言模型 (LLM)（特别是 GPT-4）在课堂对话分析中的应用，这是教学诊断和质量改进的一项重要研究任务。认识到教育研究中传统定性方法的知识密集型和劳动密集型本质，本研究调查了法学硕士简化和增强分析过程的潜力。该研究涉及一所中学的数据集，包括数学和语文课堂上的课堂对话。这些对话由教育专家手动编码，然后使用定制的 GPT-4 模型进行分析。本研究的重点是将手动注释与 GPT-4 的输出进行比较，以评估其在分析教育对话方面的功效。评估人类编码器与 GPT-4 之间的时间效率、编码器间一致性和编码器间可靠性。结果表明，GPT-4 节省了大量时间，并且模型和人类编码员之间的编码高度一致，但特定代码存在一些差异。这些发现凸显了法学硕士在教学评估和促进方面的强大潜力。</li>
</ul>

<h3>Title: Solution-oriented Agent-based Models Generation with Verifier-assisted  Iterative In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Tong Niu, Weihao Zhang, Rong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02388">https://arxiv.org/abs/2402.02388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02388">https://arxiv.org/pdf/2402.02388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02388]] Solution-oriented Agent-based Models Generation with Verifier-assisted  Iterative In-context Learning(https://arxiv.org/abs/2402.02388)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural network training, SAGE establishes a verifier-assisted iterative in-context learning process employing large language models (LLMs) to leverages their inherent cross-domain knowledge for tackling intricate demands from diverse domain scenarios. In SAGE, we introduce an semi-structured conceptual representation expliciting the intricate structures of ABMs and an objective representation to guide LLMs in modeling scenarios and proposing hypothetical solutions through in-context learning. To ensure the model executability and solution feasibility, SAGE devises a two-level verifier with chain-of-thought prompting tailored to the complex interactions and non-linear dynamics of ABMs, driving the iterative generation optimization. Moreover, we construct an evaluation dataset of solution-oriented ABMs from open sources.It contains practical models across various domains.</li>
<li><strong>摘要：</strong>基于代理的模型（ABM）是提出和验证假设解决方案或政策的重要范例，旨在解决复杂系统带来的挑战并实现各种目标。这个过程需要劳动密集型的努力和多学科的专业知识。封装跨领域知识和编程能力的大型语言模型（LLM）可能会减轻这一过程的难度。然而，法学硕士在处理顺序信息方面表现出色，这使得分析 ABM 固有的复杂相互作用和非线性动力学变得具有挑战性。另外，由于LLM缺乏自我评估能力，仅依靠LLM不足以有效完成这一过程。在本文中，我们提出了 SAGE，一种面向通用解决方案的 ABM 生成框架，旨在自动建模并生成目标问题的解决方案。与依赖专家手工或资源密集型神经网络训练的方法不同，SAGE 采用大型语言模型 (LLM) 建立了验证者辅助的迭代上下文学习过程，以利用其固有的跨领域知识来解决来自不同领域场景的复杂需求。在 SAGE 中，我们引入了一种半结构化概念表示，明确了 ABM 的复杂结构，并引入了一种客观表示，以指导法学硕士建模场景并通过上下文学习提出假设解决方案。为了确保模型的可执行性和解决方案的可行性，SAGE 针对 ABM 的复杂交互和非线性动态设计了具有思想链提示的两级验证器，驱动迭代生成优化。此外，我们还构建了一个开源的面向解决方案的 ABM 评估数据集。它包含跨各个领域的实用模型。</li>
</ul>

<h3>Title: KICGPT: Large Language Model with Knowledge in Context for Knowledge  Graph Completion</h3>
<ul>
<li><strong>Authors: </strong>Yanbin Wei, Qiushi Huang, James T. Kwok, Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02389">https://arxiv.org/abs/2402.02389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02389">https://arxiv.org/pdf/2402.02389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02389]] KICGPT: Large Language Model with Knowledge in Context for Knowledge  Graph Completion(https://arxiv.org/abs/2402.02389)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiveness of KICGPT with smaller training overhead and no finetuning.</li>
<li><strong>摘要：</strong>知识图补全（KGC）对于解决知识图不完整性和支持下游应用至关重要。已经为 KGC 提出了许多模型。它们可以分为两大类：基于三元组的方法和基于文本的方法。由于有限的结构信息和不平衡的实体分布，基于三重的方法难以处理长尾实体。基于文本的方法缓解了这个问题，但需要昂贵的语言模型训练和知识图的特定微调，这限制了它们的效率。为了缓解这些限制，在本文中，我们提出了 KICGPT，这是一个集成了大型语言模型（LLM）和基于三元组的 KGC 检索器的框架。它缓解了长尾问题，而不会产生额外的训练开销。 KICGPT 使用一种名为“知识提示”的情境学习策略，它将结构性知识编码为演示，以指导法学硕士。基准数据集的实证结果证明了 KICGPT 的有效性，训练开销较小且无需微调。</li>
</ul>

<h3>Title: DeLLMa: A Framework for Decision Making Under Uncertainty with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ollie Liu, Deqing Fu, Dani Yogatama, Willie Neiswanger</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02392">https://arxiv.org/abs/2402.02392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02392">https://arxiv.org/pdf/2402.02392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02392]] DeLLMa: A Framework for Decision Making Under Uncertainty with Large  Language Models(https://arxiv.org/abs/2402.02392)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over competing methods.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 在整个社会中越来越多地使用，包括商业、工程和医学等领域。这些领域经常需要在不确定的情况下做出决策，这是一项关键但具有挑战性的任务。在本文中，我们表明，直接促使法学硕士解决这些类型的决策问题会产生较差的结果，特别是当问题复杂性增加时。为了克服这个限制，我们提出了DeLLMa（决策大型语言模型助手），一个旨在提高不确定环境下决策准确性的框架。 DeLLMa 涉及多步骤脚手架程序，借鉴决策理论和效用理论的原理，提供最佳且可人工审计的决策过程。我们在涉及真实农业和金融数据的决策环境中验证了我们的框架。我们的结果表明，DeLLMa 可以显着提高 LLM 决策性能，与竞争方法相比，准确率提高了 40%。</li>
</ul>

<h3>Title: GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large  Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xuanchang Zhang, Zhuosheng Zhang, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02408">https://arxiv.org/abs/2402.02408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02408">https://arxiv.org/pdf/2402.02408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02408]] GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large  Language Model(https://arxiv.org/abs/2402.02408)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code, rag</a></li>
<li><strong>Abstract: </strong>Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality. To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels. Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score. Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent. Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt optimization yields effective prompts comparable to accuracy-based ones. The code is publicly available at https://github.com/thunderous77/GLaPE.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLM）取得了快速进展，但它们的任务性能仍然对即时设计敏感。最近的研究探索了利用法学硕士本身作为优化器来识别最大化任务准确性的最佳提示。然而，在评估提示时，此类方法严重依赖难以捉摸的手动注释的黄金标签来计算每个候选提示的任务准确性，这阻碍了广泛的实施和通用性。为了克服这一限制，这项工作提出了一种与金标签无关的即时评估（GLaPE），以减轻对金标签的依赖。受观察到的自我一致性和答案准确性之间的相关性的启发，我们采用自我一致性作为初始评估分数。随后，我们对产生相同答案的提示分数进行细化，使其相互一致。实验结果表明，即使没有金标签，GLaPE 也能提供一致且准确的可靠评估。此外，在六种流行的推理任务中，我们基于 GLaPE 的提示优化产生的有效提示可与基于准确性的提示相媲美。该代码可在 https://github.com/thunderous77/GLaPE 上公开获取。</li>
</ul>

<h3>Title: Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Yaodong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02416">https://arxiv.org/abs/2402.02416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02416">https://arxiv.org/pdf/2402.02416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02416]] Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction(https://arxiv.org/abs/2402.02416)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code, rag</a></li>
<li><strong>Abstract: </strong>Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on different open-source and API-based models. Remarkably, Aligner-7B improves 11 different LLMs by 18% in helpfulness and 23% in harmlessness on average (GPT-4 by 26.9% and 17.5%). When finetuning (strong) Llama2-70B with (weak) Aligner-7B's supervision, we can improve Llama2 by 8.2% in helpfulness and 61.6% in harmlessness. See our dataset and code at \url{https://aligner2024.github.io}.</li>
<li><strong>摘要：</strong>协调大型语言模型（LLM）的工作主要是通过人类反馈强化学习（RLHF）方法进行的。然而，RLHF 遇到了重大挑战，包括训练奖励模型、演员批评工程，而且重要的是，它需要访问 LLM 参数。在这里，我们介绍 Aligner，一种新的高效对齐范例，它通过学习对齐和未对齐答案之间的校正残差来绕过整个 RLHF 过程。我们的对准器具有几个关键优势。首先，它是一个自回归 seq2seq 模型，通过监督学习在查询-答案-校正数据集上进行训练；这以最少的资源提供了参数高效的对齐解决方案。其次，Aligner有利于弱到强的泛化；通过 Aligner 的监督信号对大型预训练模型进行微调，展示了强大的性能提升。第三，Aligner 充当与模型无关的即插即用模块，允许其直接应用于不同的开源和基于 API 的模型。值得注意的是，Aligner-7B 将 11 种不同的 LLM 的有用性平均提高了 18%，无害性平均提高了 23%（GPT-4 分别提高了 26.9% 和 17.5%）。当在（弱）Aligner-7B 的监督下对（强）Llama2-70B 进行微调时，我们可以将 Llama2 的有用性提高 8.2%，无害性提高 61.6%。请在 \url{https://aligner2024.github.io} 查看我们的数据集和代码。</li>
</ul>

<h3>Title: Factuality of Large Language Models in the Year 2024</h3>
<ul>
<li><strong>Authors: </strong>Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Georgi Georgiev, Rocktim Jyoti Das, Preslav Nakov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02420">https://arxiv.org/abs/2402.02420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02420">https://arxiv.org/pdf/2402.02420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02420]] Factuality of Large Language Models in the Year 2024(https://arxiv.org/abs/2402.02420)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM)，特别是针对聊天进行指令调整后，已成为我们日常生活的一部分，通过为各种问题提供简单的答案，将人们从搜索、提取和集成来自多个来源的信息的过程中解放出来。一个地方。不幸的是，在许多情况下，法学硕士的回答实际上是不正确的，这限制了它们在现实场景中的适用性。因此，评估和提高法学硕士真实性的研究最近引起了很多研究关注。在这项调查中，我们批判性地分析了现有的工作，旨在确定主要挑战及其相关原因，指出提高法学硕士真实性的潜在解决方案，并分析开放式文本生成的自动真实性评估的障碍。我们进一步对未来的研究方向进行了展望。</li>
</ul>

<h3>Title: Towards an Information Theoretic Framework of Context-Based Offline  Meta-Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Lanqing Li, Hai Zhang, Xinyu Zhang, Shatong Zhu, Junqiao Zhao, Pheng-Ann Heng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02429">https://arxiv.org/abs/2402.02429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02429">https://arxiv.org/pdf/2402.02429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02429]] Towards an Information Theoretic Framework of Context-Based Offline  Meta-Reinforcement Learning(https://arxiv.org/abs/2402.02429)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely. Among which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations. In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified information theoretic framework. Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $\boldsymbol{M}$ and its latent representation $\boldsymbol{Z}$ by implementing various approximate bounds. Based on the theoretical insight and the information bottleneck principle, we arrive at a novel algorithm dubbed UNICORN, which exhibits remarkable generalization across a broad spectrum of RL benchmarks, context shift scenarios, data qualities and deep learning architectures, attaining the new state-of-the-art. We believe that our framework could open up avenues for new optimality bounds and COMRL algorithms.</li>
<li><strong>摘要：</strong>作为离线 RL 和元 RL 的结合，离线元强化学习 (OMRL) 的出现在使 RL 代理能够执行多任务并快速适应同时安全地获取知识方面显示出了巨大的希望。其中，基于上下文的 OMRL（COMRL）作为一种流行的范式，旨在学习以有效任务表示为条件的通用策略。在这项工作中，通过研究 COMRL 领域的几个关键里程碑，我们建议将这些看似独立的方法整合到一个统一的信息论框架中。最重要的是，我们表明预先存在的 COMRL 算法本质上是通过实现各种近似边界来优化任务变量 $\boldsymbol{M}$ 及其潜在表示 $\boldsymbol{Z}$ 之间的相同互信息目标。基于理论洞察和信息瓶颈原理，我们提出了一种名为 UNICORN 的新颖算法，该算法在广泛的 RL 基准、上下文转换场景、数据质量和深度学习架构中表现出卓越的泛化能力，实现了新的状态艺术。我们相信我们的框架可以为新的最优边界和 COMRL 算法开辟途径。</li>
</ul>

<h3>Title: DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based  Trajectory Stitching</h3>
<ul>
<li><strong>Authors: </strong>Guanghe Li, Yixiang Shan, Zhengbang Zhu, Ting Long, Weinan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02439">https://arxiv.org/abs/2402.02439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02439">https://arxiv.org/pdf/2402.02439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02439]] DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based  Trajectory Stitching(https://arxiv.org/abs/2402.02439)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and trajectory optimization methods (DT).</li>
<li><strong>摘要：</strong>在离线强化学习（RL）中，学习策略的性能很大程度上取决于离线数据集的质量。然而，在许多情况下，离线数据集包含非常有限的最佳轨迹，这对离线强化学习算法提出了挑战，因为智能体必须具备转移到高奖励区域的能力。为了解决这个问题，我们引入了基于扩散的轨迹缝合（DiffStitch），这是一种新颖的基于扩散的数据增强管道，可以系统地生成轨迹之间的缝合过渡。 DiffStitch 有效地将低奖励轨迹与高奖励轨迹连接起来，形成全局最优轨迹，以解决离线 RL 算法面临的挑战。在 D4RL 数据集上进行的实证实验证明了 DiffStitch 在 RL 方法中的有效性。值得注意的是，DiffStitch 展示了一步法 (IQL)、模仿学习方法 (TD3+BC) 和轨迹优化方法 (DT) 性能的显着增强。</li>
</ul>

<h3>Title: TopoX: A Suite of Python Packages for Machine Learning on Topological  Domains</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Hajij, Mathilde Papillon, Florian Frantzen, Jens Agerberg, Ibrahem AlJabea, Ruben Ballester, Claudio Battiloro, Guillermo Bernárdez, Tolga Birdal, Aiden Brent, Peter Chin, Sergio Escalera, Odin Hoff Gardaa, Gurusankar Gopalakrishnan, Devendra Govil, Josef Hoppe, Maneel Reddy Karri, Jude Khouja, Manuel Lecha, Neal Livesay, Jan Meißner, Soham Mukherjee, Alexander Nikitin, Theodore Papamarkou, Jaro Pr'{i}lepok, Karthikeyan Natesan Ramamurthy, Paul Rosen, Aldo Guzm'{a}n-S'{a}enz, Alessandro Salatiello, Shreyas N. Samaga, Michael T. Schaub, Luca Scofano, Indro Spinelli, Lev Telyatnikov, Quang Truong, Robin Walters, Maosheng Yang, Olga Zaghen, Ghada Zamzmi, Ali Zia, Nina Miolane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02441">https://arxiv.org/abs/2402.02441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02441">https://arxiv.org/pdf/2402.02441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02441]] TopoX: A Suite of Python Packages for Machine Learning on Topological  Domains(https://arxiv.org/abs/2402.02441)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.</li>
<li><strong>摘要：</strong>我们介绍 topox，这是一个 Python 软件套件，它为扩展图的拓扑域上的计算和机器学习提供可靠且用户友好的构建块：超图、单纯图、元胞图、路径和组合复形。 topox 由三个包组成：toponetx 有助于在这些域上进行构建和计算，包括处理节点、边和高阶单元； topoembedx 提供了将拓扑域嵌入向量空间的方法，类似于流行的基于图的嵌入算法，例如 node2vec； topomodelx 建立在 PyTorch 之上，为拓扑域上的神经网络提供了一个全面的高阶消息传递函数工具箱。经广泛记录和单元测试的 topox 源代码可根据 MIT 许可在 https://github.com/pyt-team 上获取。</li>
</ul>

<h3>Title: A Momentum Accelerated Algorithm for ReLU-based Nonlinear Matrix  Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Qingsong Wang, Chunfeng Cui, Deren Han</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02442">https://arxiv.org/abs/2402.02442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02442">https://arxiv.org/pdf/2402.02442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02442]] A Momentum Accelerated Algorithm for ReLU-based Nonlinear Matrix  Decomposition(https://arxiv.org/abs/2402.02442)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, code</a></li>
<li><strong>Abstract: </strong>Recently, there has been a growing interest in the exploration of Nonlinear Matrix Decomposition (NMD) due to its close ties with neural networks. NMD aims to find a low-rank matrix from a sparse nonnegative matrix with a per-element nonlinear function. A typical choice is the Rectified Linear Unit (ReLU) activation function. To address over-fitting in the existing ReLU-based NMD model (ReLU-NMD), we propose a Tikhonov regularized ReLU-NMD model, referred to as ReLU-NMD-T. Subsequently, we introduce a momentum accelerated algorithm for handling the ReLU-NMD-T model. A distinctive feature, setting our work apart from most existing studies, is the incorporation of both positive and negative momentum parameters in our algorithm. Our numerical experiments on real-world datasets show the effectiveness of the proposed model and algorithm. Moreover, the code is available at https://github.com/nothing2wang/NMD-TM.</li>
<li><strong>摘要：</strong>最近，由于非线性矩阵分解（NMD）与神经网络的密切联系，人们对它的探索越来越感兴趣。 NMD 旨在从具有每元素非线性函数的稀疏非负矩阵中找到低秩矩阵。典型的选择是修正线性单元 (ReLU) 激活函数。为了解决现有基于 ReLU 的 NMD 模型（ReLU-NMD）中的过度拟合问题，我们提出了一种 Tikhonov 正则化 ReLU-NMD 模型，简称为 ReLU-NMD-T。随后，我们引入了用于处理 ReLU-NMD-T 模型的动量加速算法。使我们的工作与大多数现有研究不同的一个显着特征是我们的算法中结合了正动量参数和负动量参数。我们对真实世界数据集的数值实验表明了所提出的模型和算法的有效性。此外，该代码可在 https://github.com/nothing2wang/NMD-TM 上获取。</li>
</ul>

<h3>Title: LQER: Low-Rank Quantization Error Reconstruction for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Cheng Zhang, Jianyi Cheng, George A. Constantinides, Yiren Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02446">https://arxiv.org/abs/2402.02446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02446">https://arxiv.org/pdf/2402.02446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02446]] LQER: Low-Rank Quantization Error Reconstruction for LLMs(https://arxiv.org/abs/2402.02446)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的训练后量化具有挑战性。在这项工作中，我们引入了低秩量化误差减少（LQER），它结合了量化和低秩近似来恢复模型能力。 LQER 利用激活诱导的尺度矩阵将量化误差的奇异值分布驱动到理想的分布，从而在各种 LLM 和下游任务上实现近乎无损的 W4A8 量化，而无需知识蒸馏、网格搜索或基于梯度的迭代优化。与现有方法不同，LQER 的计算模式不需要专门的分散和收集过程来从不规则的内存位置收集高精度权重。我们的 W4A8 LLM 在六种流行的下游任务上实现了近乎无损的性能，同时比领先的最先进方法少使用 1.36$\times$ 的硬件资源。一旦论文被接受，我们将开源我们的框架。</li>
</ul>

<h3>Title: On the Role of Initialization on the Implicit Bias in Deep Linear  Networks</h3>
<ul>
<li><strong>Authors: </strong>Oria Gruber, Haim Avron</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02454">https://arxiv.org/abs/2402.02454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02454">https://arxiv.org/pdf/2402.02454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02454]] On the Role of Initialization on the Implicit Bias in Deep Linear  Networks(https://arxiv.org/abs/2402.02454)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Despite Deep Learning's (DL) empirical success, our theoretical understanding of its efficacy remains limited. One notable paradox is that while conventional wisdom discourages perfect data fitting, deep neural networks are designed to do just that, yet they generalize effectively. This study focuses on exploring this phenomenon attributed to the implicit bias at play. Various sources of implicit bias have been identified, such as step size, weight initialization, optimization algorithm, and number of parameters. In this work, we focus on investigating the implicit bias originating from weight initialization. To this end, we examine the problem of solving underdetermined linear systems in various contexts, scrutinizing the impact of initialization on the implicit regularization when using deep networks to solve such systems. Our findings elucidate the role of initialization in the optimization and generalization paradoxes, contributing to a more comprehensive understanding of DL's performance characteristics.</li>
<li><strong>摘要：</strong>尽管深度学习（DL）在实证上取得了成功，但我们对其功效的理论理解仍然有限。一个值得注意的悖论是，虽然传统观点不鼓励完美的数据拟合，但深度神经网络旨在做到这一点，但它们可以有效地进行泛化。本研究的重点是探索这种归因于隐性偏见的现象。隐式偏差的各种来源已被识别，例如步长、权重初始化、优化算法和参数数量。在这项工作中，我们重点研究源自权重初始化的隐式偏差。为此，我们研究了在各种情况下求解欠定线性系统的问题，仔细研究了使用深度网络求解此类系统时初始化对隐式正则化的影响。我们的研究结果阐明了初始化在优化和泛化悖论中的作用，有助于更全面地理解深度学习的性能特征。</li>
</ul>

<h3>Title: Discovering More Effective Tensor Network Structure Search Algorithms  via Large Language Models (LLMs)</h3>
<ul>
<li><strong>Authors: </strong>Junhua Zeng, Guoxu Zhou, Chao Li, Zhun Sun, Qibin Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02456">https://arxiv.org/abs/2402.02456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02456">https://arxiv.org/pdf/2402.02456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02456]] Discovering More Effective Tensor Network Structure Search Algorithms  via Large Language Models (LLMs)(https://arxiv.org/abs/2402.02456)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, lora, prompt, rag</a></li>
<li><strong>Abstract: </strong>Tensor network structure search (TN-SS), aiming at searching for suitable tensor network (TN) structures in representing high-dimensional problems, largely promotes the efficacy of TN in various machine learning applications. Nonetheless, finding a satisfactory TN structure using existing algorithms remains challenging. To develop more effective algorithms and avoid the human labor-intensive development process, we explore the knowledge embedded in large language models (LLMs) for the automatic design of TN-SS algorithms. Our approach, dubbed GPTN-SS, leverages an elaborate crafting LLM-based prompting system that operates in an evolutionary-like manner. The experimental results, derived from real-world data, demonstrate that GPTN-SS can effectively leverage the insights gained from existing methods to develop novel TN-SS algorithms that achieve a better balance between exploration and exploitation. These algorithms exhibit superior performance in searching the high-quality TN structures for natural image compression and model parameters compression while also demonstrating generalizability in their performance.</li>
<li><strong>摘要：</strong>张量网络结构搜索（TN-SS）旨在寻找合适的张量网络（TN）结构来表示高维问题，极大地提升了TN在各种机器学习应用中的功效。尽管如此，使用现有算法找到令人满意的 TN 结构仍然具有挑战性。为了开发更有效的算法并避免人工密集型开发过程，我们探索了大型语言模型（LLM）中嵌入的知识，用于 TN-SS 算法的自动设计。我们的方法被称为 GPTN-SS，利用了一个精心制作的基于 LLM 的提示系统，该系统以类似进化的方式运行。来自真实世界数据的实验结果表明，GPTN-SS 可以有效地利用从现有方法中获得的见解来开发新颖的 TN-SS 算法，从而在探索和利用之间实现更好的平衡。这些算法在搜索高质量 TN 结构以进行自然图像压缩和模型参数压缩方面表现出卓越的性能，同时也展示了其性能的通用性。</li>
</ul>

<h3>Title: A Fast Method for Lasso and Logistic Lasso</h3>
<ul>
<li><strong>Authors: </strong>Siu-Wing Cheng, Man Ting Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02463">https://arxiv.org/abs/2402.02463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02463">https://arxiv.org/pdf/2402.02463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02463]] A Fast Method for Lasso and Logistic Lasso(https://arxiv.org/abs/2402.02463)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We propose a fast method for solving compressed sensing, Lasso regression, and Logistic Lasso regression problems that iteratively runs an appropriate solver using an active set approach. We design a strategy to update the active set that achieves a large speedup over a single call of several solvers, including gradient projection for sparse reconstruction (GPSR), lassoglm of Matlab, and glmnet. For compressed sensing, the hybrid of our method and GPSR is 31.41 times faster than GPSR on average for Gaussian ensembles and 25.64 faster on average for binary ensembles. For Lasso regression, the hybrid of our method and GPSR achieves a 30.67-fold average speedup in our experiments. In our experiments on Logistic Lasso regression, the hybrid of our method and lassoglm gives an 11.95-fold average speedup, and the hybrid of our method and glmnet gives a 1.40-fold average speedup.</li>
<li><strong>摘要：</strong>我们提出了一种用于解决压缩感知、Lasso 回归和 Logistic Lasso 回归问题的快速方法，该方法使用活动集方法迭代运行适当的求解器。我们设计了一种更新活动集的策略，该策略比多个求解器的单次调用实现了大幅加速，包括稀疏重建梯度投影 (GPSR)、Matlab 的 lassoglm 和 glmnet。对于压缩感知，我们的方法和 GPSR 的混合对于高斯系综来说比 GPSR 平均快 31.41 倍，对于二元系综平均快 25.64 倍。对于 Lasso 回归，我们的方法和 GPSR 的混合在我们的实验中实现了 30.67 倍的平均加速。在我们的 Logistic Lasso 回归实验中，我们的方法和 lassoglm 的混合给出了 11.95 倍的平均加速，而我们的方法和 glmnet 的混合给出了 1.40 倍的平均加速。</li>
</ul>

<h3>Title: A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer</h3>
<ul>
<li><strong>Authors: </strong>Zhangyang Gao, Daize Dong, Cheng Tan, Jun Xia, Bozhen Hu, Stan Z. Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02464">https://arxiv.org/abs/2402.02464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02464">https://arxiv.org/pdf/2402.02464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02464]] A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer(https://arxiv.org/abs/2402.02464)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, code</a></li>
<li><strong>Abstract: </strong>Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+GraphGPT enables effective graph mixup in the Euclidean space, overcoming previously known non-Euclidean challenge. (4) Our proposed novel edge-centric GPT pretraining task is effective in graph fields, underscoring its success in both representation and generation.</li>
<li><strong>摘要：</strong>我们能否将非欧几里德图建模为纯语言甚至欧几里德向量，同时保留其固有信息？非欧几里得属性对图建模提出了长期挑战。尽管最近 GNN 和 Graphformer 努力将图编码为欧几里德向量，但从向量中恢复原始图仍然是一个挑战。我们引入了 GraphsGPT，它具有 Graph2Seq 编码器，可将非欧几里德图转换为欧几里德空间中可学习的图词，以及 GraphGPT 解码器，可从图词重建原始图以确保信息等价。我们在 1 亿个分子上预训练 GraphsGPT，并得到了一些有趣的发现：(1) 预训练的 Graph2Seq 在图表示学习方面表现出色，在 8/9 图分类和回归任务上取得了最先进的结果。 (2) 预训练的 GraphGPT 可以作为强大的图生成器，这通过其执行无条件和条件图生成的能力得到证明。 (3) Graph2Seq+GraphGPT 能够在欧几里德空间中实现有效的图混合，克服先前已知的非欧几里德挑战。 (4) 我们提出的新颖的以边缘为中心的 GPT 预训练任务在图领域是有效的，强调了它在表示和生成方面的成功。</li>
</ul>

<h3>Title: Fast Peer Adaptation with Context-aware Exploration</h3>
<ul>
<li><strong>Authors: </strong>Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, Yizhou Wang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02468">https://arxiv.org/abs/2402.02468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02468">https://arxiv.org/pdf/2402.02468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02468]] Fast Peer Adaptation with Context-aware Exploration(https://arxiv.org/abs/2402.02468)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, agent</a></li>
<li><strong>Abstract: </strong>Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games. To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation. However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon. In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes. This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident. We evaluate our method on diverse testbeds that involve competitive (Kuhn Poker), cooperative (PO-Overcooked), or mixed (Predator-Prey-W) games with peer agents. We demonstrate that our method induces more active exploration behavior, achieving faster adaptation and better outcomes than existing methods.</li>
<li><strong>摘要：</strong>快速适应具有不同策略的未知同伴（合作伙伴或对手）是多智能体博弈中的关键挑战。为此，智能体有效地探测和识别同伴的策略至关重要，因为这是在适应中执行最佳响应的先决条件。然而，探索未知同行的策略是很困难的，特别是当博弈是部分可观察的并且具有很长的视野时。在本文中，我们提出了一种同伴识别奖励，它根据学习代理在历史背景下识别同伴行为模式的能力（例如对多个事件的观察）来奖励学习代理。这种奖励激励代理学习上下文感知策略，以进行有效探索和快速适应，即在不确定其策略时积极寻求和收集来自同伴的信息反馈，并在有信心时利用上下文执行最佳响应。我们在涉及竞争性（Kuhn Poker）、合作性（PO-Overcooked）或与同伴代理的混合（Predator-Prey-W）游戏的不同测试平台上评估我们的方法。我们证明，我们的方法会引发更积极的探索行为，比现有方法实现更快的适应和更好的结果。</li>
</ul>

<h3>Title: TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Yunzhong Qiu, Li Zhang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02475">https://arxiv.org/abs/2402.02475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02475">https://arxiv.org/pdf/2402.02475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02475]] TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling(https://arxiv.org/abs/2402.02475)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Time series pre-training has recently garnered wide attention for its potential to reduce labeling expenses and benefit various downstream tasks. Prior methods are mainly based on pre-training techniques well-acknowledged in vision or language, such as masked modeling and contrastive learning. However, randomly masking time series or calculating series-wise similarity will distort or neglect inherent temporal correlations crucial in time series data. To emphasize temporal correlation modeling, this paper proposes TimeSiam as a simple but effective self-supervised pre-training framework for Time series based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to capture intrinsic temporal correlations between randomly sampled past and current subseries. With a simple data augmentation method (e.g.~masking), TimeSiam can benefit from diverse augmented subseries and learn internal time-dependent representations through a past-to-current reconstruction. Moreover, learnable lineage embeddings are also introduced to distinguish temporal distance between sampled series and further foster the learning of diverse temporal correlations. TimeSiam consistently outperforms extensive advanced pre-training baselines, demonstrating superior forecasting and classification capabilities across 13 standard benchmarks in both intra- and cross-domain scenarios.</li>
<li><strong>摘要：</strong>时间序列预训练最近因其降低标签费用和有利于各种下游任务的潜力而受到广泛关注。现有方法主要基于视觉或语言领域公认的预训练技术，例如掩模建模和对比学习。然而，随机屏蔽时间序列或计算序列相似性将扭曲或忽略时间序列数据中至关重要的内在时间相关性。为了强调时间相关性建模，本文提出了 TimeSiam 作为一种简单但有效的基于 Siamese 网络的时间序列自监督预训练框架。具体来说，TimeSiam 预训练 Siamese 编码器来捕获随机采样的过去和当前子系列之间的内在时间相关性。通过简单的数据增强方法（例如掩蔽），TimeSiam 可以从各种增强子系列中受益，并通过过去到当前的重建来学习内部的时间相关表示。此外，还引入了可学习的谱系嵌入来区分采样序列之间的时间距离，并进一步促进不同时间相关性的学习。 TimeSiam 始终优于广泛的高级预训练基线，在域内和跨域场景中的 13 个标准基准中展示了卓越的预测和分类能力。</li>
</ul>

<h3>Title: BRAIn: Bayesian Reward-conditioned Amortized Inference for natural  language generation from feedback</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Pandey, Yatin Nandwani, Tahira Naseem, Mayank Mishra, Guangxuan Xu, Dinesh Raghu, Sachindra Joshi, Asim Munawar, Ramón Fernandez Astudillo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02479">https://arxiv.org/abs/2402.02479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02479">https://arxiv.org/pdf/2402.02479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02479]] BRAIn: Bayesian Reward-conditioned Amortized Inference for natural  language generation from feedback(https://arxiv.org/abs/2402.02479)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner. These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance. However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model. In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies Bayes theorem to derive an intractable posterior distribution where the RM is explicitly represented. BRAIn then distills this posterior into an amortized inference network through self-normalized importance sampling, leading to a scalable offline algorithm that significantly outperforms prior art in summarization and AntropicHH tasks. BRAIn also has interesting connections to PPO and DPO for specific RM choices.</li>
<li><strong>摘要：</strong>继人类反馈强化学习 (RLHF) 的近端策略优化 (PPO) 的成功之后，序列似然校准 (SLiC) 和直接策略优化 (DPO) 等新技术被提出，这些技术本质上是离线的，并在间接方式。这些技术，特别是 DPO，由于其可扩展性和性能，最近已成为 LLM 对齐的首选工具。然而，它们留下了 PPO 方法的重要特征。 SLiC 或 RRHF 等方法仅将奖励模型 (RM) 用于排名/偏好，丢失细粒度信息并忽略 RM 的参数形式（例如 Bradley-Terry、Plackett-Luce），而诸如因为 DPO 甚至不使用单独的奖励模型。在这项工作中，我们提出了一种名为 BRAIn 的新方法，它将 RM 作为分布匹配方法的一部分重新引入。BRAIn 考虑以输出良好性假设为条件的 LLM 分布，并应用贝叶斯定理来导出一个棘手的后验分布，其中RM 被明确表示。 BRAIn 然后通过自归一化重要性采样将这个后验提炼成摊销推理网络，从而产生可扩展的离线算法，该算法在摘要和 AntropicHH 任务中显着优于现有技术。 BRAIn 还与 PPO 和 DPO 有有趣的联系，以实现特定的 RM 选择。</li>
</ul>

<h3>Title: Weisfeiler Leman for Euclidean Equivariant Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Snir Hordan, Tal Amir, Nadav Dym</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02484">https://arxiv.org/abs/2402.02484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02484">https://arxiv.org/pdf/2402.02484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02484]] Weisfeiler Leman for Euclidean Equivariant Machine Learning(https://arxiv.org/abs/2402.02484)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common method for assessing the expressive power of graph neural networks (GNNs). Recently, the $2$-WL test was proven to be complete on weighted graphs which encode $3\mathrm{D}$ point cloud data. Consequently, GNNs whose expressive power is equivalent to the $2$-WL test are provably universal on point clouds. Yet, this result is limited to invariant continuous functions on point clouds. In this paper we extend this result in three ways: Firstly, we show that $2$-WL tests can be extended to point clouds which include both positions and velocity, a scenario often encountered in applications. Secondly, we show that PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds with low complexity. Finally, we show that a simple modification of this PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly. Building on our results, we develop our WeLNet architecture, which can process position-velocity pairs, compute functions fully equivariant to permutations and rigid motions, and is provably complete and universal. Remarkably, WeLNet is provably complete precisely in the setting in which it is implemented in practice. Our theoretical results are complemented by experiments showing WeLNet sets new state-of-the-art results on the N-Body dynamics task and the GEOM-QM9 molecular conformation generation task.</li>
<li><strong>摘要：</strong>$k$-Weifeiler-Leman ($k$-WL) 图同构测试层次结构是评估图神经网络 (GNN) 表达能力的常用方法。最近，$2$-WL 测试被证明在编码 $3\mathrm{D}$ 点云数据的加权图上是完整的。因此，表达能力相当于 $2$-WL 测试的 GNN 在点云上被证明是通用的。然而，这个结果仅限于点云上的不变连续函数。在本文中，我们通过三种方式扩展了这一结果：首先，我们表明 $2$-WL 测试可以扩展到包括位置和速度的点云，这是应用中经常遇到的场景。其次，我们证明 PPGN (Maron et al., 2019) 可以在所有点云上以低复杂度统一模拟 $2$-WL。最后，我们证明了对该 PPGN 架构的简单修改可用于获得通用等变架构，该架构可以统一逼近所有连续等变函数。基于我们的结果，我们开发了 WeLNet 架构，它可以处理位置-速度对，计算与排列和刚性运动完全等价的函数，并且被证明是完整和通用的。值得注意的是，WeLNet 在实际实施的环境中被证明是完全完整的。我们的理论结果得到了实验的补充，实验表明 WeLNet 在 N-Body 动力学任务和 GEOM-QM9 分子构象生成任务上取得了新的最先进结果。</li>
</ul>

<h3>Title: Latent Graph Diffusion: A Unified Framework for Generation and  Prediction on Graphs</h3>
<ul>
<li><strong>Authors: </strong>Zhou Cai, Xiyuan Wang, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02518">https://arxiv.org/abs/2402.02518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02518">https://arxiv.org/pdf/2402.02518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02518]] Latent Graph Diffusion: A Unified Framework for Generation and  Prediction on Graphs(https://arxiv.org/abs/2402.02518)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) with one model. We first propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder which can also be decoded, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Then we formulate prediction tasks including regression and classification as (conditional) generation, which enables our LGD to solve tasks of all levels and all types with provable guarantees. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results across generation and regression tasks.</li>
<li><strong>摘要：</strong>在本文中，我们提出了第一个框架，能够使用一个模型解决所有级别（节点、边和图）和所有类型（生成、回归和分类）的图学习任务。我们首先提出潜在图扩散（LGD），这是一种可以同时生成所有类别的节点、边和图级特征的生成模型。我们通过利用强大的编码器（也可以解码）将图结构和特征嵌入到潜在空间中来实现这一目标，然后在潜在空间中训练扩散模型。 LGD还能够通过专门设计的交叉注意力机制进行条件生成。然后我们制定预测任务，包括回归和分类作为（条件）生成，这使得我们的 LGD 能够以可证明的保证解决所有级别和所有类型的任务。我们通过广泛的实验验证了我们框架的有效性，我们的模型在生成和回归任务中实现了最先进的或极具竞争力的结果。</li>
</ul>

<h3>Title: CompeteSMoE - Effective Training of Sparse Mixture of Experts via  Competition</h3>
<ul>
<li><strong>Authors: </strong>Quang Pham, Giang Do, Huy Nguyen, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, Nhat Ho</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02526">https://arxiv.org/abs/2402.02526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02526">https://arxiv.org/pdf/2402.02526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02526]] CompeteSMoE - Effective Training of Sparse Mixture of Experts via  Competition(https://arxiv.org/abs/2402.02526)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, effective training of SMoE has proven to be challenging due to the representation collapse issue, which causes parameter redundancy and limited representation potentials. In this work, we propose a competition mechanism to address this fundamental challenge of representation collapse. By routing inputs only to experts with the highest neural response, we show that, under mild assumptions, competition enjoys the same convergence rate as the optimal estimator. We further propose CompeteSMoE, an effective and efficient algorithm to train large language models by deploying a simple router that predicts the competition outcomes. Consequently, CompeteSMoE enjoys strong performance gains from the competition routing policy while having low computation overheads. Our extensive empirical evaluations on two transformer architectures and a wide range of tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies.</li>
<li><strong>摘要：</strong>稀疏专家混合 (SMoE) 提供了一种有吸引力的解决方案，可以将模型复杂性扩展到超出增加网络深度或宽度的范围。然而，由于表示崩溃问题，SMoE 的有效训练已被证明具有挑战性，这会导致参数冗余和有限的表示潜力。在这项工作中，我们提出了一种竞争机制来解决代表性崩溃的这一根本挑战。通过仅将输入路由给具有最高神经响应的专家，我们表明，在温和的假设下，竞争具有与最优估计器相同的收敛速度。我们进一步提出了 CompeteSMoE，这是一种有效且高效的算法，通过部署一个预测竞争结果的简单路由器来训练大型语言模型。因此，CompeteSMoE 从竞争路由策略中获得了强大的性能提升，同时计算开销较低。我们对两种 Transformer 架构和广泛的任务进行了广泛的实证评估，证明了 CompeteSMoE 与最先进的 SMoE 策略相比的有效性、稳健性和可扩展性。</li>
</ul>

<h3>Title: Knowledge Generation for Zero-shot Knowledge-based VQA</h3>
<ul>
<li><strong>Authors: </strong>Rui Cao, Jing Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02541">https://arxiv.org/abs/2402.02541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02541">https://arxiv.org/pdf/2402.02541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02541]] Knowledge Generation for Zero-shot Knowledge-based VQA(https://arxiv.org/abs/2402.02541)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Previous solutions to knowledge-based visual question answering~(K-VQA) retrieve knowledge from external knowledge bases and use supervised learning to train the K-VQA model. Recently pre-trained LLMs have been used as both a knowledge source and a zero-shot QA model for K-VQA and demonstrated promising results. However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability. Inspired by recent work on knowledge generation from LLMs for text-based QA, in this work we propose and test a similar knowledge-generation-based K-VQA method, which first generates knowledge from an LLM and then incorporates the generated knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA benchmarks and found that our method performs better than previous zero-shot K-VQA methods and our generated knowledge is generally relevant and helpful.</li>
<li><strong>摘要：</strong>之前基于知识的视觉问答~（K-VQA）的解决方案是从外部知识库检索知识，并使用监督学习来训练K-VQA模型。最近预训练的法学硕士已被用作 K-VQA 的知识源和零样本 QA 模型，并展示了有希望的结果。然而，这些最近的方法没有明确显示回答问题所需的知识，因此缺乏可解释性。受最近关于基于文本的 QA 的法学硕士知识生成工作的启发，在这项工作中，我们提出并测试了一种类似的基于知识生成的 K-VQA 方法，该方法首先从法学硕士生成知识，然后将生成的知识合并到 K-以零样本方式进行 VQA。我们在两个 K-VQA 基准上评估我们的方法，发现我们的方法比以前的零样本 K-VQA 方法表现更好，并且我们生成的知识通常是相关且有用的。</li>
</ul>

<h3>Title: Integration of cognitive tasks into artificial general intelligence test  for large models</h3>
<ul>
<li><strong>Authors: </strong>Youzhi Qu, Chen Wei, Penghui Du, Wenxin Che, Chi Zhang, Wanli Ouyang, Yatao Bian, Feiyang Xu, Bin Hu, Kai Du, Haiyan Wu, Jia Liu, Quanying Liu</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02547">https://arxiv.org/abs/2402.02547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02547">https://arxiv.org/pdf/2402.02547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02547]] Integration of cognitive tasks into artificial general intelligence test  for large models(https://arxiv.org/abs/2402.02547)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>During the evolution of large models, performance evaluation is necessarily performed on the intermediate models to assess their capabilities, and on the well-trained model to ensure safety before practical application. However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models. In this perspective, we advocate for a comprehensive framework of artificial general intelligence (AGI) test, aimed at fulfilling the testing needs of large language models and multi-modal large models with enhanced capabilities. The AGI test framework bridges cognitive science and natural language processing to encompass the full spectrum of intelligence facets, including crystallized intelligence, a reflection of amassed knowledge and experience; fluid intelligence, characterized by problem-solving and adaptive reasoning; social intelligence, signifying comprehension and adaptation within multifaceted social scenarios; and embodied intelligence, denoting the ability to interact with its physical environment. To assess the multidimensional intelligence of large models, the AGI test consists of a battery of well-designed cognitive tests adopted from human intelligence tests, and then naturally encapsulates into an immersive virtual community. We propose that the complexity of AGI testing tasks should increase commensurate with the advancements in large models. We underscore the necessity for the interpretation of test results to avoid false negatives and false positives. We believe that cognitive science-inspired AGI tests will effectively guide the targeted improvement of large models in specific dimensions of intelligence and accelerate the integration of large models into human society.</li>
<li><strong>摘要：</strong>在大型模型的演化过程中，需要对中间模型进行性能评估，以评估其能力，对训练有素的模型进行性能评估，以确保实际应用前的安全性。然而，当前的模型评估主要依赖于特定任务和数据集，缺乏评估大型模型多维智能的统一框架。从这个角度来看，我们主张建立一个全面的通用人工智能（AGI）测试框架，旨在满足具有增强功能的大型语言模型和多模态大型模型的测试需求。 AGI 测试框架将认知科学和自然语言处理联系起来，涵盖了所有的智力方面，包括结晶智力（结晶智力），它是积累的知识和经验的反映；流体智力，以解决问题和适应性推理为特征；社交智能，意味着多方面的社交场景中的理解和适应；体现智能，表示与其物理环境交互的能力。为了评估大型模型的多维智能，AGI测试由一系列精心设计的认知测试组成，这些认知测试借鉴了人类智能测试，然后自然地封装到沉浸式虚拟社区中。我们建议 AGI 测试任务的复杂性应该随着大型模型的进步而相应增加。我们强调解释测试结果以避免假阴性和假阳性的必要性。我们相信，以认知科学为灵感的AGI测试将有效指导大模型在特定智能维度上的针对性改进，加速大模型融入人类社会。</li>
</ul>

<h3>Title: "What's my model inside of?": Exploring the role of environments for  grounded natural language understanding</h3>
<ul>
<li><strong>Authors: </strong>Ronen Tamari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02548">https://arxiv.org/abs/2402.02548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02548">https://arxiv.org/pdf/2402.02548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02548]] "What's my model inside of?": Exploring the role of environments for  grounded natural language understanding(https://arxiv.org/abs/2402.02548)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>In contrast to classical cognitive science which studied brains in isolation, ecological approaches focused on the role of the body and environment in shaping cognition. Similarly, in this thesis we adopt an ecological approach to grounded natural language understanding (NLU) research. Grounded language understanding studies language understanding systems situated in the context of events, actions and precepts in naturalistic/simulated virtual environments. Where classic research tends to focus on designing new models and optimization methods while treating environments as given, we explore the potential of environment design for improving data collection and model development. We developed novel training and annotation approaches for procedural text understanding based on text-based game environments. We also drew upon embodied cognitive linguistics literature to propose a roadmap for grounded NLP research, and to inform the development of a new benchmark for measuring the progress of large language models on challenging commonsense reasoning tasks. We leveraged the richer supervision provided by text-based game environments to develop Breakpoint Transformers, a novel approach to modeling intermediate semantic information in long narrative or procedural texts. Finally, we integrated theories on the role of environments in collective human intelligence to propose a design for AI-augmented "social thinking environments" for knowledge workers like scientists.</li>
<li><strong>摘要：</strong>与孤立地研究大脑的经典认知科学相反，生态学方法侧重于身体和环境在塑造认知中的作用。同样，在本文中，我们采用生态方法进行扎根自然语言理解（NLU）研究。扎根语言理解研究位于自然/模拟虚拟环境中的事件、动作和规则背景下的语言理解系统。经典研究往往侧重于设计新模型和优化方法，同时将环境视为给定的，而我们探索环境设计在改进数据收集和模型开发方面的潜力。我们开发了基于文本的游戏环境的新颖的程序文本理解训练和注释方法。我们还利用具身认知语言学文献提出了扎根 NLP 研究的路线图，并为衡量大型语言模型在具有挑战性的常识推理任务方面的进展的新基准的开发提供了信息。我们利用基于文本的游戏环境提供的更丰富的监督来开发断点变压器，这是一种在长叙述或程序文本中建模中间语义信息的新颖方法。最后，我们整合了关于环境在人类集体智能中的作用的理论，为科学家等知识工作者提出了人工智能增强的“社会思维环境”的设计。</li>
</ul>

<h3>Title: Are Large Language Models Table-based Fact-Checkers?</h3>
<ul>
<li><strong>Authors: </strong>Hangwen Zhang, Qingyi Si, Peng Fu, Zheng Lin, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02549">https://arxiv.org/abs/2402.02549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02549">https://arxiv.org/pdf/2402.02549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02549]] Are Large Language Models Table-based Fact-Checkers?(https://arxiv.org/abs/2402.02549)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables. Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers. In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning can stimulate the TFV capability significantly. We also make some valuable findings about the format of zero-shot prompts and the number of in-context examples. Finally, we analyze some possible directions to promote the accuracy of TFV via LLMs, which is beneficial to further research of table reasoning.</li>
<li><strong>摘要：</strong>基于表的事实验证（TFV）旨在提取语句和结构化表之间的蕴含关系。现有基于小规模模型的 TFV 方法存在标记数据不足和零样本能力弱的问题。近年来，大型语言模型（LLM）的出现在研究领域引起了广泛的关注。他们在多项 NLP 任务上表现出了强大的零样本和上下文学习能力，但他们在 TFV 上的潜力仍然未知。在这项工作中，我们对法学硕士是否是基于表格的事实检查器进行了初步研究。具体来说，我们设计了不同的提示来探索情境学习如何帮助法学硕士实现 TFV，即零样本和少样本 TFV 能力。此外，我们还精心设计和构建了TFV指令来研究LLM的指令调优所带来的性能增益。实验结果表明，LLM 可以通过快速工程在零样本和少样本 TFV 上取得可接受的结果，而指令调优可以显着激发 TFV 能力。我们还对零样本提示的格式和上下文示例的数量做出了一些有价值的发现。最后，我们分析了一些通过LLM提高TFV准确性的可能方向，这有利于表推理的进一步研究。</li>
</ul>

<h3>Title: Enhancing Robustness in Biomedical NLI Models: A Probing Approach for  Clinical Trials</h3>
<ul>
<li><strong>Authors: </strong>Ata Mustafa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02558">https://arxiv.org/abs/2402.02558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02558">https://arxiv.org/pdf/2402.02558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02558]] Enhancing Robustness in Biomedical NLI Models: A Probing Approach for  Clinical Trials(https://arxiv.org/abs/2402.02558)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have revolutionized various fields and industries, such as Conversational AI, Content Generation, Information Retrieval, Business Intelligence, and Medical, to name a few. One major application in the field of medical is to analyze and investigate clinical trials for entailment tasks.However, It has been observed that Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context. Adversarial and robust testing is performed to ensure the integrity of models output. But, ambiguity still persists. In order to ensure the integrity of the reasoning performed and investigate the model has correct syntactic and semantic understanding probing is used. Here, I used mnestic probing to investigate the Sci-five model, trained on clinical trial. I investigated the model for feature learnt with respect to natural logic. To achieve the target, I trained task specific probes. Used these probes to investigate the final layers of trained model. Then, fine tuned the trained model using iterative null projection. The results shows that model accuracy improved. During experimentation, I observed that size of the probe has affect on the fine tuning process.</li>
<li><strong>摘要：</strong>大型语言模型已经彻底改变了各个领域和行业，例如对话式人工智能、内容生成、信息检索、商业智能和医疗等等。医学领域的一个主要应用是分析和研究蕴含任务的临床试验。然而，人们发现大型语言模型容易受到快捷学习、事实不一致和上下文变化很小的性能下降的影响。进行对抗性和稳健的测试以确保模型输出的完整性。但是，模糊性仍然存在。为了确保执行的推理的完整性和调查模型具有正确的句法和语义理解探测。在这里，我使用记忆强化探测来研究经过临床试验训练的 Sci-5 模型。我研究了根据自然逻辑学习的特征模型。为了实现目标，我训练了特定于任务的探针。使用这些探针来研究训练模型的最后几层。然后，使用迭代零投影对训练后的模型进行微调。结果表明模型精度有所提高。在实验过程中，我观察到探头的尺寸对微调过程有影响。</li>
</ul>

<h3>Title: NavHint: Vision and Language Navigation Agent with a Hint Generator</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhang, Quan Guo, Parisa Kordjamshidi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02559">https://arxiv.org/abs/2402.02559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02559">https://arxiv.org/pdf/2402.02559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02559]] NavHint: Vision and Language Navigation Agent with a Hint Generator(https://arxiv.org/abs/2402.02559)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment. In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions. The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent's attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment. We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The experimental results demonstrate that generating hints not only enhances the navigation performance but also helps improve the interpretability of the agent's actions.</li>
<li><strong>摘要：</strong>现有的视觉和语言导航工作主要依靠与导航相关的损失来建立视觉和语言模态之间的联系，忽略了帮助导航代理建立对视觉环境的深入理解的方面。在我们的工作中，我们通过提供详细视觉描述的提示生成器为导航代理提供间接监督。提示生成器帮助导航代理建立对视觉环境的全局理解。它将代理的注意力引导到相关的导航细节上，包括相关的子指令、识别中的潜在挑战和接地中的模糊性以及目标视点描述。为了训练提示生成器，我们根据指令中的地标以及视觉环境中的可见和独特对象构建了一个合成数据集。我们在 R2R 和 R4R 数据集上评估我们的方法，并在几个指标上达到了最先进的水平。实验结果表明，生成提示不仅可以增强导航性能，还有助于提高代理行为的可解释性。</li>
</ul>

<h3>Title: Foundation Model Makes Clustering a Better Initialization for Active  Learning</h3>
<ul>
<li><strong>Authors: </strong>Han Yuan, Chuan Hong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02561">https://arxiv.org/abs/2402.02561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02561">https://arxiv.org/pdf/2402.02561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02561]] Foundation Model Makes Clustering a Better Initialization for Active  Learning(https://arxiv.org/abs/2402.02561)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Active learning selects the most informative samples from the unlabeled dataset to annotate in the context of a limited annotation budget. While numerous methods have been proposed for subsequent sample selection based on an initialized model, scant attention has been paid to the indispensable phase of active learning: selecting samples for model initialization. Most of the previous studies resort to random sampling or naive clustering. However, random sampling is prone to fluctuation, and naive clustering suffers from convergence speed, particularly when dealing with high-dimensional data such as imaging data. In this work, we propose to integrate foundation models with clustering methods to select samples for active learning initialization. Foundation models refer to those trained on massive datasets by the self-supervised paradigm and capable of generating informative and compacted embeddings for various downstream tasks. Leveraging these embeddings to replace raw features such as pixel values, clustering quickly converges and identifies better initial samples. For a comprehensive comparison, we included a classic ImageNet-supervised model to acquire embeddings. Experiments on two clinical tasks of image classification and segmentation demonstrated that foundation model-based clustering efficiently pinpointed informative initial samples, leading to models showcasing enhanced performance than the baseline methods. We envisage that this study provides an effective paradigm for future active learning.</li>
<li><strong>摘要：</strong>主动学习从未标记的数据集中选择信息最丰富的样本，在有限的注释预算的情况下进行注释。虽然人们提出了许多基于初始化模型进行后续样本选择的方法，但人们很少关注主动学习中不可或缺的阶段：为模型初始化选择样本。以前的大多数研究都采用随机抽样或朴素聚类。然而，随机采样容易出现波动，并且朴素聚类会受到收敛速度的影响，特别是在处理成像数据等高维数据时。在这项工作中，我们建议将基础模型与聚类方法相结合，以选择用于主动学习初始化的样本。基础模型是指通过自监督范式在海量数据集上进行训练的模型，能够为各种下游任务生成信息丰富且紧凑的嵌入。利用这些嵌入来替换像素值等原始特征，聚类可以快速收敛并识别更好的初始样本。为了进行全面比较，我们使用了经典的 ImageNet 监督模型来获取嵌入。对图像分类和分割这两个临床任务的实验表明，基于基础模型的聚类可以有效地精确定位信息丰富的初始样本，从而使模型显示出比基线方法增强的性能。我们预计这项研究为未来的主动学习提供了一个有效的范例。</li>
</ul>

<h3>Title: DefInt: A Default-interventionist Framework for Efficient Reasoning with  Hybrid Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Shang, Yu Li, Fengli Xu, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02563">https://arxiv.org/abs/2402.02563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02563">https://arxiv.org/pdf/2402.02563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02563]] DefInt: A Default-interventionist Framework for Efficient Reasoning with  Hybrid Large Language Models(https://arxiv.org/abs/2402.02563)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code, rag, chain-of-thought, tree-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on five representative reasoning tasks show that DefInt consistently achieves state-of-the-art reasoning accuracy and solution diversity. More importantly, it substantially reduces the token cost by 49%-79% compared to the second accurate baselines. Specifically, the open-ended tasks have an average 75% token cost reduction. Code repo with all prompts will be released upon publication.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在广泛的任务中表现出了令人印象深刻的新兴能力，但在处理复杂的推理问题时仍然面临挑战。之前的思想链（CoT）和思想树（ToT）等工作主要侧重于提高准确性，但忽视了快速增加的代币成本，这对于具有巨大规模的开放式现实世界任务来说尤其成问题。解空间。受人类认知双重过程理论的启发，我们提出了一个默认干预主义框架（DefInt）来释放混合法学硕士的协同潜力。默认情况下，DefInt 使用较小规模的语言模型来生成低成本推理思想，这类似于系统 1 产生的快速直觉。如果直觉被认为置信度较低，DefInt 将调用放大语言模型的反射推理，如下所示系统2的干预，可以推翻默认想法并纠正推理过程。对五个代表性推理任务的实验表明，DefInt 始终实现了最先进的推理准确性和解决方案多样性。更重要的是，与第二个准确基线相比，它大幅降低了代币成本 49%-79%。具体来说，开放式任务的代币成本平均降低了 75%。包含所有提示的代码存储库将在发布后发布。</li>
</ul>

<h3>Title: A Truly Joint Neural Architecture for Segmentation and Parsing</h3>
<ul>
<li><strong>Authors: </strong>Danit Yshaayahu Levi, Reut Tsarfaty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02564">https://arxiv.org/abs/2402.02564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02564">https://arxiv.org/pdf/2402.02564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02564]] A Truly Joint Neural Architecture for Segmentation and Parsing(https://arxiv.org/abs/2402.02564)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art performance on parsing, tagging and segmentation of the Hebrew section of UD, using a single model. This proposed architecture is LLM-based and language agnostic, providing a solid foundation for MRLs to obtain further performance improvements and bridge the gap with other languages.</li>
<li><strong>摘要：</strong>当代多语言依存解析器可以解析多种语言，但对于形态丰富的语言（MRL），性能被证明低于其他语言。关键的挑战是，由于空间分隔的输入标记的形态复杂性和模糊性高，作为树中节点的语言单元事先是未知的。 MRL 的前神经依存解析器遵循联合形态句法假设，指出形态分割和句法解析应该联合解决，而不是作为分割先于解析的管道。然而，迄今为止最先进的神经解析器使用严格的管道。在本文中，我们介绍了一种联合神经架构，其中保留输入的所有形态歧义的基于格的表示被提供给弧因子模型，然后该模型立即解决形态分割和句法解析任务。我们对希伯来语（一种丰富且高度模糊的 MRL）进行的实验展示了使用单一模型对 UD 希伯来语部分进行解析、标记和分段的最先进性能。该架构基于法学硕士且与语言无关，为 MRL 获得进一步的性能改进并缩小与其他语言的差距提供了坚实的基础。</li>
</ul>

<h3>Title: On the performance of phonetic algorithms in microtext normalization</h3>
<ul>
<li><strong>Authors: </strong>Yerai Doval, Manuel Vilares, Jesús Vilares</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02591">https://arxiv.org/abs/2402.02591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02591">https://arxiv.org/pdf/2402.02591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02591]] On the performance of phonetic algorithms in microtext normalization(https://arxiv.org/abs/2402.02591)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>User-generated content published on microblogging social networks constitutes a priceless source of information. However, microtexts usually deviate from the standard lexical and grammatical rules of the language, thus making its processing by traditional intelligent systems very difficult. As an answer, microtext normalization consists in transforming those non-standard microtexts into standard well-written texts as a preprocessing step, allowing traditional approaches to continue with their usual processing. Given the importance of phonetic phenomena in non-standard text formation, an essential element of the knowledge base of a normalizer would be the phonetic rules that encode these phenomena, which can be found in the so-called phonetic algorithms. In this work we experiment with a wide range of phonetic algorithms for the English language. The aim of this study is to determine the best phonetic algorithms within the context of candidate generation for microtext normalization. In other words, we intend to find those algorithms that taking as input non-standard terms to be normalized allow us to obtain as output the smallest possible sets of normalization candidates which still contain the corresponding target standard words. As it will be stated, the choice of the phonetic algorithm will depend heavily on the capabilities of the candidate selection mechanism which we usually find at the end of a microtext normalization pipeline. The faster it can make the right choices among big enough sets of candidates, the more we can sacrifice on the precision of the phonetic algorithms in favour of coverage in order to increase the overall performance of the normalization system. KEYWORDS: microtext normalization; phonetic algorithm; fuzzy matching; Twitter; texting</li>
<li><strong>摘要：</strong>在微博社交网络上发布的用户生成内容构成了无价的信息来源。然而，缩微文本通常偏离该语言的标准词汇和语法规则，从而使得传统智能系统对其处理非常困难。作为一个答案，缩微文本规范化包括将那些非标准缩微文本转换为标准的编写良好的文本作为预处理步骤，从而允许传统方法继续其通常的处理。考虑到语音现象在非标准文本形成中的重要性，规范化器知识库的一个基本元素将是编码这些现象的语音规则，这可以在所谓的语音算法中找到。在这项工作中，我们尝试了多种英语语音算法。本研究的目的是在缩微文本标准化候选生成的背景下确定最佳语音算法。换句话说，我们打算找到那些将非标准术语作为输入进行标准化的算法，使我们能够获得仍然包含相应目标标准单词的尽可能最小的标准化候选集作为输出。正如将要指出的，语音算法的选择将在很大程度上取决于我们通常在缩微文本标准化管道末端找到的候选选择机制的功能。它在足够大的候选集合中做出正确选择的速度越快，我们就越能牺牲语音算法的精度以支持覆盖范围，以提高标准化系统的整体性能。关键词：缩微文本规范化；语音算法；模糊匹配；推特;发短信</li>
</ul>

<h3>Title: Unified Training of Universal Time Series Forecasting Transformers</h3>
<ul>
<li><strong>Authors: </strong>Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02592">https://arxiv.org/abs/2402.02592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02592">https://arxiv.org/pdf/2402.02592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02592]] Unified Training of Universal Time Series Forecasting Transformers(https://arxiv.org/abs/2402.02592)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, model weights, and data will be released.</li>
<li><strong>摘要：</strong>用于时间序列预测的深度学习传统上在每个数据集一个模型的框架内运行，限制了其利用大型预训练模型改变游戏规则的影响的潜力。通用预测的概念源于对大量时间序列数据集的预训练，设想了一个能够解决各种下游预测任务的单一大型时间序列模型。然而，构建这样的模型对时间序列数据提出了独特的挑战：i）跨频率学习，ii）为多元时间序列容纳任意数量的变量，以及iii）解决大规模数据固有的不同分布特性。为了应对这些挑战，我们对传统时间序列 Transformer 架构进行了新颖的增强，从而提出了基于掩码编码器的通用时间序列预测 Transformer (Moirai)。 Moirai 在我们新推出的大规模开放时间序列存档 (LOTSA) 上进行训练，该模型具有跨九个域的超过 27B 观测值，与全样本模型相比，Moirai 作为零样本预测器实现了具有竞争力或卓越的性能。代码、模型权重和数据将被发布。</li>
</ul>

<h3>Title: Leveraging Continuously Differentiable Activation Functions for Learning  in Quantized Noisy Environments</h3>
<ul>
<li><strong>Authors: </strong>Vivswan Shah, Nathan Youngblood</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02593">https://arxiv.org/abs/2402.02593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02593">https://arxiv.org/pdf/2402.02593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02593]] Leveraging Continuously Differentiable Activation Functions for Learning  in Quantized Noisy Environments(https://arxiv.org/abs/2402.02593)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Real-world analog systems intrinsically suffer from noise that can impede model convergence and accuracy on a variety of deep learning models. We demonstrate that differentiable activations like GELU and SiLU enable robust propagation of gradients which help to mitigate analog quantization error that is ubiquitous to all analog systems. We perform analysis and training of convolutional, linear, and transformer networks in the presence of quantized noise. Here, we are able to demonstrate that continuously differentiable activation functions are significantly more noise resilient over conventional rectified activations. As in the case of ReLU, the error in gradients are 100x higher than those in GELU near zero. Our findings provide guidance for selecting appropriate activations to realize performant and reliable hardware implementations across several machine learning domains such as computer vision, signal processing, and beyond.</li>
<li><strong>摘要：</strong>现实世界的模拟系统本质上会受到噪声的影响，这些噪声可能会阻碍各种深度学习模型的模型收敛和准确性。我们证明了像 GELU 和 SiLU 这样的可微激活能够实现梯度的稳健传播，这有助于减轻所有模拟系统中普遍存在的模拟量化误差。我们在存在量化噪声的情况下对卷积网络、线性网络和变压器网络进行分析和训练。在这里，我们能够证明连续可微的激活函数比传统的校正激活函数具有显着更高的抗噪声能力。与 ReLU 的情况一样，梯度误差比 GELU 中接近零的误差高 100 倍。我们的研究结果为选择适当的激活提供了指导，以在计算机视觉、信号处理等多个机器学习领域实现高性能和可靠的硬件实现。</li>
</ul>

<h3>Title: Accelerating Inverse Reinforcement Learning with Expert Bootstrapping</h3>
<ul>
<li><strong>Authors: </strong>David Wu, Sanjiban Choudhury</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02608">https://arxiv.org/abs/2402.02608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02608">https://arxiv.org/pdf/2402.02608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02608]] Accelerating Inverse Reinforcement Learning with Expert Bootstrapping(https://arxiv.org/abs/2402.02608)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Existing inverse reinforcement learning methods (e.g. MaxEntIRL, $f$-IRL) search over candidate reward functions and solve a reinforcement learning problem in the inner loop. This creates a rather strange inversion where a harder problem, reinforcement learning, is in the inner loop of a presumably easier problem, imitation learning. In this work, we show that better utilization of expert demonstrations can reduce the need for hard exploration in the inner RL loop, hence accelerating learning. Specifically, we propose two simple recipes: (1) placing expert transitions into the replay buffer of the inner RL algorithm (e.g. Soft-Actor Critic) which directly informs the learner about high reward states instead of forcing the learner to discover them through extensive exploration, and (2) using expert actions in Q value bootstrapping in order to improve the target Q value estimates and more accurately describe high value expert states. Our methods show significant gains over a MaxEntIRL baseline on the benchmark MuJoCo suite of tasks, speeding up recovery to 70\% of deterministic expert performance by 2.13x on HalfCheetah-v2, 2.6x on Ant-v2, 18x on Hopper-v2, and 3.36x on Walker2d-v2.</li>
<li><strong>摘要：</strong>现有的逆强化学习方法（例如 MaxEntIRL、$f$-IRL）搜索候选奖励函数并解决内循环中的强化学习问题。这造成了一种相当奇怪的倒置，即更难的问题（强化学习）位于一个可能更简单的问题（模仿学习）的内循环中。在这项工作中，我们表明，更好地利用专家演示可以减少内部 RL 循环中艰苦探索的需要，从而加速学习。具体来说，我们提出了两个简单的方法：（1）将专家转换放入内部 RL 算法（例如 Soft-Actor Critic）的重播缓冲区中，它直接告知学习者有关高奖励状态的信息，而不是强迫学习者通过广泛的探索来发现它们，(2) 在 Q 值引导中使用专家动作，以改进目标 Q 值估计并更准确地描述高值专家状态。我们的方法在 MuJoCo 任务套件基准上显示出相对于 MaxEntIRL 基线的显着提升，将确定性专家性能的恢复速度加快了 2.13 倍（HalfCheetah-v2 上为 2.13 倍）、2.6 倍（Ant-v2 上为 2.6 倍）、18 倍（Hopper-v2 上为），以及Walker2d-v2 上为 3.36 倍。</li>
</ul>

<h3>Title: PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial  Reasoning Problems?</h3>
<ul>
<li><strong>Authors: </strong>Chinmay Mittal, Krishna Kartik, Mausam, Parag Singla</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02611">https://arxiv.org/abs/2402.02611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02611">https://arxiv.org/pdf/2402.02611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02611]] PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial  Reasoning Problems?(https://arxiv.org/abs/2402.02611)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such challenging problems. We also show how feedback from smaller solved instances can help improve this reasoning ability.</li>
<li><strong>摘要：</strong>最近的工作探索了使用法学硕士进行推理任务，重点关注相对简单的问题，例如逻辑问题回答。在我们的工作中，我们希望解决更复杂的问题，显着扩展这些模型的功能。特别是，我们探索法学硕士是否可以解决具有挑战性的一阶组合推理问题，一个例子是流行的数独谜题。这些问题具有由自然语言的一般描述描述的底层一阶结构，并且可以实例化为不同大小的实例。此外，这些问题的计算量很大，需要多个推理步骤才能得出解决方案。我们向 PuzzleBench 提供了包含 31 个此类挑战性谜题的数据集。我们观察到，法学硕士即使在符号求解器的帮助下，在我们的基准测试中表现也相当差。作为回应，我们提出了一种新方法，即 Puzzle-LM，它将法学硕士与符号求解器和程序解释器相结合，使他们能够推理此类具有挑战性的问题。我们还展示了较小的已解决实例的反馈如何帮助提高这种推理能力。</li>
</ul>

<h3>Title: The Virtues of Pessimism in Inverse Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>David Wu, Gokul Swamy, J. Andrew Bagnell, Zhiwei Steven Wu, Sanjiban Choudhury</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02616">https://arxiv.org/abs/2402.02616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02616">https://arxiv.org/pdf/2402.02616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02616]] The Virtues of Pessimism in Inverse Reinforcement Learning(https://arxiv.org/abs/2402.02616)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>Inverse Reinforcement Learning (IRL) is a powerful framework for learning complex behaviors from expert demonstrations. However, it traditionally requires repeatedly solving a computationally expensive reinforcement learning (RL) problem in its inner loop. It is desirable to reduce the exploration burden by leveraging expert demonstrations in the inner-loop RL. As an example, recent work resets the learner to expert states in order to inform the learner of high-reward expert states. However, such an approach is infeasible in the real world. In this work, we consider an alternative approach to speeding up the RL subroutine in IRL: \emph{pessimism}, i.e., staying close to the expert's data distribution, instantiated via the use of offline RL algorithms. We formalize a connection between offline RL and IRL, enabling us to use an arbitrary offline RL algorithm to improve the sample efficiency of IRL. We validate our theory experimentally by demonstrating a strong correlation between the efficacy of an offline RL algorithm and how well it works as part of an IRL procedure. By using a strong offline RL algorithm as part of an IRL procedure, we are able to find policies that match expert performance significantly more efficiently than the prior art.</li>
<li><strong>摘要：</strong>逆强化学习 (IRL) 是一个强大的框架，用于从专家演示中学习复杂行为。然而，传统上它需要在其内部循环中重复解决计算成本昂贵的强化学习（RL）问题。我们希望通过利用内循环 RL 中的专家演示来减轻探索负担。例如，最近的工作将学习者重置为专家状态，以便告知学习者高回报的专家状态。然而，这种方法在现实世界中是不可行的。在这项工作中，我们考虑了一种在 IRL 中加速 RL 子例程的替代方法：\emph{pessimism}，即接近专家的数据分布，通过使用离线 RL 算法进行实例化。我们形式化了离线 RL 和 IRL 之间的联系，使我们能够使用任意离线 RL 算法来提高 IRL 的样本效率。我们通过实验验证了我们的理论，证明了离线 RL 算法的功效与其作为 IRL 程序一部分的工作效果之间的强相关性。通过使用强大的离线 RL 算法作为 IRL 过程的一部分，我们能够比现有技术更有效地找到匹配专家表现的策略。</li>
</ul>

<h3>Title: Increasing Trust in Language Models through the Reuse of Verified  Circuits</h3>
<ul>
<li><strong>Authors: </strong>Philip Quirke, Clement Neo, Fazl Barez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02619">https://arxiv.org/abs/2402.02619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02619">https://arxiv.org/pdf/2402.02619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02619]] Increasing Trust in Language Models through the Reuse of Verified  Circuits(https://arxiv.org/abs/2402.02619)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of language models built using them. The reuse of verified circuits reduces the effort to verify more complex composite models which we believe to be a significant step towards safety of language models.</li>
<li><strong>摘要：</strong>语言模型 (LM) 越来越多地用于各种预测任务，但它们的训练通常会忽略罕见的边缘情况，从而降低了其可靠性。在这里，我们定义了严格的可信度标准，必须验证任务算法和电路实现，考虑到边缘情况，没有已知的故障模式。我们表明，如果使用数学和逻辑上指定的框架构建变压器模型，则可以对其进行训练以满足此标准。在本文中，我们充分验证了n位整数加法的模型。为了展示已验证模块的可重用性，我们将经过训练的整数加法模型插入到未经训练的模型中，并训练组合模型来执行加法和减法。我们发现这两项任务都广泛重用了加法电路，从而简化了更复杂的减法器模型的验证。我们讨论如何将经过验证的任务模块插入语言模型中，从而利用模型重用来提高使用它们构建的语言模型的可验证性和可信度。已验证电路的重用减少了验证更复杂的复合模型的工作量，我们认为这是朝着语言模型安全性迈出的重要一步。</li>
</ul>

<h3>Title: DenseFormer: Enhancing Information Flow in Transformers via Depth  Weighted Averaging</h3>
<ul>
<li><strong>Authors: </strong>Matteo Pagliardini, Amirkeivan Mohtashami, Francois Fleuret, Martin Jaggi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02622">https://arxiv.org/abs/2402.02622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02622">https://arxiv.org/pdf/2402.02622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02622]] DenseFormer: Enhancing Information Flow in Transformers via Depth  Weighted Averaging(https://arxiv.org/abs/2402.02622)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms of memory efficiency and inference time.</li>
<li><strong>摘要：</strong>Vaswani 等人的变压器架构。 （2017）现在在从自然语言处理到语音处理和图像理解的应用领域中无处不在。我们提出了 DenseFormer，这是对标准架构的简单修改，可以在不增加模型大小的情况下改善模型的复杂性——为 100B 参数范围内的大型模型添加数千个参数。我们的方法依赖于每个变压器块之后的额外平均步骤，该步骤计算当前和过去表示的加权平均值——我们将此操作称为深度加权平均（DWA）。学习到的 DWA 权重表现出连贯的信息流模式，揭示了对来自远处层的激活的强大且结构化的重用。实验表明，DenseFormer 的数据效率更高，达到了与更深层次的 Transformer 模型相同的困惑度，并且对于相同的困惑度，这些新模型在内存效率和推理时间方面优于 Transformer 基线。</li>
</ul>

<h3>Title: Enhancing Transformer RNNs with Multiple Temporal Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Razvan-Gabriel Dumitru, Darius Peteleaza, Mihai Surdeanu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02625">https://arxiv.org/abs/2402.02625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02625">https://arxiv.org/pdf/2402.02625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02625]] Enhancing Transformer RNNs with Multiple Temporal Perspectives(https://arxiv.org/abs/2402.02625)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code</a></li>
<li><strong>Abstract: </strong>We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, ensuring consistent efficiency across various sequence lengths. The empirical results and ablation studies included in our research validate the effectiveness of our approach, showcasing improved performance across multiple benchmarks. The code, model weights and datasets are open-sourced at: https://github.com/RazvanDu/TemporalRNNs.</li>
<li><strong>摘要：</strong>我们引入了多时间视角的概念，这是一种适用于循环神经网络（RNN）架构的新颖方法，用于增强对顺序数据的理解。该方法涉及维护先前遇到的文本的不同时间视图，显着丰富了语言模型解释上下文的能力。为了展示这种方法的有效性，我们将其纳入接收加权键值（RWKV）架构中，解决其在单个隐藏状态中保留所有历史信息的固有挑战。值得注意的是，这种改进是通过最小程度地增加参数数量来实现的——甚至只有原始参数数量的 $0.04\%$。此外，多个时间视角所需的附加参数以最小的计算开销进行微调，从而避免了完整预训练的需要。生成的模型在即时推理期间保持线性计算复杂性，确保在不同序列长度上保持一致的效率。我们的研究中包含的实证结果和消融研究验证了我们方法的有效性，展示了多个基准的性能改进。代码、模型权重和数据集均开源于：https://github.com/RazvanDu/TemporalRNNs。</li>
</ul>

<h3>Title: PROSAC: Provably Safe Certification for Machine Learning Models under  Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ziquan Liu, Zhuo Zhi, Ilija Bogunovic, Carsten Gerner-Beuerle, Miguel Rodrigues</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02629">https://arxiv.org/abs/2402.02629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02629">https://arxiv.org/pdf/2402.02629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02629]] PROSAC: Provably Safe Certification for Machine Learning Models under  Adversarial Attacks(https://arxiv.org/abs/2402.02629)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>It is widely known that state-of-the-art machine learning models, including vision and language models, can be seriously compromised by adversarial perturbations. It is therefore increasingly relevant to develop capabilities to certify their performance in the presence of the most effective adversarial attacks. Our paper offers a new approach to certify the performance of machine learning models in the presence of adversarial attacks with population level risk guarantees. In particular, we introduce the notion of $(\alpha,\zeta)$ machine learning model safety. We propose a hypothesis testing procedure, based on the availability of a calibration set, to derive statistical guarantees providing that the probability of declaring that the adversarial (population) risk of a machine learning model is less than $\alpha$ (i.e. the model is safe), while the model is in fact unsafe (i.e. the model adversarial population risk is higher than $\alpha$), is less than $\zeta$. We also propose Bayesian optimization algorithms to determine efficiently whether a machine learning model is $(\alpha,\zeta)$-safe in the presence of an adversarial attack, along with statistical guarantees. We apply our framework to a range of machine learning models including various sizes of vision Transformer (ViT) and ResNet models impaired by a variety of adversarial attacks, such as AutoAttack, SquareAttack and natural evolution strategy attack, to illustrate the operation of our approach. Importantly, we show that ViT's are generally more robust to adversarial attacks than ResNets, and ViT-large is more robust than smaller models. Our approach goes beyond existing empirical adversarial risk-based certification guarantees. It formulates rigorous (and provable) performance guarantees that can be used to satisfy regulatory requirements mandating the use of state-of-the-art technical tools.</li>
<li><strong>摘要：</strong>众所周知，最先进的机器学习模型，包括视觉和语言模型，可能会受到对抗性扰动的严重损害。因此，开发能力来证明其在最有效的对抗性攻击下的性能变得越来越重要。我们的论文提供了一种新方法，可以在存在对抗性攻击的情况下验证机器学习模型的性能，并提供群体级别的风险保证。特别是，我们引入了 $(\alpha,\zeta)$ 机器学习模型安全性的概念。我们提出了一个假设检验程序，基于校准集的可用性，以得出统计保证，前提是声明机器学习模型的对抗（群体）风险的概率小于 $\alpha$ （即该模型是安全），而模型实际上是不安全的（即模型对抗性群体风险高于 $\alpha$），但低于 $\zeta$。我们还提出了贝叶斯优化算法，以有效地确定机器学习模型在存在对抗性攻击的情况下是否是 $(\alpha,\zeta)$ 安全的，以及统计保证。我们将我们的框架应用于一系列机器学习模型，包括受到各种对抗性攻击（例如 AutoAttack、SquareAttack 和自然进化策略攻击）损害的各种大小的视觉 Transformer (ViT) 和 ResNet 模型，以说明我们方法的操作。重要的是，我们表明 ViT 通常比 ResNet 对对抗性攻击更稳健，并且 ViT-large 比小型模型更稳健。我们的方法超越了现有的基于经验的对抗性风险的认证保证。它制定了严格的（且可证明的）性能保证，可用于满足强制使用最先进技术工具的监管要求。</li>
</ul>

<h3>Title: Learning to Understand: Identifying Interactions via the Mobius  Transform</h3>
<ul>
<li><strong>Authors: </strong>Justin S. Kang, Yigit E. Erginbas, Landon Butler, Ramtin Pedarsani, Kannan Ramchandran</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02631">https://arxiv.org/abs/2402.02631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02631">https://arxiv.org/pdf/2402.02631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02631]] Learning to Understand: Identifying Interactions via the Mobius  Transform(https://arxiv.org/abs/2402.02631)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>One of the most fundamental problems in machine learning is finding interpretable representations of the functions we learn. The Mobius transform is a useful tool for this because its coefficients correspond to unique importance scores on sets of input variables. The Mobius Transform is strongly related (and in some cases equivalent) to the concept of Shapley value, which is a widely used game-theoretic notion of importance. This work focuses on the (typical) regime where the fraction of non-zero Mobius coefficients (and thus interactions between inputs) is small compared to the set of all $2^n$ possible interactions between $n$ inputs. When there are $K = O(2^{n \delta})$ with $\delta \leq \frac{1}{3}$ non-zero coefficients chosen uniformly at random, our algorithm exactly recovers the Mobius transform in $O(Kn)$ samples and $O(Kn^2)$ time with vanishing error as $K \rightarrow \infty$, the first non-adaptive algorithm to do so. We also uncover a surprising connection between group testing and the Mobius transform. In the case where all interactions are between at most $t = \Theta(n^{\alpha})$ inputs, for $\alpha < 0.409$, we are able to leverage results from group testing to provide the first algorithm that computes the Mobius transform in $O(Kt\log n)$ sample complexity and $O(K\mathrm{poly}(n))$ time with vanishing error as $K \rightarrow \infty$. Finally, we present a robust version of this algorithm that achieves the same sample and time complexity under some assumptions, but with a factor depending on noise variance. Our work is deeply interdisciplinary, drawing from tools spanning across signal processing, algebra, information theory, learning theory and group testing to address this important problem at the forefront of machine learning.</li>
<li><strong>摘要：</strong>机器学习中最基本的问题之一是找到我们所学函数的可解释表示。莫比乌斯变换是一个有用的工具，因为它的系数对应于输入变量集的独特重要性得分。莫比乌斯变换与沙普利值的概念密切相关（在某些情况下是等效的），沙普利值是一种广泛使用的博弈论重要性概念。这项工作重点关注（典型）机制，其中非零莫比乌斯系数（以及输入之间的相互作用）的比例与 $n$ 输入之间所有 $2^n$ 可能的相互作用的集合相比很小。当存在 $K = O(2^{n \delta})$ 且 $\delta \leq \frac{1}{3}$ 随机均匀选择的非零系数时，我们的算法精确地恢复 $\delta \leq \frac{1}{3}$ 中的莫比乌斯变换O(Kn)$ 个样本和 $O(Kn^2)$ 时间，误差消失为 $K \rightarrow \infty$，这是第一个这样做的非自适应算法。我们还发现了群体测试和莫比乌斯变换之间令人惊讶的联系。在所有交互最多在 $t = \Theta(n^{\alpha})$ 输入之间的情况下，对于 $\alpha < 0.409$，我们能够利用分组测试的结果来提供第一个计算的算法$O(Kt\log n)$ 样本复杂度和 $O(K\mathrm{poly}(n))$ 时间的莫比乌斯变换，消失误差为 $K \rightarrow \infty$。最后，我们提出了该算法的鲁棒版本，该算法在某些假设下实现了相同的样本和时间复杂度，但具有取决于噪声方差的因素。我们的工作是深度跨学科的，利用信号处理、代数、信息论、学习理论和小组测试等领域的工具来解决机器学习前沿的这一重要问题。</li>
</ul>

<h3>Title: Predicting Machine Translation Performance on Low-Resource Languages:  The Role of Domain Similarity</h3>
<ul>
<li><strong>Authors: </strong>Eric Khiu, Hasti Toossi, David Anugraha, Jinyu Liu, Jiaxu Li, Juan Armando Parra Flores, Leandro Acros Roman, A. Seza Doğruöz, En-Shiun Annie Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02633">https://arxiv.org/abs/2402.02633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02633">https://arxiv.org/pdf/2402.02633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02633]] Predicting Machine Translation Performance on Low-Resource Languages:  The Role of Domain Similarity(https://arxiv.org/abs/2402.02633)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning and testing a multilingual large language model is expensive and challenging for low-resource languages (LRLs). While previous studies have predicted the performance of natural language processing (NLP) tasks using machine learning methods, they primarily focus on high-resource languages, overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate three factors: the size of the fine-tuning corpus, the domain similarity between fine-tuning and testing corpora, and the language similarity between source and target languages. We employ classical regression models to assess how these factors impact the model's performance. Our results indicate that domain similarity has the most critical impact on predicting the performance of Machine Translation models.</li>
<li><strong>摘要：</strong>对于低资源语言 (LRL) 来说，微调和测试多语言大语言模型成本高昂且具有挑战性。虽然之前的研究已经使用机器学习方法预测了自然语言处理 (NLP) 任务的性能，但它们主要关注高资源语言，忽略了 LRL 和跨领域的转换。针对LRL，我们研究了三个因素：微调语料库的大小、微调语料库和测试语料库之间的领域相似性以及源语言和目标语言之间的语言相似性。我们采用经典回归模型来评估这些因素如何影响模型的性能。我们的结果表明，领域相似性对预测机器翻译模型的性能具有最关键的影响。</li>
</ul>

<h3>Title: Can Large Language Models Learn Independent Causal Mechanisms?</h3>
<ul>
<li><strong>Authors: </strong>Gaël Gendron, Bao Trung Nguyen, Alex Yuxuan Peng, Michael Witbrock, Gillian Dobbie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02636">https://arxiv.org/abs/2402.02636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02636">https://arxiv.org/pdf/2402.02636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02636]] Can Large Language Models Learn Independent Causal Mechanisms?(https://arxiv.org/abs/2402.02636)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability. This issue has usually been alleviated by feeding more training data into the LLM. However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training. By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language modelling modules. We introduce a routing scheme to induce specialisation of the network into domain-specific modules. We also present a Mutual Information minimisation objective that trains a separate module to learn abstraction and domain-invariant mechanisms. We show that such causal constraints can improve out-of-distribution performance on abstract and causal reasoning tasks.</li>
<li><strong>摘要：</strong>尽管在语言建模和复杂推理任务上表现出色，但大型语言模型 (LLM) 在不常见的环境或分布变化的相同任务上表现不佳，表现出泛化能力的缺乏。通常可以通过向法​​学硕士提供更多训练数据来缓解这个问题。然而，这种方法很脆弱，因为任务的范围可能不容易预测或可能发生变化，并且用新数据更新模型通常需要大量的额外训练。相比之下，学习抽象变量和因果关系的系统（例如因果模型）可以表现出针对分布变化的更强的鲁棒性。这一成功的原因之一是独立因果机制 (ICM) 的存在和使用，这些机制代表了稀疏交互的高级概念。在这项工作中，我们应用因果关系中的两个概念来学习法学硕士中的 ICM。我们开发了一种新的法学硕士架构，由多个稀疏交互的语言建模模块组成。我们引入了一种路由方案，将网络的专业化引入特定于域的模块。我们还提出了一个互信息最小化目标，该目标训练一个单独的模块来学习抽象和领域不变机制。我们表明，这种因果约束可以提高抽象和因果推理任务的分布外性能。</li>
</ul>

<h3>Title: Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses</h3>
<ul>
<li><strong>Authors: </strong>Jinwoo Ahn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02648">https://arxiv.org/abs/2402.02648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02648">https://arxiv.org/pdf/2402.02648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02648]] Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses(https://arxiv.org/abs/2402.02648)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the other hand, to mitigate the effects of the aforementioned inconsistencies, we present a novel method of recursively revising the initial incorrect reasoning provided by the LLM by repetitively breaking down each incorrect step into smaller individual problems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 经常遇到知识密集型问题，尽管给出相同的输入，但提供不同的输出通常会导致不一致。当用户表达坚定的反对立场时，响应质量会恶化，导致法学硕士调整其响应，尽管最初的响应是正确的。这些行为降低了这些模型提供的响应的可靠性和有效性。在本文中，我们试图 1) 通过展示反馈链 (CoF) 如何触发法学硕士更多地偏离实际答案，提高人们对过度依赖 ChatGPT 等人工智能代理所带来的固有风险的认识，2) 建议新颖的提示方法，递归反馈链（R-CoF），我们正在进一步研究。 CoF 系统采用开放式多步骤问题。然后，我们不断地提供毫无意义的反馈，要求再次尝试。我们的初步实验表明，此类反馈只会降低响应的质量。另一方面，为了减轻上述不一致的影响，我们提出了一种新颖的方法，通过将每个不正确的步骤重复分解为更小的单个问题，递归地修正法学硕士提供的最初错误推理。</li>
</ul>

<h3>Title: Vision-Language Models Provide Promptable Representations for  Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>William Chen, Oier Mees, Aviral Kumar, Sergey Levine</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02651">https://arxiv.org/abs/2402.02651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02651">https://arxiv.org/pdf/2402.02651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02651]] Vision-Language Models Provide Promptable Representations for  Reinforcement Learning(https://arxiv.org/abs/2402.02651)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code, rag, agent</a></li>
<li><strong>Abstract: </strong>Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings.</li>
<li><strong>摘要：</strong>人类可以通过利用背景世界知识快速学习新行为。相比之下，经过强化学习（RL）训练的智能体通常从头开始学习行为。因此，我们提出了一种新颖的方法，该方法使用视觉语言模型（VLM）中编码的大量通用且可索引的世界知识，这些知识是在互联网规模的数据上预先训练的，用于具体的强化学习。我们通过使用 VLM 作为提示表示来初始化策略：基于视觉观察的嵌入，并根据 VLM 的内部知识对语义特征进行编码，通过提供任务上下文和辅助信息的提示来引出。我们评估了我们在 Minecraft 中视觉复杂、长视野 RL 任务和 Habitat 中机器人导航的方法。我们发现，我们的针对从通用 VLM 中提取的嵌入进行训练的策略优于针对通用、非提示图像嵌入进行训练的同等策略。我们还发现我们的方法优于指令跟踪方法，并且与特定领域的嵌入相当。</li>
</ul>

<h3>Title: Learning with Mixture of Prototypes for Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Haodong Lu, Dong Gong, Shuo Wang, Jason Xue, Lina Yao, Kristen Moore</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02653">https://arxiv.org/abs/2402.02653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02653">https://arxiv.org/pdf/2402.02653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02653]] Learning with Mixture of Prototypes for Out-of-Distribution Detection(https://arxiv.org/abs/2402.02653)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation learning. They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes. However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data. Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance. To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) which models each class with multiple prototypes to capture the sample diversities, and learns more faithful and compact samples embeddings to enhance OOD detection. Our method automatically identifies and dynamically updates prototypes, assigning each sample to a subset of prototypes via reciprocal neighbor soft assignment weights. PALM optimizes a maximum likelihood estimation (MLE) loss to encourage the sample embeddings to be compact around the associated prototypes, as well as a contrastive loss on all prototypes to enhance intra-class compactness and inter-class discrimination at the prototype level. Moreover, the automatic estimation of prototypes enables our approach to be extended to the challenging OOD detection task with unlabelled ID data. Extensive experiments demonstrate the superiority of PALM, achieving state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark. Code is available at https://github.com/jeff024/PALM.</li>
<li><strong>摘要：</strong>分布外（OOD）检测旨在检测远离分布内（ID）训练数据的测试样本，这对于机器学习模型在现实世界中的安全部署至关重要。随着深度表示学习的增强，基于距离的 OOD 检测方法应运而生。他们通过测量与 ID 类质心或原型的距离来识别看不见的 OOD 样本。然而，现有的方法依赖于过于简化的数据假设来学习表示，例如，使用一个质心类原型对每一类的 ID 数据进行建模，或者使用并非为 OOD 检测而设计的损失函数，这忽略了数据内的自然多样性。天真地强制每一类的数据样本仅围绕一个原型紧凑，导致对实际数据的建模不充分并且性能有限。为了解决这些问题，我们提出了原型混合学习（PALM），它用多个原型对每个类进行建模以捕获样本多样性，并学习更忠实和紧凑的样本嵌入以增强 OOD 检测。我们的方法自动识别并动态更新原型，通过互邻软分配权重将每个样本分配给原型的子集。 PALM 优化了最大似然估计 (MLE) 损失，以鼓励样本嵌入在相关原型周围紧凑，并优化所有原型上的对比损失，以增强原型级别的类内紧凑性和类间区分度。此外，原型的自动估计使我们的方法能够扩展到具有未标记 ID 数据的具有挑战性的 OOD 检测任务。大量实验证明了 PALM 的优越性，在具有挑战性的 CIFAR-100 基准上实现了最先进的平均 AUROC 性能 93.82。代码可在 https://github.com/jeff024/PALM 获取。</li>
</ul>

<h3>Title: RACER: An LLM-powered Methodology for Scalable Analysis of  Semi-structured Mental Health Interviews</h3>
<ul>
<li><strong>Authors: </strong>Satpreet Harcharan Singh, Kevin Jiang, Kanchan Bhasin, Ashutosh Sabharwal, Nidal Moukaddam, Ankit B Patel</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02656">https://arxiv.org/abs/2402.02656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02656">https://arxiv.org/pdf/2402.02656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02656]] RACER: An LLM-powered Methodology for Scalable Analysis of  Semi-structured Mental Health Interviews(https://arxiv.org/abs/2402.02656)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Semi-structured interviews (SSIs) are a commonly employed data-collection method in healthcare research, offering in-depth qualitative insights into subject experiences. Despite their value, the manual analysis of SSIs is notoriously time-consuming and labor-intensive, in part due to the difficulty of extracting and categorizing emotional responses, and challenges in scaling human evaluation for large populations. In this study, we develop RACER, a Large Language Model (LLM) based expert-guided automated pipeline that efficiently converts raw interview transcripts into insightful domain-relevant themes and sub-themes. We used RACER to analyze SSIs conducted with 93 healthcare professionals and trainees to assess the broad personal and professional mental health impacts of the COVID-19 crisis. RACER achieves moderately high agreement with two human evaluators (72%), which approaches the human inter-rater agreement (77%). Interestingly, LLMs and humans struggle with similar content involving nuanced emotional, ambivalent/dialectical, and psychological statements. Our study highlights the opportunities and challenges in using LLMs to improve research efficiency and opens new avenues for scalable analysis of SSIs in healthcare research.</li>
<li><strong>摘要：</strong>半结构化访谈 (SSI) 是医疗保健研究中常用的数据收集方法，可为受试者体验提供深入的定性见解。尽管有其价值，但 SSI 的手动分析非常耗时且费力，部分原因是情绪反应的提取和分类困难，以及对大量人群进行人类评估的挑战。在这项研究中，我们开发了 RACER，这是一种基于大型语言模型 (LLM) 的专家引导自动化管道，可有效地将原始采访记录转换为富有洞察力的领域相关主题和子主题。我们使用 RACER 分析了 93 名医疗保健专业人员和学员进行的 SSI，以评估 COVID-19 危机对个人和专业心理健康的广泛影响。 RACER 与两名人类评估者达成了中等高度的一致性 (72%)，接近人类评估者间的一致性 (77%)。有趣的是，法学硕士和人类都在处理类似的内容，涉及微妙的情感、矛盾/辩证和心理陈述。我们的研究强调了利用法学硕士提高研究效率的机遇和挑战，并为医疗保健研究中 SSI 的可扩展分析开辟了新途径。</li>
</ul>

<h3>Title: Counterfactual Explanations of Black-box Machine Learning Models using  Causal Discovery with Applications to Credit Rating</h3>
<ul>
<li><strong>Authors: </strong>Daisuke Takahashi, Shohei Shimizu, Takuma Tanaka</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02678">https://arxiv.org/abs/2402.02678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02678">https://arxiv.org/pdf/2402.02678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02678]] Counterfactual Explanations of Black-box Machine Learning Models using  Causal Discovery with Applications to Credit Rating(https://arxiv.org/abs/2402.02678)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions. Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features. The majority of these models have been based their explanations on counterfactual probabilities, assuming that the causal graph is known. However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases. Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known. This framework leveraged counterfactual probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classification model. Furthermore, explanatory scores were estimated based on counterfactual probabilities. Numerical experiments conducted employing artificial data confirmed the possibility of estimating the explanatory score more accurately than in the absence of a causal graph. Finally, as an application to real data, we constructed a classification model of credit ratings assigned by Shiga Bank, Shiga prefecture, Japan. We demonstrated the effectiveness of the proposed method in cases where the causal graph is unknown.</li>
<li><strong>摘要：</strong>可解释的人工智能（XAI）有助于阐明机器学习算法的内部机制，通过展示其预测的基础来增强其可靠性。一些 XAI 模型通过检查预测模型的输入输出关系以及特征之间的依赖关系来考虑因果关系来解释模型。这些模型中的大多数都是基于反事实概率的解释，假设因果图是已知的。然而，鉴于大多数情况下特征之间的因果关系是未知的，这种假设使此类模型在实际数据中的应用变得复杂。因此，本研究提出了一种新颖的 XAI 框架，放宽了因果图已知的约束。该框架利用反事实概率和关于因果结构的附加先验信息，促进通过因果发现方法和黑盒分类模型估计的因果图的集成。此外，解释性分数是根据反事实概率估计的。使用人工数据进行的数值实验证实了比没有因果图的情况下更准确地估计解释性分数的可能性。最后，作为实际数据的应用，我们构建了日本滋贺县滋贺银行信用评级的分类模型。我们证明了所提出的方法在因果图未知的情况下的有效性。</li>
</ul>

<h3>Title: Large Language Models are Geographically Biased</h3>
<ul>
<li><strong>Authors: </strong>Rohin Manvi, Samar Khanna, Marshall Burke, David Lobell, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02680">https://arxiv.org/abs/2402.02680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02680">https://arxiv.org/pdf/2402.02680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02680]] Large Language Models are Geographically Biased(https://arxiv.org/abs/2402.02680)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 本质上带有其训练语料库中包含的偏见，这可能导致社会危害持续存在。随着这些基础模型影响力的增长，理解和评估它们的偏见对于实现公平性和准确性变得至关重要。我们建议通过地理学的视角研究法学硕士对我们生活的世界的了解。这种方法特别有效，因为人类生活的许多方面都有基本事实，这些方面有意义地投射到地理空间上，例如文化、种族、语言、政治和宗教。我们展示了各种有问题的地理偏差，我们将其定义为地理空间预测中的系统错误。最初，我们证明法学硕士能够以评级的形式进行准确的零样本地理空间预测，这些评级显示出与地面事实的强单调相关性（Spearman 的 $\rho$ 高达 0.89）。然后我们表明法学硕士在一系列客观和主观主题上表现出共同的偏见。特别是，法学硕士在吸引力、道德和智力等各种敏感的主观主题上明显对社会经济条件较低的地区（例如非洲大部分地区）存在偏见（斯皮尔曼的 $\rho$ 高达 0.70）。最后，我们引入了偏差分数来量化这一点，并发现现有法学硕士的偏差程度存在显着差异。</li>
</ul>

<h3>Title: Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit  Models for High-dimensional Gaussian Mixtures</h3>
<ul>
<li><strong>Authors: </strong>Zenan Ling, Longbo Li, Zhanbo Feng, Yixuan Zhang, Feng Zhou, Robert C. Qiu, Zhenyu Liao</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02697">https://arxiv.org/abs/2402.02697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02697">https://arxiv.org/pdf/2402.02697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02697]] Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit  Models for High-dimensional Gaussian Mixtures(https://arxiv.org/abs/2402.02697)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Deep equilibrium models (DEQs), as a typical implicit neural network, have demonstrated remarkable success on various tasks. There is, however, a lack of theoretical understanding of the connections and differences between implicit DEQs and explicit neural network models. In this paper, leveraging recent advances in random matrix theory (RMT), we perform an in-depth analysis on the eigenspectra of the conjugate kernel (CK) and neural tangent kernel (NTK) matrices for implicit DEQs, when the input data are drawn from a high-dimensional Gaussian mixture. We prove, in this setting, that the spectral behavior of these Implicit-CKs and NTKs depend on the DEQ activation function and initial weight variances, but only via a system of four nonlinear equations. As a direct consequence of this theoretical result, we demonstrate that a shallow explicit network can be carefully designed to produce the same CK or NTK as a given DEQ. Despite derived here for Gaussian mixture data, empirical results show the proposed theory and design principle also apply to popular real-world datasets.</li>
<li><strong>摘要：</strong>深度均衡模型（DEQ）作为一种典型的隐式神经网络，在各种任务上都取得了显着的成功。然而，对于隐式 DEQ 和显式神经网络模型之间的联系和差异缺乏理论理解。在本文中，利用随机矩阵理论（RMT）的最新进展，当绘制输入数据时，我们对隐式DEQ的共轭核（CK）和神经正切核（NTK）矩阵的特征谱进行了深入分析来自高维高斯混合。我们证明，在这种情况下，这些隐式 CK 和 NTK 的谱行为取决于 DEQ 激活函数和初始权重方差，但仅通过四个非线性方程组。作为这一理论结果的直接结果，我们证明了可以仔细设计浅层显式网络来产生与给定 DEQ 相同的 CK 或 NTK。尽管此处是针对高斯混合数据得出的，但实证结果表明所提出的理论和设计原则也适用于流行的现实世界数据集。</li>
</ul>

<h3>Title: Architectural Strategies for the optimization of Physics-Informed Neural  Networks</h3>
<ul>
<li><strong>Authors: </strong>Hemanth Saratchandran, Shin-Fang Chng, Simon Lucey</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02711">https://arxiv.org/abs/2402.02711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02711">https://arxiv.org/pdf/2402.02711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02711]] Architectural Strategies for the optimization of Physics-Informed Neural  Networks(https://arxiv.org/abs/2402.02711)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Physics-informed neural networks (PINNs) offer a promising avenue for tackling both forward and inverse problems in partial differential equations (PDEs) by incorporating deep learning with fundamental physics principles. Despite their remarkable empirical success, PINNs have garnered a reputation for their notorious training challenges across a spectrum of PDEs. In this work, we delve into the intricacies of PINN optimization from a neural architecture perspective. Leveraging the Neural Tangent Kernel (NTK), our study reveals that Gaussian activations surpass several alternate activations when it comes to effectively training PINNs. Building on insights from numerical linear algebra, we introduce a preconditioned neural architecture, showcasing how such tailored architectures enhance the optimization process. Our theoretical findings are substantiated through rigorous validation against established PDEs within the scientific literature.</li>
<li><strong>摘要：</strong>物理信息神经网络 (PINN) 通过将深度学习与基本物理原理相结合，为解决偏微分方程 (PDE) 中的正向和逆向问题提供了一条有前途的途径。尽管 PINN 在实证上取得了显着的成功，但因其在一系列偏微分方程中臭名昭著的训练挑战而赢得了声誉。在这项工作中，我们从神经架构的角度深入研究了 PINN 优化的复杂性。利用神经正切内核 (NTK)，我们的研究表明，在有效训练 PINN 方面，高斯激活超过了几种替代激活。基于数值线性代数的见解，我们引入了一种预处理神经架构，展示了这种定制的架构如何增强优化过程。我们的理论发现通过对科学文献中已建立的偏微分方程的严格验证得到证实。</li>
</ul>

<h3>Title: Position Paper: What Can Large Language Models Tell Us about Time Series  Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui Pan, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02713">https://arxiv.org/abs/2402.02713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02713">https://arxiv.org/pdf/2402.02713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02713]] Position Paper: What Can Large Language Models Tell Us about Time Series  Analysis(https://arxiv.org/abs/2402.02713)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including modality switching and time series question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.</li>
<li><strong>摘要：</strong>时间序列分析对于理解各种现实世界系统和应用程序固有的复杂性至关重要。尽管大型语言模型（LLM）最近取得了重大进展，但配备时间序列分析功能的通用人工智能（AGI）的发展仍处于起步阶段。大多数现有时间序列模型严重依赖领域知识和广泛的模型调整，主要关注预测任务。在本文中，我们认为当前的法学硕士有可能彻底改变时间序列分析，从而促进高效的决策并朝着更通用的时间序列分析智能形式迈进。这种进步可以释放广泛的可能性，包括模态切换和时间序列问答。我们鼓励研究人员和从业者认识到法学硕士在推进时​​间序列分析方面的潜力，并强调对这些相关努力的信任的必要性。此外，我们详细介绍了时间序列分析与现有法学硕士技术的无缝集成，并概述了未来研究的有希望的途径。</li>
</ul>

<h3>Title: Understanding the planning of LLM agents: A survey</h3>
<ul>
<li><strong>Authors: </strong>Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02716">https://arxiv.org/abs/2402.02716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02716">https://arxiv.org/pdf/2402.02716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02716]] Understanding the planning of LLM agents: A survey(https://arxiv.org/abs/2402.02716)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）显示出显着的智能，利用 LLM 作为自主代理的规划模块的进展引起了更多关注。这项调查提供了基于法学硕士的代理人规划的第一个系统视图，涵盖了旨在提高规划能力的最新工作。我们提供了 LLM-Agent 规划现有工作的分类，可分为任务分解、计划选择、外部模块、反思和记忆。对每个方向进行了综合分析，并讨论了研究领域的进一步挑战。</li>
</ul>

<h3>Title: KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache</h3>
<ul>
<li><strong>Authors: </strong>Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, Xia Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02750">https://arxiv.org/abs/2402.02750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02750">https://arxiv.org/pdf/2402.02750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02750]] KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache(https://arxiv.org/abs/2402.02750)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm, named KIVI. With the hardware-friendly implementation, KIVI can enable Llama (Llama-2), Falcon, and Mistral models to maintain almost the same quality while using $\mathbf{2.6\times}$ less peak memory usage (including the model weight). This reduction in memory usage enables up to $\mathbf{4\times}$ larger batch size, bringing $\mathbf{2.35\times \sim 3.47\times}$ throughput on real LLM inference workload. The source code is available at https://github.com/jy-yuan/KIVI.</li>
<li><strong>摘要：</strong>高效地服务大型语言模型 (LLM) 需要将许多请求一起批处理，以降低每个请求的成本。然而，存储注意力键和值以避免重新计算的键值（KV）缓存显着增加了内存需求，并成为速度和内存使用的新瓶颈。这种内存需求随着批量大小和上下文长度的增加而增加。此外，推理速度受到 KV 缓存大小的限制，因为 GPU 的 SRAM 必须为每个生成的令牌从主 GPU 内存加载整个 KV 缓存，导致计算核心在此过程中处于空闲状态。减少 KV 缓存大小的一个直接有效的解决方案是量化，这会减少 KV 缓存占用的总字节数。然而，缺乏深入研究KV缓存的元素分布来了解KV缓存量化的难度和局限性。为了填补这一空白，我们对流行的LLM的KV缓存中的元素分布进行了全面的研究。我们的研究结果表明，密钥缓存应该按通道进行量化，即沿通道维度对元素进行分组并将它们一起量化。相反，值缓存应该按令牌进行量化。根据这一分析，我们开发了一种免调优的 2 位 KV 缓存量化算法，名为 KIVI。通过硬件友好的实现，KIVI 可以使 Llama (Llama-2)、Falcon 和 Mistral 模型保持几乎相同的质量，同时减少 $\mathbf{2.6\times}$ 峰值内存使用量（包括模型权重）。内存使用量的减少可实现高达 $\mathbf{4\times}$ 的更大批量大小，从而为实际 LLM 推理工作负载带来 $\mathbf{2.35\times \sim 3.47\times}$ 吞吐量。源代码可在 https://github.com/jy-yuan/KIVI 获取。</li>
</ul>

<h3>Title: Contrastive Diffuser: Planning Towards High Return States via  Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Yixiang Shan, Zhengbang Zhu, Ting Long, Qifan Liang, Yi Chang, Weinan Zhang, Liang Yin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02772">https://arxiv.org/abs/2402.02772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02772">https://arxiv.org/pdf/2402.02772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02772]] Contrastive Diffuser: Planning Towards High Return States via  Contrastive Learning(https://arxiv.org/abs/2402.02772)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return contrast mechanism to pull the states in generated trajectories towards high-return states while pushing them away from low-return states to improve the base distribution. Experiments on 14 commonly used D4RL benchmarks demonstrate the effectiveness of our proposed method. Our code is publicly available at https://anonymous.4open.science/r/ContrastiveDiffuser.</li>
<li><strong>摘要：</strong>在强化学习中应用扩散模型进行长期规划最近引起了广泛关注。几种基于扩散的方法已经成功地利用了任意分布的扩散建模功能。这些方法生成了后续的规划轨迹，并已显示出显着的改进。然而，这些方法受到其简单的基础分布和忽视样本多样性的限制，其中不同的状态有不同的回报。他们只是利用扩散来学习离线数据集的分布，生成状态与离线数据集具有相同分布的轨迹。因此，这些模型达到高回报状态的概率在很大程度上取决于数据集分布。即使配备制导模型，性能依然受到压制。为了解决这些限制，在本文中，我们提出了一种称为 CDiffuser 的新方法，它设计了一种返回对比机制，将生成轨迹中的状态拉向高返回状态，同时将它们推离低返回状态，以改善基础分布。在 14 个常用的 D4RL 基准上进行的实验证明了我们提出的方法的有效性。我们的代码可在 https://anonymous.4open.science/r/ContrastiveDiffuser 上公开获取。</li>
</ul>

<h3>Title: From Partial to Strictly Incremental Constituent Parsing</h3>
<ul>
<li><strong>Authors: </strong>Ana Ezquerro, Carlos Gómez-Rodríguez, David Vilares</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02782">https://arxiv.org/abs/2402.02782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02782">https://arxiv.org/pdf/2402.02782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02782]] From Partial to Strictly Incremental Constituent Parsing(https://arxiv.org/abs/2402.02782)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>We study incremental constituent parsers to assess their capacity to output trees based on prefix representations alone. Guided by strictly left-to-right generative language models and tree-decoding modules, we build parsers that adhere to a strong definition of incrementality across languages. This builds upon work that asserted incrementality, but that mostly only enforced it on either the encoder or the decoder. Finally, we conduct an analysis against non-incremental and partially incremental models.</li>
<li><strong>摘要：</strong>我们研究增量成分解析器，以评估它们仅基于前缀表示输出树的能力。在严格从左到右的生成语言模型和树解码模块的指导下，我们构建了遵循跨语言增量的强大定义的解析器。这是建立在主张增量性的工作基础上的，但大多数情况下只在编码器或解码器上强制执行增量性。最后，我们对非增量模型和部分增量模型进行了分析。</li>
</ul>

<h3>Title: Rethinking Optimization and Architecture for Tiny Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02791">https://arxiv.org/abs/2402.02791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02791">https://arxiv.org/pdf/2402.02791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02791]] Rethinking Optimization and Architecture for Tiny Language Models(https://arxiv.org/abs/2402.02791)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag</a></li>
<li><strong>Abstract: </strong>The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingual corpora, following the established formulas. Experimental results demonstrate the improved optimization and architecture yield a notable average improvement of 8.87 on benchmark evaluation sets for PanGu-$\pi$-1B Pro. Besides, PanGu-$\pi$-1.5B Pro surpasses a range of SOTA models with larger model sizes, validating its superior performance. The code will be released soon (https://github.com/YuchuanTian/RethinkTinyLM).</li>
<li><strong>摘要：</strong>大量数据和计算资源已经证明了大型语言模型 (LLM) 的强大功能。然而，语言模型在移动设备上的应用面临着计算和内存成本的巨大挑战，即迫切需要高性能的微型语言模型。受限于高度复杂的训练过程，优化语言模型有很多细节很少被仔细研究。在本研究中，基于具有1B个参数的微小语言模型，我们精心设计了一系列实证研究来分析每个组件的效果。主要讨论三个角度，即神经架构、参数初始化和优化策略。经验证明，一些设计公式对于微型语言模型特别有效，包括分词器压缩、架构调整、参数继承和多轮训练。然后我们按照既定公式在1.6T多语言语料库上训练PanGu-$\pi$-1B Pro和PanGu-$\pi$-1.5B Pro。实验结果表明，改进的优化和架构在 PanGu-$\pi$-1B Pro 的基准评估集上产生了 8.87 的显着平均提升。此外，PanGu-$\pi$-1.5B Pro超越了一系列较大模型尺寸的SOTA模型，验证了其优越的性能。代码即将发布（https://github.com/YuchuanTian/RethinkTinyLM）。</li>
</ul>

<h3>Title: KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Fei Yuan, Chang Ma, Shuai Yuan, Qiushi Sun, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02801">https://arxiv.org/abs/2402.02801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02801">https://arxiv.org/pdf/2402.02801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02801]] KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language  Models(https://arxiv.org/abs/2402.02801)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, code</a></li>
<li><strong>Abstract: </strong>The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embedding of LLaMA suffices to reach the fine-tuning translation performance. Code and model will be released to the public.</li>
<li><strong>摘要：</strong>彩票假设假设随机初始化的神经网络中存在“中奖彩票”。在微调场景中，LLM 是否存在中奖机会？我们怎样才能找到这样的中奖彩票呢？在本文中，我们提出了 KS-Lottery，一种识别 LLM 参数的小子集的方法，该方法在多语言微调中非常有效。我们的核心思想是使用Kolmogorov-Smirnov Test来分析微调前后参数的分布变化。我们进一步从理论上证明KS-Lottery可以在嵌入层中找到经过认证的中奖彩票，对找到的参数进行微调可以保证与完全微调一样好。将 KS-Lottery 与翻译任务上的其他参数高效调整算法进行比较，实验结果表明，KS-Lottery 找到了小得多的参数集进行微调，同时实现了与完全微调 LLM 相当的性能。令人惊讶的是，我们发现微调 LLaMA 的 18 个 token 嵌入足以达到微调翻译性能。代码和模型将向公众发布。</li>
</ul>

<h3>Title: Graph-enhanced Large Language Models in Asynchronous Plan Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Fangru Lin, Emanuele La Malfa, Valentin Hofmann, Elle Michelle Yang, Anthony Cohn, Janet B. Pierrehumbert</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02805">https://arxiv.org/abs/2402.02805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02805">https://arxiv.org/pdf/2402.02805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02805]] Graph-enhanced Large Language Models in Asynchronous Plan Reasoning(https://arxiv.org/abs/2402.02805)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.</li>
<li><strong>摘要：</strong>推理异步计划具有挑战性，因为它需要顺序和并行计划来优化时间成本。大型语言模型 (LLM) 能否成功完成这项任务？在这里，我们提出了第一个调查这个问题的大规模研究。我们发现，一组具有代表性的封闭式和开源 LLM（包括 GPT-4 和 LLaMA-2）在我们的基准 AsyncHow 中未提供有关任务解决过程的说明时表现不佳。我们提出了一种称为“像图一样规划”（PLaG）的新技术，它将图与自然语言提示相结合，并取得了最先进的结果。我们表明，尽管 PLaG 可以提高模型性能，但当任务复杂性增加时，LLM 仍然会急剧退化，这凸显了利用 LLM 来模拟数字设备的局限性。我们认为我们的研究是迈向使用法学硕士作为高效自主代理的激动人心的一步。</li>
</ul>

<h3>Title: Are Sounds Sound for Phylogenetic Reconstruction?</h3>
<ul>
<li><strong>Authors: </strong>Luise Häuser, Gerhard Jäger, Taraka Rama, Johann-Mattis List, Alexandros Stamatakis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02807">https://arxiv.org/abs/2402.02807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02807">https://arxiv.org/pdf/2402.02807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02807]] Are Sounds Sound for Phylogenetic Reconstruction?(https://arxiv.org/abs/2402.02807)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In traditional studies on language evolution, scholars often emphasize the importance of sound laws and sound correspondences for phylogenetic inference of language family trees. However, to date, computational approaches have typically not taken this potential into account. Most computational studies still rely on lexical cognates as major data source for phylogenetic reconstruction in linguistics, although there do exist a few studies in which authors praise the benefits of comparing words at the level of sound sequences. Building on (a) ten diverse datasets from different language families, and (b) state-of-the-art methods for automated cognate and sound correspondence detection, we test, for the first time, the performance of sound-based versus cognate-based approaches to phylogenetic reconstruction. Our results show that phylogenies reconstructed from lexical cognates are topologically closer, by approximately one third with respect to the generalized quartet distance on average, to the gold standard phylogenies than phylogenies reconstructed from sound correspondences.</li>
<li><strong>摘要：</strong>在传统的语言演化研究中，学者们常常强调声音规律和声音对应对于语言家谱系统发育推断的重要性。然而，迄今为止，计算方法通常没有考虑到这种潜力。大多数计算研究仍然依赖词汇同源词作为语言学中系统发育重建的主要数据源，尽管确实存在一些研究，其中作者赞扬了在声音序列水平上比较单词的好处。基于（a）来自不同语系的十个不同数据集，以及（b）自动同源和声音对应检测的最先进方法，我们首次测试了基于声音与同源的性能基于系统发育重建的方法。我们的结果表明，从词汇同源词重建的系统发育在拓扑上比从声音对应重建的系统发育更接近金标准系统发育，平均而言，广义四重奏距离大约接近三分之一。</li>
</ul>

<h3>Title: Revisiting VAE for Unsupervised Time Series Anomaly Detection: A  Frequency Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zexin Wang, Changhua Pei, Minghua Ma, Xin Wang, Zhihan Li, Dan Pei, Saravan Rajmohan, Dongmei Zhang, Qingwei Lin, Haiming Zhang, Jianhui Li, Gaogang Xie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02820">https://arxiv.org/abs/2402.02820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02820">https://arxiv.org/pdf/2402.02820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02820]] Revisiting VAE for Unsupervised Time Series Anomaly Detection: A  Frequency Perspective(https://arxiv.org/abs/2402.02820)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Time series Anomaly Detection (AD) plays a crucial role for web systems. Various web systems rely on time series data to monitor and identify anomalies in real time, as well as to initiate diagnosis and remediation procedures. Variational Autoencoders (VAEs) have gained popularity in recent decades due to their superior de-noising capabilities, which are useful for anomaly detection. However, our study reveals that VAE-based methods face challenges in capturing long-periodic heterogeneous patterns and detailed short-periodic trends simultaneously. To address these challenges, we propose Frequency-enhanced Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for univariate time series. To ensure an accurate AD, FCVAE exploits an innovative approach to concurrently integrate both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) to significantly increase the accuracy of reconstructing the normal data. Together with a carefully designed "target attention" mechanism, our approach allows the model to pick the most useful information from the frequency domain for better short-periodic trend construction. Our FCVAE has been evaluated on public datasets and a large-scale cloud system, and the results demonstrate that it outperforms state-of-the-art methods. This confirms the practical applicability of our approach in addressing the limitations of current VAE-based anomaly detection models.</li>
<li><strong>摘要：</strong>时间序列异常检测（AD）对于网络系统起着至关重要的作用。各种网络系统依靠时间序列数据来实时监控和识别异常情况，以及启动诊断和补救程序。变分自动编码器（VAE）由于其卓越的去噪能力，在近几十年来越来越受欢迎，这对于异常检测非常有用。然而，我们的研究表明，基于 VAE 的方法在同时捕获长周期异质模式和详细的短周期趋势方面面临挑战。为了解决这些挑战，我们提出了频率增强条件变分自动编码器（FCVAE），这是一种用于单变量时间序列的新型无监督 AD 方法。为了确保准确的 AD，FCVAE 采用创新方法将全局和局部频率特征同时集成到条件变分自动编码器（CVAE）的条件中，以显着提高重建正常数据的准确性。结合精心设计的“目标注意”机制，我们的方法允许模型从频域中挑选最有用的信息，以更好地构建短期趋势。我们的 FCVAE 已在公共数据集和大型云系统上进行了评估，结果表明它优于最先进的方法。这证实了我们的方法在解决当前基于 VAE 的异常检测模型的局限性方面的实际适用性。</li>
</ul>

<h3>Title: Evading Data Contamination Detection for Language Models is (too) Easy</h3>
<ul>
<li><strong>Authors: </strong>Jasper Dekoninck, Mark Niklas Müller, Maximilian Baader, Marc Fischer, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02823">https://arxiv.org/abs/2402.02823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02823">https://arxiv.org/pdf/2402.02823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02823]] Evading Data Contamination Detection for Language Models is (too) Easy(https://arxiv.org/abs/2402.02823)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.</li>
<li><strong>摘要：</strong>大型语言模型很普遍，它们在基准测试中的性能经常引导用户对一种模型的偏好而不是另一种模型。然而，这些模型训练所用的大量数据可能会无意中导致公共基准的污染，从而影响性能测量。虽然最近开发的污染检测方法试图解决这个问题，但它们忽视了旨在逃避检测的恶意模型提供商故意污染的可能性。我们认为，这种设置至关重要，因为它让人对公共基准的可靠性产生怀疑。为了更严格地研究这个问题，我们提出了模型提供者和污染检测方法的分类。这揭示了我们利用 EAL 所利用的现有方法中的漏洞，EAL 是一种简单而有效的污染技术，可显着提高基准性能，同时完全规避当前的检测方法。</li>
</ul>

<h3>Title: PowerGraph: A power grid benchmark dataset for graph neural networks</h3>
<ul>
<li><strong>Authors: </strong>Anna Varbella, Kenza Amara, Blazhe Gjorgiev, Giovanni Sansavini</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02827">https://arxiv.org/abs/2402.02827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02827">https://arxiv.org/pdf/2402.02827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02827]] PowerGraph: A power grid benchmark dataset for graph neural networks(https://arxiv.org/abs/2402.02827)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Public Graph Neural Networks (GNN) benchmark datasets facilitate the use of GNN and enhance GNN applicability to diverse disciplines. The community currently lacks public datasets of electrical power grids for GNN applications. Indeed, GNNs can potentially capture complex power grid phenomena over alternative machine learning techniques. Power grids are complex engineered networks that are naturally amenable to graph representations. Therefore, GNN have the potential for capturing the behavior of power grids over alternative machine learning techniques. To this aim, we develop a graph dataset for cascading failure events, which are the major cause of blackouts in electric power grids. Historical blackout datasets are scarce and incomplete. The assessment of vulnerability and the identification of critical components are usually conducted via computationally expensive offline simulations of cascading failures. Instead, we propose using machine learning models for the online detection of cascading failures leveraging the knowledge of the system state at the onset of the cascade. We develop PowerGraph, a graph dataset modeling cascading failures in power grids, designed for two purposes, namely, i) training GNN models for different graph-level tasks including multi-class classification, binary classification, and regression, and ii) explaining GNN models. The dataset generated via a physics-based cascading failure model ensures the generality of the operating and environmental conditions by spanning diverse failure scenarios. In addition, we foster the use of the dataset to benchmark GNN explainability methods by assigning ground-truth edge-level explanations. PowerGraph helps the development of better GNN models for graph-level tasks and explainability, critical in many domains ranging from chemistry to biology, where the systems and processes can be described as graphs.</li>
<li><strong>摘要：</strong>公共图神经网络 (GNN) 基准数据集促进了 GNN 的使用，并增强了 GNN 对不同学科的适用性。该社区目前缺乏用于 GNN 应用的公共电网数据集。事实上，相比其他机器学习技术，GNN 有可能捕获复杂的电网现象。电网是复杂的工程网络，自然适合图形表示。因此，与其他机器学习技术相比，GNN 有潜力捕获电网的行为。为此，我们开发了级联故障事件的图形数据集，这是电网停电的主要原因。历史停电数据集稀缺且不完整。脆弱性评估和关键组件的识别通常是通过计算成本昂贵的级联故障离线模拟来进行的。相反，我们建议使用机器学习模型来在线检测级联故障，利用级联开始时的系统状态知识。我们开发了 PowerGraph，这是一个对电网级联故障进行建模的图形数据集，其设计有两个目的，即 i）为不同的图形级任务（包括多类分类、二元分类和回归）训练 GNN 模型，以及 ii）解释 GNN 模型。通过基于物理的级联故障模型生成的数据集通过跨越不同的故障场景确保了操作和环境条件的通用性。此外，我们还通过分配真实的边缘级别解释来促进使用数据集来对 GNN 可解释性方法进行基准测试。 PowerGraph 有助于开发更好的 GNN 模型，用于图级任务和可解释性，这在从化学到生物学的许多领域中至关重要，这些领域的系统和过程可以用图来描述。</li>
</ul>

<h3>Title: Shortened LLaMA: A Simple Depth Pruning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu Song</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02834">https://arxiv.org/abs/2402.02834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02834">https://arxiv.org/pdf/2402.02834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02834]] Shortened LLaMA: A Simple Depth Pruning for Large Language Models(https://arxiv.org/abs/2402.02834)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that a simple depth pruning approach can compete with recent width pruning methods in terms of zero-shot task performance. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. We hope this work can help deploy LLMs on local and edge devices.</li>
<li><strong>摘要：</strong>现代大型语言模型（LLM）的结构化修剪已成为减少其高计算需求的一种方法。宽度修剪减少了投影权重矩阵的大小（例如，通过移除注意力头），同时保持层数。相反，深度修剪会删除整个层或块，同时保持剩余权重的大小不变。目前大多数研究都集中于仅宽度修剪或宽度和深度修剪的混合，很少对这两个单元（宽度与深度）之间关于它们对 LLM 推理效率的影响进行比较分析。在这项工作中，我们证明了一种简单的深度剪枝方法可以在零样本任务性能方面与最近的宽度剪枝方法竞争。我们的剪枝方法提高了推理速度，特别是在内存受限的情况下，需要有限的批量大小来运行 LLM，而宽度剪枝是无效的。我们希望这项工作能够帮助在本地和边缘设备上部署 LLM。</li>
</ul>

<h3>Title: Deep autoregressive density nets vs neural ensembles for model-based  offline reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Abdelhakim Benechehab, Albert Thomas, Balázs Kégl</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02858">https://arxiv.org/abs/2402.02858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02858">https://arxiv.org/pdf/2402.02858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02858]] Deep autoregressive density nets vs neural ensembles for model-based  offline reinforcement learning(https://arxiv.org/abs/2402.02858)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization. Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark. We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.</li>
<li><strong>摘要：</strong>我们考虑离线强化学习的问题，其中只有一组系统转换可用于策略优化。随着该领域的最新进展，我们考虑一种基于模型的强化学习算法，该算法从可用数据中推断系统动态，并对假想模型的推出执行策略优化。这种方法很容易受到模型错误的利用，从而导致实际系统出现灾难性故障。标准解决方案是依靠集成来实现不确定性启发法，并避免在过于不确定的情况下利用模型。我们通过证明在 D4RL 基准上使用单个经过良好校准的自回归模型可以获得更好的性能，从而挑战了我们必须诉诸集成的流行观念。我们还分析模型学习的静态指标，并得出对代理最终性能重要的模型属性的结论。</li>
</ul>

<h3>Title: How do Large Language Models Learn In-Context? Query and Key Matrices of  In-Context Heads are Two Towers for Metric Learning</h3>
<ul>
<li><strong>Authors: </strong>Zeping Yu, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02872">https://arxiv.org/abs/2402.02872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02872">https://arxiv.org/pdf/2402.02872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02872]] How do Large Language Models Learn In-Context? Query and Key Matrices of  In-Context Heads are Two Towers for Metric Learning(https://arxiv.org/abs/2402.02872)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, code</a></li>
<li><strong>Abstract: </strong>We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provides a new method and a reasonable hypothesis for understanding the mechanism of in-context learning. Our code will be released on github.</li>
<li><strong>摘要：</strong>我们探索情境学习的机制，并使用定位和投影方法提出假设。在浅层中，演示的特征被合并到相应的标签中，输入文本的特征被聚合到最后一个标记中。在深层，上下文中的头脑做出了巨大的贡献。在每个上下文头中，值输出矩阵提取标签的特征。查询和关键矩阵计算输入文本和每个演示之间的注意力权重。注意力权重越大，转移到最后一个 token 中用于预测下一个单词的标签信息就越多。查询矩阵和关键矩阵可以被视为两个塔，用于学习输入文本和每个演示之间的相似性度量。基于这个假设，我们解释了为什么不平衡的标签和演示顺序会影响预测。我们在 GPT2 Large、Llama 7B、13B 和 30B 上进行了实验。结果可以支持我们的分析。总的来说，我们的研究为理解情境学习的机制提供了一种新的方法和合理的假设。我们的代码将在github上发布。</li>
</ul>

<h3>Title: Approximate Attributions for Off-the-Shelf Siamese Transformers</h3>
<ul>
<li><strong>Authors: </strong>Lucas Möller, Dmitry Nikolaev, Sebastian Padó</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02883">https://arxiv.org/abs/2402.02883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02883">https://arxiv.org/pdf/2402.02883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02883]] Approximate Attributions for Off-the-Shelf Siamese Transformers(https://arxiv.org/abs/2402.02883)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Siamese encoders such as sentence transformers are among the least understood deep models. Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (M\"oller et al., 2023). However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model's predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models. We extensively compare approximate and exact attributions and use them to analyze the models' attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically opposite adjectives, and find that they exhibit lexical bias.</li>
<li><strong>摘要：</strong>诸如句子转换器之类的连体编码器是最难理解的深度模型之一。已建立的归因方法无法处理该模型类，因为它比较两个输入而不是处理单个输入。为了解决这个差距，我们最近提出了一种专门针对暹罗编码器的归因方法（M\"oller et al., 2023）。但是，它需要对模型进行调整和微调，因此不能直接应用于离线-现成模型。在这项工作中，我们重新评估这些限制并提出（i）具有精确归因能力的模型，保留原始模型的预测性能，以及（ii）计算现成模型的近似归因的方法。我们广泛地比较近似归因和精确归因，并用它们来分析模型对不同语言方面的关注。我们深入了解连体变形者关注哪些句法角色，确认它们大多忽略否定，探索它们如何判断语义相反的形容词，并发现它们表现出词汇偏见。</li>
</ul>

<h3>Title: LLM Agents in Interaction: Measuring Personality Consistency and  Linguistic Alignment in Interacting Populations of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ivar Frisch, Mario Giulianelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02896">https://arxiv.org/abs/2402.02896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02896">https://arxiv.org/pdf/2402.02896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02896]] LLM Agents in Interaction: Measuring Personality Consistency and  Linguistic Alignment in Interacting Populations of Large Language Models(https://arxiv.org/abs/2402.02896)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.</li>
<li><strong>摘要：</strong>虽然代理交互和个性化都是大型语言模型 (LLM) 研究中充满活力的主题，但人们对语言交互对角色条件 LLM 代理行为的影响的关注有限。这样的努力对于确保智能体保持与其指定特征一致但能够进行开放、自然的对话非常重要。在我们的实验中，我们通过提示对人格概况进行 GPT-3.5 调节，并使用简单的变异性诱导采样算法创建两组 LLM 代理群体。然后，我们进行性格测试，并将代理提交给协作写作任务，发现不同的个人资料与对话伙伴表现出不同程度的性格一致性和语言一致性。我们的研究旨在为更好地理解法学硕士之间基于对话的互动奠定基础，并强调需要新的方法来为互动环境打造强大的、更人性化的法学硕士角色。</li>
</ul>

<h3>Title: Kernel PCA for Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Kun Fang, Qinghua Tao, Kexin Lv, Mingzhen He, Xiaolin Huang, Jie Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02949">https://arxiv.org/abs/2402.02949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02949">https://arxiv.org/pdf/2402.02949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02949]] Kernel PCA for Out-of-Distribution Detection(https://arxiv.org/abs/2402.02949)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs). Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings. In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD and InD features are allocated with significantly different patterns. We devise two feature mappings that induce non-linear kernels in KPCA to advocate the separability between InD and OoD data in the subspace spanned by the principal components. Given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\mathcal{O}(1)$ time complexity in inference. Extensive empirical results on multiple OoD data sets and network structures verify the superiority of our KPCA-based detector in efficiency and efficacy with state-of-the-art OoD detection performances.</li>
<li><strong>摘要：</strong>分布外 (OoD) 检测对于深度神经网络 (DNN) 的可靠性至关重要。现有的研究表明，直接应用于 DNN 特征的主成分分析 (PCA) 在从分布内 (InD) 数据中检测 OoD 数据方面存在不足。 PCA 的失败表明，OoD 和 InD 中的网络特征不能通过简单地在线性子空间中进行很好地分离，而是可以通过适当的非线性映射来解决。在这项工作中，我们利用内核 PCA (KPCA) 框架进行 OoD 检测，寻找 OoD 和 InD 特征分配有显着不同模式的子空间。我们设计了两种特征映射，它们在 KPCA 中引入非线性内核，以提倡主成分跨越的子空间中 InD 和 OoD 数据之间的可分离性。给定任何测试样本，然后利用该子空间中的重建误差以 $\mathcal{O}(1)$ 推理时间复杂度有效地获得检测结果。对多个 OoD 数据集和网络结构的广泛实证结果验证了我们基于 KPCA 的检测器在效率和功效方面的优越性以及最先进的 OoD 检测性能。</li>
</ul>

<h3>Title: Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine  Workers</h3>
<ul>
<li><strong>Authors: </strong>Ron Dorfman, Naseem Yehya, Kfir Y. Levy</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02951">https://arxiv.org/abs/2402.02951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02951">https://arxiv.org/pdf/2402.02951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02951]] Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine  Workers(https://arxiv.org/abs/2402.02951)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques consider the static setting, wherein the identity of Byzantine machines remains fixed during the learning process. This assumption does not capture real-world dynamic Byzantine behaviors, which may include transient malfunctions or targeted temporal attacks. Addressing this limitation, we propose $\textsf{DynaBRO}$ -- a new method capable of withstanding $\mathcal{O}(\sqrt{T})$ rounds of Byzantine identity alterations (where $T$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting. Our method combines a multi-level Monte Carlo (MLMC) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic Byzantine strategies. Additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentage of Byzantine workers.</li>
<li><strong>摘要：</strong>拜占庭鲁棒学习已成为一种突出的容错分布式机器学习框架。然而，大多数技术考虑静态设置，其中拜占庭机器的身份在学习过程中保持固定。这种假设并没有捕获现实世界的动态拜占庭行为，其中可能包括瞬时故障或有针对性的时间攻击。为了解决这个限制，我们提出了 $\textsf{DynaBRO}$ ——一种能够承受 $\mathcal{O}(\sqrt{T})$ 轮拜占庭身份变更的新方法（其中 $T$ 是拜占庭身份变更的总数）训练轮次），同时匹配静态设置的渐近收敛速度。我们的方法将多级蒙特卡罗（MLMC）梯度估计技术与工作人员更新的稳健聚合相结合，并采用自动防故障过滤器来限制动态拜占庭策略的偏差。此外，通过利用自适应学习率，我们的方法消除了了解拜占庭工人百分比的需要。</li>
</ul>

<h3>Title: Boosting, Voting Classifiers and Randomized Sample Compression Schemes</h3>
<ul>
<li><strong>Authors: </strong>Arthur da Cunha, Kasper Green Larsen, Martin Ritzert</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02976">https://arxiv.org/abs/2402.02976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02976">https://arxiv.org/pdf/2402.02976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02976]] Boosting, Voting Classifiers and Randomized Sample Compression Schemes(https://arxiv.org/abs/2402.02976)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In boosting, we aim to leverage multiple weak learners to produce a strong learner. At the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners. While many successful boosting algorithms, such as the iconic AdaBoost, produce voting classifiers, their theoretical performance has long remained sub-optimal: the best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners. In this work, we break this barrier by proposing a randomized boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the sample size. We obtain this result by building a general framework that extends sample compression methods to support randomized learning algorithms based on sub-sampling.</li>
<li><strong>摘要：</strong>在 boosting 中，我们的目标是利用多个弱学习器来产生一个强学习器。该范式的核心是将强学习器构建为投票分类器的概念，该分类器输出弱学习器的加权多数投票。虽然许多成功的 boosting 算法（例如标志性的 AdaBoost）产生了投票分类器，但它们的理论性能长期以来一直处于次优状态：投票分类器获得给定精度所需的训练示例数量的已知界限迄今为止始终是有限的。包含至少两个对数因子，高于已知的一般弱到强学习器可以实现的水平。在这项工作中，我们通过提出一种随机增强算法来打破这一障碍，该算法输出投票分类器，其泛化误差包含对样本大小的单个对数依赖性。我们通过构建一个通用框架来获得这个结果，该框架扩展了样本压缩方法以支持基于子采样的随机学习算法。</li>
</ul>

<h3>Title: A Safety-Adapted Loss for Pedestrian Detection in Automated Driving</h3>
<ul>
<li><strong>Authors: </strong>Maria Lyssenko, Piyush Pimplikar, Maarten Bieshaar, Farzad Nozarian, Rudolph Triebel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02986">https://arxiv.org/abs/2402.02986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02986">https://arxiv.org/pdf/2402.02986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02986]] A Safety-Adapted Loss for Pedestrian Detection in Automated Driving(https://arxiv.org/abs/2402.02986)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In safety-critical domains like automated driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU). As common evaluation metrics are not an adequate safety indicator, recent works employ approaches to identify safety-critical VRU and back-annotate the risk to the object detector. However, those approaches do not consider the safety factor in the deep neural network (DNN) training process. Thus, state-of-the-art DNN penalizes all misdetections equally irrespective of their criticality. Subsequently, to mitigate the occurrence of critical failure cases, i.e., false negatives, a safety-aware training strategy might be required to enhance the detection performance for critical pedestrians. In this paper, we propose a novel safety-aware loss variation that leverages the estimated per-pedestrian criticality scores during training. We exploit the reachability set-based time-to-collision (TTC-RSB) metric from the motion domain along with distance information to account for the worst-case threat quantifying the criticality. Our evaluation results using RetinaNet and FCOS on the nuScenes dataset demonstrate that training the models with our safety-aware loss function mitigates the misdetection of critical pedestrians without sacrificing performance for the general case, i.e., pedestrians outside the safety-critical zone.</li>
<li><strong>摘要：</strong>在自动驾驶 (AD) 等安全关键领域，物体检测器的错误可能会危及行人和其他易受伤害的道路使用者 (VRU)。由于常见的评估指标并不是足够的安全指标，因此最近的工作采用了一些方法来识别安全关键的 VRU 并反向注释对象检测器的风险。然而，这些方法没有考虑深度神经网络（DNN）训练过程中的安全因素。因此，最先进的 DNN 会同等地惩罚所有误检，无论其严重性如何。随后，为了减少关键故障案例（即漏报）的发生，可能需要安全意识训练策略来增强关键行人的检测性能。在本文中，我们提出了一种新颖的安全意识损失变化，该变化利用训练期间估计的每个行人关键性分数。我们利用运动域中基于可达性集的碰撞时间 (TTC-RSB) 度量以及距离信息来考虑量化关键性的最坏情况威胁。我们在 nuScenes 数据集上使用 RetinaNet 和 FCOS 进行的评估结果表明，使用我们的安全感知损失函数训练模型可以减少对关键行人的误检测，而不会牺牲一般情况（即安全关键区域外的行人）的性能。</li>
</ul>

<h3>Title: Decoding-time Realignment of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello, Quentin Berthet, Felipe Llinares, Jessica Hoffmann, Lucas Dixon, Michal Valko, Mathieu Blondel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02992">https://arxiv.org/abs/2402.02992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02992">https://arxiv.org/pdf/2402.02992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02992]] Decoding-time Realignment of Language Models(https://arxiv.org/abs/2402.02992)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset.</li>
<li><strong>摘要：</strong>将语言模型与人类偏好保持一致对于减少这些模型中的错误和偏见至关重要。对齐技术，例如来自人类反馈的强化学习（RLHF），通常被视为优化人类偏好奖励和鼓励接近未对齐模型的接近正则化项之间的权衡。选择适当的正则化级别至关重要：正则化不足可能会因奖励黑客行为而导致模型能力下降，而过度正则化则会阻碍对齐。寻找最佳正则化水平的传统方法需要重新训练具有不同正则化强度的多个模型。然而，这个过程是资源密集型的，特别是对于大型模型。为了应对这一挑战，我们提出了解码时重新对齐（DeRa），这是一种无需重新训练即可探索和评估对齐模型中不同正则化强度的简单方法。 DeRa 可以控制对齐程度，允许用户在未对齐和对齐模型之间平滑过渡。它还通过使用验证数据集识别有效的正则化强度来提高超参数调整的效率。</li>
</ul>

<h3>Title: Text-Guided Image Clustering</h3>
<ul>
<li><strong>Authors: </strong>Andreas Stephan, Lukas Miklautz, Kevin Sidak, Jan Philip Wahle, Bela Gipp, Claudia Plant, Benjamin Roth</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02996">https://arxiv.org/abs/2402.02996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02996">https://arxiv.org/pdf/2402.02996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02996]] Text-Guided Image Clustering(https://arxiv.org/abs/2402.02996)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the question of using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, this research challenges traditional approaches and paves the way for a paradigm shift in image clustering, using generated text.</li>
<li><strong>摘要：</strong>图像聚类将图像集合划分为有意义的组，通常通过人类给出的注释进行事后解释。这些通常采用文本形式，这就引出了使用文本作为图像聚类抽象的问题。然而，当前的图像聚类方法忽略了生成的文本描述的使用。因此，我们提出文本引导图像聚类，即使用图像字幕和视觉问答（VQA）模型生成文本，然后对生成的文本进行聚类。此外，我们引入了一种新方法，通过提示 VQA 模型来注入任务或领域知识以进行聚类。在八个不同的图像聚类数据集中，我们的结果表明，获得的文本表示通常优于图像特征。此外，我们提出了一种基于计数的聚类可解释性方法。我们的评估表明，派生的基于关键字的解释比相应的聚类准确度所建议的更好地描述了聚类。总的来说，这项研究挑战了传统方法，并为使用生成文本的图像聚类范式转变铺平了道路。</li>
</ul>

<h3>Title: Careful with that Scalpel: Improving Gradient Surgery with an EMA</h3>
<ul>
<li><strong>Authors: </strong>Yu-Guan Hsieh, James Thornton, Eugene Ndiaye, Michal Klein, Marco Cuturi, Pierre Ablin</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02998">https://arxiv.org/abs/2402.02998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02998">https://arxiv.org/pdf/2402.02998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02998]] Careful with that Scalpel: Improving Gradient Surgery with an EMA(https://arxiv.org/abs/2402.02998)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Beyond minimizing a single training loss, many deep learning estimation pipelines rely on an auxiliary objective to quantify and encourage desirable properties of the model (e.g. performance on another dataset, robustness, agreement with a prior). Although the simplest approach to incorporating an auxiliary loss is to sum it with the training loss as a regularizer, recent works have shown that one can improve performance by blending the gradients beyond a simple sum; this is known as gradient surgery. We cast the problem as a constrained minimization problem where the auxiliary objective is minimized among the set of minimizers of the training loss. To solve this bilevel problem, we follow a parameter update direction that combines the training loss gradient and the orthogonal projection of the auxiliary gradient to the training gradient. In a setting where gradients come from mini-batches, we explain how, using a moving average of the training loss gradients, we can carefully maintain this critical orthogonality property. We demonstrate that our method, Bloop, can lead to much better performances on NLP and vision experiments than other gradient surgery methods without EMA.</li>
<li><strong>摘要：</strong>除了最小化单个训练损失之外，许多深度学习估计管道还依赖于辅助目标来量化和鼓励模型的理想属性（例如，在另一个数据集上的性能、鲁棒性、与先验的一致性）。尽管合并辅助损失的最简单方法是将其与作为正则化器的训练损失求和，但最近的工作表明，可以通过混合梯度而不是简单求和来提高性能；这称为梯度手术。我们将该问题视为约束最小化问题，其中辅助目标在训练损失的最小化器集合中最小化。为了解决这个双层问题，我们遵循一个参数更新方向，该方向结合了训练损失梯度和辅助梯度到训练梯度的正交投影。在梯度来自小批量的情况下，我们解释了如何使用训练损失梯度的移动平均值来小心地维护这种关键的正交性属性。我们证明，与其他没有 EMA 的梯度手术方法相比，我们的方法 Bloop 在 NLP 和视觉实验上可以带来更好的性能。</li>
</ul>

<h3>Title: UniMem: Towards a Unified View of Long-Context Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junjie Fang, Likai Tang, Hongzhe Bi, Yujia Qin, Si Sun, Zhenyu Li, Haolun Li, Yongjian Li, Xin Cong, Yukun Yan, Xiaodong Shi, Sen Song, Yankai Lin, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03009">https://arxiv.org/abs/2402.03009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03009">https://arxiv.org/pdf/2402.03009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03009]] UniMem: Towards a Unified View of Long-Context Large Language Models(https://arxiv.org/abs/2402.03009)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Long-context processing is a critical ability that constrains the applicability of large language models. Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs. UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an innovative approach that integrates the strengths of these algorithms. Experimental results show that UniMix achieves superior performance in handling long contexts with significantly lower perplexity than baselines.</li>
<li><strong>摘要：</strong>长上下文处理是制约大型语言模型适用性的关键能力。尽管目前存在多种致力于增强大语言模型（LLM）长上下文处理能力的方法，但它们都是以孤立的方式发展，缺乏对各自优势的系统分析和整合，阻碍了进一步的发展。在本文中，我们介绍了 UniMem，一个统一的框架，它从法学硕士的记忆增强的角度重新制定了现有的长上下文方法。 UniMem 具有四个关键维度：内存管理、内存写入、内存读取和内存注入，为理解各种长上下文方法提供了系统的理论。我们基于 UniMem 重新制定了 16 种现有方法，并将四种代表性方法：Transformer-XL、Memorizing Transformer、RMT 和 Longformer 分析为等效的 UniMem 形式，以揭示其设计原理和优势。基于这些分析，我们提出了 UniMix，这是一种集成了这些算法优势的创新方法。实验结果表明，UniMix 在处理长上下文方面取得了优异的性能，并且困惑度显着低于基线。</li>
</ul>

<h3>Title: Whom to Trust? Elective Learning for Distributed Gaussian Process  Regression</h3>
<ul>
<li><strong>Authors: </strong>Zewen Yang, Xiaobing Dai, Akshat Dubey, Sandra Hirche, Georges Hattab</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03014">https://arxiv.org/abs/2402.03014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03014">https://arxiv.org/pdf/2402.03014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03014]] Whom to Trust? Elective Learning for Distributed Gaussian Process  Regression(https://arxiv.org/abs/2402.03014)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>This paper introduces an innovative approach to enhance distributed cooperative learning using Gaussian process (GP) regression in multi-agent systems (MASs). The key contribution of this work is the development of an elective learning algorithm, namely prior-aware elective distributed GP (Pri-GP), which empowers agents with the capability to selectively request predictions from neighboring agents based on their trustworthiness. The proposed Pri-GP effectively improves individual prediction accuracy, especially in cases where the prior knowledge of an agent is incorrect. Moreover, it eliminates the need for computationally intensive variance calculations for determining aggregation weights in distributed GP. Furthermore, we establish a prediction error bound within the Pri-GP framework, ensuring the reliability of predictions, which is regarded as a crucial property in safety-critical MAS applications.</li>
<li><strong>摘要：</strong>本文介绍了一种在多智能体系统 (MAS) 中使用高斯过程 (GP) 回归来增强分布式协作学习的创新方法。这项工作的关键贡献是开发了一种选择性学习算法，即先验感知选择性分布式 GP (Pri-GP)，它使代理能够根据邻近代理的可信度有选择地请求预测。所提出的 Pri-GP 有效地提高了个体预测的准确性，特别是在代理的先验知识不正确的情况下。此外，它消除了在分布式 GP 中确定聚合权重时进行计算密集型方差计算的需要。此外，我们在 Pri-GP 框架内建立了预测误差界限，确保预测的可靠性，这被视为安全关键 MAS 应用中的关键属性。</li>
</ul>

<h3>Title: Automatic Combination of Sample Selection Strategies for Few-Shot  Learning</h3>
<ul>
<li><strong>Authors: </strong>Branislav Pecher, Ivan Srba, Maria Bielikova, Joaquin Vanschoren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03038">https://arxiv.org/abs/2402.03038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03038">https://arxiv.org/pdf/2402.03038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03038]] Automatic Combination of Sample Selection Strategies for Few-Shot  Learning(https://arxiv.org/abs/2402.03038)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a strong modality, dataset and approach dependence for the majority of strategies as well as their dependence on the number of shots - demonstrating that the sample selection strategies play a significant role for lower number of shots, but regresses to random selection at higher number of shots.</li>
<li><strong>摘要：</strong>在少样本学习中，例如元学习、少样本微调或上下文学习，用于训练模型的样本数量有限对整体成功有重大影响。尽管存在大量的样本选择策略，但它们对小样本学习性能的影响并不广为人知，因为到目前为止，大多数策略仅在典型的监督设置中进行了评估。在本文中，我们深入研究了 20 种样本选择策略对 8 个图像和 6 个文本数据集上的 5 种小样本学习方法性能的影响。此外，我们提出了一种自动组合样本选择策略（ACSESS）的新方法，该方法利用各个策略的优势和互补信息。实验结果表明，我们的方法始终优于个体选择策略，以及最近提出的为上下文学习选择支持示例的方法。我们还展示了大多数策略的强烈模态、数据集和方法依赖性以及它们对镜头数量的依赖性——表明样本选择策略对于较低镜头数量发挥着重要作用，但在较高镜头数量时回归到随机选择拍摄次数。</li>
</ul>

<h3>Title: SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach</h3>
<ul>
<li><strong>Authors: </strong>Mohammad N.S. Jahromi, Satya. M. Muddamsetty, Asta Sofie Stage Jarlner, Anna Murphy Høgenhaug, Thomas Gammeltoft-Hansen, Thomas B. Moeslund</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03043">https://arxiv.org/abs/2402.03043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03043">https://arxiv.org/pdf/2402.03043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03043]] SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach(https://arxiv.org/abs/2402.03043)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Explainable AI (XAI) aids in deciphering 'black-box' models. While several methods have been proposed and evaluated primarily in the image domain, the exploration of explainability in the text domain remains a growing research area. In this paper, we delve into the applicability of XAI methods for the text domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU) XAI method, recognized for its superior capability in localizing entire salient regions in image-based classification is extended to textual data. The extended method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to generate heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements crucial for model predictions. Given the absence of a unified standard for assessing XAI methods, this study applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded, to assess the effectiveness of the proposed SIDU-TXT across various experiments. We find that, in sentiment analysis task of a movie review dataset, SIDU-TXT excels in both functionally and human-grounded evaluations, demonstrating superior performance through quantitative and qualitative analyses compared to benchmarks like Grad-CAM and LIME. In the application-grounded evaluation within the sensitive and complex legal domain of asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable performances, each with its own set of strengths and weaknesses. However, both methods fall short of entirely fulfilling the sophisticated criteria of expert expectations, highlighting the imperative need for additional research in XAI methods suitable for such domains.</li>
<li><strong>摘要：</strong>可解释的人工智能 (XAI) 有助于破译“黑匣子”模型。虽然主要在图像领域提出并评估了几种方法，但文本领域可解释性的探索仍然是一个不断发展的研究领域。在本文中，我们深入研究了 XAI 方法在文本领域的适用性。在此背景下，“相似性、差异性和唯一性”(SIDU) XAI 方法因其在基于图像的分类中定位整个显着区域的卓越能力而被认可，并扩展到文本数据。扩展方法 SIDU-TXT 利用“黑盒”模型的特征激活图在基于单词的粒度级别生成热图，从而提供突出显示对模型预测至关重要的上下文重要文本元素的解释。鉴于缺乏评估XAI方法的统一标准，本研究采用整体三层综合评估框架：功能接地、人性接地和应用接地，来评估所提出的SIDU-TXT在各种实验中的有效性。我们发现，在电影评论数据集的情感分析任务中，SIDU-TXT 在功能和人性化评估方面均表现出色，与 Grad-CAM 和 LIME 等基准相比，通过定量和定性分析展示了优越的性能。在庇护决策这一敏感而复杂的法律领域内基于应用的评估中，SIDU-TXT 和 Grad-CAM 表现出可比的表现，各有其优点和缺点。然而，这两种方法都未能完全满足专家期望的复杂标准，这凸显了对适合这些领域的 XAI 方法进行额外研究的迫切需要。</li>
</ul>

<h3>Title: EasyInstruct: An Easy-to-use Instruction Processing Framework for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Zhen Bi, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03049">https://arxiv.org/abs/2402.03049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03049">https://arxiv.org/pdf/2402.03049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03049]] EasyInstruct: An Easy-to-use Instruction Processing Framework for Large  Language Models(https://arxiv.org/abs/2402.03049)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along with a running demo App at https://huggingface.co/spaces/zjunlp/EasyInstruct for quick-start, calling for broader research centered on instruction data.</li>
<li><strong>摘要：</strong>近年来，指令调优受到越来越多的关注，并成为增强大型语言模型（LLM）能力的关键技术。为了构建高质量的指令数据集，人们提出了许多指令处理方法，旨在实现数据数量和数据质量之间的微妙平衡。然而，由于各种指令处理方法之间存在不一致，导致社区没有标准的开源指令处理实现框架，这阻碍了从业者的进一步发展和进步。为了促进指令处理研究和开发，我们推出了 EasyInstruct，这是一个易于使用的 LLM 指令处理框架，它将指令生成、选择和提示模块化，同时还考虑了它们的组合和交互。 EasyInstruct 在 https://github.com/zjunlp/EasyInstruct 上公开发布并积极维护，并在 https://huggingface.co/spaces/zjunlp/EasyInstruct 上运行演示应用程序以实现快速启动，呼吁以更广泛的研究为中心关于指令数据。</li>
</ul>

<h3>Title: Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for  Semantic Representations</h3>
<ul>
<li><strong>Authors: </strong>Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03053">https://arxiv.org/abs/2402.03053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03053">https://arxiv.org/pdf/2402.03053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03053]] Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for  Semantic Representations(https://arxiv.org/abs/2402.03053)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>In this work, we present a comprehensive exploration of finetuning Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs. We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG). For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets. In the realm of RAG models, our approach proves competitive with OpenAI text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the "Melayu" keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset. These findings underscore the effectiveness of our finetuning strategy and highlight the performance gains in both Semantic Similarity and RAG tasks. All models released at https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99</li>
<li><strong>摘要：</strong>在这项工作中，我们对马来西亚语言模型（特别是 Llama2 和 Mistral）在涉及负对和正对的嵌入任务上进行了全面的探索。我们发布了两个专为语义相似性和检索增强生成（RAG）量身定制的不同模型。对于语义相似度，我们的 6 亿参数 Llama2 模型在 b.cari.com.my、c.cari.com.my、马来新闻和马来西亚 Twitter 测试集的所有recall@k 指标上均优于 OpenAI text-embedding-ada-002 。在 RAG 模型领域，我们的方法在马来西亚环境中与 OpenAI text-embedding-ada-002 相比具有竞争力。值得注意的是，我们的 20 亿参数 Llama2 模型在“Melayu”关键词研究论文数据集上实现了卓越的 Recall@5、Recall@10，并在 lom.agc.gov.my 的 Recall@3、Recall@5 和 Recall@10 上表现出色数据集。这些发现强调了我们微调策略的有效性，并强调了语义相似性和 RAG 任务的性能提升。所有模型均在 https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99 发布</li>
</ul>

<h3>Title: Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes  Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Bahareh Tasdighi, Nicklas Werge, Yi-Shan Wu, Melih Kandemir</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03055">https://arxiv.org/abs/2402.03055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03055">https://arxiv.org/pdf/2402.03055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03055]] Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes  Uncertainty(https://arxiv.org/abs/2402.03055)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement learning. We report empirical evaluations demonstrating PAC's enhanced stability and improved performance over the state of the art in diverse continuous control problems.</li>
<li><strong>摘要：</strong>我们引入了概率 Actor-Critic (PAC)，这是一种新颖的强化学习算法，由于其能够减轻探索-利用权衡，因此具有改进的连续控制性能。 PAC 通过无缝集成随机政策和批评者来实现这一目标，在批评者不确定性估计和参与者培训之间创建动态协同作用。我们的 PAC 算法的关键贡献在于，它通过可能近似正确贝叶斯 (PAC-Bayes) 分析明确地建模和推断批评者的认知不确定性。这种批评者不确定性的结合使 PAC 能够在学习时调整其探索策略，指导参与者的决策过程。 PAC 与现有技术的固定或预先安排的勘探方案相比具有优势。在 PAC-Bayes 分析的指导下，随机政策和批评家之间的协同作用代表了深度强化学习中更具适应性和更有效的探索策略的基本一步。我们报告的实证评估表明，在各种连续控制问题中，PAC 的稳定性和性能优于现有技术。</li>
</ul>

<h3>Title: Intent-based Prompt Calibration: Enhancing prompt optimization with  synthetic boundary cases</h3>
<ul>
<li><strong>Authors: </strong>Elad Levi, Eli Brosh, Matan Friedmann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03099">https://arxiv.org/abs/2402.03099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03099">https://arxiv.org/pdf/2402.03099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03099]] Intent-based Prompt Calibration: Enhancing prompt optimization with  synthetic boundary cases(https://arxiv.org/abs/2402.03099)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation. Our method outperforms state-of-the-art methods with a limited number of annotated samples. Furthermore, we validate the advantages of each one of the system's key components. Our system is built in a modular way, facilitating easy adaptation to other tasks. The code is available $\href{https://github.com/Eladlev/AutoPrompt}{here}$.</li>
<li><strong>摘要：</strong>由于大型语言模型 (LLM) 对给定提示的高度敏感性以及文本任务指令固有的模糊性，提示工程是一项具有挑战性且重要的任务。自动提示工程对于法学硕士实现优化性能至关重要。最近的研究证明了法学硕士能够通过采用元提示来自动进行提示工程，该元提示结合了上次试验的结果并提出了改进的提示。然而，这需要一个高质量的基准来比较不同的提示，这在许多现实世界的用例中获取起来既困难又昂贵。在这项工作中，我们引入了一种自动提示工程的新方法，使用校准过程来迭代地根据用户意图细化提示。在优化过程中，系统联合生成边界用例的综合数据，并根据生成的数据集优化提示。我们证明了我们的方法对于现实世界任务（例如调节和生成）的强大专有模型的有效性。我们的方法在带注释的样本数量有限的情况下优于最先进的方法。此外，我们验证了系统每个关键组件的优点。我们的系统以模块化方式构建，便于轻松适应其他任务。代码可在 $\href{https://github.com/Eladlev/AutoPrompt}{here}$ 获取。</li>
</ul>

<h3>Title: Constrained Decoding for Cross-lingual Label Projection</h3>
<ul>
<li><strong>Authors: </strong>Duong Minh Le, Yang Chen, Alan Ritter, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03131">https://arxiv.org/abs/2402.03131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03131">https://arxiv.org/pdf/2402.03131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03131]] Constrained Decoding for Cross-lingual Label Projection(https://arxiv.org/abs/2402.03131)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, rag</a></li>
<li><strong>Abstract: </strong>Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues. Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies. This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data. We evaluate on two cross-lingual transfer tasks, namely Named Entity Recognition and Event Argument Extraction, spanning 20 languages. The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment.</li>
<li><strong>摘要：</strong>利用多语言法学硕士的零样本跨语言迁移已成为无标记训练数据的低资源语言的流行学习范例。然而，对于涉及单词和短语的细粒度预测的 NLP 任务，零样本跨语言迁移学习的性能远远落后于监督微调方法。因此，通常利用翻译和标签投影来进一步提高性能，方法是：（1）将以高资源语言（例如英语）提供的训练数据与黄金标签一起翻译成低资源语言，和/ （2）将低资源语言的测试数据翻译为高源语言以运行推理，然后将预测的跨度标签投影回原始测试数据。然而，由于在翻译模型的输入中注入了额外的标签标记，最先进的基于标记的标签投影方法会遭受翻译质量下降的影响。在这项工作中，我们探索了一个新的方向，利用标签投影的约束解码来克服上述问题。我们的新方法不仅可以保持翻译文本的质量，而且具有适用于翻译训练和翻译测试数据策略的多功能性。这种多功能性至关重要，因为我们的实验表明，与仅翻译训练数据相比，翻译测试数据可以显着提高性能。我们评估了两项跨语言迁移任务，即命名实体识别和事件参数提取，涵盖 20 种语言。结果表明，我们的方法大大优于最先进的基于标记的方法，并且比其他依赖外部单词对齐的标签投影方法表现出更好的性能。</li>
</ul>

<h3>Title: Mastering Zero-Shot Interactions in Cooperative and Competitive  Simultaneous Games</h3>
<ul>
<li><strong>Authors: </strong>Yannik Mahlau, Frederik Schubert, Bodo Rosenhahn</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03136">https://arxiv.org/abs/2402.03136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03136">https://arxiv.org/pdf/2402.03136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03136]] Mastering Zero-Shot Interactions in Cooperative and Competitive  Simultaneous Games(https://arxiv.org/abs/2402.03136)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>The combination of self-play and planning has achieved great successes in sequential games, for instance in Chess and Go. However, adapting algorithms such as AlphaZero to simultaneous games poses a new challenge. In these games, missing information about concurrent actions of other agents is a limiting factor as they may select different Nash equilibria or do not play optimally at all. Thus, it is vital to model the behavior of the other agents when interacting with them in simultaneous games. To this end, we propose Albatross: AlphaZero for Learning Bounded-rational Agents and Temperature-based Response Optimization using Simulated Self-play. Albatross learns to play the novel equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which enables cooperation and competition with agents of any playing strength. We perform an extensive evaluation of Albatross on a set of cooperative and competitive simultaneous perfect-information games. In contrast to AlphaZero, Albatross is able to exploit weak agents in the competitive game of Battlesnake. Additionally, it yields an improvement of 37.6% compared to previous state of the art in the cooperative Overcooked benchmark.</li>
<li><strong>摘要：</strong>自我对弈和计划的结合在连续游戏中取得了巨大的成功，例如在国际象棋和围棋中。然而，使 AlphaZero 等算法适应同时进行的游戏提出了新的挑战。在这些游戏中，缺少有关其他智能体并发操作的信息是一个限制因素，因为它们可能会选择不同的纳什均衡或根本无法发挥最佳效果。因此，在同步游戏中与其他智能体交互时，对其他智能体的行为进行建模至关重要。为此，我们提出 Albatross：AlphaZero，用于使用模拟自我游戏学习有界理性代理和基于温度的响应优化。 Albatross 学习如何玩平滑最佳响应 Logit 均衡 (SBRLE) 的新颖均衡概念，它可以与任何游戏强度的代理进行合作和竞争。我们在一组合作和竞争的同步完美信息博弈中对 Albatross 进行了广泛的评估。与 AlphaZero 相比，Albatross 能够在 Battlesnake 的竞技游戏中利用弱代理。此外，与之前合作的 Overcooked 基准测试中的最新技术相比，它的性能提高了 37.6%。</li>
</ul>

<h3>Title: Sociolinguistically Informed Interpretability: A Case Study on Hinglish  Emotion Classification</h3>
<ul>
<li><strong>Authors: </strong>Kushal Tatariya, Heather Lent, Johannes Bjerva, Miryam de Lhoneux</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03137">https://arxiv.org/abs/2402.03137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03137">https://arxiv.org/pdf/2402.03137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03137]] Sociolinguistically Informed Interpretability: A Case Study on Hinglish  Emotion Classification(https://arxiv.org/abs/2402.03137)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Emotion classification is a challenging task in NLP due to the inherent idiosyncratic and subjective nature of linguistic expression, especially with code-mixed data. Pre-trained language models (PLMs) have achieved high performance for many tasks and languages, but it remains to be seen whether these models learn and are robust to the differences in emotional expression across languages. Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions. To understand if language models can learn these associations, we study the effect of language on emotion prediction across 3 PLMs on a Hinglish emotion classification dataset. Using LIME and token level language ID, we find that models do learn these associations between language choice and emotional expression. Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce. We also conclude from the misclassifications that the models may overgeneralise this heuristic to other infrequent examples where this sociolinguistic phenomenon does not apply.</li>
<li><strong>摘要：</strong>由于语言表达固有的特殊性和主观性，尤其是代码混合数据，情感分类是 NLP 中的一项具有挑战性的任务。预训练的语言模型（PLM）已经在许多任务和语言上取得了高性能，但这些模型是否能够学习并且对不同语言的情感表达差异具有鲁棒性还有待观察。社会语言学研究表明，说印度英语的人在表达消极情绪时会改用印地语，而在表达积极情绪时会改用英语。为了了解语言模型是否可以学习这些关联，我们研究了语言对印度英语情感分类数据集上 3 个 PLM 的情感预测的影响。使用 LIME 和 token 级语言 ID，我们发现模型确实学习了语言选择和情感表达之间的这些关联。此外，在预训练中存在代码混合数据可以在特定任务数据稀缺时增强学习效果。我们还从错误分类中得出结论，模型可能会过度概括这种启发式到其他不常见的社会语言现象不适用的例子。</li>
</ul>

<h3>Title: Just Cluster It: An Approach for Exploration in High-Dimensions using  Clustering and Pre-Trained Representations</h3>
<ul>
<li><strong>Authors: </strong>Stefan Sylvius Wagner, Stefan Harmeling</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03138">https://arxiv.org/abs/2402.03138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03138">https://arxiv.org/pdf/2402.03138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03138]] Just Cluster It: An Approach for Exploration in High-Dimensions using  Clustering and Pre-Trained Representations(https://arxiv.org/abs/2402.03138)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-trained biases into exploration. We evaluate our approach on the VizDoom and Habitat environments, demonstrating that our method surpasses other well-known exploration methods in these settings.</li>
<li><strong>摘要：</strong>在本文中，我们采用以表示为中心的视角来探索强化学习，将探索从根本上视为密度估计问题。我们研究了聚类表示在 3D 环境中探索的有效性，基于这样的观察：与 2D 环境相比，过渡之间的像素变化的重要性在 3D 环境中不太明显，在 2D 环境中，过渡之间的像素变化通常是不同的且意义重大。我们提出了一种方法，对随机表示和预训练的 DINO 表示执行情景和全局聚类来计数状态，即估计伪计数。令人惊讶的是，即使是随机特征也可以有效地聚类来计算 3D 环境中的状态，但是当这些特征在视觉上变得更加复杂时，由于表示中预先训练的归纳偏差，预训练的 DINO 表示会更加有效。总的来说，这提供了一种将预先训练的偏差整合到探索中的途径。我们在 VizDoom 和 Habitat 环境中评估我们的方法，证明我们的方法在这些环境中超越了其他众所周知的探索方法。</li>
</ul>

<h3>Title: Boosting Long-Delayed Reinforcement Learning with Auxiliary  Short-Delayed Task</h3>
<ul>
<li><strong>Authors: </strong>Qingyuan Wu, Simon Sinong Zhan, Yixuan Wang, Chung-Wei Lin, Chen Lv, Qi Zhu, Chao Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03141">https://arxiv.org/abs/2402.03141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03141">https://arxiv.org/pdf/2402.03141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03141]] Boosting Long-Delayed Reinforcement Learning with Auxiliary  Short-Delayed Task(https://arxiv.org/abs/2402.03141)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays. State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments. To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments. Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task. We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task. On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficiency and policy performance.</li>
<li><strong>摘要：</strong>强化学习在延迟场景中具有挑战性，延迟场景是现实世界中观察和交互发生延迟的常见情况。最先进的（SOTA）状态增强技术要么会受到状态空间爆炸以及延迟步骤的影响，要么会在随机环境中出现性能退化。为了应对这些挑战，我们新颖的辅助延迟强化学习（AD-RL）利用辅助短延迟任务来加速长延迟任务的学习，而不会影响随机环境中的性能。具体来说，AD-RL 学习短延迟任务中的价值函数，然后将其与长延迟任务中的引导和策略改进技术一起使用。我们从理论上证明，与直接在原始长延迟任务上学习相比，这可以大大降低样本复杂性。在确定性和随机基准上，我们的方法在样本效率和策略性能方面都明显优于 SOTA。</li>
</ul>

<h3>Title: Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michele Mastromattei, Fabio Massimo Zanzotto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03142">https://arxiv.org/abs/2402.03142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03142">https://arxiv.org/pdf/2402.03142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03142]] Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for  Large Language Models(https://arxiv.org/abs/2402.03142)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other pruning and PEFT algorithms confirm KEN effectiveness. Furthermore, we introduce KEN_viz, an explainable tool that visualizes the optimized model composition and the subnetwork selected by KEN.</li>
<li><strong>摘要：</strong>由于神经网络模型的复杂性及其在各个领域的广泛应用，神经网络剪枝变得越来越重要。现有的剪枝算法常常受到架构特殊性、过于复杂以及对复杂计算的依赖等限制，使得它们对于实际应用来说不切实际。在本文中，我们提出了 KEN：一种基于核密度估计（KDE）的简单、通用和非结构化剪枝算法。 KEN 旨在通过有选择地保留最重要的参数，同时将其他参数恢复到训练前状态来构建优化的 Transformer 模型。这种方法保持模型性能，同时允许仅存储优化的子网，从而显着节省内存。对七个变压器模型的广泛评估表明，KEN 的性能与原始模型相同或更好，参数最小减少了 25%。与其他剪枝和 PEFT 算法的深入比较证实了 KEN 的有效性。此外，我们还引入了 KEN_viz，这是一个可解释的工具，可以可视化优化的模型组合和 KEN 选择的子网络。</li>
</ul>

<h3>Title: A Multi-step Loss Function for Robust Learning of the Dynamics in  Model-based Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Abdelhakim Benechehab, Albert Thomas, Giuseppe Paolo, Maurizio Filippone, Balázs Kégl</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03146">https://arxiv.org/abs/2402.03146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03146">https://arxiv.org/pdf/2402.03146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03146]] A Multi-step Loss Function for Robust Learning of the Dynamics in  Model-based Reinforcement Learning(https://arxiv.org/abs/2402.03146)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows. In this paper we tackle this issue by using a multi-step objective to train one-step models. Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons. We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments. To support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system. Second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons. Finally, in the pure batch reinforcement learning setting, we demonstrate that one-step models serve as strong baselines when dynamics are deterministic, while multi-step models would be more advantageous in the presence of noise, highlighting the potential of our approach in real-world applications.</li>
<li><strong>摘要：</strong>在基于模型的强化学习中，大多数算法依赖于从数据中学习到的动力学的一步模型来模拟轨迹。这种方法的一个关键挑战是随着轨迹长度的增长，一步预测误差会加剧。在本文中，我们通过使用多步目标来训练单步模型来解决这个问题。我们的目标是未来各个时期的均方误差 (MSE) 损失的加权和。我们发现，当数据有噪声（观测中的加性高斯噪声）时，这种新的损失特别有用，这在现实生活环境中经常出现。为了支持多步损失，首先我们在两种易于处理的情况下研究其属性：i）一维线性系统，ii）双参数非线性系统。其次，我们在各种任务（环境或数据集）中表明，通过这种损失学习的模型在未来预测范围内的平均 R2 分数方面取得了显着的改进。最后，在纯批量强化学习设置中，我们证明，当动态确定时，单步模型可以作为强大的基线，而多步模型在存在噪声的情况下将更具优势，这凸显了我们的方法在现实中的潜力。世界应用。</li>
</ul>

<h3>Title: Linguistic features for sentence difficulty prediction in ABSA</h3>
<ul>
<li><strong>Authors: </strong>Adrian-Gabriel Chifu, Sébastien Fournier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03163">https://arxiv.org/abs/2402.03163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03163">https://arxiv.org/pdf/2402.03163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03163]] Linguistic features for sentence difficulty prediction in ABSA(https://arxiv.org/abs/2402.03163)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>One of the challenges of natural language understanding is to deal with the subjectivity of sentences, which may express opinions and emotions that add layers of complexity and nuance. Sentiment analysis is a field that aims to extract and analyze these subjective elements from text, and it can be applied at different levels of granularity, such as document, paragraph, sentence, or aspect. Aspect-based sentiment analysis is a well-studied topic with many available data sets and models. However, there is no clear definition of what makes a sentence difficult for aspect-based sentiment analysis. In this paper, we explore this question by conducting an experiment with three data sets: "Laptops", "Restaurants", and "MTSC" (Multi-Target-dependent Sentiment Classification), and a merged version of these three datasets. We study the impact of domain diversity and syntactic diversity on difficulty. We use a combination of classifiers to identify the most difficult sentences and analyze their characteristics. We employ two ways of defining sentence difficulty. The first one is binary and labels a sentence as difficult if the classifiers fail to correctly predict the sentiment polarity. The second one is a six-level scale based on how many of the top five best-performing classifiers can correctly predict the sentiment polarity. We also define 9 linguistic features that, combined, aim at estimating the difficulty at sentence level.</li>
<li><strong>摘要：</strong>自然语言理解的挑战之一是处理句子的主观性，这可能会表达增加复杂性和细微差别的观点和情感。情感分析是一个旨在从文本中提取和分析这些主观元素的领域，它可以应用于不同粒度级别，例如文档、段落、句子或方面。基于方面的情感分析是一个经过深入研究的主题，有许多可用的数据集和模型。然而，对于什么使得句子难以进行基于方面的情感分析，目前还没有明确的定义。在本文中，我们通过使用三个数据集：“笔记本电脑”、“餐厅”和“MTSC”（多目标相关情感分类）以及这三个数据集的合并版本来进行实验来探讨这个问题。我们研究领域多样性和句法多样性对难度的影响。我们使用分类器的组合来识别最困难的句子并分析它们的特征。我们采用两种方法来定义句子难度。第一个是二元的，如果分类器无法正确预测情感极性，则将句子标记为困难。第二个是六级量表，基于前五个表现最好的分类器中有多少可以正确预测情感极性。我们还定义了 9 个语言特征，这些特征组合起来旨在估计句子级别的难度。</li>
</ul>

<h3>Title: Homograph Attacks on Maghreb Sentiment Analyzers</h3>
<ul>
<li><strong>Authors: </strong>Fatima Zahra Qachfar, Rakesh M. Verma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03171">https://arxiv.org/abs/2402.03171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03171">https://arxiv.org/pdf/2402.03171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03171]] Homograph Attacks on Maghreb Sentiment Analyzers(https://arxiv.org/abs/2402.03171)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries. Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in "Arabizi". The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning.</li>
<li><strong>摘要：</strong>我们研究了同形异义词攻击对北非马格里布国家不同阿拉伯方言的情感分析 (SA) 任务的影响。当数据以“Arabizi”写入时，同形异义词攻击导致 Transformer 分类下降 65.3%，从 F1 分数 0.95 降至 0.33。这项研究的目的是强调法学硕士的弱点，并优先考虑道德和负责任的机器学习。</li>
</ul>

<h3>Title: Accurate and Well-Calibrated ICD Code Assignment Through Attention Over  Diverse Label Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Gonçalo Gomes, Isabel Coutinho, Bruno Martins</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03172">https://arxiv.org/abs/2402.03172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03172">https://arxiv.org/pdf/2402.03172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03172]] Accurate and Well-Calibrated ICD Code Assignment Through Attention Over  Diverse Label Embeddings(https://arxiv.org/abs/2402.03172)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches. This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work. We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently. The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms. Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance. Our approach also leads to properly calibrated classification results, which can effectively inform downstream tasks such as quantification.</li>
<li><strong>摘要：</strong>尽管国际疾病分类 (ICD) 已在全球范围内采用，但手动将 ICD 代码分配给临床文本非常耗时、容易出错且成本高昂，这促使了自动化方法的开发。本文描述了一种自动 ICD 编码的新方法，结合了以前相关工作的一些想法。我们特别采用基于 Transformer 的强大模型作为文本编码器，为了处理冗长的临床叙述，我们探索了 (a) 将基本编码器模型改编为 Longformer，或 (b) 将文本划分为块并独立处理每个块。编码器生成的表示与探索各种 ICD 代码同义词的标签嵌入机制相结合。对 MIMIC-III 数据集的不同分割进行的实验表明，所提出的方法优于 ICD 编码中当前最先进的模型，标签嵌入对良好的性能做出了显着贡献。我们的方法还可以得到正确校准的分类结果，可以有效地为量化等下游任务提供信息。</li>
</ul>

<h3>Title: Multi: Multimodal Understanding Leaderboard with Text and Images</h3>
<ul>
<li><strong>Authors: </strong>Zichen Zhu, Yang Xu, Lu Chen, Jingkai Yang, Yichuan Ma, Yiming Sun, Hailin Wen, Jiaqi Liu, Jinyu Cai, Yingzi Ma, Situo Zhang, Zihan Zhao, Liangtai Sun, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03173">https://arxiv.org/abs/2402.03173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03173">https://arxiv.org/pdf/2402.03173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03173]] Multi: Multimodal Understanding Leaderboard with Text and Images(https://arxiv.org/abs/2402.03173)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4,500 knowledge pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and 53.7%. Multi serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）的快速进展凸显了向学术界引入具有挑战性但现实的基准的必要性。现有的基准主要侧重于简单的自然图像理解，但 Multi 成为 MLLM 的前沿基准，提供了一个全面的数据集，用于评估 MLLM 是否理解复杂的图形和表格以及科学问题。该基准反映了当前现实的考试风格，提供了多模式输入，并要求精确或开放式的回答，类似于现实生活中的学校测试。它向 MLLM 提出各种任务的挑战，从公式推导到图像细节分析，再到跨模态推理。 Multi 包含超过 18,000 个问题，重点是多种形式的基于科学的 QA。我们还引入了 Multi-Elite（一个包含 500 个问题的子集，用于测试 MLLM 的极限）和 Multi-Extend（它通过 4,500 多个知识片段增强了情境学习研究）。我们的评估表明 MLLM 具有巨大的进步潜力，GPT-4V 在 Multi 上实现了 63.7% 的准确率，而其他 MLLM 的得分在 31.3% 到 53.7% 之间。 Multi不仅是一个强大的评估平台，而且为专家级人工智能的发展铺平了道路。</li>
</ul>

<h3>Title: The Matrix: A Bayesian learning model for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Siddhartha Dalal, Vishal Misra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03175">https://arxiv.org/abs/2402.03175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03175">https://arxiv.org/pdf/2402.03175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03175]] The Matrix: A Bayesian learning model for LLMs(https://arxiv.org/abs/2402.03175)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights into their functioning and potential applications.</li>
<li><strong>摘要：</strong>在本文中，我们引入了贝叶斯学习模型来理解大型语言模型（LLM）的行为。我们探索了 LLM 的优化指标，该指标基于预测下一个 token，并开发了一个基于这一原理的新颖模型。我们的方法涉及构建一个理想的生成文本模型，该模型由具有先验的多项转移概率矩阵表示，并且我们研究法学硕士如何近似该矩阵。我们讨论了嵌入和多项分布之间映射的连续性，并提出了狄利克雷近似定理来近似任何先验。此外，我们还演示了法学硕士的文本生成如何与贝叶斯学习原理保持一致，并深入研究了上下文学习的含义，特别解释了为什么上下文学习出现在较大的模型中，其中提示被视为要更新的样本。我们的研究结果表明，法学硕士的行为与贝叶斯学习一致，为其功能和潜在应用提供了新的见解。</li>
</ul>

<h3>Title: CIDAR: Culturally Relevant Instruction Dataset For Arabic</h3>
<ul>
<li><strong>Authors: </strong>Zaid Alyafeai, Khalid Almubarak, Ahmed Ashraf, Deema Alnuhait, Saied Alshahrani, Gubran A. Q. Abdulrahman, Gamil Ahmed, Qais Gawah, Zead Saleh, Mustafa Ghaleb, Yousef Ali, Maged S. Al-Shaibani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03177">https://arxiv.org/abs/2402.03177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03177">https://arxiv.org/pdf/2402.03177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03177]] CIDAR: Culturally Relevant Instruction Dataset For Arabic(https://arxiv.org/abs/2402.03177)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to other models fine-tuned on other datasets. Our experiments show that CIDAR can help enrich research efforts in aligning LLMs with the Arabic culture. All the code is available at https://github.com/ARBML/CIDAR.</li>
<li><strong>摘要：</strong>指令调优已成为教授大型语言模型 (LLM) 遵循指令的重要方法。然而，当前的教学数据集主要迎合英语或源自以英语为主的法学硕士，导致对西方文化的固有偏见。这种偏见极大地影响了阿拉伯语等非英语语言的语言结构，阿拉伯语具有反映整个阿拉伯地区不同文化的独特语法。本文通过引入 CIDAR 来解决这一限制：https://hf.co/datasets/arbml/CIDAR，这是第一个由人类审阅者在文化上保持一致的开放阿拉伯语指令调整数据集。 CIDAR 包含代表阿拉伯地区的 10,000 个指令和输出对。我们通过分析和与在其他数据集上微调的其他模型进行比较来讨论 CIDAR 的文化相关性。我们的实验表明，CIDAR 可以帮助丰富研究工作，使法学硕士与阿拉伯文化保持一致。所有代码均可在 https://github.com/ARBML/CIDAR 上获取。</li>
</ul>

<h3>Title: C-RAG: Certified Generation Risks for Retrieval-Augmented Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Mintong Kang, Nezihe Merve Gürel, Ning Yu, Dawn Song, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03181">https://arxiv.org/abs/2402.03181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03181">https://arxiv.org/pdf/2402.03181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03181]] C-RAG: Certified Generation Risks for Retrieval-Augmented Language  Models(https://arxiv.org/abs/2402.03181)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, rag</a></li>
<li><strong>Abstract: </strong>Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 在不同的应用程序中具有令人印象深刻的功能，但它们仍然存在可信度问题，例如幻觉和错位。检索增强语言模型（RAG）已被提出来通过基于外部知识来提高世代的可信度，但对其世代风险的理论理解仍有待探索。在本文中，我们回答：1）RAG是否确实可以带来低发电风险，2）如何为RAG和普通法学硕士的发电风险提供可证明的保证，以及3）什么充分条件使RAG模型能够降低发电风险。我们提出 C-RAG，这是第一个认证 RAG 模型发电风险的框架。具体来说，我们为 RAG 模型提供共形风险分析，并验证发电风险的置信上限，我们将其称为共形发电风险。我们还为测试分布变化下的一般有界风险函数的共形生成风险提供了理论保证。我们证明，当检索模型和转换器的质量非常重要时，RAG 比单个 LLM 实现了更低的保形生成风险。我们深入的实证结果证明了我们在四个最先进的检索模型上的四个广泛使用的 NLP 数据集上的保形生成风险保证的健全性和严格性。</li>
</ul>

<h3>Title: Empowering Time Series Analysis with Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, Dongjin Song</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03182">https://arxiv.org/abs/2402.03182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03182">https://arxiv.org/pdf/2402.03182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03182]] Empowering Time Series Analysis with Large Language Models: A Survey(https://arxiv.org/abs/2402.03182)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.</li>
<li><strong>摘要：</strong>最近，大型语言模型（LLM）取得了显着的进展，展示了它们在各种自然语言任务中前所未有的能力。然而，由于时间序列数据量大且种类繁多，以及导致概念漂移的非平稳性阻碍了连续模型的适应和重新构建，从头开始完全训练大型通用模型对于时间序列分析来说是一项挑战。 -训练。最近的进展表明，可以利用预先训练的法学硕士来捕获时间序列数据中的复杂依赖性并促进各种应用。在本次调查中，我们系统地概述了利用法学硕士进行时间序列分析的现有方法。具体来说，我们首先阐述了在时间序列背景下应用语言模型的挑战和动机，以及法学硕士的简要介绍。接下来，我们总结了基于 LLM 的时间序列分析的一般流程，将现有方法分为不同的组（即直接查询、标记化、提示设计、微调和模型集成），并强调每个组中的关键思想。我们还讨论了法学硕士在一般数据和时空时间序列数据上的应用，并针对特定领域进行了定制。最后，我们深入讨论了未来的研究机会，以支持法学硕士的时间序列分析。</li>
</ul>

<h3>Title: Unified Hallucination Detection for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Jinjie Gu, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03190">https://arxiv.org/abs/2402.03190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03190">https://arxiv.org/pdf/2402.03190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03190]] Unified Hallucination Detection for Multimodal Large Language Models(https://arxiv.org/abs/2402.03190)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, rag</a></li>
<li><strong>Abstract: </strong>Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.</li>
<li><strong>摘要：</strong>尽管多模态任务取得了重大进展，但多模态大语言模型（MLLM）仍受到幻觉这一关键问题的困扰。因此，可靠检测 MLLM 中的此类幻觉已成为模型评估和保障实际应用部署的重要方面。该领域的先前研究受到对单一任务的狭隘关注、所处理的幻觉类别范围不足以及缺乏详细粒度的限制。为了应对这些挑战，我们的工作扩大了幻觉检测的研究视野。我们提出了一种新颖的元评估基准 MHaluBench，它经过精心设计，旨在促进幻觉检测方法进展的评估。此外，我们还推出了一种新颖的统一多模式幻觉检测框架 UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过细致的评估和综合分析来证明UNIHD的有效性。我们还提供有关应用特定工具来解决各类幻觉的战略见解。</li>
</ul>

<h3>Title: Guidance with Spherical Gaussian Constraint for Conditional Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Lingxiao Yang, Shutong Ding, Yifan Cai, Jingyi Yu, Jingya Wang, Ye Shi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03201">https://arxiv.org/abs/2402.03201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03201">https://arxiv.org/pdf/2402.03201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03201]] Guidance with Spherical Gaussian Constraint for Conditional Diffusion(https://arxiv.org/abs/2402.03201)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form solution for DSG denoising with the Spherical Gaussian constraint. Notably, DSG can seamlessly integrate as a plugin module within existing training-free conditional diffusion methods. Implementing DSG merely involves a few lines of additional code with almost no extra computational overhead, yet it leads to significant performance improvements. Comprehensive experimental results in various conditional generation tasks validate the superiority and adaptability of DSG in terms of both sample quality and time efficiency.</li>
<li><strong>摘要：</strong>扩散模型的最新进展试图通过利用可微损失函数作为指导来处理条件生成任务，而无需额外的训练。虽然这些方法取得了一定的成功，但它们常常会损害样本质量并且需要较小的指导步长，从而导致采样过程更长。本文揭示了采用损失引导时采样过程中的流形偏差是根本问题。我们通过为损失指导的估计误差建立一定的下界，从理论上证明了流形偏差的存在。为了缓解这个问题，我们从高维高斯分布中的集中现象中汲取灵感，提出了带有球面高斯约束的扩散（DSG）。 DSG 通过优化有效地限制中间数据流形内的引导步骤，并允许使用更大的引导步骤。此外，我们提出了一种带有球面高斯约束的 DSG 去噪的封闭式解决方案。值得注意的是，DSG 可以作为插件模块无缝集成到现有的免训练条件扩散方法中。实现 DSG 仅涉及几行附加代码，几乎没有额外的计算开销，但却带来了显着的性能改进。各种条件生成任务的综合实验结果验证了DSG在样本质量和时间效率方面的优越性和适应性。</li>
</ul>

<h3>Title: Light and Optimal Schrödinger Bridge Matching</h3>
<ul>
<li><strong>Authors: </strong>Nikita Gushchin, Sergei Kholkin, Evgeny Burnaev, Alexander Korotin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03207">https://arxiv.org/abs/2402.03207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03207">https://arxiv.org/pdf/2402.03207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03207]] Light and Optimal Schrödinger Bridge Matching(https://arxiv.org/abs/2402.03207)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Schr\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB exploit the pervasive bridge matching procedures. Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them. In particular, given the EOT plan, these procedures can be adapted to solve SB. This fact is heavily exploited by recent works giving rives to matching-based SB solvers. The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training. We address these limitations and propose a novel procedure to learn SB which we call the \textbf{optimal Schr\"odinger bridge matching}. It exploits the optimal parameterization of the diffusion process and provably recovers the SB process \textbf{(a)} with a single bridge matching step and \textbf{(b)} with arbitrary transport plan as the input. Furthermore, we show that the optimal bridge matching objective coincides with the recently discovered energy-based modeling (EBM) objectives to learn EOT/SB. Inspired by this observation, we develop a light solver (which we call LightSB-M) to implement optimal matching in practice using the Gaussian mixture parameterization of the Schr\"odinger potential. We experimentally showcase the performance of our solver in a range of practical tasks. The code for the LightSB-M solver can be found at \url{https://github.com/SKholkin/LightSB-Matching}.</li>
<li><strong>摘要：</strong>薛定格桥 (SB) 最近引起了 ML 社区的关注，作为经典扩散模型的有希望的扩展，该模型也与熵最优传输 (EOT) 互连。最近的 SB 求解器利用了普遍的桥匹配程序。程序旨在恢复在分布之间传输质量的随机过程，仅给出它们之间的传输计划。特别是，考虑到 EOT 计划，这些程序可以适应解决 SB。最近的工作充分利用了这一事实，使 rives 能够匹配-基于 SB 求解器。这里的基石是恢复 EOT 计划：最近的工作要么使用启发式近似（例如，小批量 OT），要么建立迭代匹配程序，通过设计在训练期间累积误差。我们解决了这些限制并提出了一种新颖的方法学习 SB 的过程我们称之为 \textbf{optimal Schr\"odinger 桥匹配}。它利用扩散过程的最佳参数化，并可证明地恢复具有单个桥匹配步骤的 SB 过程 \textbf{(a)} 和具有任意传输计划作为输入的 \textbf{(b)} 。此外，我们表明最佳桥梁匹配目标与最近发现的用于学习 EOT/SB 的基于能量的建模 (EBM) 目标一致。受这一观察的启发，我们开发了一种光求解器（我们称之为 LightSB-M），以使用薛定格势的高斯混合参数化在实践中实现最佳匹配。我们通过实验展示了我们的求解器在一系列实际应用中的性能。 LightSB-M 求解器的代码可以在 \url{https://github.com/SKholkin/LightSB-Matching} 找到。</li>
</ul>

<h3>Title: BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity  Text Embeddings Through Self-Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03216">https://arxiv.org/abs/2402.03216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03216">https://arxiv.org/pdf/2402.03216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03216]] BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity  Text Embeddings Through Self-Knowledge Distillation(https://arxiv.org/abs/2402.03216)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings. To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility. The model and code will be publicly available at https://github.com/FlagOpen/FlagEmbedding.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种新的嵌入模型，称为 M3-Embedding，它以其在多语言、多功能和多粒度方面的多功能性而著称。它可以支持 100 多种工作语言，在多语言和跨语言检索任务上带来最先进的性能。它可以同时执行嵌入模型的三种常见检索功能：密集检索、多向量检索和稀疏检索，为现实世界的红外应用提供了统一的模型基础。它能够处理不同粒度的输入，从短句子到最多 8192 个标记的长文档。 M3-Embedding 的有效训练涉及以下技术贡献。我们提出了一种新颖的自我知识蒸馏方法，其中来自不同检索功能的相关性分数可以被整合为教师信号以提高训练质量。我们还优化了批处理策略，实现大批量和高训练吞吐量，以确保嵌入的辨别力。据我们所知，M3-Embedding 是第一个实现如此强大通用性的嵌入模型。该模型和代码将在 https://github.com/FlagOpen/FlagEmbedding 上公开提供。</li>
</ul>

<h3>Title: "Define Your Terms" : Enhancing Efficient Offensive Speech  Classification with Definition</h3>
<ul>
<li><strong>Authors: </strong>Huy Nghiem, Umang Gupta, Fred Morstatter</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03221">https://arxiv.org/abs/2402.03221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03221">https://arxiv.org/pdf/2402.03221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03221]] "Define Your Terms" : Enhancing Efficient Offensive Speech  Classification with Definition(https://arxiv.org/abs/2402.03221)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The propagation of offensive content through social media channels has garnered attention of the research community. Multiple works have proposed various semantically related yet subtle distinct categories of offensive speech. In this work, we explore meta-earning approaches to leverage the diversity of offensive speech corpora to enhance their reliable and efficient detection. We propose a joint embedding architecture that incorporates the input's label and definition for classification via Prototypical Network. Our model achieves at least 75% of the maximal F1-score while using less than 10% of the available training data across 4 datasets. Our experimental findings also provide a case study of training strategies valuable to combat resource scarcity.</li>
<li><strong>摘要：</strong>通过社交媒体渠道传播攻击性内容引起了研究界的关注。多部著作提出了各种语义相关但微妙不同的攻击性言论类别。在这项工作中，我们探索元学习方法，以利用攻击性语音语料库的多样性来增强其可靠和高效的检测。我们提出了一种联合嵌入架构，其中包含输入的标签和通过原型网络进行分类的定义。我们的模型至少实现了最大 F1 分数的 75%，同时在 4 个数据集中使用了不到 10% 的可用训练数据。我们的实验结果还提供了对应对资源稀缺有价值的培训策略的案例研究。</li>
</ul>

<h3>Title: English Prompts are Better for NLI-based Zero-Shot Emotion  Classification than Target-Language Prompts</h3>
<ul>
<li><strong>Authors: </strong>Patrick Barreiß, Roman Klinger, Jeremy Barnes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03223">https://arxiv.org/abs/2402.03223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03223">https://arxiv.org/pdf/2402.03223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03223]] English Prompts are Better for NLI-based Zero-Shot Emotion  Classification than Target-Language Prompts(https://arxiv.org/abs/2402.03223)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus. In addition, the set of emotion categories is highly domain-specific. For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories. This renders the task an interesting field for zero-shot classifications, in which the label set is not known at model development time. Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve prompting language models for text labels. This leaves us with a research gap that we address in this paper: In which language should we prompt for emotion labels on non-English texts? This is particularly of interest when we have access to a multilingual large language model, because we could request labels with English prompts even for non-English data. Our experiments with natural language inference-based language models show that it is consistently better to use English prompts even if the data is in a different language.</li>
<li><strong>摘要：</strong>文本中的情感分类是一项具有挑战性和主观性的任务，因为解释文本刺激需要涉及认知推理过程。此外，情感类别集是高度特定于领域的。例如，文学分析可能需要使用审美情感（例如，发现美丽的东西），而与基本情感类别相比，社交媒体分析可以受益于细粒度的集合（例如，将愤怒与烦恼分开）。这使得该任务成为零样本分类的一个有趣领域，其中标签集在模型开发时是未知的。不幸的是，大多数情感分析的资源都是英语，因此，大多数情感分析的研究都是用英语进行的，包括那些涉及文本标签提示语言模型的研究。这给我们留下了本文要解决的研究空白：我们应该用哪种语言提示非英语文本上的情感标签？当我们可以访问多语言大语言模型时，这一点尤其令人感兴趣，因为即使对于非英语数据，我们也可以请求带有英语提示的标签。我们对基于自然语言推理的语言模型的实验表明，即使数据采用不同的语言，使用英语提示始终会更好。</li>
</ul>

<h3>Title: FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion</h3>
<ul>
<li><strong>Authors: </strong>Xing Han, Huy Nguyen, Carl Harris, Nhat Ho, Suchi Saria</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03226">https://arxiv.org/abs/2402.03226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03226">https://arxiv.org/pdf/2402.03226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03226]] FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion(https://arxiv.org/abs/2402.03226)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.</li>
<li><strong>摘要：</strong>随着关键领域的机器学习模型越来越多地处理多模态数据，它们面临着处理各种模态的双重挑战，这些模态通常由于元素缺失而不完整，以及收集样本的时间不规则性和稀疏性。成功利用这些复杂的数据，同时克服高质量训练样本的稀缺性，是提高这些模型预测性能的关键。我们推出“FuseMoE”，这是一个结合了创新门控功能的专家混合框架。 FuseMoE 旨在集成多种模态，可有效管理模态缺失和采样数据轨迹不规则的场景。理论上，我们独特的门函数有助于提高收敛速度，从而在多个下游任务中获得更好的性能。 FuseMoE 在现实世界中的实用性通过一组具有挑战性的临床风险预测任务得到了验证。</li>
</ul>

<h3>Title: JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance  Skill Matching</h3>
<ul>
<li><strong>Authors: </strong>Antoine Magron, Anna Dai, Mike Zhang, Syrielle Montariol, Antoine Bosselut</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03242">https://arxiv.org/abs/2402.03242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03242">https://arxiv.org/pdf/2402.03242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03242]] JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance  Skill Matching(https://arxiv.org/abs/2402.03242)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies. We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability.</li>
<li><strong>摘要：</strong>最近的技能匹配方法，采用合成训练数据进行分类或相似性模型训练，已经显示出有希望的结果，减少了对耗时且昂贵的注释的需求。然而，以前的合成数据集有局限性，例如每个句子仅包含一项技能并且通常包含短句子。在本文中，我们介绍了 JobSkape，这是一个生成合成数据的框架，可以解决这些限制，专门用于增强技能与分类的匹配。在此框架内，我们创建了 SkillSkape，这是一个专为技能匹配任务量身定制的招聘信息综合开源综合数据集。我们引入了几个离线指标，表明我们的数据集类似于现实世界的数据。此外，我们还提出了一个使用大型语言模型（LLM）进行技能提取和匹配任务的多步骤管道，与已知的监督方法进行基准测试。我们概述了现实世界数据的下游评估结果可以超越基线，强调其有效性和适应性。</li>
</ul>

<h3>Title: PINN-BO: A Black-box Optimization Algorithm using Physics-Informed  Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Dat Phan-Trong, Hung The Tran, Alistair Shilton, Sunil Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03243">https://arxiv.org/abs/2402.03243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03243">https://arxiv.org/pdf/2402.03243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03243]] PINN-BO: A Black-box Optimization Algorithm using Physics-Informed  Neural Networks(https://arxiv.org/abs/2402.03243)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Black-box optimization is a powerful approach for discovering global optima in noisy and expensive black-box functions, a problem widely encountered in real-world scenarios. Recently, there has been a growing interest in leveraging domain knowledge to enhance the efficacy of machine learning methods. Partial Differential Equations (PDEs) often provide an effective means for elucidating the fundamental principles governing the black-box functions. In this paper, we propose PINN-BO, a black-box optimization algorithm employing Physics-Informed Neural Networks that integrates the knowledge from Partial Differential Equations (PDEs) to improve the sample efficiency of the optimization. We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory and prove that the use of the PDE alongside the black-box function evaluations, PINN-BO leads to a tighter regret bound. We perform several experiments on a variety of optimization tasks and show that our algorithm is more sample-efficient compared to existing methods.</li>
<li><strong>摘要：</strong>黑盒优化是一种在嘈杂且昂贵的黑盒函数中发现全局最优值的强大方法，这是现实场景中广泛遇到的问题。最近，人们对利用领域知识来提高机器学习方法的效率越来越感兴趣。偏微分方程 (PDE) 通常提供一种有效的方法来阐明控制黑盒函数的基本原理。在本文中，我们提出了 PINN-BO，一种采用物理信息神经网络的黑盒优化算法，该算法集成了偏微分方程 (PDE) 的知识，以提高优化的样本效率。我们利用 NTK 理论的进步分析了我们算法在后悔界限方面的理论行为，并证明使用偏微分方程以及黑盒函数评估 PINN-BO 会导致更严格的后悔界限。我们对各种优化任务进行了多次实验，结果表明，与现有方法相比，我们的算法具有更高的样本效率。</li>
</ul>

<h3>Title: Skill Set Optimization: Reinforcing Language Model Behavior via  Transferable Skills</h3>
<ul>
<li><strong>Authors: </strong>Kolby Nottingham, Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Sameer Singh, Peter Clark, Roy Fox</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03244">https://arxiv.org/abs/2402.03244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03244">https://arxiv.org/pdf/2402.03244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03244]] Skill Set Optimization: Reinforcing Language Model Behavior via  Transferable Skills(https://arxiv.org/abs/2402.03244)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）最近被用于交互式环境中的顺序决策。然而，利用环境奖励信号来持续提高 LLM 参与者并不简单。我们提出技能集优化（SSO），通过构建和完善可转移技能集来提高 LLM 参与者的表现。 SSO 通过提取具有高奖励的常见子轨迹并生成代表每个技能的子目标和指令来构建技能。这些技能是在上下文中向法学硕士参与者提供的，以强化高回报的行为。然后，SSO 通过修剪不会继续带来高回报的技能来进一步细化技能集。我们在经典视频游戏 NetHack 和文本环境 ScienceWorld 中评估我们的方法，以证明 SSO 优化一组技能和执行上下文策略改进的能力。在我们的自定义 NetHack 任务中，SSO 的性能比基线高出 40%，并且比 ScienceWorld 中之前的最先进技术高出 35%。</li>
</ul>

<h3>Title: MobilityGPT: Enhanced Human Mobility Modeling with a GPT model</h3>
<ul>
<li><strong>Authors: </strong>Ammar Haydari, Dongjie Chen, Zhengfeng Lai, Chen-Nee Chuah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03264">https://arxiv.org/abs/2402.03264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03264">https://arxiv.org/pdf/2402.03264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03264]] MobilityGPT: Enhanced Human Mobility Modeling with a GPT model(https://arxiv.org/abs/2402.03264)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, rag</a></li>
<li><strong>Abstract: </strong>Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits. To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT. We propose a gravity-based sampling method to train a transformer for semantic sequence similarity. Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits. Lastly, we constructed a Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel distance between training and the synthetically generated trajectories. Our experiments on real-world datasets demonstrate that MobilityGPT outperforms state-of-the-art methods in generating high-quality mobility trajectories that are closest to real data in terms of origin-destination similarity, trip length, travel radius, link, and gravity distributions.</li>
<li><strong>摘要：</strong>生成模型在捕捉人类移动特征和生成合成轨迹方面显示出了有希望的结果。然而，确保生成的地理空间移动数据在语义上是真实的（包括一致的位置序列）并反映现实世界的特征（例如对地理空间限制的约束）仍然具有挑战性。为了解决这些问题，我们利用生成式预训练 Transformer (GPT) 将人员流动建模重新格式化为自回归生成任务。为了确保其可控生成以缓解上述挑战，我们提出了一种地理空间感知的生成模型MobilityGPT。我们提出了一种基于重力的采样方法来训练变压器的语义序列相似性。然后，我们通过道路连接矩阵来约束训练过程，该矩阵提供轨迹生成中序列的连接性，从而将生成的轨迹保持在地理空间限制内。最后，我们构建了轨迹反馈强化学习（RLTF），以最小化训练和综合生成的轨迹之间的行进距离。我们对真实世界数据集的实验表明，MobilityGPT 在生成高质量移动轨迹方面优于最先进的方法，这些轨迹在起点-目的地相似性、行程长度、行程半径、链路和重力方面最接近真实数据分布。</li>
</ul>

<h3>Title: Understanding the Reasoning Ability of Language Models From the  Perspective of Reasoning Paths Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03268">https://arxiv.org/abs/2402.03268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03268">https://arxiv.org/pdf/2402.03268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03268]] Understanding the Reasoning Ability of Language Models From the  Perspective of Reasoning Paths Aggregation(https://arxiv.org/abs/2402.03268)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance.</li>
<li><strong>摘要：</strong>预训练的语言模型 (LM) 无需显式微调即可执行复杂的推理。为了理解具有下一个标记预测目标的预训练如何有助于这种推理能力的出现，我们建议我们可以将 LM 视为通过聚合在预训练时看到的间接推理路径来得出新的结论。我们发现这种观点在两个重要的推理案例中有效：使用知识图（KG）的逻辑推理和使用数学应用题（MWP）的数学推理。更具体地说，我们将推理路径形式化为知识/推理图上的随机游走路径。对学习到的 LM 分布的分析表明，相关随机游走路径概率的加权和是解释 LM 推理方式的合理方法。对多个 KG 和 MWP 数据集的实验和分析揭示了随机游走路径训练的效果，并表明增强未标记的随机游走推理路径可以提高现实世界的多步推理性能。</li>
</ul>

<h3>Title: Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information  Seeking in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03271">https://arxiv.org/abs/2402.03271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03271">https://arxiv.org/pdf/2402.03271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03271]] Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information  Seeking in Large Language Models(https://arxiv.org/abs/2402.03271)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).</li>
<li><strong>摘要：</strong>面对不确定性，寻求信息的能力至关重要。在许多实际应用中，例如医疗诊断和故障排除，解决任务所需的信息最初并不是给出的，必须通过提出后续问题来主动寻求（例如，医生向患者询问有关其病情的更多详细信息）症状）。在这项工作中，我们介绍了思想的不确定性（UoT），这是一种增强大型语言模型的算法，使其能够通过提出有效的问题来主动寻找信息。 UoT 结合了 1) 不确定性感知模拟方法，使模型能够模拟未来可能出现的场景及其发生的可能性，2) 基于不确定性的奖励，信息增益激励模型寻求信息，3) 奖励传播方案以最大化预期奖励的方式选择要提出的最佳问题。在医疗诊断、故障排除和“20个问题”游戏的实验中，与直接提示相比，UoT在多个LLM的任务成功完成率上平均提高了57.8%，并且还提高了效率（即问题数量）完成任务所需的）。</li>
</ul>

<h3>Title: A Framework for Partially Observed Reward-States in RLHF</h3>
<ul>
<li><strong>Authors: </strong>Chinmaya Kausik, Mirco Mutti, Aldo Pacchiano, Ambuj Tewari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03282">https://arxiv.org/abs/2402.03282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03282">https://arxiv.org/pdf/2402.03282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03282]] A Framework for Partially Observed Reward-States in RLHF(https://arxiv.org/abs/2402.03282)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regret. We then present the first explicit reduction that converts guarantees for cardinal regret to dueling regret. We show that our models and guarantees in both settings generalize and extend existing ones. Finally, we identify a recursive structure on our model that could improve the statistical and computational tractability of PORRL, giving examples from past work on RLHF as well as learning perfect reward machines, which PORRL subsumes.</li>
<li><strong>摘要：</strong>近年来，基于人类反馈的强化学习（RLHF）研究因其在法学硕士发展中的作用而受到重视。神经科学研究表明，人类对刺激的反应取决于部分观察到的“内部状态”。不幸的是，当前的 RLHF 模型没有考虑到这一点。此外，大多数 RLHF 模型不考虑中间反馈，这在实证工作中变得越来越重要，并且有助于提高样本复杂性和一致性。为了解决这些限制，我们将 RLHF 建模为具有部分观察到的奖励状态 (PORRL) 的强化学习。我们展示了 RLHF 中人类反馈的两种主要形式（主要反馈和决斗反馈）到 PORRL 的减少。对于基本反馈，我们开发了通用的统计有效算法并将其实例化以呈现 POR-UCRL 和 POR-UCBVI。对于决斗反馈，我们表明天真地减少基本反馈无法实现亚线性决斗遗憾。然后，我们提出了第一个明确的减少，将基本遗憾的保证转变为决斗遗憾。我们表明，我们的模型和保证在这两种情况下都可以概括和扩展现有的模型和保证。最后，我们在模型上确定了一个递归结构，可以提高 PORRL 的统计和计算易处理性，给出了过去 RLHF 工作的例子以及学习 PORRL 所包含的完美奖励机器。</li>
</ul>

<h3>Title: Deal, or no deal (or who knows)? Forecasting Uncertainty in  Conversations using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anthony Sicilia, Hyunwoo Kim, Khyathi Raghavi Chandu, Malihe Alikhani, Jack Hessel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03284">https://arxiv.org/abs/2402.03284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03284">https://arxiv.org/pdf/2402.03284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03284]] Deal, or no deal (or who knows)? Forecasting Uncertainty in  Conversations using Large Language Models(https://arxiv.org/abs/2402.03284)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing "conversation forecasting" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their size.</li>
<li><strong>摘要：</strong>有效的对话者会考虑他人不确定的目标、信念和情绪。但即使是最优秀的人类对话者也无法完美地预测对话的轨迹。语言模型能在多大程度上表示对话中固有的不确定性？我们提出了 FortUne Dial，这是长期存在的“对话预测”任务的扩展：不仅仅是准确性，而是使用不确定性感知指标进行评估，有效地实现对个别实例的弃权。我们研究了语言模型潜在地表示结果不确定性的两种方式（内部使用分数和直接使用标记），并提出了微调策略来改进两种表示的校准。对八个困难的谈判语料库的实验表明，我们提出的微调策略（传统的监督策略和离策略强化学习策略）可以校准较小的开源模型，以与 10 倍大小的预训练模型竞争。</li>
</ul>

<h3>Title: Make Every Move Count: LLM-based High-Quality RTL Code Generation Using  MCTS</h3>
<ul>
<li><strong>Authors: </strong>Matthew DeLorenzo, Animesh Basak Chowdhury, Vasudev Gohil, Shailja Thakur, Ramesh Karri, Siddharth Garg, Jeyavijayan Rajendran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03289">https://arxiv.org/abs/2402.03289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03289">https://arxiv.org/pdf/2402.03289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03289]] Make Every Move Count: LLM-based High-Quality RTL Code Generation Using  MCTS(https://arxiv.org/abs/2402.03289)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional transformer decoding algorithms. In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code. Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models. For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.</li>
<li><strong>摘要：</strong>现有的用于寄存器传输级代码生成的大型语言模型 (LLM) 面临编译失败以及功耗、性能和面积 (PPA) 效率欠佳等挑战。这是由于传统 Transformer 解码算法缺乏 PPA 意识。为此，我们提出了一种自动 Transformer 解码算法，该算法集成了用于前瞻的 Monte Carlo 树搜索，指导 Transformer 生成可编译、功能正确且经过 PPA 优化的代码。在 RTL 代码集上使用微调语言模型进行的实证评估表明，与仅提示方法相比，我们提出的技术始终能够生成功能正确的代码，并有效解决了朴素大型语言模型的 PPA 无意识缺陷。对于由最先进的 LLM（16 位加法器）生成的最大设计，我们的技术可以将面积延迟乘积实现 31.8% 的改进。</li>
</ul>

<h3>Title: Flora: Low-Rank Adapters Are Secretly Gradient Compressors</h3>
<ul>
<li><strong>Authors: </strong>Yongchang Hao, Yanshuai Cao, Lili Mou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03293">https://arxiv.org/abs/2402.03293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03293">https://arxiv.org/pdf/2402.03293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03293]] Flora: Low-Rank Adapters Are Secretly Gradient Compressors(https://arxiv.org/abs/2402.03293)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.</li>
<li><strong>摘要：</strong>尽管大型神经网络表现出完成不同任务的卓越能力，但它们需要使用过多的内存来存储训练的优化状态。为了缓解这一问题，提出了低秩自适应（LoRA），通过训练更少的参数来减少优化状态。然而，LoRA 将整体权重更新矩阵限制为低秩，从而限制了模型性能。在这项工作中，我们研究了 LoRA 的动态，并发现它可以通过随机投影来近似。基于这一观察，我们提出了 Flora，它能够通过重新采样投影矩阵来实现高秩更新，同时享受优化状态的亚线性空间复杂度。我们在不同的任务和模型架构中进行实验，以验证我们方法的有效性。</li>
</ul>

<h3>Title: Ginger: An Efficient Curvature Approximation with Linear Complexity for  General Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yongchang Hao, Yanshuai Cao, Lili Mou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03295">https://arxiv.org/abs/2402.03295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03295">https://arxiv.org/pdf/2402.03295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03295]] Ginger: An Efficient Curvature Approximation with Linear Complexity for  General Neural Networks(https://arxiv.org/abs/2402.03295)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices. Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning. The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix. These requirements are infeasible even with state-of-the-art hardware. In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our method enjoys efficient linear memory and time complexity for each iteration. Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate. We provide the convergence result of Ginger for non-convex objectives. Our experiments on different tasks with different model architectures verify the effectiveness of our method. Our code is publicly available.</li>
<li><strong>摘要：</strong>像广义高斯-牛顿方法这样的二阶优化方法被认为更强大，因为它们利用带有预处理矩阵的目标函数的曲率信息。尽管提供了诱人的理论优势，但它们并不容易应用于现代深度学习。主要原因是计算矩阵逆的二次内存和三次时间复杂度。即使使用最先进的硬件，这些要求也是不可行的。在这项工作中，我们提出了 Ginger，一种广义高斯-牛顿矩阵逆的特征分解。我们的方法具有高效的线性内存和每次迭代的时间复杂度。我们不近似条件矩阵，而是直接维护其逆矩阵，以使近似更加准确。我们提供了 Ginger 对于非凸目标的收敛结果。我们对不同模型架构的不同任务进行的实验验证了我们方法的有效性。我们的代码是公开的。</li>
</ul>

<h3>Title: GUARD: Role-playing to Generate Natural-language Jailbreakings to Test  Guideline Adherence of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03299">https://arxiv.org/abs/2402.03299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03299">https://arxiv.org/pdf/2402.03299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03299]] GUARD: Role-playing to Generate Natural-language Jailbreakings to Test  Guideline Adherence of Large Language Models(https://arxiv.org/abs/2402.03299)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, rag</a></li>
<li><strong>Abstract: </strong>The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.</li>
<li><strong>摘要：</strong>绕过大型语言模型 (LLM) 安全过滤器的“越狱”和有害响应的发现鼓励社区实施安全措施。一项主要的安全措施是在发布之前通过越狱主动测试法学硕士。因此，此类测试需要一种能够大规模且高效地生成越狱的方法。在本文中，我们遵循一种新颖而直观的策略来生成人类一代风格的越狱。我们提出了一个角色扮演系统，为用户法学硕士分配四个不同的角色，以协作完成新的越狱。此外，我们收集现有的越狱，并使用聚类频率和语义模式逐句将它们分成不同的独立特征。我们将这些特征组织成知识图谱，使它们更易于访问和检索。我们的不同角色系统将利用这个知识图谱来生成新的越狱，事实证明，这可以有效地诱导法学硕士产生不道德或违反指南的反应。此外，我们还在我们的系统中开创了一个设置，该设置将自动遵循政府发布的指导方针来生成越狱，以测试法学硕士是否相应地遵循指导方针。我们将我们的系统称为 GUARD（通过自适应角色扮演诊断维护指南）。我们通过实证验证了 GUARD 在三个尖端开源 LLM（Vicuna-13B、LongChat-7B 和 Llama-2-7B）以及广泛使用的商业 LLM (ChatGPT) 上的有效性。此外，我们的工作扩展到视觉语言模型（MiniGPT-v2 和 Gemini Vision Pro）领域，展示了 GUARD 的多功能性，并为跨不同模式开发更安全、更可靠的基于 LLM 的应用程序提供了宝贵的见解。</li>
</ul>

<h3>Title: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03300">https://arxiv.org/abs/2402.03300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03300">https://arxiv.org/pdf/2402.03300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03300]] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open  Language Models(https://arxiv.org/abs/2402.03300)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, code</a></li>
<li><strong>Abstract: </strong>Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.</li>
<li><strong>摘要：</strong>由于其复杂性和结构化的性质，数学推理对语言模型提出了重大挑战。在本文中，我们介绍了 DeepSeekMath 7B，它继续使用来自 Common Crawl 的 120B 数学相关标记以及自然语言和代码数据来预训练 DeepSeek-Coder-Base-v1.5 7B。 DeepSeekMath 7B 在不依赖外部工具包和投票技术的情况下，在竞赛级 MATH 基准测试中取得了 51.7% 的骄人成绩，接近 Gemini-Ultra 和 GPT-4 的性能水平。 DeepSeekMath 7B 的 64 个样本的自一致性在 MATH 上达到 60.9%。 DeepSeekMath 的数学推理能力归因于两个关键因素：首先，我们通过精心设计的数据选择管道来利用公开可用的网络数据的巨大潜力。其次，我们介绍组相对策略优化（GRPO），它是近端策略优化（PPO）的一种变体，它可以增强数学推理能力，同时优化 PPO 的内存使用。</li>
</ul>

<h3>Title: Nevermind: Instruction Override and Moderation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Edward Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03303">https://arxiv.org/abs/2402.03303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03303">https://arxiv.org/pdf/2402.03303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03303]] Nevermind: Instruction Override and Moderation in Large Language Models(https://arxiv.org/abs/2402.03303)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines. Thus, we postulate the most effective approach for safe, trustworthy AI should be dealt external to the LLM itself.</li>
<li><strong>摘要：</strong>鉴于最近的大型语言模型（LLM）令人印象深刻的功能，我们对最流行的专有和不同规模的开源模型进行了调查和基准测试，以在冲突情况下遵循显式指令，例如覆盖。这些包括模型覆盖模型权重内的知识的能力、覆盖（或缓和）提示中提取的知识的能力，以及最后执行完全越狱的能力。进行的实验表明了改善指令遵循的几个关键发现——较大的模型在遵循指令方面表现最好，这些指令覆盖内部和上下文指令，并且是服从的，甚至是错误的。当通过绳缩放扩展到更长的上下文时，需要从困惑悬崖的边缘维护一个重要的缓冲区，以维持指令跟踪能力。最后，我们观察到改进指令遵循，以及随后的指令覆盖/越狱，从根本上与语言模型遵循给定安全过滤器或指南的能力相矛盾。因此，我们假设安全、值得信赖的人工智能的最有效方法应该在法学硕士本身的外部进行处理。</li>
</ul>

<h3>Title: V-IRL: Grounding Virtual Intelligence in Real Life</h3>
<ul>
<li><strong>Authors: </strong>Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, Saining Xie</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03310">https://arxiv.org/abs/2402.03310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03310">https://arxiv.org/pdf/2402.03310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03310]] V-IRL: Grounding Virtual Intelligence in Real Life(https://arxiv.org/abs/2402.03310)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.</li>
<li><strong>摘要：</strong>人类居住的地球与创建现代人工智能代理的数字领域之间存在着感官鸿沟。为了开发能够在现实世界中像人类一样灵活感知、思考和行动的人工智能代理，必须弥合数字世界和物理世界之间的现实差距。我们如何才能在像我们所居住的环境一样丰富多样的环境中体现代理，而不受真实硬件和控制的限制？为此，我们引入了 V-IRL：一个平台，使代理能够在虚拟而现实的环境中与现实世界进行可扩展的交互。我们的平台是开发能够完成各种实际任务的代理的游乐场，也是衡量感知、决策以及与全球真实数据交互等能力进展的巨大测试平台。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
