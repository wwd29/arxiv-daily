<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2023-12-19</h1>
<h2>language model</h2>
<h3>Title: Do LVLMs Understand Charts? Analyzing and Correcting Factual Errors in Chart Captioning. (arXiv:2312.10160v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10160">http://arxiv.org/abs/2312.10160</a></li>
<li>Code URL: <a href="https://github.com/khuangaf/chocolate">https://github.com/khuangaf/chocolate</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10160]] Do LVLMs Understand Charts? Analyzing and Correcting Factual Errors in Chart Captioning(http://arxiv.org/abs/2312.10160)</code></li>
<li>Summary: <p>Recent advancements in large vision-language models (LVLMs) have led to
significant progress in generating natural language descriptions for visual
content and thus enhancing various applications. One issue with these powerful
models is that they sometimes produce texts that are factually inconsistent
with the visual input. While there has been some effort to mitigate such
inconsistencies in natural image captioning, the factuality of generated
captions for structured document images, such as charts, has not received as
much scrutiny, posing a potential threat to information reliability in critical
applications. This work delves into the factuality aspect by introducing a
comprehensive typology of factual errors in generated chart captions. A
large-scale human annotation effort provides insight into the error patterns
and frequencies in captions crafted by various chart captioning models,
ultimately forming the foundation of a novel dataset, CHOCOLATE. Our analysis
reveals that even state-of-the-art models, including GPT-4V, frequently produce
captions laced with factual inaccuracies. In response to this challenge, we
establish the new task of Chart Caption Factual Error Correction and introduce
CHARTVE, a model for visual entailment that outperforms proprietary and
open-source LVLMs in evaluating factual consistency. Furthermore, we propose
C2TFEC, an interpretable two-stage framework that excels at correcting factual
errors. This work inaugurates a new domain in factual error correction for
chart captions, presenting a novel evaluation mechanism, and demonstrating an
effective approach to ensuring the factuality of generated chart captions.
</p></li>
</ul>

<h3>Title: Pipeline and Dataset Generation for Automated Fact-checking in Almost Any Language. (arXiv:2312.10171v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10171">http://arxiv.org/abs/2312.10171</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10171]] Pipeline and Dataset Generation for Automated Fact-checking in Almost Any Language(http://arxiv.org/abs/2312.10171)</code></li>
<li>Summary: <p>This article presents a pipeline for automated fact-checking leveraging
publicly available Language Models and data. The objective is to assess the
accuracy of textual claims using evidence from a ground-truth evidence corpus.
The pipeline consists of two main modules -- the evidence retrieval and the
claim veracity evaluation. Our primary focus is on the ease of deployment in
various languages that remain unexplored in the field of automated
fact-checking. Unlike most similar pipelines, which work with evidence
sentences, our pipeline processes data on a paragraph level, simplifying the
overall architecture and data requirements. Given the high cost of annotating
language-specific fact-checking training data, our solution builds on the
Question Answering for Claim Generation (QACG) method, which we adapt and use
to generate the data for all models of the pipeline. Our strategy enables the
introduction of new languages through machine translation of only two fixed
datasets of moderate size. Subsequently, any number of training samples can be
generated based on an evidence corpus in the target language. We provide open
access to all data and fine-tuned models for Czech, English, Polish, and Slovak
pipelines, as well as to our codebase that may be used to reproduce the
results.We comprehensively evaluate the pipelines for all four languages,
including human annotations and per-sample difficulty assessment using
Pointwise V-information. The presented experiments are based on full Wikipedia
snapshots to promote reproducibility. To facilitate implementation and user
interaction, we develop the FactSearch application featuring the proposed
pipeline and the preliminary feedback on its performance.
</p></li>
</ul>

<h3>Title: Student as an Inherent Denoiser of Noisy Teacher. (arXiv:2312.10185v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10185">http://arxiv.org/abs/2312.10185</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10185]] Student as an Inherent Denoiser of Noisy Teacher(http://arxiv.org/abs/2312.10185)</code></li>
<li>Summary: <p>Knowledge distillation (KD) has been widely employed to transfer knowledge
from a large language model (LLM) to a specialized model in low-data regimes
through pseudo label learning. However, pseudo labels generated by teacher
models are usually noisy and may influence KD performance. This study delves
into KD with noisy teachers and uncovers that the student model can already
generate more accurate predictions than the teacher labels used to train it
during KD, indicating its inherent ability to denoise noisy teacher labels.
Motivated by this finding, we propose Peer-Advised KD to improve vanilla KD
from noisy teachers. Experiments show that Peer-Advised KD can outperform LLM
by approximately 5% with 50 human-labeled data, and even competitive to
standard supervised finetuning with 750 human-labeled data.
</p></li>
</ul>

<h3>Title: Low-resource classification of mobility functioning information in clinical sentences using large language models. (arXiv:2312.10202v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10202">http://arxiv.org/abs/2312.10202</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10202]] Low-resource classification of mobility functioning information in clinical sentences using large language models(http://arxiv.org/abs/2312.10202)</code></li>
<li>Summary: <p>Objective: Function is increasingly recognized as an important indicator of
whole-person health. This study evaluates the ability of publicly available
large language models (LLMs) to accurately identify the presence of functioning
information from clinical notes. We explore various strategies to improve the
performance on this task. Materials and Methods: We collect a balanced binary
classification dataset of 1000 sentences from the Mobility NER dataset, which
was curated from n2c2 clinical notes. For evaluation, we construct zero-shot
and few-shot prompts to query the LLMs whether a given sentence contains
mobility functioning information. Two sampling techniques, random sampling and
k-nearest neighbor (kNN)-based sampling, are used to select the few-shot
examples. Furthermore, we apply a parameter-efficient prompt-based fine-tuning
method to the LLMs and evaluate their performance under various training
settings. Results: Flan-T5-xxl outperforms all other models in both zero-shot
and few-shot settings, achieving a F1 score of 0.865 with a single
demonstrative example selected by kNN sampling. In prompt-based fine-tuning
experiments, this foundation model also demonstrates superior performance
across all low-resource settings, particularly achieving an impressive F1 score
of 0.922 using the full training dataset. The smaller model, Flan-T5-xl,
requires fine-tuning with only 2.3M additional parameters to achieve comparable
performance to the fully fine-tuned Gatortron-base model, both surpassing 0.9
F1 score. Conclusion: Open-source instruction-tuned LLMs demonstrate impressive
in-context learning capability in the mobility functioning classification task.
The performance of these models can be further improved by continuing
fine-tuning on a task-specific dataset.
</p></li>
</ul>

<h2>gpt</h2>
<h2>llm</h2>
<h3>Title: Assessing LLMs for Moral Value Pluralism. (arXiv:2312.10075v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10075">http://arxiv.org/abs/2312.10075</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10075]] Assessing LLMs for Moral Value Pluralism(http://arxiv.org/abs/2312.10075)</code></li>
<li>Summary: <p>The fields of AI current lacks methods to quantitatively assess and
potentially alter the moral values inherent in the output of large language
models (LLMs). However, decades of social science research has developed and
refined widely-accepted moral value surveys, such as the World Values Survey
(WVS), eliciting value judgments from direct questions in various geographies.
We have turned those questions into value statements and use NLP to compute to
how well popular LLMs are aligned with moral values for various demographics
and cultures. While the WVS is accepted as an explicit assessment of values, we
lack methods for assessing implicit moral and cultural values in media, e.g.,
encountered in social media, political rhetoric, narratives, and generated by
AI systems such as LLMs that are increasingly present in our daily lives. As we
consume online content and utilize LLM outputs, we might ask, which moral
values are being implicitly promoted or undercut, or -- in the case of LLMs --
if they are intending to represent a cultural identity, are they doing so
consistently? In this paper we utilize a Recognizing Value Resonance (RVR) NLP
model to identify WVS values that resonate and conflict with a given passage of
output text. We apply RVR to the text generated by LLMs to characterize
implicit moral values, allowing us to quantify the moral/cultural distance
between LLMs and various demographics that have been surveyed using the WVS. In
line with other work we find that LLMs exhibit several Western-centric value
biases; they overestimate how conservative people in non-Western countries are,
they are less accurate in representing gender for non-Western countries, and
portray older populations as having more traditional values. Our results
highlight value misalignment and age groups, and a need for social science
informed technological solutions addressing value plurality in LLMs.
</p></li>
</ul>

<h2>long context</h2>
<h2>lora</h2>
<h2>hallucination</h2>
<h2>prompt</h2>
<h2>code</h2>
<h3>Title: Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10144">http://arxiv.org/abs/2312.10144</a></li>
<li>Code URL: <a href="https://github.com/layer6ai-labs/fusemix">https://github.com/layer6ai-labs/fusemix</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10144]] Data-Efficient Multimodal Fusion on a Single GPU(http://arxiv.org/abs/2312.10144)</code></li>
<li>Summary: <p>The goal of multimodal alignment is to learn a single latent space that is
shared between multimodal inputs. The most powerful models in this space have
been trained using massive datasets of paired inputs and large-scale
computational resources, making them prohibitively expensive to train in many
practical scenarios. We surmise that existing unimodal encoders pre-trained on
large amounts of unimodal data should provide an effective bootstrap to create
multimodal models from unimodal ones at much lower costs. We therefore propose
FuseMix, a multimodal augmentation scheme that operates on the latent spaces of
arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal
alignment, we achieve competitive performance -- and in certain cases
outperform state-of-the art methods -- in both image-text and audio-text
retrieval, with orders of magnitude less compute and data: for example, we
outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \!
600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs.
Additionally, we show how our method can be applied to convert pre-trained
text-to-image generative models into audio-to-image ones. Code is available at:
https://github.com/layer6ai-labs/fusemix.
</p></li>
</ul>

<h3>Title: Arithmetics-Based Decomposition of Numeral Words -- Arithmetic Conditions give the Unpacking Strategy. (arXiv:2312.10097v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10097">http://arxiv.org/abs/2312.10097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10097]] Arithmetics-Based Decomposition of Numeral Words -- Arithmetic Conditions give the Unpacking Strategy(http://arxiv.org/abs/2312.10097)</code></li>
<li>Summary: <p>In this paper we present a novel numeral decomposer that is designed to
revert Hurford's Packing Strategy. The Packing Strategy is a model on how
numeral words are formed out of smaller numeral words by recursion. The
decomposer does not simply check decimal digits but it also works for numerals
formed on base 20 or any other base or even combinations of different bases.
All assumptions that we use are justified with Hurford's Packing Strategy. The
decomposer reads through the numeral. When it finds a sub-numeral, it checks
arithmetic conditions to decide whether or not to unpack the sub-numeral. The
goal is to unpack those numerals that can sensibly be substituted by similar
numerals. E.g., in 'twenty-seven thousand and two hundred and six' it should
unpack 'twenty-seven' and 'two hundred and six', as those could each be
sensibly replaced by any numeral from 1 to 999. Our most used condition is: If
S is a substitutable sub-numeral of a numeral N, then 2*value(S) &lt; value(N). We
have tested the decomposer on numeral systems in 254 different natural
languages. We also developed a reinforcement learning algorithm based on the
decomposer. Both algorithms' code and the results are open source on GitHub.
</p></li>
</ul>

<h3>Title: Building symmetries into data-driven manifold dynamics models for complex flows. (arXiv:2312.10235v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10235">http://arxiv.org/abs/2312.10235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10235]] Building symmetries into data-driven manifold dynamics models for complex flows(http://arxiv.org/abs/2312.10235)</code></li>
<li>Summary: <p>Symmetries in a dynamical system provide an opportunity to dramatically
improve the performance of data-driven models. For fluid flows, such models are
needed for tasks related to design, understanding, prediction, and control. In
this work we exploit the symmetries of the Navier-Stokes equations (NSE) and
use simulation data to find the manifold where the long-time dynamics live,
which has many fewer degrees of freedom than the full state representation, and
the evolution equation for the dynamics on that manifold. We call this method
''symmetry charting''. The first step is to map to a ''fundamental chart'',
which is a region in the state space of the flow to which all other regions can
be mapped by a symmetry operation. To map to the fundamental chart we identify
a set of indicators from the Fourier transform that uniquely identify the
symmetries of the system. We then find a low-dimensional coordinate
representation of the data in the fundamental chart with the use of an
autoencoder. We use a variation called an implicit rank minimizing autoencoder
with weight decay, which in addition to compressing the dimension of the data,
also gives estimates of how many dimensions are needed to represent the data:
i.e. the dimension of the invariant manifold of the long-time dynamics.
Finally, we learn dynamics on this manifold with the use of neural ordinary
differential equations. We apply symmetry charting to two-dimensional
Kolmogorov flow in a chaotic bursting regime. This system has a continuous
translation symmetry, and discrete rotation and shift-reflect symmetries. With
this framework we observe that less data is needed to learn accurate
data-driven models, more robust estimates of the manifold dimension are
obtained, equivariance of the NSE is satisfied, better short-time tracking with
respect to the true data is observed, and long-time statistics are correctly
captured.
</p></li>
</ul>

<h2>chat</h2>
<h2>retrieval augmented generation</h2>
<h2>rag</h2>
<h3>Title: Aspect-Level Sentiment Analysis Based on Knowledge Graph and Recurrent Attention Network. (arXiv:2312.10048v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10048">http://arxiv.org/abs/2312.10048</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10048]] Aspect-Level Sentiment Analysis Based on Knowledge Graph and Recurrent Attention Network(http://arxiv.org/abs/2312.10048)</code></li>
<li>Summary: <p>In this paper, we propose a novel method to enhance sentiment analysis by
addressing the challenge of context-specific word meanings. It combines the
advantages of a bidirectional long short-term memory network (Bi-LSTM) with a
knowledge graph's synonym data. This synergy leverages a dynamic attention
mechanism to develop a knowledge-driven state vector. For classifying
sentiments linked to specific aspects, the approach constructs a memory bank
integrating positional data. This data is then analyzed using a multi-layer
gated recurrent unit (GRU) to pinpoint sentiment characteristics related to
specific aspect terms. Tests on three widely available datasets demonstrate
this method's superior performance in sentiment classification.
</p></li>
</ul>

<h3>Title: Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension. (arXiv:2312.10126v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10126">http://arxiv.org/abs/2312.10126</a></li>
<li>Code URL: <a href="https://github.com/sweta20/ats-eval">https://github.com/sweta20/ats-eval</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10126]] Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension(http://arxiv.org/abs/2312.10126)</code></li>
<li>Summary: <p>Automatic text simplification (TS) aims to automate the process of rewriting
text to make it easier for people to read. A pre-requisite for TS to be useful
is that it should convey information that is consistent with the meaning of the
original text. However, current TS evaluation protocols assess system outputs
for simplicity and meaning preservation without regard for the document context
in which output sentences occur and for how people understand them. In this
work, we introduce a human evaluation framework to assess whether simplified
texts preserve meaning using reading comprehension questions. With this
framework, we conduct a thorough human evaluation of texts by humans and by
nine automatic systems. Supervised systems that leverage pre-training knowledge
achieve the highest scores on the reading comprehension (RC) tasks amongst the
automatic controllable TS systems. However, even the best-performing supervised
system struggles with at least 14% of the questions, marking them as
"unanswerable'' based on simplified content. We further investigate how
existing TS evaluation metrics and automatic question-answering systems
approximate the human judgments we obtained.
</p></li>
</ul>

<h3>Title: VK-G2T: Vision and Context Knowledge enhanced Gloss2Text. (arXiv:2312.10210v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10210">http://arxiv.org/abs/2312.10210</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10210]] VK-G2T: Vision and Context Knowledge enhanced Gloss2Text(http://arxiv.org/abs/2312.10210)</code></li>
<li>Summary: <p>Existing sign language translation methods follow a two-stage pipeline: first
converting the sign language video to a gloss sequence (i.e. Sign2Gloss) and
then translating the generated gloss sequence into a spoken language sentence
(i.e. Gloss2Text). While previous studies have focused on boosting the
performance of the Sign2Gloss stage, we emphasize the optimization of the
Gloss2Text stage. However, this task is non-trivial due to two distinct
features of Gloss2Text: (1) isolated gloss input and (2) low-capacity gloss
vocabulary. To address these issues, we propose a vision and context knowledge
enhanced Gloss2Text model, named VK-G2T, which leverages the visual content of
the sign language video to learn the properties of the target sentence and
exploit the context knowledge to facilitate the adaptive translation of gloss
words. Extensive experiments conducted on a Chinese benchmark validate the
superiority of our model.
</p></li>
</ul>

<h3>Title: Bayesian Estimate of Mean Proper Scores for Diversity-Enhanced Active Learning. (arXiv:2312.10116v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10116">http://arxiv.org/abs/2312.10116</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10116]] Bayesian Estimate of Mean Proper Scores for Diversity-Enhanced Active Learning(http://arxiv.org/abs/2312.10116)</code></li>
<li>Summary: <p>The effectiveness of active learning largely depends on the sampling
efficiency of the acquisition function. Expected Loss Reduction (ELR) focuses
on a Bayesian estimate of the reduction in classification error, and more
general costs fit in the same framework. We propose Bayesian Estimate of Mean
Proper Scores (BEMPS) to estimate the increase in strictly proper scores such
as log probability or negative mean square error within this framework. We also
prove convergence results for this general class of costs. To facilitate better
experimentation with the new acquisition functions, we develop a complementary
batch AL algorithm that encourages diversity in the vector of expected changes
in scores for unlabeled data. To allow high-performance classifiers, we combine
deep ensembles, and dynamic validation set construction on pretrained models,
and further speed up the ensemble process with the idea of Monte Carlo Dropout.
Extensive experiments on both texts and images show that the use of mean square
error and log probability with BEMPS yields robust acquisition functions and
well-calibrated classifiers, and consistently outperforms the others tested.
The advantages of BEMPS over the others are further supported by a set of
qualitative analyses, where we visualise their sampling behaviour using data
maps and t-SNE plots.
</p></li>
</ul>

<h3>Title: WordScape: a Pipeline to extract multilingual, visually rich Documents with Layout Annotations from Web Crawl Data. (arXiv:2312.10188v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10188">http://arxiv.org/abs/2312.10188</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10188]] WordScape: a Pipeline to extract multilingual, visually rich Documents with Layout Annotations from Web Crawl Data(http://arxiv.org/abs/2312.10188)</code></li>
<li>Summary: <p>We introduce WordScape, a novel pipeline for the creation of
cross-disciplinary, multilingual corpora comprising millions of pages with
annotations for document layout detection. Relating visual and textual items on
document pages has gained further significance with the advent of multimodal
models. Various approaches proved effective for visual question answering or
layout segmentation. However, the interplay of text, tables, and visuals
remains challenging for a variety of document understanding tasks. In
particular, many models fail to generalize well to diverse domains and new
languages due to insufficient availability of training data. WordScape
addresses these limitations. Our automatic annotation pipeline parses the Open
XML structure of Word documents obtained from the web, jointly providing
layout-annotated document images and their textual representations. In turn,
WordScape offers unique properties as it (1) leverages the ubiquity of the Word
file format on the internet, (2) is readily accessible through the Common Crawl
web corpus, (3) is adaptive to domain-specific documents, and (4) offers
culturally and linguistically diverse document pages with natural semantic
structure and high-quality text. Together with the pipeline, we will
additionally release 9.5M urls to word documents which can be processed using
WordScape to create a dataset of over 40M pages. Finally, we investigate the
quality of text and layout annotations extracted by WordScape, assess the
impact on document understanding benchmarks, and demonstrate that manual
labeling costs can be substantially reduced.
</p></li>
</ul>

<h3>Title: Pareto Envelope Augmented with Reinforcement Learning: Multi-objective reinforcement learning-based approach for Large-Scale Constrained Pressurized Water Reactor optimization. (arXiv:2312.10194v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10194">http://arxiv.org/abs/2312.10194</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10194]] Pareto Envelope Augmented with Reinforcement Learning: Multi-objective reinforcement learning-based approach for Large-Scale Constrained Pressurized Water Reactor optimization(http://arxiv.org/abs/2312.10194)</code></li>
<li>Summary: <p>A novel method, the Pareto Envelope Augmented with Reinforcement Learning
(PEARL), has been developed to address the challenges posed by multi-objective
problems, particularly in the field of engineering where the evaluation of
candidate solutions can be time-consuming. PEARL distinguishes itself from
traditional policy-based multi-objective Reinforcement Learning methods by
learning a single policy, eliminating the need for multiple neural networks to
independently solve simpler sub-problems. Several versions inspired from deep
learning and evolutionary techniques have been crafted, catering to both
unconstrained and constrained problem domains. Curriculum Learning is harnessed
to effectively manage constraints in these versions. PEARL's performance is
first evaluated on classical multi-objective benchmarks. Additionally, it is
tested on two practical PWR core Loading Pattern optimization problems to
showcase its real-world applicability. The first problem involves optimizing
the Cycle length and the rod-integrated peaking factor as the primary
objectives, while the second problem incorporates the mean average enrichment
as an additional objective. Furthermore, PEARL addresses three types of
constraints related to boron concentration, peak pin burnup, and peak pin
power. The results are systematically compared against a conventional approach,
the Non-dominated Sorting Genetic Algorithm. Notably, PEARL, specifically the
PEARL-NdS variant, efficiently uncovers a Pareto front without necessitating
additional efforts from the algorithm designer, as opposed to a single
optimization with scaled objectives. It also outperforms the classical approach
across multiple performance metrics, including the Hyper-volume.
</p></li>
</ul>

<h2>multi-run</h2>
<h2>chain-of-thought</h2>
<h2>tree-of-thought</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
