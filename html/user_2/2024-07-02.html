<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-02</h1>
<h3>Title: The Qiyas Benchmark: Measuring ChatGPT Mathematical and Language Understanding in Arabic</h3>
<ul>
<li><strong>Authors: </strong>Shahad Al-Khalifa, Hend Al-Khalifa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00146">https://arxiv.org/abs/2407.00146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00146">https://arxiv.org/pdf/2407.00146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00146]] The Qiyas Benchmark: Measuring ChatGPT Mathematical and Language Understanding in Arabic(https://arxiv.org/abs/2407.00146)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>Despite the growing importance of Arabic as a global language, there is a notable lack of language models pre-trained exclusively on Arabic data. This shortage has led to limited benchmarks available for assessing language model performance in Arabic. To address this gap, we introduce two novel benchmarks designed to evaluate models' mathematical reasoning and language understanding abilities in Arabic. These benchmarks are derived from a General Aptitude Test (GAT) called Qiyas exam, a standardized test widely used for university admissions in Saudi Arabia. For validation purposes, we assess the performance of ChatGPT-3.5-trubo and ChatGPT-4 on our benchmarks. Our findings reveal that these benchmarks pose a significant challenge, with ChatGPT-4 achieving an overall average accuracy of 64%, while ChatGPT-3.5-trubo achieved an overall accuracy of 49% across the various question types in the Qiyas benchmark. We believe the release of these benchmarks will pave the way for enhancing the mathematical reasoning and language understanding capabilities of future models tailored for the low-resource Arabic language.</li>
<li><strong>摘要：</strong>尽管阿拉伯语作为全球语言的重要性日益增加，但专门针对阿拉伯语数据进行预训练的语言模型却明显不足。这种短缺导致可用于评估阿拉伯语语言模型性能的基准有限。为了弥补这一差距，我们引入了两个新的基准，旨在评估模型在阿拉伯语中的数学推理和语言理解能力。这些基准源自名为 Qiyas 考试的通用能力倾向测试 (GAT)，这是沙特阿拉伯大学入学广泛使用的标准化考试。为了进行验证，我们在基准上评估了 ChatGPT-3.5-trubo 和 ChatGPT-4 的性能。我们的研究结果表明，这些基准带来了重大挑战，ChatGPT-4 的总体平均准确率为 64%，而 ChatGPT-3.5-trubo 在 Qiyas 基准的各种问题类型中实现了 49% 的总体准确率。我们相信，这些基准的发布将为增强未来针对资源匮乏的阿拉伯语量身定制的模型的数学推理和语言理解能力铺平道路。</li>
</ul>

<h3>Title: Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach</h3>
<ul>
<li><strong>Authors: </strong>Sai Krishna Revanth Vuruma, Dezhi Wu, Saborny Sen Gupta, Lucas Aust, Valerie Lookingbill, Wyatt Bellamy, Yang Ren, Erin Kasson, Li-Shiun Chen, Patricia Cavazos-Rehg, Dian Hu, Ming Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, cs.HC, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00167">https://arxiv.org/abs/2407.00167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00167">https://arxiv.org/pdf/2407.00167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00167]] Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach(https://arxiv.org/abs/2407.00167)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In recent years, the United States has witnessed a significant surge in the popularity of vaping or e-cigarette use, leading to a notable rise in cases of e-cigarette and vaping use-associated lung injury (EVALI) that caused hospitalizations and fatalities during the EVALI outbreak in 2019, highlighting the urgency to comprehend vaping behaviors and develop effective strategies for cessation. Due to the ubiquity of social media platforms, over 4.7 billion users worldwide use them for connectivity, communications, news, and entertainment with a significant portion of the discourse related to health, thereby establishing social media data as an invaluable organic data resource for public health research. In this study, we extracted a sample dataset from one vaping sub-community on Reddit to analyze users' quit-vaping intentions. Leveraging OpenAI's latest large language model GPT-4 for sentence-level quit vaping intention detection, this study compares the outcomes of this model against layman and clinical expert annotations. Using different prompting strategies such as zero-shot, one-shot, few-shot and chain-of-thought prompting, we developed 8 prompts with varying levels of detail to explain the task to GPT-4 and also evaluated the performance of the strategies against each other. These preliminary findings emphasize the potential of GPT-4 in social media data analysis, especially in identifying users' subtle intentions that may elude human detection.</li>
<li><strong>摘要：</strong>近年来，美国电子烟使用量大幅增长，导致电子烟和电子烟相关肺损伤 (EVALI) 病例显著增加，在 2019 年 EVALI 爆发期间导致住院和死亡，凸显了了解电子烟行为和制定有效戒烟策略的紧迫性。由于社交媒体平台无处不在，全球超过 47 亿用户使用它们进行连接、通信、新闻和娱乐，其中很大一部分讨论与健康有关，从而使社交媒体数据成为公共卫生研究的宝贵有机数据资源。在本研究中，我们从 Reddit 上的一个电子烟子社区中提取了一个样本数据集，以分析用户的戒烟意图。本研究利用 OpenAI 最新的大型语言模型 GPT-4 进行句子级戒烟意图检测，将该模型的结果与外行和临床专家的注释进行了比较。我们使用不同的提示策略（例如零次提示、一次提示、几次提示和思路链提示），开发了 8 个细节程度各异的提示，向 GPT-4 解释任务，并评估了这些策略的性能。这些初步发现强调了 GPT-4 在社交媒体数据分析方面的潜力，尤其是在识别可能逃避人类检测的用户细微意图方面。</li>
</ul>

<h3>Title: MetaKP: On-Demand Keyphrase Generation</h3>
<ul>
<li><strong>Authors: </strong>Di Wu, Xiaoxian Shen, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00191">https://arxiv.org/abs/2407.00191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00191">https://arxiv.org/pdf/2407.00191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00191]] MetaKP: On-Demand Keyphrase Generation(https://arxiv.org/abs/2407.00191)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Traditional keyphrase prediction methods predict a single set of keyphrases per document, failing to cater to the diverse needs of users and downstream applications. To bridge the gap, we introduce on-demand keyphrase generation, a novel paradigm that requires keyphrases that conform to specific high-level goals or intents. For this task, we present MetaKP, a large-scale benchmark comprising four datasets, 7500 documents, and 3760 goals across news and biomedical domains with human-annotated keyphrases. Leveraging MetaKP, we design both supervised and unsupervised methods, including a multi-task fine-tuning approach and a self-consistency prompting method with large language models. The results highlight the challenges of supervised fine-tuning, whose performance is not robust to distribution shifts. By contrast, the proposed self-consistency prompting approach greatly improves the performance of large language models, enabling GPT-4o to achieve 0.548 SemF1, surpassing the performance of a fully fine-tuned BART-base model. Finally, we demonstrate the potential of our method to serve as a general NLP infrastructure, exemplified by its application in epidemic event detection from social media.</li>
<li><strong>摘要：</strong>传统的关键短语预测方法针对每个文档预测一组关键短语，无法满足用户和下游应用程序的多样化需求。为了弥补这一差距，我们引入了按需关键短语生成，这是一种新范式，需要符合特定高级目标或意图的关键短语。为了完成这项任务，我们提出了 MetaKP，这是一个大规模基准，包含四个数据集、7500 份文档和 3760 个目标，涵盖新闻和生物医学领域，并带有人工注释的关键短语。利用 MetaKP，我们设计了监督和无监督方法，包括多任务微调方法和具有大型语言模型的自洽提示方法。结果突出了监督微调的挑战，其性能对分布变化不具有鲁棒性。相比之下，提出的自洽性提示方法极大地提高了大型语言模型的性能，使 GPT-4o 达到了 0.548 SemF1，超过了完全微调的 BART 基础模型的性能。最后，我们展示了我们的方法作为通用 NLP 基础设施的潜力，并以它在社交媒体流行病事件检测中的应用为例。</li>
</ul>

<h3>Title: Detection and Measurement of Syntactic Templates in Generated Text</h3>
<ul>
<li><strong>Authors: </strong>Chantal Shaib, Yanai Elazar, Junyi Jessy Li, Byron C. Wallace</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00211">https://arxiv.org/abs/2407.00211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00211">https://arxiv.org/pdf/2407.00211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00211]] Detection and Measurement of Syntactic Templates in Generated Text(https://arxiv.org/abs/2407.00211)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Recent work on evaluating the diversity of text generated by LLMs has focused on word-level features. Here we offer an analysis of syntactic features to characterize general repetition in models, beyond frequent n-grams. Specifically, we define syntactic templates and show that models tend to produce templated text in downstream tasks at a higher rate than what is found in human-reference texts. We find that most (76%) templates in model-generated text can be found in pre-training data (compared to only 35% of human-authored text), and are not overwritten during fine-tuning processes such as RLHF. This connection to the pre-training data allows us to analyze syntactic templates in models where we do not have the pre-training data. We also find that templates as features are able to differentiate between models, tasks, and domains, and are useful for qualitatively evaluating common model constructions. Finally, we demonstrate the use of templates as a useful tool for analyzing style memorization of training data in LLMs.</li>
<li><strong>摘要：</strong>最近，评估 LLM 生成的文本多样性的工作主要集中在词级特征上。在这里，我们提供了句法特征的分析，以表征模型中的一般重复，而不仅仅是频繁的 n-gram。具体来说，我们定义了句法模板，并表明模型在下游任务中生成模板文本的速率往往高于人类参考文本。我们发现，模型生成文本中的大多数 (76%) 模板都可以在预训练数据中找到（而人类编写的文本只有 35%），并且在 RLHF 等微调过程中不会被覆盖。这种与预训练数据的联系使我们能够在没有预训练数据的模型中分析句法模板。我们还发现，模板作为特征能够区分模型、任务和领域，并且可用于定性评估常见的模型构造。最后，我们展示了模板作为分析 LLM 中训练数据风格记忆的有用工具的用途。</li>
</ul>

<h3>Title: Evaluating Human Alignment and Model Faithfulness of LLM Rationale</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Fayyaz, Fan Yin, Jiao Sun, Nanyun Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00219">https://arxiv.org/abs/2407.00219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00219">https://arxiv.org/pdf/2407.00219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00219]] Evaluating Human Alignment and Model Faithfulness of LLM Rationale(https://arxiv.org/abs/2407.00219)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We study how well large language models (LLMs) explain their generations with rationales -- a set of tokens extracted from the input texts that reflect the decision process of LLMs. We examine LLM rationales extracted with two methods: 1) attribution-based methods that use attention or gradients to locate important tokens, and 2) prompting-based methods that guide LLMs to extract rationales using prompts. Through extensive experiments, we show that prompting-based rationales align better with human-annotated rationales than attribution-based rationales, and demonstrate reasonable alignment with humans even when model performance is poor. We additionally find that the faithfulness limitations of prompting-based methods, which are identified in previous work, may be linked to their collapsed predictions. By fine-tuning these models on the corresponding datasets, both prompting and attribution methods demonstrate improved faithfulness. Our study sheds light on more rigorous and fair evaluations of LLM rationales, especially for prompting-based ones.</li>
<li><strong>摘要：</strong>我们研究大型语言模型 (LLM) 如何很好地用理论依据（从输入文本中提取的一组标记，反映了 LLM 的决策过程）来解释其生成。我们研究了用两种方法提取的 LLM 理论依据：1) 使用注意力或梯度来定位重要标记的归因方法，以及 2) 引导 LLM 使用提示提取理论依据的基于提示的方法。通过大量实验，我们表明基于提示的理论依据比基于归因的理论依据与人类注释的理论依据更一致，并且即使在模型性能较差时也表现出与人类的合理一致性。我们还发现，在以前的研究中发现的基于提示的方法的忠诚度限制可能与它们的预测崩溃有关。通过在相应的数据集上对这些模型进行微调，提示和归因方法都表现出更高的忠诚度。我们的研究揭示了对 LLM 理论依据（尤其是基于提示的理论依据）进行更严格和公平的评估。</li>
</ul>

<h3>Title: EHRmonize: A Framework for Medical Concept Abstraction from Electronic Health Records using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>João Matos, Jack Gallifant, Jian Pei, A. Ian Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00242">https://arxiv.org/abs/2407.00242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00242">https://arxiv.org/pdf/2407.00242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00242]] EHRmonize: A Framework for Medical Concept Abstraction from Electronic Health Records using Large Language Models(https://arxiv.org/abs/2407.00242)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Electronic health records (EHRs) contain vast amounts of complex data, but harmonizing and processing this information remains a challenging and costly task requiring significant clinical expertise. While large language models (LLMs) have shown promise in various healthcare applications, their potential for abstracting medical concepts from EHRs remains largely unexplored. We introduce EHRmonize, a framework leveraging LLMs to abstract medical concepts from EHR data. Our study uses medication data from two real-world EHR databases to evaluate five LLMs on two free-text extraction and six binary classification tasks across various prompting strategies. GPT-4o's with 10-shot prompting achieved the highest performance in all tasks, accompanied by Claude-3.5-Sonnet in a subset of tasks. GPT-4o achieved an accuracy of 97% in identifying generic route names, 82% for generic drug names, and 100% in performing binary classification of antibiotics. While EHRmonize significantly enhances efficiency, reducing annotation time by an estimated 60%, we emphasize that clinician oversight remains essential. Our framework, available as a Python package, offers a promising tool to assist clinicians in EHR data abstraction, potentially accelerating healthcare research and improving data harmonization processes.</li>
<li><strong>摘要：</strong>电子健康记录 (EHR) 包含大量复杂数据，但协调和处理这些信息仍然是一项具有挑战性且成本高昂的任务，需要大量的临床专业知识。虽然大型语言模型 (LLM) 在各种医疗保健应用中都显示出良好的前景，但它们从 EHR 中抽象医学概念的潜力在很大程度上仍未得到开发。我们引入了 EHRmonize，这是一个利用 LLM 从 EHR 数据中抽象医学概念的框架。我们的研究使用来自两个真实世界 EHR 数据库的药物数据来评估五个 LLM 在两个自由文本提取和六个二元分类任务中在不同提示策略下的表现。具有 10 次提示的 GPT-4o 在所有任务中都取得了最高的性能，并在部分任务中伴随着 Claude-3.5-Sonnet。GPT-4o 在识别通用路线名称方面的准确率为 97%，在识别通用药物名称方面的准确率为 82%，在执行抗生素二元分类方面的准确率为 100%。虽然 EHRmonize 显着提高了效率，将注释时间缩短了约 60%，但我们强调临床医生的监督仍然至关重要。我们的框架以 Python 包的形式提供，它提供了一种很有前途的工具来协助临床医生进行 EHR 数据抽象，从而有可能加速医疗保健研究并改善数据协调流程。</li>
</ul>

<h3>Title: DiffuseDef: Improved Robustness to Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Zhenhao Li, Marek Rei, Lucia Specia</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00248">https://arxiv.org/abs/2407.00248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00248">https://arxiv.org/pdf/2407.00248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00248]] DiffuseDef: Improved Robustness to Adversarial Attacks(https://arxiv.org/abs/2407.00248)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pretrained language models have significantly advanced performance across various natural language processing tasks. However, adversarial attacks continue to pose a critical challenge to system built using these models, as they can be exploited with carefully crafted adversarial texts. Inspired by the ability of diffusion models to predict and reduce noise in computer vision, we propose a novel and flexible adversarial defense method for language classification tasks, DiffuseDef, which incorporates a diffusion layer as a denoiser between the encoder and the classifier. During inference, the adversarial hidden state is first combined with sampled noise, then denoised iteratively and finally ensembled to produce a robust text representation. By integrating adversarial training, denoising, and ensembling techniques, we show that DiffuseDef improves over different existing adversarial defense methods and achieves state-of-the-art performance against common adversarial attacks.</li>
<li><strong>摘要：</strong>预训练语言模型在各种自然语言处理任务中都具有显著的先进性能。然而，对抗性攻击仍然对使用这些模型构建的系统构成严峻挑战，因为它们可以通过精心制作的对抗性文本进行利用。受扩散模型在计算机视觉中预测和降低噪声的能力的启发，我们提出了一种用于语言分类任务的新颖而灵活的对抗性防御方法 DiffuseDef，它在编码器和分类器之间加入了一个扩散层作为降噪器。在推理过程中，对抗性隐藏状态首先与采样噪声相结合，然后迭代降噪，最后进行集成以产生鲁棒的文本表示。通过整合对抗性训练、降噪和集成技术，我们表明 DiffuseDef 比现有的不同对抗性防御方法有所改进，并在对抗常见对抗性攻击方面实现了最先进的性能。</li>
</ul>

<h3>Title: From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mehar Bhatia, Sahithya Ravi, Aditya Chinchure, Eunjeong Hwang, Vered Shwartz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00263">https://arxiv.org/abs/2407.00263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00263">https://arxiv.org/pdf/2407.00263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00263]] From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models(https://arxiv.org/abs/2407.00263)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models' cultural inclusivity, but they have limited coverage of cultures and do not adequately assess cultural diversity across universal as well as culture-specific local concepts. To address these limitations, we introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures -- underscoring the necessity for enhancing multicultural understanding in vision-language models.</li>
<li><strong>摘要：</strong>尽管视觉语言模型最近取得了进展，但由于训练数据集中代表性不足，它们在非西方文化的图像上的表现仍然不是最佳的。人们提出了各种基准来测试模型的文化包容性，但它们对文化的覆盖范围有限，不能充分评估普遍和特定文化的局部概念的文化多样性。为了解决这些限制，我们引入了 GlobalRG 基准，它包含两个具有挑战性的任务：跨普遍性检索和文化视觉基础。前一项任务需要从 50 个国家/地区检索文化多样化的通用概念图像，而后一项任务旨在将特定文化的概念扎根于 15 个国家/地区的图像中。我们对各种模型的评估表明，不同文化之间的性能差异很大——强调了在视觉语言模型中增强多元文化理解的必要性。</li>
</ul>

<h3>Title: LiteSearch: Efficacious Tree Search for LLM</h3>
<ul>
<li><strong>Authors: </strong>Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Dian Yu, Haitao Mi, Jinsong Su, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00320">https://arxiv.org/abs/2407.00320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00320">https://arxiv.org/pdf/2407.00320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00320]] LiteSearch: Efficacious Tree Search for LLM(https://arxiv.org/abs/2407.00320)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Recent research suggests that tree search algorithms (e.g. Monte Carlo Tree Search) can dramatically boost LLM performance on complex mathematical reasoning tasks. However, they often require more than 10 times the computational resources of greedy decoding due to wasteful search strategies, making them difficult to be deployed in practical applications. This study introduces a novel guided tree search algorithm with dynamic node selection and node-level exploration budget (maximum number of children) calculation to tackle this issue. By considering the search progress towards the final answer (history) and the guidance from a value network (future) trained without any step-wise annotations, our algorithm iteratively selects the most promising tree node before expanding it within the boundaries of the allocated computational budget. Experiments conducted on the GSM8K and TabMWP datasets demonstrate that our approach not only offers competitive performance but also enjoys significantly lower computational costs compared to baseline methods.</li>
<li><strong>摘要：</strong>最近的研究表明，树搜索算法（例如蒙特卡洛树搜索）可以显著提高 LLM 在复杂数学推理任务上的性能。然而，由于浪费的搜索策略，它们通常需要比贪婪解码多 10 倍的计算资源，这使得它们很难在实际应用中部署。这项研究引入了一种新颖的引导树搜索算法，该算法具有动态节点选择和节点级探索预算（最大子节点数）计算来解决此问题。通过考虑朝着最终答案（历史）的搜索进度和在没有任何逐步注释的情况下训练的价值网络（未来）的指导，我们的算法会迭代地选择最有希望的树节点，然后在分配的计算预算范围内扩展它。在 GSM8K 和 TabMWP 数据集上进行的实验表明，与基线方法相比，我们的方法不仅提供了具有竞争力的性能，而且计算成本也显著降低。</li>
</ul>

<h3>Title: LLM-Generated Natural Language Meets Scaling Laws: New Explorations and Data Augmentation Methods</h3>
<ul>
<li><strong>Authors: </strong>Zhenhua Wang, Guang Xu, Ming Ren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00322">https://arxiv.org/abs/2407.00322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00322">https://arxiv.org/pdf/2407.00322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00322]] LLM-Generated Natural Language Meets Scaling Laws: New Explorations and Data Augmentation Methods(https://arxiv.org/abs/2407.00322)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>With the ascent of large language models (LLM), natural language processing has witnessed enhancements, such as LLM-based data augmentation. Nonetheless, prior research harbors two primary concerns: firstly, a lack of contemplation regarding whether the natural language generated by LLM (LLMNL) truly aligns with human natural language (HNL), a critical foundational question; secondly, an oversight that augmented data is randomly generated by LLM, implying that not all data may possess equal training value, that could impede the performance of classifiers. To address these challenges, we introduce the scaling laws to intrinsically calculate LLMNL and HNL. Through extensive experiments, we reveal slight deviations (approximately 0.2 Mandelbrot exponent) from Mandelbrot's law in LLMNL, underscore a complexity advantage in HNL, and supplement an interpretive discussion on language style. This establishes a solid foundation for LLM's expansion. Further, we introduce a novel data augmentation method for few-shot text classification, termed ZGPTDA, which leverages fuzzy computing mechanisms driven by the conformity to scaling laws to make decisions about GPT-4 augmented data. Extensive experiments, conducted in real-world scenarios, confirms the effectiveness (improving F1 of Bert and RoBerta by 7-10%) and competitiveness (surpassing recent AugGPT and GENCO methods by about 2% accuracy on DeBerta) of ZGPTDA. In addition, we reveal some interesting insights, e.g., Hilberg's law and Taylor's law can impart more benefits to text classification, etc.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的兴起，自然语言处理也得到了增强，例如基于 LLM 的数据增强。然而，先前的研究存在两个主要问题：首先，缺乏对 LLM 生成的自然语言（LLMNL）是否真正与人类自然语言（HNL）一致的思考，这是一个关键的基础问题；其次，人们忽略了增强数据是由 LLM 随机生成的，这意味着并非所有数据都具有相同的训练价值，这可能会妨碍分类器的性能。为了解决这些挑战，我们引入了缩放定律来内在计算 LLMNL 和 HNL。通过大量实验，我们发现 LLMNL 与曼德布洛特定律略有偏差（约 0.2 曼德布洛特指数），强调了 HNL 的复杂性优势，并补充了对语言风格的解释性讨论。这为 LLM 的扩展奠定了坚实的基础。此外，我们引入了一种用于小样本文本分类的新型数据增强方法，称为 ZGPTDA，该方法利用符合缩放定律的模糊计算机制来对 GPT-4 增强数据进行决策。在现实场景中进行的大量实验证实了 ZGPTDA 的有效性（将 Bert 和 RoBerta 的 F1 提高了 7-10%）和竞争力（在 DeBerta 上的准确率比最近的 AugGPT 和 GENCO 方法高出约 2%）。此外，我们还揭示了一些有趣的见解，例如希尔伯格定律和泰勒定律可以为文本分类带来更多好处等。</li>
</ul>

<h3>Title: Iterative Data Augmentation with Large Language Models for Aspect-based Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Haiyun Li, Qihuang Zhong, Ke Zhu, Juhua Liu, Bo Du, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00341">https://arxiv.org/abs/2407.00341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00341">https://arxiv.org/pdf/2407.00341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00341]] Iterative Data Augmentation with Large Language Models for Aspect-based Sentiment Analysis(https://arxiv.org/abs/2407.00341)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Aspect-based Sentiment Analysis (ABSA) is an important sentiment analysis task, which aims to determine the sentiment polarity towards an aspect in a sentence. Due to the expensive and limited labeled data, data augmentation (DA) has become the standard for improving the performance of ABSA. However, current DA methods usually have some shortcomings: 1) poor fluency and coherence, 2) lack of diversity of generated data, and 3) reliance on some existing labeled data, hindering its applications in real-world scenarios. In response to these problems, we propose a systematic Iterative Data augmentation framework, namely IterD, to boost the performance of ABSA. The core of IterD is to leverage the powerful ability of large language models (LLMs) to iteratively generate more fluent and diverse synthetic labeled data, starting from an unsupervised sentence corpus. Extensive experiments on 4 widely-used ABSA benchmarks show that IterD brings consistent and significant performance gains among 5 baseline ABSA models. More encouragingly, the synthetic data generated by IterD can achieve comparable or even better performance against the manually annotated data.</li>
<li><strong>摘要：</strong>基于方面的情绪分析 (ABSA) 是一项重要的情绪分析任务，旨在确定句子中针对某一方面的情绪极性。由于标记数据昂贵且有限，数据增强 (DA) 已成为提高 ABSA 性能的标准。然而，当前的 DA 方法通常存在一些缺点：1) 流畅性和连贯性较差，2) 生成数据缺乏多样性，3) 依赖于一些现有的标记数据，阻碍了其在实际场景中的应用。针对这些问题，我们提出了一个系统的迭代数据增强框架，即 IterD，以提高 ABSA 的性能。IterD 的核心是利用大型语言模型 (LLM) 的强大能力，从无监督句子语料库开始迭代生成更流畅、更多样化的合成标记数据。在 4 个广泛使用的 ABSA 基准上进行的大量实验表明，IterD 在 5 个基线 ABSA 模型中带来了一致且显着的性能提升。更令人鼓舞的是，IterD 生成的合成数据可以实现与手动注释数据相当甚至更好的性能。</li>
</ul>

<h3>Title: From RAG to RICHES: Retrieval Interlaced with Sequence Generation</h3>
<ul>
<li><strong>Authors: </strong>Palak Jain, Livio Baldini Soares, Tom Kwiatkowski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00361">https://arxiv.org/abs/2407.00361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00361">https://arxiv.org/pdf/2407.00361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00361]] From RAG to RICHES: Retrieval Interlaced with Sequence Generation(https://arxiv.org/abs/2407.00361)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>We present RICHES, a novel approach that interleaves retrieval with sequence generation tasks. RICHES offers an alternative to conventional RAG systems by eliminating the need for separate retriever and generator. It retrieves documents by directly decoding their contents, constrained on the corpus. Unifying retrieval with generation allows us to adapt to diverse new tasks via prompting alone. RICHES can work with any Instruction-tuned model, without additional training. It provides attributed evidence, supports multi-hop retrievals and interleaves thoughts to plan on what to retrieve next, all within a single decoding pass of the LLM. We demonstrate the strong performance of RICHES across ODQA tasks including attributed and multi-hop QA.</li>
<li><strong>摘要：</strong>我们提出了 RICHES，这是一种将检索与序列生成任务交错在一起的新方法。RICHES 无需单独的检索器和生成器，因此为传统的 RAG 系统提供了一种替代方案。它通过直接解码文档内容来检索文档，并限制在语料库中。将检索与生成统一起来，使我们能够仅通过提示就适应各种新任务。RICHES 可以与任何指令调整模型一起使用，而无需额外的训练。它提供归因证据，支持多跳检索，并交错思考以计划下一步要检索的内容，所有这些都在 LLM 的一次解码过程中完成。我们展示了 RICHES 在 ODQA 任务（包括归因和多跳 QA）中的强大性能。</li>
</ul>

<h3>Title: Financial Knowledge Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Cehao Yang, Chengjin Xu, Yiyan Qi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00365">https://arxiv.org/abs/2407.00365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00365">https://arxiv.org/pdf/2407.00365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00365]] Financial Knowledge Large Language Model(https://arxiv.org/abs/2407.00365)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Artificial intelligence is making significant strides in the finance industry, revolutionizing how data is processed and interpreted. Among these technologies, large language models (LLMs) have demonstrated substantial potential to transform financial services by automating complex tasks, enhancing customer service, and providing detailed financial analysis. Firstly, we introduce IDEA-FinBench, an evaluation benchmark specifically tailored for assessing financial knowledge in large language models (LLMs). This benchmark utilizes questions from two globally respected and authoritative financial professional exams, aimimg to comprehensively evaluate the capability of LLMs to directly address exam questions pertinent to the finance sector. Secondly, we propose IDEA-FinKER, a Financial Knowledge Enhancement framework designed to facilitate the rapid adaptation of general LLMs to the financial domain, introducing a retrieval-based few-shot learning method for real-time context-level knowledge injection, and a set of high-quality financial knowledge instructions for fine-tuning any general LLM. Finally, we present IDEA-FinQA, a financial question-answering system powered by LLMs. This system is structured around a scheme of real-time knowledge injection and factual enhancement using external knowledge. IDEA-FinQA is comprised of three main modules: the data collector, the data querying module, and LLM-based agents tasked with specific functions.</li>
<li><strong>摘要：</strong>人工智能正在金融行业取得重大进展，彻底改变了数据的处理和解释方式。在这些技术中，大型语言模型 (LLM) 已显示出巨大的潜力，可以自动执行复杂任务、增强客户服务并提供详细的财务分析，从而改变金融服务。首先，我们介绍了 IDEA-FinBench，这是一个专门为评估大型语言模型 (LLM) 中的金融知识而量身定制的评估基准。该基准采用了两个全球知名的权威金融专业考试的问题，旨在全面评估 LLM 直接解决与金融行业相关的考试问题的能力。其次，我们提出了 IDEA-FinKER，这是一个金融知识增强框架，旨在促进通用 LLM 快速适应金融领域，引入一种基于检索的少量学习方法用于实时上下文级知识注入，以及一组用于微调任何通用 LLM 的高质量金融知识指令。最后，我们介绍了 IDEA-FinQA，一个由 LLM 驱动的金融问答系统。该系统采用实时知识注入和利用外部知识增强事实的方案构建。IDEA-FinQA 由三个主要模块组成：数据收集器、数据查询模块和负责特定功能的基于 LLM 的代理。</li>
</ul>

<h3>Title: How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models</h3>
<ul>
<li><strong>Authors: </strong>Jaeyoung Lee, Ximing Lu, Jack Hessel, Faeze Brahman, Youngjae Yu, Yonatan Bisk, Yejin Choi, Saadia Gabriel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00369">https://arxiv.org/abs/2407.00369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00369">https://arxiv.org/pdf/2407.00369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00369]] How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models(https://arxiv.org/abs/2407.00369)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Given the growing influx of misinformation across news and social media, there is a critical need for systems that can provide effective real-time verification of news claims. Large language or multimodal model based verification has been proposed to scale up online policing mechanisms for mitigating spread of false and harmful content. While these can potentially reduce burden on human fact-checkers, such efforts may be hampered by foundation model training data becoming outdated. In this work, we test the limits of improving foundation model performance without continual updating through an initial study of knowledge transfer using either existing intra- and inter- domain benchmarks or explanations generated from large language models (LLMs). We evaluate on 12 public benchmarks for fact-checking and misinformation detection as well as two other tasks relevant to content moderation -- toxicity and stance detection. Our results on two recent multi-modal fact-checking benchmarks, Mocheg and Fakeddit, indicate that knowledge transfer strategies can improve Fakeddit performance over the state-of-the-art by up to 1.7% and Mocheg performance by up to 2.9%.</li>
<li><strong>摘要：</strong>鉴于新闻和社交媒体中虚假信息的不断涌入，迫切需要能够有效实时验证新闻声明的系统。有人提出了基于大型语言或多模态模型的验证，以扩大在线监管机制，从而减轻虚假和有害内容的传播。虽然这些方法可以减轻人工事实核查员的负担，但基础模型训练数据过时可能会阻碍这些努力。在这项工作中，我们通过初步研究知识转移来测试在不持续更新的情况下提高基础模型性能的极限，该研究使用现有的域内和域间基准或从大型语言模型 (LLM) 生成的解释。我们评估了 12 个公共基准，用于事实核查和错误信息检测以及与内容审核相关的另外两个任务——毒性和立场检测。我们对两个最近的多模式事实核查基准 Mocheg 和 Fakeddit 的结果表明，知识转移策略可以将 Fakeddit 的性能提高 1.7%，将 Mocheg 的性能提高 2.9%。</li>
</ul>

<h3>Title: The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention</h3>
<ul>
<li><strong>Authors: </strong>Yixin Wan, Di Wu, Haoran Wang, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00377">https://arxiv.org/abs/2407.00377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00377">https://arxiv.org/pdf/2407.00377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00377]] The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention(https://arxiv.org/abs/2407.00377)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Prompt-based "diversity interventions" are commonly adopted to improve the diversity of Text-to-Image (T2I) models depicting individuals with various racial or gender traits. However, will this strategy result in nonfactual demographic distribution, especially when generating real historical figures? In this work, we propose DemOgraphic FActualIty Representation (DoFaiR), a benchmark to systematically quantify the trade-off between using diversity interventions and preserving demographic factuality in T2I models. DoFaiR consists of 756 meticulously fact-checked test instances to reveal the factuality tax of various diversity prompts through an automated evidence-supported evaluation pipeline. Experiments on DoFaiR unveil that diversity-oriented instructions increase the number of different gender and racial groups in DALLE-3's generations at the cost of historically inaccurate demographic distributions. To resolve this issue, we propose Fact-Augmented Intervention (FAI), which instructs a Large Language Model (LLM) to reflect on verbalized or retrieved factual information about gender and racial compositions of generation subjects in history, and incorporate it into the generation context of T2I models. By orienting model generations using the reflected historical truths, FAI significantly improves the demographic factuality under diversity interventions while preserving diversity.</li>
<li><strong>摘要：</strong>基于提示的“多样性干预”通常用于提高文本转图像 (T2I) 模型的多样性，这些模型描绘了具有各种种族或性别特征的个人。但是，这种策略是否会导致人口分布不真实，尤其是在生成真实的历史人物时？在这项工作中，我们提出了人口统计事实表示 (DoFaiR)，这是一个基准，用于系统地量化使用多样性干预措施和在 T2I 模型中保留人口统计事实之间的权衡。DoFaiR 由 756 个经过严格事实检查的测试实例组成，通过自动化证据支持的评估管道揭示各种多样性提示的事实性负担。对 DoFaiR 的实验表明，以多样性为导向的指令增加了 DALLE-3 代中不同性别和种族群体的数量，但代价是历史上不准确的人口分布。为了解决这一问题，我们提出了事实增强干预 (FAI)，它指示大型语言模型 (LLM) 反思历史上代际主体的性别和种族构成的口头或检索事实信息，并将其纳入 T2I 模型的代际背景中。通过使用反映的历史事实来指导模型代际，FAI 在保持多样性的同时，显著提高了多样性干预下的人口统计真实性。</li>
</ul>

<h3>Title: Advancing Process Verification for Large Language Models via Tree-Based Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingqian He, Yongliang Shen, Wenqi Zhang, Zeqi Tan, Weiming Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00390">https://arxiv.org/abs/2407.00390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00390">https://arxiv.org/pdf/2407.00390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00390]] Advancing Process Verification for Large Language Models via Tree-Based Preference Learning(https://arxiv.org/abs/2407.00390)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales.Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% to 82.79%), MATH (17.00% to 26.80%), CSQA (68.14% to 72.97%), and StrategyQA (82.86% to 83.25%).Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通过生成分步原理，在处理复杂推理任务方面表现出了巨大的潜力。一些方法已被证明可以通过引入额外的验证器来评估这些路径来有效提高准确性。然而，现有的验证器通常是在二元标记的推理路径上进行训练的，无法充分利用中间步骤的相对优点，从而限制了所提供反馈的有效性。为了克服这一限制，我们提出了基于树的偏好学习验证器 (Tree-PLV)，这是一种新颖的方法，它通过最佳优先搜索算法构建推理树并收集步骤级配对数据进行偏好训练。与传统的二元分类相比，步骤级偏好可以更精细地捕捉推理步骤之间的细微差别，从而可以更精确地评​​估完整的推理路径。我们在一系列算术和常识推理任务中对 Tree-PLV 进行了实证评估，它的表现明显优于现有的基准。例如，Tree-PLV 在 GSM8K（67.55% 到 82.79%）、MATH（17.00% 到 26.80%）、CSQA（68.14% 到 72.97%）和 StrategyQA（82.86% 到 83.25%）上比 Mistral-7B 自洽基线取得了显著的性能提升。此外，我们的研究探索了应用偏好学习的适当粒度，结果表明分步指导提供的反馈与推理过程的评估更加一致。</li>
</ul>

<h3>Title: A Study on Effect of Reference Knowledge Choice in Generating Technical Content Relevant to SAPPhIRE Model Using Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Kausik Bhattacharya, Anubhab Majumder, Amaresh Chakrabarti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00396">https://arxiv.org/abs/2407.00396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00396">https://arxiv.org/pdf/2407.00396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00396]] A Study on Effect of Reference Knowledge Choice in Generating Technical Content Relevant to SAPPhIRE Model Using Large Language Model(https://arxiv.org/abs/2407.00396)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Representation of systems using the SAPPhIRE model of causality can be an inspirational stimulus in design. However, creating a SAPPhIRE model of a technical or a natural system requires sourcing technical knowledge from multiple technical documents regarding how the system works. This research investigates how to generate technical content accurately relevant to the SAPPhIRE model of causality using a Large Language Model, also called LLM. This paper, which is the first part of the two-part research, presents a method for hallucination suppression using Retrieval Augmented Generating with LLM to generate technical content supported by the scientific information relevant to a SAPPhIRE con-struct. The result from this research shows that the selection of reference knowledge used in providing context to the LLM for generating the technical content is very important. The outcome of this research is used to build a software support tool to generate the SAPPhIRE model of a given technical system.</li>
<li><strong>摘要：</strong>使用 SAPPhIRE 因果关系模型表示系统可以激发设计的灵感。但是，创建技术或自然系统的 SAPPhIRE 模型需要从多个技术文档中获取有关系统工作原理的技术知识。本研究探讨了如何使用大型语言模型（也称为 LLM）生成与 SAPPhIRE 因果关系模型准确相关的技术内容。本文是两部分研究的第一部分，介绍了一种使用 LLM 的检索增强生成来抑制幻觉的方法，以生成由与 SAPPhIRE 构造相关的科学信息支持的技术内容。本研究的结果表明，选择用于为 LLM 提供生成技术内容背景的参考知识非常重要。本研究的结果用于构建一个软件支持工具，以生成给定技术系统的 SAPPhIRE 模型。</li>
</ul>

<h3>Title: Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP</h3>
<ul>
<li><strong>Authors: </strong>Omer Goldman, Alon Jacovi, Aviv Slobodkin, Aviya Maimon, Ido Dagan, Reut Tsarfaty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00402">https://arxiv.org/abs/2407.00402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00402">https://arxiv.org/pdf/2407.00402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00402]] Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP(https://arxiv.org/abs/2407.00402)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context</a></li>
<li><strong>Abstract: </strong>Improvements in language models' capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use-cases are grouped together under the umbrella term of "long-context", defined simply by the total length of the model's input, including - for example - Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different. We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Diffusion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long-context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly diffused within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long-context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context.</li>
<li><strong>摘要：</strong>语言模型能力的提升推动了其应用向更长上下文的转变，使得长上下文评估和开发成为一个活跃的研究领域。然而，许多不同的用例被归类在“长上下文”这一总称下，其定义仅仅在于模型输入的总长度，包括 - 例如 - 大海捞针任务、书籍摘要和信息聚合。鉴于它们的难度各不相同，我们在本立场文件中认为，将不同的任务按上下文长度混为一谈是没有成效的。作为一个社区，我们需要更精确的词汇来理解是什么让长上下文任务相似或不同。我们建议根据使它们在较长上下文中更难的属性来解开长上下文的分类。我们提出了两个正交的难度轴：（I）扩散：在上下文中找到必要的信息有多难？（II）范围：有多少必要的信息需要查找？我们调查了有关长语境的文献，为该分类法作为信息描述符提供了依据，并根据该分类法对文献进行了定位。我们得出的结论是，最困难和最有趣的设置（其必要信息非常长且在输入中高度分散）尚未得到充分探索。通过使用描述性词汇并讨论长语境中难度的相关属性，我们可以在这一领域开展更明智的研究。我们呼吁仔细设计具有明显长语境的任务和基准，同时考虑到使其在质量上不同于短语境的特征。</li>
</ul>

<h3>Title: Too Late to Train, Too Early To Use? A Study on Necessity and Viability of Low-Resource Bengali LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tamzeed Mahfuz, Satak Kumar Dey, Ruwad Naswan, Hasnaen Adil, Khondker Salman Sayeed, Haz Sameen Shahgir</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00416">https://arxiv.org/abs/2407.00416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00416">https://arxiv.org/pdf/2407.00416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00416]] Too Late to Train, Too Early To Use? A Study on Necessity and Viability of Low-Resource Bengali LLMs(https://arxiv.org/abs/2407.00416)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Each new generation of English-oriented Large Language Models (LLMs) exhibits enhanced cross-lingual transfer capabilities and significantly outperforms older LLMs on low-resource languages. This prompts the question: Is there a need for LLMs dedicated to a particular low-resource language? We aim to explore this question for Bengali, a low-to-moderate resource Indo-Aryan language native to the Bengal region of South Asia. We compare the performance of open-weight and closed-source LLMs such as LLaMA-3 and GPT-4 against fine-tuned encoder-decoder models across a diverse set of Bengali downstream tasks, including translation, summarization, paraphrasing, question-answering, and natural language inference. Our findings reveal that while LLMs generally excel in reasoning tasks, their performance in tasks requiring Bengali script generation is inconsistent. Key challenges include inefficient tokenization of Bengali script by existing LLMs, leading to increased computational costs and potential performance degradation. Additionally, we highlight biases in machine-translated datasets commonly used for Bengali NLP tasks. We conclude that there is a significant need for a Bengali-oriented LLM, but the field currently lacks the high-quality pretraining and instruction-tuning datasets necessary to develop a highly effective model.</li>
<li><strong>摘要：</strong>每一代面向英语的大型语言模型 (LLM) 都表现出增强的跨语言迁移能力，并且在资源匮乏的语言上的表现明显优于旧版 LLM。这引发了一个问题：是否需要专门针对特定资源匮乏语言的 LLM？我们旨在针对孟加拉语探索这个问题，孟加拉语是一种资源稀缺到中等的印度-雅利安语，原产于南亚的孟加拉地区。我们将 LLaMA-3 和 GPT-4 等开源和闭源 LLM 与经过微调的编码器-解码器模型在一系列孟加拉语下游任务（包括翻译、摘要、释义、问答和自然语言推理）中的表现进行比较。我们的研究结果表明，虽然 LLM 通常在推理任务中表现出色，但它们在需要生成孟加拉语脚本的任务中的表现并不一致。关键挑战包括现有 LLM 对孟加拉语脚本的标记效率低下，导致计算成本增加和潜在的性能下降。此外，我们还强调了常用于孟加拉语 NLP 任务的机器翻译数据集中的偏差。我们得出的结论是，孟加拉语方向的 LLM 需求很大，但该领域目前缺乏开发高效模型所需的高质量预训练和指令调整数据集。</li>
</ul>

<h3>Title: Brevity is the soul of wit: Pruning long files for code generation</h3>
<ul>
<li><strong>Authors: </strong>Aaditya K. Singh, Yu Yang, Kushal Tirumala, Mostafa Elhoushi, Ari S. Morcos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00434">https://arxiv.org/abs/2407.00434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00434">https://arxiv.org/pdf/2407.00434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00434]] Brevity is the soul of wit: Pruning long files for code generation(https://arxiv.org/abs/2407.00434)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Data curation is commonly considered a "secret-sauce" for LLM training, with higher quality data usually leading to better LLM performance. Given the scale of internet-scraped corpora, data pruning has become a larger and larger focus. Specifically, many have shown that de-duplicating data, or sub-selecting higher quality data, can lead to efficiency or performance improvements. Generally, three types of methods are used to filter internet-scale corpora: embedding-based, heuristic-based, and classifier-based. In this work, we contrast the former two in the domain of finetuning LLMs for code generation. We find that embedding-based methods are often confounded by length, and that a simple heuristic--pruning long files--outperforms other methods in compute-limited regimes. Our method can yield up to a 2x efficiency benefit in training (while matching performance) or a 3.5% absolute performance improvement on HumanEval (while matching compute). However, we find that perplexity on held-out long files can increase, begging the question of whether optimizing data mixtures for common coding benchmarks (HumanEval, MBPP) actually best serves downstream use cases. Overall, we hope our work builds useful intuitions about code data (specifically, the low quality of extremely long code files) provides a compelling heuristic-based method for data pruning, and brings to light questions in how we evaluate code generation models.</li>
<li><strong>摘要：</strong>数据管理通常被认为是 LLM 训练的“秘诀”，更高质量的数据通常会带来更好的 LLM 性能。鉴于互联网抓取语料库的规模，数据修剪已成为越来越受关注的焦点。具体而言，许多人已经表明，重复数据删除或子选择更高质量的数据可以提高效率或性能。通常，使用三种方法来过滤互联网规模的语料库：基于嵌入、基于启发式和基于分类器。在这项工作中，我们在微调 LLM 以生成代码的领域对比了前两者。我们发现基于嵌入的方法通常会因长度而混淆，而简单的启发式方法（修剪长文件）在计算受限的情况下优于其他方法。我们的方法可以在训练中产生高达 2 倍的效率优势（同时匹配性能）或在 HumanEval 上产生 3.5% 的绝对性能改进（同时匹配计算）。然而，我们发现保留的长文件的困惑度可能会增加，这引出了一个问题：针对常见编码基准（HumanEval、MBPP）优化数据混合是否真的最能服务于下游用例。总的来说，我们希望我们的工作能够建立关于代码数据的有用直觉（特别是极长代码文件的低质量），为数据修剪提供一种令人信服的基于启发式的方法，并揭示我们如何评估代码生成模型的问题。</li>
</ul>

<h3>Title: A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Peiqin Lin, André F. T. Martins, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00436">https://arxiv.org/abs/2407.00436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00436">https://arxiv.org/pdf/2407.00436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00436]] A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models(https://arxiv.org/abs/2407.00436)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent studies have highlighted the potential of exploiting parallel corpora to enhance multilingual large language models, improving performance in both bilingual tasks, e.g., machine translation, and general-purpose tasks, e.g., text classification. Building upon these findings, our comprehensive study aims to identify the most effective strategies for leveraging parallel corpora. We investigate the impact of parallel corpora quality and quantity, training objectives, and model size on the performance of multilingual large language models enhanced with parallel corpora across diverse languages and tasks. Our analysis reveals several key insights: (i) filtering noisy translations is essential for effectively exploiting parallel corpora, while language identification and short sentence filtering have little effect; (ii) even a corpus containing just 10K parallel sentences can yield results comparable to those obtained from much larger datasets; (iii) employing only the machine translation objective yields the best results among various training objectives and their combinations; (iv) larger multilingual language models benefit more from parallel corpora than smaller models due to their stronger capacity for cross-task transfer. Our study offers valuable insights into the optimal utilization of parallel corpora to enhance multilingual large language models, extending the generalizability of previous findings from limited languages and tasks to a broader range of scenarios.</li>
<li><strong>摘要：</strong>最近的研究强调了利用平行语料库来增强多语言大型语言模型的潜力，从而提高双语任务（例如机器翻译）和通用任务（例如文本分类）的性能。基于这些发现，我们的全面研究旨在确定利用平行语料库的最有效策略。我们研究了平行语料库质量和数量、训练目标和模型大小对使用平行语料库增强的多语言大型语言模型在不同语言和任务中性能的影响。我们的分析揭示了几个关键见解：（i）过滤嘈杂的翻译对于有效利用平行语料库至关重要，而语言识别和短句过滤效果甚微；（ii）即使只包含 10K 个平行句子的语料库也能产生与从更大的数据集中获得的结果相当的结果；（iii）在各种训练目标及其组合中，仅使用机器翻译目标会产生最佳结果；（iv）大型多语言语言模型比小型模型从平行语料库中受益更多，因为它们具有更强的跨任务迁移能力。我们的研究为如何最佳地利用平行语料库来增强多语言大型语言模型提供了宝贵的见解，将以前的研究结果从有限的语言和任务推广到更广泛的场景。</li>
</ul>

<h3>Title: Self-Translate-Train: A Simple but Strong Baseline for Cross-lingual Transfer of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ryokan Ri, Shun Kiyono, Sho Takase</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00454">https://arxiv.org/abs/2407.00454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00454">https://arxiv.org/pdf/2407.00454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00454]] Self-Translate-Train: A Simple but Strong Baseline for Cross-lingual Transfer of Large Language Models(https://arxiv.org/abs/2407.00454)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Cross-lingual transfer is a promising technique for utilizing data in a source language to improve performance in a target language. However, current techniques often require an external translation system or suffer from suboptimal performance due to over-reliance on cross-lingual generalization of multi-lingual pretrained language models. In this study, we propose a simple yet effective method called Self-Translate-Train. It leverages the translation capability of a large language model to generate synthetic training data in the target language and fine-tunes the model with its own generated data. We evaluate the proposed method on a wide range of tasks and show substantial performance gains across several non-English languages.</li>
<li><strong>摘要：</strong>跨语言迁移是一种很有前途的技术，它利用源语言的数据来提高目标语言的性能。然而，当前的技术通常需要外部翻译系统，或者由于过度依赖多语言预训练语言模型的跨语言泛化而导致性能不佳。在本研究中，我们提出了一种简单而有效的方法，称为 Self-Translate-Train。它利用大型语言模型的翻译能力来生成目标语言的合成训练数据，并使用自己生成的数据对模型进行微调。我们在广泛的任务上评估了所提出的方法，并显示在几种非英语语言中都有显著的性能提升。</li>
</ul>

<h3>Title: BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science</h3>
<ul>
<li><strong>Authors: </strong>Xinna Lin, Siqi Ma, Junjie Shan, Xiaojing Zhang, Shell Xu Hu, Tiannan Guo, Stan Z. Li, Kaicheng Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00466">https://arxiv.org/abs/2407.00466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00466">https://arxiv.org/pdf/2407.00466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00466]] BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science(https://arxiv.org/abs/2407.00466)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, people either rely on direct Question-Answering (QA) to the LLM itself, or in a biomedical experimental manner. How to precisely benchmark biomedical agents from an AI Scientist perspective remains largely unexplored. To this end, we draw inspiration from one most important abilities of scientists, understanding the literature, and introduce BioKGBench. In contrast to traditional evaluation benchmark that only focuses on factual QA, where the LLMs are known to have hallucination issues, we first disentangle "Understanding Literature" into two atomic abilities, i) "Understanding" the unstructured text from research papers by performing scientific claim verification, and ii) Ability to interact with structured Knowledge-Graph Question-Answering (KGQA) as a form of "Literature" grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify the factual errors of existing large-scale knowledge graph databases. We collect over two thousand data for two atomic tasks and 225 high-quality annotated data for the agent task. Surprisingly, we discover that state-of-the-art agents, both daily scenarios and biomedical ones, have either failed or inferior performance on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent. On the widely used popular knowledge graph, we discover over 90 factual errors which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach. The code and data are available at this https URL.</li>
<li><strong>摘要：</strong>追求生物医学科学的人工智能，即 AI Scientist，受到越来越多的关注，其中一种常见的方法是构建由大型语言模型 (LLM) 驱动的副驾驶代理。然而，为了评估这样的系统，人们要么依靠对 LLM 本身的直接问答 (QA)，要么以生物医学实验的方式进行评估。如何从 AI Scientist 的角度精确地对生物医学代理进行基准测试仍在很大程度上尚未探索。为此，我们从科学家最重要的能力之一——理解文献中汲取灵感，并引入了 BioKGBench。与仅关注事实 QA 的传统评估基准（其中 LLM 已知存在幻觉问题）相比，我们首先将“理解文献”分解为两种原子能力，i) 通过执行科学声明验证来“理解”研究论文中的非结构化文本，以及 ii) 能够与结构化知识图谱问答 (KGQA) 交互作为“文献”基础的一种形式。然后，我们制定了一个新的代理任务，称为 KGCheck，使用 KGQA 和基于领域的检索增强生成 (RAG) 来识别现有大型知识图谱数据库的事实错误。我们为两个原子任务收集了两千多个数据，为代理任务收集了 225 个高质量注释数据。令人惊讶的是，我们发现最先进的代理（无论是日常场景还是生物医学场景）在我们的基准测试中要么失败，要么表现不佳。然后，我们引入了一个简单但有效的基线，称为 BKGAgent。在广泛使用的流行知识图谱上，我们发现了 90 多个事实错误，这些错误为代理提供了发现场景，并证明了我们方法的有效性。代码和数据可在此 https URL 上找到。</li>
</ul>

<h3>Title: Large Language Models for Power Scheduling: A User-Centric Approach</h3>
<ul>
<li><strong>Authors: </strong>Thomas Mongaillard, Samson Lasaulce, Othman Hicheur, Chao Zhang, Lina Bariah, Vineeth S. Varma, Hang Zou, Qiyang Zhao, Merouane Debbah</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00476">https://arxiv.org/abs/2407.00476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00476">https://arxiv.org/pdf/2407.00476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00476]] Large Language Models for Power Scheduling: A User-Centric Approach(https://arxiv.org/abs/2407.00476)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>While traditional optimization and scheduling schemes are designed to meet fixed, predefined system requirements, future systems are moving toward user-driven approaches and personalized services, aiming to achieve high quality-of-experience (QoE) and flexibility. This challenge is particularly pronounced in wireless and digitalized energy networks, where users' requirements have largely not been taken into consideration due to the lack of a common language between users and machines. The emergence of powerful large language models (LLMs) marks a radical departure from traditional system-centric methods into more advanced user-centric approaches by providing a natural communication interface between users and devices. In this paper, for the first time, we introduce a novel architecture for resource scheduling problems by constructing three LLM agents to convert an arbitrary user's voice request (VRQ) into a resource allocation vector. Specifically, we design an LLM intent recognition agent to translate the request into an optimization problem (OP), an LLM OP parameter identification agent, and an LLM OP solving agent. To evaluate system performance, we construct a database of typical VRQs in the context of electric vehicle (EV) charging. As a proof of concept, we primarily use Llama 3 8B. Through testing with different prompt engineering scenarios, the obtained results demonstrate the efficiency of the proposed architecture. The conducted performance analysis allows key insights to be extracted. For instance, having a larger set of candidate OPs to model the real-world problem might degrade the final performance because of a higher recognition/OP classification noise level. All results and codes are open source.</li>
<li><strong>摘要：</strong>虽然传统的优化和调度方案旨在满足固定的、预定义的系统要求，但未来的系统正朝着用户驱动的方法和个性化服务的方向发展，旨在实现高体验质量 (QoE) 和灵活性。这一挑战在无线和数字化能源网络中尤为明显，由于用户和机器之间缺乏共同语言，用户的需求在很大程度上没有得到考虑。强大的大型语言模型 (LLM) 的出现标志着从传统的以系统为中心的方法向更先进的以用户为中心的方法的彻底转变，它提供了用户和设备之间的自然通信接口。在本文中，我们首次通过构建三个 LLM 代理将任意用户的语音请求 (VRQ) 转换为资源分配向量，为资源调度问题引入了一种新颖的架构。具体来说，我们设计了一个 LLM 意图识别代理来将请求转换为优化问题 (OP)、一个 LLM OP 参数识别代理和一个 LLM OP 求解代理。为了评估系统性能，我们在电动汽车 (EV) 充电的背景下构建了一个典型 VRQ 的数据库。作为概念验证，我们主要使用 Llama 3 8B。通过对不同的即时工程场景进行测试，获得的结果证明了所提出的架构的效率。进行的性能分析可以提取关键见解。例如，使用更大的候选 OP 集来模拟现实世界的问题可能会降低最终性能，因为识别/OP 分类噪声水平更高。所有结果和代码都是开源的。</li>
</ul>

<h3>Title: It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization</h3>
<ul>
<li><strong>Authors: </strong>Bingdong Li, Zixiang Di, Yanting Yang, Hong Qian, Peng Yang, Hao Hao, Ke Tang, Aimin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00487">https://arxiv.org/abs/2407.00487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00487">https://arxiv.org/pdf/2407.00487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00487]] It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization(https://arxiv.org/abs/2407.00487)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a novel approach for large language model merging via black-box multi-objective optimization algorithms. The goal of model merging is to combine multiple models, each excelling in different tasks, into a single model that outperforms any of the individual source models. However, model merging faces two significant challenges: First, existing methods rely heavily on human intuition and customized strategies. Second, parameter conflicts often arise during merging, and while methods like DARE [1] can alleviate this issue, they tend to stochastically drop parameters, risking the loss of important delta parameters. To address these challenges, we propose the MM-MO method, which automates the search for optimal merging configurations using multi-objective optimization algorithms, eliminating the need for human intuition. During the configuration searching process, we use estimated performance across multiple diverse tasks as optimization objectives in order to alleviate the parameter conflicting between different source models without losing crucial delta parameters. We conducted comparative experiments with other mainstream model merging methods, demonstrating that our method consistently outperforms them. Moreover, our experiments reveal that even task types not explicitly targeted as optimization objectives show performance improvements, indicating that our method enhances the overall potential of the model rather than merely overfitting to specific task types. This approach provides a significant advancement in model merging techniques, offering a robust and plug-and-play solution for integrating diverse models into a unified, high-performing model.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了一种通过黑盒多目标优化算法进行大型语言模型合并的新方法。模型合并的目标是将多个模型（每个模型在不同任务上表现优异）组合成一个优于任何单个源模型的单一模型。然而，模型合并面临两个重大挑战：首先，现有方法严重依赖人类直觉和定制策略。其次，合并过程中经常会出现参数冲突，虽然像 DARE [1] 这样的方法可以缓解这个问题，但它们往往会随机丢弃参数，从而有丢失重要增量参数的风险。为了应对这些挑战，我们提出了 MM-MO 方法，该方法使用多目标优化算法自动搜索最佳合并配置，从而无需人类直觉。在配置搜索过程中，我们使用跨多个不同任务的估计性能作为优化目标，以缓解不同源模型之间的参数冲突，而不会丢失关键的增量参数。我们与其他主流模型合并方法进行了比较实验，表明我们的方法始终优于它们。此外，我们的实验表明，即使没有明确作为优化目标的任务类型也表现出性能提升，这表明我们的方法增强了模型的整体潜力，而不仅仅是过度拟合特定任务类型。这种方法在模型合并技术方面取得了重大进展，为将各种模型集成为统一的高性能模型提供了一种强大且即插即用的解决方案。</li>
</ul>

<h3>Title: PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kunquan Deng, Zeyu Huang, Chen Li, Chenghua Lin, Min Gao, Wenge Rong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00488">https://arxiv.org/abs/2407.00488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00488">https://arxiv.org/pdf/2407.00488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00488]] PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models(https://arxiv.org/abs/2407.00488)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in fluency but risk producing inaccurate content, called "hallucinations." This paper outlines a standardized process for categorizing fine-grained hallucination types and proposes an innovative framework--the Progressive Fine-grained Model Editor (PFME)--specifically designed to detect and correct fine-grained hallucinations in LLMs. PFME consists of two collaborative modules: the Real-time Fact Retrieval Module and the Fine-grained Hallucination Detection and Editing Module. The former identifies key entities in the document and retrieves the latest factual evidence from credible sources. The latter further segments the document into sentence-level text and, based on relevant evidence and previously edited context, identifies, locates, and edits each sentence's hallucination type. Experimental results on FavaBench and FActScore demonstrate that PFME outperforms existing methods in fine-grained hallucination detection tasks. Particularly, when using the Llama3-8B-Instruct model, PFME's performance in fine-grained hallucination detection with external knowledge assistance improves by 8.7 percentage points (pp) compared to ChatGPT. In editing tasks, PFME further enhances the FActScore of FActScore-Alpaca13B and FActScore-ChatGPT datasets, increasing by 16.2pp and 4.6pp, respectively.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的流畅度很高，但可能会产生不准确的内容，即所谓的“幻觉”。本文概述了对细粒度幻觉类型进行分类的标准化流程，并提出了一个创新框架——渐进式细粒度模型编辑器 (PFME)，专门用于检测和纠正 LLM 中的细粒度幻觉。PFME 由两个协作模块组成：实时事实检索模块和细粒度幻觉检测和编辑模块。前者识别文档中的关键实体并从可靠来源检索最新的事实证据。后者将文档进一步细分为句子级文本，并根据相关证据和先前编辑的上下文识别、定位和编辑每个句子的幻觉类型。FavaBench 和 FActScore 上的实验结果表明，PFME 在细粒度幻觉检测任务中的表现优于现有方法。特别地，在使用 Llama3-8B-Instruct 模型时，PFME 在外部知识辅助的细粒度幻觉检测中的表现比 ChatGPT 提高了 8.7 个百分点（pp）。在编辑任务中，PFME 进一步提升了 FActScore-Alpaca13B 和 FActScore-ChatGPT 数据集的 FActScore，分别提高了 16.2pp 和 4.6pp。</li>
</ul>

<h3>Title: LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Ying, Mingbao Lin, Yixin Cao, Wei Tang, Bo Wang, Qianru Sun, Xuanjing Huang, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00497">https://arxiv.org/abs/2407.00497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00497">https://arxiv.org/pdf/2407.00497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00497]] LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement(https://arxiv.org/abs/2407.00497)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>This paper introduces the innovative "LLMs-as-Instructors" framework, which leverages the advanced Large Language Models (LLMs) to autonomously enhance the training of smaller target models. Inspired by the theory of "Learning from Errors", this framework employs an instructor LLM to meticulously analyze the specific errors within a target model, facilitating targeted and efficient training cycles. Within this framework, we implement two strategies: "Learning from Error," which focuses solely on incorrect responses to tailor training data, and "Learning from Error by Contrast", which uses contrastive learning to analyze both correct and incorrect responses for a deeper understanding of errors. Our empirical studies, conducted with several open-source models, demonstrate significant improvements across multiple benchmarks, including mathematical reasoning, coding abilities, and factual knowledge. Notably, the refined Llama-3-8b-Instruction has outperformed ChatGPT, illustrating the effectiveness of our approach. By leveraging the strengths of both strategies, we have attained a more balanced performance improvement on both in-domain and out-of-domain benchmarks. Our code can be found at this https URL.</li>
<li><strong>摘要：</strong>本文介绍了创新的“LLM 作为讲师”框架，该框架利用先进的大型语言模型 (LLM) 自主增强较小目标模型的训练。受“从错误中学习”理论的启发，该框架采用讲师 LLM 来细致分析目标模型中的具体错误，从而促进有针对性和高效的训练周期。在这个框架内，我们实施了两种策略：“从错误中学习”，它只关注错误的回答以定制训练数据，以及“通过对比从错误中学习”，它使用对比学习来分析正确和错误的回答，以更深入地理解错误。我们对几个开源模型进行的实证研究表明，在多个基准测试中都有显著的改进，包括数学推理、编码能力和事实知识。值得注意的是，改进后的 Llama-3-8b-Instruction 的表现优于 ChatGPT，说明了我们方法的有效性。通过利用两种策略的优势，我们在域内和域外基准测试中实现了更均衡的性能改进。我们的代码可在此 https URL 中找到。</li>
</ul>

<h3>Title: ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Wang, Jinhao Duan, Lu Cheng, Yue Zhang, Qingni Wang, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00499">https://arxiv.org/abs/2407.00499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00499">https://arxiv.org/pdf/2407.00499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00499]] ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees(https://arxiv.org/abs/2407.00499)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification (UQ) in natural language generation (NLG) tasks remains an open challenge, exacerbated by the intricate nature of the recent large language models (LLMs). This study investigates adapting conformal prediction (CP), which can convert any heuristic measure of uncertainty into rigorous theoretical guarantees by constructing prediction sets, for black-box LLMs in open-ended NLG tasks. We propose a sampling-based uncertainty measure leveraging self-consistency and develop a conformal uncertainty criterion by integrating the uncertainty condition aligned with correctness into the design of the CP algorithm. Experimental results indicate that our uncertainty measure generally surpasses prior state-of-the-art methods. Furthermore, we calibrate the prediction sets within the model's unfixed answer distribution and achieve strict control over the correctness coverage rate across 6 LLMs on 4 free-form NLG datasets, spanning general-purpose and medical domains, while the small average set size further highlights the efficiency of our method in providing trustworthy guarantees for practical open-ended NLG applications.</li>
<li><strong>摘要：</strong>自然语言生成 (NLG) 任务中的不确定性量化 (UQ) 仍然是一个开放的挑战，而最近大型语言模型 (LLM) 的复杂性加剧了这一挑战。本研究调查了针对开放式 NLG 任务中的黑盒 LLM 调整共形预测 (CP)，CP 可以通过构建预测集将任何启发式不确定性度量转换为严格的理论保证。我们提出了一种利用自洽性的基于采样的不确定性度量，并通过将与正确性一致的不确定性条件整合到 CP 算法的设计中来开发共形不确定性标准。实验结果表明，我们的不确定性度量通常优于先前的最先进的方法。此外，我们在模型的不固定答案分布内校准预测集，并在 4 个自由形式 NLG 数据集上的 6 个 LLM 中实现对正确性覆盖率的严格控制，涵盖通用和医学领域，而较小的平均集大小进一步凸显了我们的方法在为实际开放式 NLG 应用提供可靠保证方面的效率。</li>
</ul>

<h3>Title: Answering real-world clinical questions using large language model based systems</h3>
<ul>
<li><strong>Authors: </strong>Yen Sia Low (1), Michael L. Jackson (1), Rebecca J. Hyde (1), Robert E. Brown (1), Neil M. Sanghavi (1), Julian D. Baldwin (1), C. William Pike (1), Jananee Muralidharan (1), Gavin Hui (1 and 2), Natasha Alexander (3), Hadeel Hassan (3), Rahul V. Nene (4), Morgan Pike (5), Courtney J. Pokrzywa (6), Shivam Vedak (7), Adam Paul Yan (3), Dong-han Yao (7), Amy R. Zipursky (3), Christina Dinh (1), Philip Ballentine (1), Dan C. Derieg (1), Vladimir Polony (1), Rehan N. Chawdry (1), Jordan Davies (1), Brigham B. Hyde (1), Nigam H. Shah (1 and 7), Saurabh Gombar (1 and 8) ((1) Atropos Health, New York NY, USA, (2) Department of Medicine, University of California, Los Angeles CA, USA, (3) Department of Pediatrics, The Hospital for Sick Children, Toronto ON, Canada, (4) Department of Emergency Medicine, University of California, San Diego CA, USA, (5) Department of Emergency Medicine, University of Michigan, Ann Arbor MI, USA, (6) Department of Surgery, Columbia University, New York NY, USA, (7) Center for Biomedical Informatics Research, Stanford University, Stanford CA, USA (8) Department of Pathology, Stanford University, Stanford CA, USA)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00541">https://arxiv.org/abs/2407.00541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00541">https://arxiv.org/pdf/2407.00541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00541]] Answering real-world clinical questions using large language model based systems(https://arxiv.org/abs/2407.00541)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, retrieval augmented generation, agent</a></li>
<li><strong>Abstract: </strong>Evidence to guide healthcare decisions is often limited by a lack of relevant and trustworthy literature as well as difficulty in contextualizing existing research for a specific patient. Large language models (LLMs) could potentially address both challenges by either summarizing published literature or generating new studies based on real-world data (RWD). We evaluated the ability of five LLM-based systems in answering 50 clinical questions and had nine independent physicians review the responses for relevance, reliability, and actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus, Gemini Pro 1.5) rarely produced answers that were deemed relevant and evidence-based (2% - 10%). In contrast, retrieval augmented generation (RAG)-based and agentic LLM systems produced relevant and evidence-based answers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic ChatRWD was able to answer novel questions compared to other LLMs (65% vs. 0-9%). These results suggest that while general-purpose LLMs should not be used as-is, a purpose-built system for evidence summarization based on RAG and one for generating novel evidence working synergistically would improve availability of pertinent evidence for patient care.</li>
<li><strong>摘要：</strong>指导医疗保健决策的证据通常受到缺乏相关和可信文献以及难以将现有研究与特定患者联系起来的限制。大型语言模型 (LLM) 可以通过总结已发表的文献或基于真实世界数据 (RWD) 生成新研究来解决这两个挑战。我们评估了五个基于 LLM 的系统回答 50 个临床问题的能力，并让九名独立医生审查了答案的相关性、可靠性和可操作性。目前，通用 LLM（ChatGPT-4、Claude 3 Opus、Gemini Pro 1.5）很少产生被认为相关且基于证据的答案（2% - 10%）。相比之下，基于检索增强生成 (RAG) 和代理 LLM 系统为 24%（OpenEvidence）至 58%（ChatRWD）的问题提供了相关且基于证据的答案。与其他 LLM 相比，只有代理 ChatRWD 能够回答新问题（65% vs. 0-9%）。这些结果表明，虽然不应按原样使用通用 LLM，但基于 RAG 专门构建的证据总结系统和用于生成新证据的系统协同工作将提高患者护理相关证据的可用性。</li>
</ul>

<h3>Title: MasonTigers at SemEval-2024 Task 10: Emotion Discovery and Flip Reasoning in Conversation with Ensemble of Transformers and Prompting</h3>
<ul>
<li><strong>Authors: </strong>Al Nahian Bin Emran, Amrita Ganguly, Sadiya Sayara Chowdhury Puspo, Nishat Raihan, Dhiman Goswami</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00581">https://arxiv.org/abs/2407.00581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00581">https://arxiv.org/pdf/2407.00581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00581]] MasonTigers at SemEval-2024 Task 10: Emotion Discovery and Flip Reasoning in Conversation with Ensemble of Transformers and Prompting(https://arxiv.org/abs/2407.00581)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we present MasonTigers' participation in SemEval-2024 Task 10, a shared task aimed at identifying emotions and understanding the rationale behind their flips within monolingual English and Hindi-English code-mixed dialogues. This task comprises three distinct subtasks - emotion recognition in conversation for Hindi-English code-mixed dialogues, emotion flip reasoning for Hindi-English code-mixed dialogues, and emotion flip reasoning for English dialogues. Our team, MasonTigers, contributed to each subtask, focusing on developing methods for accurate emotion recognition and reasoning. By leveraging our approaches, we attained impressive F1-scores of 0.78 for the first task and 0.79 for both the second and third tasks. This performance not only underscores the effectiveness of our methods across different aspects of the task but also secured us the top rank in the first and third subtasks, and the 2nd rank in the second subtask. Through extensive experimentation and analysis, we provide insights into our system's performance and contributions to each subtask.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 MasonTigers 参与 SemEval-2024 任务 10 的情况，这是一项共享任务，旨在识别情绪并理解在单语英语和印地语-英语代码混合对话中情绪翻转背后的原因。这项任务包含三个不同的子任务——印地语-英语代码混合对话中的情绪识别、印地语-英语代码混合对话的情绪翻转推理以及英语对话的情绪翻转推理。我们的团队 MasonTigers 为每个子任务做出了贡献，专注于开发准确的情绪识别和推理方法。通过利用我们的方法，我们在第一项任务中获得了令人印象深刻的 F1 分数 0.78，在第二项和第三项任务中都获得了 0.79。这一表现不仅凸显了我们的方法在任务不同方面的有效性，而且还使我们在第一项和第三项子任务中排名第一，在第二项子任务中排名第二。通过大量的实验和分析，我们深入了解了我们的系统的性能以及对每个子任务的贡献。</li>
</ul>

<h3>Title: DP-MLM: Differentially Private Text Rewriting Using Masked Language Models</h3>
<ul>
<li><strong>Authors: </strong>Stephen Meisenbacher, Maulik Chevli, Juraj Vladika, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00637">https://arxiv.org/abs/2407.00637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00637">https://arxiv.org/pdf/2407.00637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00637]] DP-MLM: Differentially Private Text Rewriting Using Masked Language Models(https://arxiv.org/abs/2407.00637)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The task of text privatization using Differential Privacy has recently taken the form of $\textit{text rewriting}$, in which an input text is obfuscated via the use of generative (large) language models. While these methods have shown promising results in the ability to preserve privacy, these methods rely on autoregressive models which lack a mechanism to contextualize the private rewriting process. In response to this, we propose $\textbf{DP-MLM}$, a new method for differentially private text rewriting based on leveraging masked language models (MLMs) to rewrite text in a semantically similar $\textit{and}$ obfuscated manner. We accomplish this with a simple contextualization technique, whereby we rewrite a text one token at a time. We find that utilizing encoder-only MLMs provides better utility preservation at lower $\varepsilon$ levels, as compared to previous methods relying on larger models with a decoder. In addition, MLMs allow for greater customization of the rewriting mechanism, as opposed to generative approaches. We make the code for $\textbf{DP-MLM}$ public and reusable, found at this https URL .</li>
<li><strong>摘要：</strong>使用差异隐私进行文本私有化的任务最近采用了文本重写的形式，其中通过使用生成（大型）语言模型对输入文本进行混淆。虽然这些方法在保护隐私方面表现出了良好的效果，但这些方法依赖于自回归模型，而自回归模型缺乏将隐私重写过程情境化的机制。针对这一问题，我们提出了一种新的差异隐私文本重写方法，该方法基于利用掩码语言模型 (MLM) 以语义上类似的混淆方式重写文本。我们使用一种简单的情境化技术来实现这一点，即一次重写一个标记的文本。我们发现，与以前依赖带有解码器的较大模型的方法相比，使用仅编码器的 MLM 可以在较低的 $\varepsilon$ 级别提供更好的效用保存。此外，与生成方法相比，MLM 允许对重写机制进行更大的定制。我们将 $\textbf{DP-MLM}$ 的代码公开并可重复使用，可在此 https URL 上找到。</li>
</ul>

<h3>Title: Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yifei Zhang, Xintao Wang, Jiaqing Liang, Sirui Xia, Lida Chen, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00653">https://arxiv.org/abs/2407.00653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00653">https://arxiv.org/pdf/2407.00653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00653]] Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs(https://arxiv.org/abs/2407.00653)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new knowledge from existing one.While it has been widely studied in the context of knowledge graphs (KGs), knowledge reasoning in LLMs remains underexplored. In this paper, we introduce Chain-of-Knowledge, a comprehensive framework for knowledge reasoning, including methodologies for both dataset construction and model learning. For dataset construction, we create KnowReason via rule mining on KGs. For model learning, we observe rule overfitting induced by naive training. Hence, we enhance CoK with a trial-and-error mechanism that simulates the human process of internal knowledge exploration. We conduct extensive experiments with KnowReason. Our results show the effectiveness of CoK in refining LLMs in not only knowledge reasoning, but also general reasoning benchmarkms.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种自然语言处理 (NLP) 任务中表现出令人印象深刻的性能，这些任务涉及越来越复杂的推理。知识推理是一种主要的推理类型，旨在从现有知识中获取新知识。虽然知识推理在知识图谱 (KG) 的背景下得到了广泛的研究，但 LLM 中的知识推理仍未得到充分探索。在本文中，我们介绍了知识链，这是一个全面的知识推理框架，包括数据集构建和模型学习的方法。对于数据集构建，我们通过对 KG 进行规则挖掘创建 KnowReason。对于模型学习，我们观察到由朴素训练引起的规则过度拟合。因此，我们通过一种模拟人类内部知识探索过程的试错机制增强了 CoK。我们对 KnowReason 进行了广泛的实验。我们的结果表明，CoK 不仅在知识推理方面，而且在一般推理基准方面都能有效地改进 LLM。</li>
</ul>

<h3>Title: HRDE: Retrieval-Augmented Large Language Models for Chinese Health Rumor Detection and Explainability</h3>
<ul>
<li><strong>Authors: </strong>Yanfang Chen, Ding Chen, Shichao Song, Simin Niu, Hanyu Wang, Zeyun Tang, Feiyu Xiong, Zhiyu Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00668">https://arxiv.org/abs/2407.00668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00668">https://arxiv.org/pdf/2407.00668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00668]] HRDE: Retrieval-Augmented Large Language Models for Chinese Health Rumor Detection and Explainability(https://arxiv.org/abs/2407.00668)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>As people increasingly prioritize their health, the speed and breadth of health information dissemination on the internet have also grown. At the same time, the presence of false health information (health rumors) intermingled with genuine content poses a significant potential threat to public health. However, current research on Chinese health rumors still lacks a large-scale, public, and open-source dataset of health rumor information, as well as effective and reliable rumor detection methods. This paper addresses this gap by constructing a dataset containing 1.12 million health-related rumors (HealthRCN) through web scraping of common health-related questions and a series of data processing steps. HealthRCN is the largest known dataset of Chinese health information rumors to date. Based on this dataset, we propose retrieval-augmented large language models for Chinese health rumor detection and explainability (HRDE). This model leverages retrieved relevant information to accurately determine whether the input health information is a rumor and provides explanatory responses, effectively aiding users in verifying the authenticity of health information. In evaluation experiments, we compared multiple models and found that HRDE outperformed them all, including GPT-4-1106-Preview, in rumor detection accuracy and answer quality. HRDE achieved an average accuracy of 91.04% and an F1 score of 91.58%.</li>
<li><strong>摘要：</strong>随着人们越来越重视健康，互联网上健康信息的传播速度和广度也随之增长。与此同时，虚假健康信息（健康谣言）混杂在真实内容中，对公共健康构成了重大的潜在威胁。但目前对中文健康谣言的研究仍然缺乏大规模、公开、开源的健康谣言信息数据集，以及有效可靠的谣言检测方法。本文通过网络抓取常见健康相关问题和一系列数据处理步骤，构建了一个包含112万条健康相关谣言的数据集（HealthRCN），解决了这一空白。HealthRCN是迄今为止已知的最大的中文健康信息谣言数据集。基于该数据集，我们提出了用于中文健康谣言检测和可解释性（HRDE）的检索增强大型语言模型。该模型利用检索到的相关信息准确判断输入的健康信息是否为谣言并提供解释性响应，有效帮助用户验证健康信息的真实性。在评估实验中，我们比较了多个模型，发现 HRDE 在谣言检测准确率和答案质量方面均优于所有模型（包括 GPT-4-1106-Preview）。HRDE 的平均准确率为 91.04%，F1 得分为 91.58%。</li>
</ul>

<h3>Title: Scaling Technology Acceptance Analysis with Large Language Model (LLM) Annotation Systems</h3>
<ul>
<li><strong>Authors: </strong>Pawel Robert Smolinski, Joseph Januszewicz, Jacek Winiarski</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00702">https://arxiv.org/abs/2407.00702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00702">https://arxiv.org/pdf/2407.00702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00702]] Scaling Technology Acceptance Analysis with Large Language Model (LLM) Annotation Systems(https://arxiv.org/abs/2407.00702)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Technology acceptance models effectively predict how users will adopt new technology products. Traditional surveys, often expensive and cumbersome, are commonly used for this assessment. As an alternative to surveys, we explore the use of large language models for annotating online user-generated content, like digital reviews and comments. Our research involved designing an LLM annotation system that transform reviews into structured data based on the Unified Theory of Acceptance and Use of Technology model. We conducted two studies to validate the consistency and accuracy of the annotations. Results showed moderate-to-strong consistency of LLM annotation systems, improving further by lowering the model temperature. LLM annotations achieved close agreement with human expert annotations and outperformed the agreement between experts for UTAUT variables. These results suggest that LLMs can be an effective tool for analyzing user sentiment, offering a practical alternative to traditional survey methods and enabling deeper insights into technology design and adoption.</li>
<li><strong>摘要：</strong>技术接受模型可以有效地预测用户将如何采用新技术产品。传统调查通常昂贵而繁琐，常用于这种评估。作为调查的替代方案，我们探索使用大型语言模型来注释在线用户生成的内容，如数字评论和意见。我们的研究涉及设计一个 LLM 注释系统，该系统基于技术接受和使用统一理论模型将评论转换为结构化数据。我们进行了两项研究来验证注释的一致性和准确性。结果显示 LLM 注释系统的一致性为中等到强，通过降低模型温度可以进一步提高。LLM 注释与人类专家注释非常一致，并且优于 UTAUT 变量专家之间的一致性。这些结果表明，LLM 可以成为分析用户情绪的有效工具，为传统调查方法提供实用的替代方案，并能够更深入地了解技术设计和采用。</li>
</ul>

<h3>Title: Large Language Models Struggle in Token-Level Clinical Named Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Qiuhao Lu, Rui Li, Andrew Wen, Jinlian Wang, Liwei Wang, Hongfang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00731">https://arxiv.org/abs/2407.00731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00731">https://arxiv.org/pdf/2407.00731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00731]] Large Language Models Struggle in Token-Level Clinical Named Entity Recognition(https://arxiv.org/abs/2407.00731)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized various sectors, including healthcare where they are employed in diverse applications. Their utility is particularly significant in the context of rare diseases, where data scarcity, complexity, and specificity pose considerable challenges. In the clinical domain, Named Entity Recognition (NER) stands out as an essential task and it plays a crucial role in extracting relevant information from clinical texts. Despite the promise of LLMs, current research mostly concentrates on document-level NER, identifying entities in a more general context across entire documents, without extracting their precise location. Additionally, efforts have been directed towards adapting ChatGPT for token-level NER. However, there is a significant research gap when it comes to employing token-level NER for clinical texts, especially with the use of local open-source LLMs. This study aims to bridge this gap by investigating the effectiveness of both proprietary and local LLMs in token-level clinical NER. Essentially, we delve into the capabilities of these models through a series of experiments involving zero-shot prompting, few-shot prompting, retrieval-augmented generation (RAG), and instruction-fine-tuning. Our exploration reveals the inherent challenges LLMs face in token-level NER, particularly in the context of rare diseases, and suggests possible improvements for their application in healthcare. This research contributes to narrowing a significant gap in healthcare informatics and offers insights that could lead to a more refined application of LLMs in the healthcare sector.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已经彻底改变了各个领域，包括医疗保健领域，它们在这些领域有多种应用。它们的实用性在罕见疾病领域尤其重要，因为数据稀缺性、复杂性和特异性带来了巨大挑战。在临床领域，命名实体识别 (NER) 是一项必不可少的任务，它在从临床文本中提取相关信息方面起着至关重要的作用。尽管 LLM 前景光明，但当前的研究主要集中在文档级 NER 上，即在整个文档中以更一般的上下文识别实体，而不提取它们的精确位置。此外，人们一直在努力将 ChatGPT 调整为 token 级 NER。然而，在将 token 级 NER 用于临床文本方面存在显著的研究差距，尤其是使用本地开源 LLM 时。本研究旨在通过研究专有和本地 LLM 在 token 级临床 NER 中的有效性来弥补这一差距。本质上，我们通过一系列涉及零样本提示、少样本提示、检索增强生成 (RAG) 和指令微调的实验深入研究了这些模型的功能。我们的探索揭示了 LLM 在 token 级 NER 中面临的固有挑战，特别是在罕见疾病的背景下，并提出了其在医疗保健领域应用的可能改进。这项研究有助于缩小医疗信息学的巨大差距，并提供了可能使 LLM 在医疗保健领域得到更精细应用的见解。</li>
</ul>

<h3>Title: Locate&Edit: Energy-based Text Editing for Efficient, Flexible, and Faithful Controlled Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Hye Ryung Son, Jay-Yoon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00740">https://arxiv.org/abs/2407.00740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00740">https://arxiv.org/pdf/2407.00740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00740]] Locate&Edit: Energy-based Text Editing for Efficient, Flexible, and Faithful Controlled Text Generation(https://arxiv.org/abs/2407.00740)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent approaches to controlled text generation (CTG) often involve manipulating the weights or logits of base language models (LMs) at decoding time. However, these methods are inapplicable to latest black-box LMs and ineffective at preserving the core semantics of the base LM's original generations. In this work, we propose Locate&Edit(L&E), an efficient and flexible energy-based approach to CTG, which edits text outputs from a base LM using off-the-shelf energy models. Given text outputs from the base LM, L&E first locates spans that are most relevant to constraints (e.g., toxicity) utilizing energy models, and then edits these spans by replacing them with more suitable alternatives. Importantly, our method is compatible with black-box LMs, as it requires only the text outputs. Also, since L&E doesn't mandate specific architecture for its component models, it can work with a diverse combination of available off-the-shelf models. Moreover, L&E preserves the base LM's original generations, by selectively modifying constraint-related aspects of the texts and leaving others unchanged. These targeted edits also ensure that L&E operates efficiently. Our experiments confirm that L&E achieves superior semantic preservation of the base LM generations and speed, while simultaneously obtaining competitive or improved constraint satisfaction. Furthermore, we analyze how the granularity of energy distribution impacts CTG performance and find that fine-grained, regression-based energy models improve constraint satisfaction, compared to conventional binary classifier energy models.</li>
<li><strong>摘要：</strong>受控文本生成 (CTG) 的最新方法通常涉及在解码时操纵基础语言模型 (LM) 的权重或逻辑。然而，这些方法不适用于最新的黑盒 LM，并且无法有效保留基础 LM 原始生成的核心语义。在这项工作中，我们提出了 Locate&Edit(L&E)，这是一种高效且灵活的基于能量的 CTG 方法，它使用现成的能量模型编辑来自基础 LM 的文本输出。给定来自基础 LM 的文本输出，L&E 首先利用能量模型找到与约束（例如毒性）最相关的跨度，然后通过用更合适的替代方案替换它们来编辑这些跨度。重要的是，我们的方法与黑盒 LM 兼容，因为它只需要文本输出。此外，由于 L&E 不要求其组件模型使用特定的架构，因此它可以与各种可用的现成模型组合一起使用。此外，L&E 通过选择性地修改文本中与约束相关的方面并保持其他方面不变，保留了基础 LM 的原始生成。这些有针对性的编辑还确保了 L&E 高效运行。我们的实验证实，L&E 实现了基础 LM 生成和速度的卓越语义保存，同时获得了有竞争力或改进的约束满足。此外，我们分析了能量分布的粒度如何影响 CTG 性能，并发现与传统的二元分类器能量模型相比，细粒度的基于回归的能量模型可以提高约束满足。</li>
</ul>

<h3>Title: A Comparative Study of Quality Evaluation Methods for Text Summarization</h3>
<ul>
<li><strong>Authors: </strong>Huyen Nguyen, Haihua Chen, Lavanya Pobbathi, Junhua Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00747">https://arxiv.org/abs/2407.00747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00747">https://arxiv.org/pdf/2407.00747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00747]] A Comparative Study of Quality Evaluation Methods for Text Summarization(https://arxiv.org/abs/2407.00747)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Evaluating text summarization has been a challenging task in natural language processing (NLP). Automatic metrics which heavily rely on reference summaries are not suitable in many situations, while human evaluation is time-consuming and labor-intensive. To bridge this gap, this paper proposes a novel method based on large language models (LLMs) for evaluating text summarization. We also conducts a comparative study on eight automatic metrics, human evaluation, and our proposed LLM-based method. Seven different types of state-of-the-art (SOTA) summarization models were evaluated. We perform extensive experiments and analysis on datasets with patent documents. Our results show that LLMs evaluation aligns closely with human evaluation, while widely-used automatic metrics such as ROUGE-2, BERTScore, and SummaC do not and also lack consistency. Based on the empirical comparison, we propose a LLM-powered framework for automatically evaluating and improving text summarization, which is beneficial and could attract wide attention among the community.</li>
<li><strong>摘要：</strong>评估文本摘要一直是自然语言处理 (NLP) 中的一项艰巨任务。严重依赖参考摘要的自动指标在许多情况下并不适用，而人工评估则耗时且费力。为了弥补这一差距，本文提出了一种基于大型语言模型 (LLM) 的评估文本摘要的新方法。我们还对八个自动指标、人工评估和我们提出的基于 LLM 的方法进行了比较研究。对七种不同类型的最先进 (SOTA) 摘要模型进行了评估。我们对包含专利文档的数据集进行了广泛的实验和分析。我们的结果表明，LLM 评估与人工评估非常接近，而广泛使用的自动指标（如 ROUGE-2、BERTScore 和 SummaC）则不接近，而且缺乏一致性。基于实证比较，我们提出了一个基于 LLM 的自动评估和改进文本摘要的框架，这是有益的，可能会引起社区的广泛关注。</li>
</ul>

<h3>Title: Characterizing Stereotypical Bias from Privacy-preserving Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Stefan Arnold, Rene Gröbner, Annika Schreiner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00764">https://arxiv.org/abs/2407.00764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00764">https://arxiv.org/pdf/2407.00764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00764]] Characterizing Stereotypical Bias from Privacy-preserving Pre-Training(https://arxiv.org/abs/2407.00764)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Differential Privacy (DP) can be applied to raw text by exploiting the spatial arrangement of words in an embedding space. We investigate the implications of such text privatization on Language Models (LMs) and their tendency towards stereotypical associations. Since previous studies documented that linguistic proficiency correlates with stereotypical bias, one could assume that techniques for text privatization, which are known to degrade language modeling capabilities, would cancel out undesirable biases. By testing BERT models trained on texts containing biased statements primed with varying degrees of privacy, our study reveals that while stereotypical bias generally diminishes when privacy is tightened, text privatization does not uniformly equate to diminishing bias across all social domains. This highlights the need for careful diagnosis of bias in LMs that undergo text privatization.</li>
<li><strong>摘要：</strong>通过利用嵌入空间中单词的空间排列，可以将差异隐私 (DP) 应用于原始文本。我们研究了这种文本私有化对语言模型 (LM) 的影响及其对刻板联想的倾向。由于先前的研究表明语言能力与刻板偏见相关，因此可以假设文本私有化技术（已知会降低语言建模能力）会消除不良偏见。通过测试在包含具有不同程度隐私的偏见陈述的文本上训练的 BERT 模型，我们的研究表明，虽然刻板偏见通常会在隐私收紧时减少，但文本私有化并不等同于在所有社交领域减少偏见。这凸显了仔细诊断经过文本私有化的 LM 中的偏见的必要性。</li>
</ul>

<h3>Title: Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00782">https://arxiv.org/abs/2407.00782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00782">https://arxiv.org/pdf/2407.00782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00782]] Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning(https://arxiv.org/abs/2407.00782)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has proven effective at improving the performance of large language models (LLMs) on downstream tasks such as reasoning and alignment. In this work, we propose Step-Controlled DPO (SCDPO), a method for automatically providing stepwise error supervision by creating negative samples of mathematical reasoning rationales that start making errors at a specified step. By applying these samples in DPO training, SCDPO can better align the model to understand reasoning errors and output accurate reasoning steps. We apply SCDPO to both code-integrated and chain-of-thought solutions, empirically showing that it consistently improves the performance compared to naive DPO on three different SFT models, including one existing SFT model and two models we finetuned. Qualitative analysis of the credit assignment of SCDPO and DPO demonstrates the effectiveness of SCDPO at identifying errors in mathematical solutions. We then apply SCDPO to an InternLM2-20B model, resulting in a 20B model that achieves high scores of 88.5% on GSM8K and 58.1% on MATH, rivaling all other open-source LLMs, showing the great potential of our method.</li>
<li><strong>摘要：</strong>直接偏好优化 (DPO) 已被证明可有效提高大型语言模型 (LLM) 在推理和对齐等下游任务上的性能。在这项工作中，我们提出了分步控制 DPO (SCDPO)，这是一种通过创建在指定步骤开始出错的数学推理原理负样本来自动提供分步错误监督的方法。通过在 DPO 训练中应用这些样本，SCDPO 可以更好地对齐模型以了解推理错误并输出准确的推理步骤。我们将 SCDPO 应用于代码集成和思路链解决方案，经验表明，与朴素 DPO 相比，它在三种不同的 SFT 模型上持续提高了性能，包括一个现有的 SFT 模型和两个我们微调的模型。对 SCDPO 和 DPO 信用分配的定性分析证明了 SCDPO 在识别数学解决方案中的错误方面的有效性。然后，我们将 SCDPO 应用于 InternLM2-20B 模型，结果 20B 模型在 GSM8K 上取得了 88.5% 的高分，在 MATH 上取得了 58.1% 的高分，可与所有其他开源 LLM 相媲美，展示了我们方法的巨大潜力。</li>
</ul>

<h3>Title: NAIST Simultaneous Speech Translation System for IWSLT 2024</h3>
<ul>
<li><strong>Authors: </strong>Yuka Ko, Ryo Fukuda, Yuta Nishikawa, Yasumasa Kano, Tomoya Yanagita, Kosuke Doi, Mana Makinae, Haotian Tan, Makoto Sakai, Sakriani Sakti, Katsuhito Sudoh, Satoshi Nakamura</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00826">https://arxiv.org/abs/2407.00826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00826">https://arxiv.org/pdf/2407.00826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00826]] NAIST Simultaneous Speech Translation System for IWSLT 2024(https://arxiv.org/abs/2407.00826)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper describes NAIST's submission to the simultaneous track of the IWSLT 2024 Evaluation Campaign: English-to-{German, Japanese, Chinese} speech-to-text translation and English-to-Japanese speech-to-speech translation. We develop a multilingual end-to-end speech-to-text translation model combining two pre-trained language models, HuBERT and mBART. We trained this model with two decoding policies, Local Agreement (LA) and AlignAtt. The submitted models employ the LA policy because it outperformed the AlignAtt policy in previous models. Our speech-to-speech translation method is a cascade of the above speech-to-text model and an incremental text-to-speech (TTS) module that incorporates a phoneme estimation model, a parallel acoustic model, and a parallel WaveGAN vocoder. We improved our incremental TTS by applying the Transformer architecture with the AlignAtt policy for the estimation model. The results show that our upgraded TTS module contributed to improving the system performance.</li>
<li><strong>摘要：</strong>本文介绍了 NAIST 向 IWSLT 2024 评估活动同步轨道提交的论文：英语到{德语、日语、中文}语音到文本翻译和英语到日语语音到语音翻译。我们开发了一种多语言端到端语音到文本翻译模型，该模型结合了两个预训练语言模型 HuBERT 和 mBART。我们使用两种解码策略（局部一致性 (LA) 和 AlignAtt）训练了该模型。提交的模型采用 LA 策略，因为它的表现优于以前模型中的 AlignAtt 策略。我们的语音到语音翻译方法是上述语音到文本模型和增量文本到语音 (TTS) 模块的级联，该模块包含音素估计模型、并行声学模型和并行 WaveGAN 声码器。我们通过将 Transformer 架构与估计模型的 AlignAtt 策略一起应用于增量 TTS，从而改进了增量 TTS。结果表明，我们升级的 TTS 模块有助于提高系统性能。</li>
</ul>

<h3>Title: Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00869">https://arxiv.org/abs/2407.00869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00869">https://arxiv.org/pdf/2407.00869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00869]] Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks(https://arxiv.org/abs/2407.00869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.</li>
<li><strong>摘要：</strong>我们发现语言模型难以生成谬误和欺骗性推理。当被要求生成欺骗性输出时，语言模型往往会泄露诚实的对应物，但认为它们是错误的。利用这一缺陷，我们提出了一种越狱攻击方法，该方法可以引出一个对齐的语言模型来进行恶意输出。具体来说，我们查询模型以生成一个错误但具有欺骗性的真实程序来处理有害行为。由于 LLM 通常认为谬误程序是假的，因此无害，因此它有助于绕过保护机制。然而，输出实际上是有害的，因为 LLM 不能编造错误的解决方案，但会提出真实的解决方案。我们在五个安全对齐的大型语言模型上评估了我们的方法，比较了四种以前的越狱方法，并表明我们的方法在更具危害性的输出下实现了具有竞争力的性能。我们相信这些发现可以扩展到模型安全性之外，例如自我验证和幻觉。</li>
</ul>

<h3>Title: Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles</h3>
<ul>
<li><strong>Authors: </strong>Ryan Louie (1), Ananjan Nandi (1), William Fang (1), Cheng Chang (1), Emma Brunskill (1), Diyi Yang (1) ((1) Stanford University)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00870">https://arxiv.org/abs/2407.00870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00870">https://arxiv.org/pdf/2407.00870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00870]] Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles(https://arxiv.org/abs/2407.00870)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. To address this, we develop Roleplay-doh, a novel human-LLM collaboration pipeline that elicits qualitative feedback from a domain-expert, which is transformed into a set of principles, or natural language rules, that govern an LLM-prompted roleplay. We apply this pipeline to enable senior mental health supporters to create customized AI patients for simulated practice partners for novice counselors. After uncovering issues in GPT-4 simulations not adhering to expert-defined principles, we also introduce a novel principle-adherence prompting pipeline which shows 30\% improvements in response quality and principle following for the downstream task. Via a user study with 25 counseling experts, we demonstrate that the pipeline makes it easy and effective to create AI patients that more faithfully resemble real patients, as judged by creators and third-party counselors.</li>
<li><strong>摘要：</strong>最近的研究利用 LLM 来扮演现实的社交场景，帮助新手练习社交技能。然而，模拟敏感的互动（例如心理健康方面的互动）具有挑战性。隐私问题限制了数据访问，而收集专家反馈虽然至关重要，但却很费力。为了解决这个问题，我们开发了 Roleplay-doh，这是一种新颖的人机 LLM 协作管道，可以从领域专家那里获得定性反馈，并将其转化为一组原则或自然语言规则，用于管理 LLM 提示的角色扮演。我们应用此管道使高级心理健康支持者能够为新手咨询师的模拟实践伙伴创建定制的 AI 患者。在发现 GPT-4 模拟中不遵循专家定义的原则的问题后，我们还引入了一种新颖的原则遵循提示管道，该管道显示下游任务的响应质量和原则遵循率提高了 30\%。通过对 25 位咨询专家进行的用户研究，我们证明该流程可以轻松有效地创建更忠实地类似于真实患者的 AI 患者，这由创造者和第三方咨询师进行评判。</li>
</ul>

<h3>Title: MoE-CT: A Novel Approach For Large Language Models Training With Resistance To Catastrophic Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Li, Shangjie Li, Binbin Xie, Deyi Xiong, Baosong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00875">https://arxiv.org/abs/2407.00875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00875">https://arxiv.org/pdf/2407.00875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00875]] MoE-CT: A Novel Approach For Large Language Models Training With Resistance To Catastrophic Forgetting(https://arxiv.org/abs/2407.00875)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) has predominantly catered to high-resource languages, leaving a disparity in performance for low-resource languages. Conventional Continual Training (CT) approaches to bridge this gap often undermine a model's original linguistic proficiency when expanding to multilingual contexts. Addressing this issue, we introduce a novel MoE-CT architecture, a paradigm that innovatively separates the base model's learning from the multilingual expansion process. Our design freezes the original LLM parameters, thus safeguarding its performance in high-resource languages, while an appended MoE module, trained on diverse language datasets, augments low-resource language proficiency. Our approach significantly outperforms conventional CT methods, as evidenced by our experiments, which show marked improvements in multilingual benchmarks without sacrificing the model's original language performance. Moreover, our MoE-CT framework demonstrates enhanced resistance to forgetting and superior transfer learning capabilities. By preserving the base model's integrity and focusing on strategic parameter expansion, our methodology advances multilingual language modeling and represents a significant step forward for low-resource language inclusion in LLMs, indicating a fruitful direction for future research in language technologies.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现主要针对高资源语言，而低资源语言的性能则存在差距。传统的持续训练 (CT) 方法弥补了这一差距，但在扩展到多语言环境时，往往会破坏模型的原始语言能力。为了解决这个问题，我们引入了一种新颖的 MoE-CT 架构，这种范式创新地将基础模型的学习与多语言扩展过程分开。我们的设计冻结了原始 LLM 参数，从而保证了其在高资源语言中的性能，而附加的 MoE 模块（在不同语言数据集上进行训练）则增强了低资源语言能力。我们的方法明显优于传统的 CT 方法，我们的实验证明了这一点，实验在多语言基准测试中显示出显着的改进，而不会牺牲模型的原始语言性能。此外，我们的 MoE-CT 框架表现出增强的抗遗忘能力和卓越的迁移学习能力。通过保持基础模型的完整性并专注于战略参数扩展，我们的方法推动了多语言建模的发展，并代表了 LLM 中低资源语言纳入的重要一步，为未来语言技术的研究指明了富有成效的方向。</li>
</ul>

<h3>Title: How to Leverage Digit Embeddings to Represent Numbers?</h3>
<ul>
<li><strong>Authors: </strong>Jasivan Alex Sivakumar, Nafise Sadat Moosavi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00894">https://arxiv.org/abs/2407.00894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00894">https://arxiv.org/pdf/2407.00894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00894]] How to Leverage Digit Embeddings to Represent Numbers?(https://arxiv.org/abs/2407.00894)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Apart from performing arithmetic operations, understanding numbers themselves is still a challenge for existing language models. Simple generalisations, such as solving 100+200 instead of 1+2, can substantially affect model performance (Sivakumar and Moosavi, 2023). Among various techniques, character-level embeddings of numbers have emerged as a promising approach to improve number representation. However, this method has limitations as it leaves the task of aggregating digit representations to the model, which lacks direct supervision for this process. In this paper, we explore the use of mathematical priors to compute aggregated digit embeddings and explicitly incorporate these aggregates into transformer models. This can be achieved either by adding a special token to the input embeddings or by introducing an additional loss function to enhance correct predictions. We evaluate the effectiveness of incorporating this explicit aggregation, analysing its strengths and shortcomings, and discuss future directions to better benefit from this approach. Our methods, while simple, are compatible with any pretrained model and require only a few lines of code, which we have made publicly available.</li>
<li><strong>摘要：</strong>除了执行算术运算之外，理解数字本身仍然是现有语言模型面临的挑战。简单的概括，例如解决 100+200 而不是 1+2，可以显著影响模型性能（Sivakumar 和 Moosavi，2023 年）。在各种技术中，字符级数字嵌入已成为一种有前途的改进数字表示的方法。然而，这种方法有局限性，因为它将聚合数字表示的任务留给了模型，而模型缺乏对这一过程的直接监督。在本文中，我们探索了使用数学先验来计算聚合数字嵌入，并将这些聚合明确地合并到 Transformer 模型中。这可以通过在输入嵌入中添加特殊标记或引入额外的损失函数来增强正确预测来实现。我们评估了合并这种显式聚合的有效性，分析了它的优点和缺点，并讨论了更好地从这种方法中受益的未来方向。我们的方法虽然简单，但与任何预训练模型兼容，并且只需要几行代码，我们已经将其公开。</li>
</ul>

<h3>Title: FineSurE: Fine-grained Summarization Evaluation using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, Saab Mansour</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] FineSurE: Fine-grained Summarization Evaluation using LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Automated evaluation is crucial for streamlining text summarization benchmarking and model development, given the costly and time-consuming nature of human evaluation. Traditional methods like ROUGE do not correlate well with human judgment, while recently proposed LLM-based metrics provide only summary-level assessment using Likert-scale scores. This limits deeper model analysis, e.g., we can only assign one hallucination score at the summary level, while at the sentence level, we can count sentences containing hallucinations. To remedy those limitations, we propose FineSurE, a fine-grained evaluator specifically tailored for the summarization task using large language models (LLMs). It also employs completeness and conciseness criteria, in addition to faithfulness, enabling multi-dimensional assessment. We compare various open-source and proprietary LLMs as backbones for FineSurE. In addition, we conduct extensive benchmarking of FineSurE against SOTA methods including NLI-, QA-, and LLM-based methods, showing improved performance especially on the completeness and conciseness dimensions. The code is available at this https URL.</li>
<li><strong>摘要：</strong>鉴于人工评估成本高昂且耗时，自动评估对于简化文本摘要基准测试和模型开发至关重要。传统方法（如 ROUGE）与人类判断的相关性不高，而最近提出的基于 LLM 的指标仅使用李克特量表分数提供摘要级别的评估。这限制了更深层次的模型分析，例如，我们只能在摘要级别分配一个幻觉分数，而在句子级别，我们可以计算包含幻觉的句子。为了弥补这些限制，我们提出了 FineSurE，这是一种细粒度评估器，专门针对使用大型语言模型 (LLM) 的摘要任务量身定制。除了忠实度之外，它还采用完整性和简洁性标准，从而实现多维评估。我们将各种开源和专有 LLM 作为 FineSurE 的主干进行比较。此外，我们对 FineSurE 与 SOTA 方法（包括基于 NLI、QA 和 LLM 的方法）进行了广泛的基准测试，结果显示其性能有所提高，尤其是在完整性和简洁性维度上。代码可在此 https URL 上获取。</li>
</ul>

<h3>Title: EXCGEC: A Benchmark of Edit-wise Explainable Chinese Grammatical Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Jingheng Ye, Shang Qin, Yinghui Li, Xuxin Cheng, Libo Qin, Hai-Tao Zheng, Peng Xing, Zishan Xu, Guo Cheng, Zhao Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00924">https://arxiv.org/abs/2407.00924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00924">https://arxiv.org/pdf/2407.00924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00924]] EXCGEC: A Benchmark of Edit-wise Explainable Chinese Grammatical Error Correction(https://arxiv.org/abs/2407.00924)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Existing studies explore the explainability of Grammatical Error Correction (GEC) in a limited scenario, where they ignore the interaction between corrections and explanations. To bridge the gap, this paper introduces the task of EXplainable GEC (EXGEC), which focuses on the integral role of both correction and explanation tasks. To facilitate the task, we propose EXCGEC, a tailored benchmark for Chinese EXGEC consisting of 8,216 explanation-augmented samples featuring the design of hybrid edit-wise explanations. We benchmark several series of LLMs in multiple settings, covering post-explaining and pre-explaining. To promote the development of the task, we introduce a comprehensive suite of automatic metrics and conduct human evaluation experiments to demonstrate the human consistency of the automatic metrics for free-text explanations. All the codes and data will be released after the review.</li>
<li><strong>摘要：</strong>现有研究在有限的场景中探索了语法错误纠正 (GEC) 的可解释性，忽略了纠正和解释之间的相互作用。为了弥补这一空白，本文引入了可解释 GEC (EXGEC) 任务，该任务侧重于纠正和解释任务的综合作用。为了促进该任务的完成，我们提出了 EXCGEC，这是一个针对中文 EXGEC 量身定制的基准测试，由 8,216 个解释增强样本组成，具有混合编辑式解释的设计。我们在多种设置中对几个系列的 LLM 进行了基准测试，涵盖了后解释和预解释。为了促进该任务的发展，我们引入了一套全面的自动指标并进行人工评估实验，以证明自由文本解释的自动指标与人类的一致性。所有代码和数据将在评审后发布。</li>
</ul>

<h3>Title: Large Language Model Enhanced Knowledge Representation Learning: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Zirui Chen, Haofen Wang, Leong Hou U, Zhao Li, Wenbin Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00936">https://arxiv.org/abs/2407.00936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00936">https://arxiv.org/pdf/2407.00936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00936]] Large Language Model Enhanced Knowledge Representation Learning: A Survey(https://arxiv.org/abs/2407.00936)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) with Knowledge Representation Learning (KRL) signifies a pivotal advancement in the field of artificial intelligence, enhancing the ability to capture and utilize complex knowledge structures. This synergy leverages the advanced linguistic and contextual understanding capabilities of LLMs to improve the accuracy, adaptability, and efficacy of KRL, thereby expanding its applications and potential. Despite the increasing volume of research focused on embedding LLMs within the domain of knowledge representation, a thorough review that examines the fundamental components and processes of these enhanced models is conspicuously absent. Our survey addresses this by categorizing these models based on three distinct Transformer architectures, and by analyzing experimental data from various KRL downstream tasks to evaluate the strengths and weaknesses of each approach. Finally, we identify and explore potential future research directions in this emerging yet underexplored domain, proposing pathways for continued progress.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 与知识表示学习 (KRL) 的结合标志着人工智能领域的一项关键进步，增强了捕获和利用复杂知识结构的能力。这种协同作用利用 LLM 先进的语言和上下文理解能力来提高 KRL 的准确性、适应性和有效性，从而扩大其应用和潜力。尽管越来越多的研究专注于将 LLM 嵌入知识表示领域，但对这些增强模型的基本组件和过程进行全面研究的综述却明显缺失。我们的调查通过基于三种不同的 Transformer 架构对这些模型进行分类，并通过分析来自各种 KRL 下游任务的实验数据来评估每种方法的优缺点来解决这个问题。最后，我们确定并探索这个新兴但尚未充分探索的领域未来的潜在研究方向，并提出继续取得进展的途径。</li>
</ul>

<h3>Title: MalAlgoQA: A Pedagogical Approach for Evaluating Counterfactual Reasoning Abilities</h3>
<ul>
<li><strong>Authors: </strong>Naiming Liu, Shashank Sonkar, Myco Le, Richard Baraniuk</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00938">https://arxiv.org/abs/2407.00938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00938">https://arxiv.org/pdf/2407.00938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00938]] MalAlgoQA: A Pedagogical Approach for Evaluating Counterfactual Reasoning Abilities(https://arxiv.org/abs/2407.00938)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>This paper introduces MalAlgoQA, a novel dataset designed to evaluate the counterfactual reasoning capabilities of Large Language Models (LLMs) through a pedagogical approach. The dataset comprises mathematics and reading comprehension questions, each accompanied by four answer choices and their corresponding rationales. We focus on the incorrect answer rationales, termed "malgorithms", which highlights flawed reasoning steps leading to incorrect answers and offers valuable insights into erroneous thought processes. We also propose the Malgorithm Identification task, where LLMs are assessed based on their ability to identify corresponding malgorithm given an incorrect answer choice. To evaluate the model performance, we introduce two metrics: Algorithm Identification Accuracy (AIA) for correct answer rationale identification, and Malgorithm Identification Accuracy (MIA) for incorrect answer rationale identification. The task is challenging since state-of-the-art LLMs exhibit significant drops in MIA as compared to AIA. Moreover, we find that the chain-of-thought prompting technique not only fails to consistently enhance MIA, but can also lead to underperformance compared to simple prompting. These findings hold significant implications for the development of more cognitively-inspired LLMs to improve their counterfactual reasoning abilities, particularly through a pedagogical perspective where understanding and rectifying student misconceptions are crucial.</li>
<li><strong>摘要：</strong>本文介绍了 MalAlgoQA，这是一种新颖的数据集，旨在通过教学方法评估大型语言模型 (LLM) 的反事实推理能力。该数据集包括数学和阅读理解问题，每个问题都附有四个答案选项及其相应的理由。我们专注于错误答案的理由，称为“malgorithms”，它突出了导致错误答案的有缺陷的推理步骤，并提供了对错误思维过程的宝贵见解。我们还提出了 Malgorithm 识别任务，其中 LLM 的评估基于它们在给定错误答案选项的情况下识别相应 malgorithm 的能力。为了评估模型性能，我们引入了两个指标：用于正确答案理由识别的算法识别准确度 (AIA) 和用于错误答案理由识别的 Malgorithm 识别准确度 (MIA)。这项任务具有挑战性，因为与 AIA 相比，最先进的 LLM 在 MIA 方面表现出显著下降。此外，我们发现，思路链提示法不仅无法持续提高 MIA，而且与简单提示法相比，还可能导致表现不佳。这些发现对于培养更多受认知启发的法学硕士以提高他们的反事实推理能力具有重要意义，特别是从教学角度来看，理解和纠正学生的错误观念至关重要。</li>
</ul>

<h3>Title: The House Always Wins: A Framework for Evaluating Strategic Deception in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tanush Chopra, Michael Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00948">https://arxiv.org/abs/2407.00948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00948">https://arxiv.org/pdf/2407.00948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00948]] The House Always Wins: A Framework for Evaluating Strategic Deception in LLMs(https://arxiv.org/abs/2407.00948)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>We propose a framework for evaluating strategic deception in large language models (LLMs). In this framework, an LLM acts as a game master in two scenarios: one with random game mechanics and another where it can choose between random or deliberate actions. As an example, we use blackjack because the action space nor strategies involve deception. We benchmark Llama3-70B, GPT-4-Turbo, and Mixtral in blackjack, comparing outcomes against expected distributions in fair play to determine if LLMs develop strategies favoring the "house." Our findings reveal that the LLMs exhibit significant deviations from fair play when given implicit randomness instructions, suggesting a tendency towards strategic manipulation in ambiguous scenarios. However, when presented with an explicit choice, the LLMs largely adhere to fair play, indicating that the framing of instructions plays a crucial role in eliciting or mitigating potentially deceptive behaviors in AI systems.</li>
<li><strong>摘要：</strong>我们提出了一个用于评估大型语言模型 (LLM) 中的战略欺骗的框架。在这个框架中，LLM 在两种情况下充当游戏大师：一种是随机游戏机制，另一种是可以选择随机或故意行动。例如，我们使用二十一点，因为行动空间和策略都涉及欺骗。我们在二十一点中对 Llama3-70B、GPT-4-Turbo 和 Mixtral 进行了基准测试，将结果与公平竞争中的预期分布进行比较，以确定 LLM 是否制定了有利于“庄家”的策略。我们的研究结果表明，当给出隐含的随机性指令时，LLM 表现出与公平竞争的显著偏差，这表明在模糊场景中倾向于战略操纵。然而，当给出明确的选择时，LLM 基本上遵循公平竞争，这表明指令的框架在引发或减轻 AI 系统中潜在的欺骗行为方面起着至关重要的作用。</li>
</ul>

<h3>Title: LLM Uncertainty Quantification through Directional Entailment Graph and Claim Level Response Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Longchao Da, Tiejin Chen, Lu Cheng, Hua Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00994">https://arxiv.org/abs/2407.00994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00994">https://arxiv.org/pdf/2407.00994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00994]] LLM Uncertainty Quantification through Directional Entailment Graph and Claim Level Response Augmentation(https://arxiv.org/abs/2407.00994)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>The Large language models (LLMs) have showcased superior capabilities in sophisticated tasks across various domains, stemming from basic question-answer (QA), they are nowadays used as decision assistants or explainers for unfamiliar content. However, they are not always correct due to the data sparsity in specific domain corpus, or the model's hallucination problems. Given this, how much should we trust the responses from LLMs? This paper presents a novel way to evaluate the uncertainty that captures the directional instability, by constructing a directional graph from entailment probabilities, and we innovatively conduct Random Walk Laplacian given the asymmetric property of a constructed directed graph, then the uncertainty is aggregated by the derived eigenvalues from the Laplacian process. We also provide a way to incorporate the existing work's semantics uncertainty with our proposed layer. Besides, this paper identifies the vagueness issues in the raw response set and proposes an augmentation approach to mitigate such a problem, we conducted extensive empirical experiments and demonstrated the superiority of our proposed solutions.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 源自基本的问答 (QA)，已在各个领域的复杂任务中展现出卓越的能力，如今它们被用作决策助手或不熟悉内容的解释器。然而，由于特定领域语料库的数据稀疏性或模型的幻觉问题，它们并不总是正确的。鉴于此，我们应该在多大程度上信任 LLM 的回应？本文提出了一种评估捕捉方向不稳定性不确定性的新方法，通过从蕴涵概率构建方向图，并且我们创新地根据构造的有向图的不对称性质进行随机游走拉普拉斯算子，然后通过从拉普拉斯过程中得出的特征值聚合不确定性。我们还提供了一种将现有工作的语义不确定性与我们提出的层结合起来的方法。此外，本文确定了原始响应集中的模糊性问题，并提出了一种增强方法来缓解这种问题，我们进行了大量的实证实验并证明了我们提出的解决方案的优越性。</li>
</ul>

<h3>Title: Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?</h3>
<ul>
<li><strong>Authors: </strong>Nicy Scaria, Silvester John Joseph Kennedy, Deepak Subramani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00996">https://arxiv.org/abs/2407.00996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00996">https://arxiv.org/pdf/2407.00996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00996]] Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?(https://arxiv.org/abs/2407.00996)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Small Language Models (SLMs) are generally considered to be more compact versions of large language models (LLMs), typically having fewer than 7 billion parameters. This study investigates the ability of small language models to learn, retain, and subsequently eliminate noise that is typically not found on the internet, where most pretraining datasets are sourced. For this, four pre-trained SLMs were utilized: Olmo 1B, Qwen1.5 1.8B, Gemma 2B, and Phi2 2.7B. The models were instruction-tuned without noise and tested for task execution with in-context learning. Afterward, noise patterns were introduced to evaluate the models' learning and unlearning capabilities. We evaluated the models' performance at various training levels. Phi consistently excelled with word-level noise but performed the worst with character-level noise. Despite being the smallest with approximately 1 billion parameters, Olmo performed consistently well on tasks.</li>
<li><strong>摘要：</strong>小型语言模型 (SLM) 通常被认为是大型语言模型 (LLM) 的更紧凑版本，通常具有少于 70 亿个参数。本研究调查了小型语言模型学习、保留和随后消除互联网上通常找不到的噪音的能力，大多数预训练数据集都来自互联网。为此，使用了四个预训练的 SLM：Olmo 1B、Qwen1.5 1.8B、Gemma 2B 和 Phi2 2.7B。这些模型在没有噪音的情况下进行了指令调整，并通过上下文学习测试了任务执行情况。之后，引入了噪声模式来评估模型的学习和反学习能力。我们在不同的训练级别评估了模型的表现。Phi 在单词级噪音方面始终表现出色，但在字符级噪音方面表现最差。尽管 Olmo 是最小的，只有大约 10 亿个参数，但它在任务上始终表现良好。</li>
</ul>

<h3>Title: Engineering Conversational Search Systems: A Review of Applications, Architectures, and Functional Components</h3>
<ul>
<li><strong>Authors: </strong>Phillip Schneider, Wessel Poelman, Michael Rovatsos, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.00997">https://arxiv.org/abs/2407.00997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.00997">https://arxiv.org/pdf/2407.00997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.00997]] Engineering Conversational Search Systems: A Review of Applications, Architectures, and Functional Components(https://arxiv.org/abs/2407.00997)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Conversational search systems enable information retrieval via natural language interactions, with the goal of maximizing users' information gain over multiple dialogue turns. The increasing prevalence of conversational interfaces adopting this search paradigm challenges traditional information retrieval approaches, stressing the importance of better understanding the engineering process of developing these systems. We undertook a systematic literature review to investigate the links between theoretical studies and technical implementations of conversational search systems. Our review identifies real-world application scenarios, system architectures, and functional components. We consolidate our results by presenting a layered architecture framework and explaining the core functions of conversational search systems. Furthermore, we reflect on our findings in light of the rapid progress in large language models, discussing their capabilities, limitations, and directions for future research.</li>
<li><strong>摘要：</strong>对话式搜索系统通过自然语言交互实现信息检索，目的是在多轮对话中最大限度地提高用户的信息获取。采用这种搜索模式的对话式界面越来越流行，这对传统的信息检索方法提出了挑战，强调了更好地理解开发这些系统的工程过程的重要性。我们进行了系统的文献综述，以调查对话式搜索系统的理论研究与技术实现之间的联系。我们的综述确定了现实世界的应用场景、系统架构和功能组件。我们通过展示分层架构框架和解释对话式搜索系统的核心功能来巩固我们的成果。此外，我们根据大型语言模型的快速发展反思我们的研究结果，讨论它们的能力、局限性和未来研究的方向。</li>
</ul>

<h3>Title: DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiabao Pan, Yan Zhang, Chen Zhang, Zuozhu Liu, Hongwei Wang, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01009">https://arxiv.org/abs/2407.01009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01009">https://arxiv.org/pdf/2407.01009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01009]] DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models(https://arxiv.org/abs/2407.01009)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with complicated problems, while a thorough method, which considers multiple reasoning pathways and verifies each step carefully, results in slower inference. This paper addresses the challenge of enabling LLMs to autonomously select between fast and slow inference methods, thereby optimizing both efficiency and effectiveness. We introduce a dynamic decision-making framework that categorizes tasks into two distinct pathways: 'Fast', designated for tasks where the LLM quickly identifies a high-confidence solution, and 'Slow', allocated for tasks that the LLM perceives as complex and for which it has low confidence in immediate solutions as well as requiring more reasoning paths to verify. Experiments on five popular reasoning benchmarks demonstrated the superiority of the DynaThink over baselines.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已通过流行的思维链 (COT) 提示展示了跨各种推理任务的新兴能力。然而，这种简单快速的 COT 方法在处理复杂问题时往往会遇到限制，而考虑多种推理路径并仔细验证每个步骤的全面方法会导致推理速度变慢。本文解决了使 LLM 能够自主选择快速和慢速推理方法的挑战，从而优化效率和有效性。我们引入了一个动态决策框架，将任务分为两种不同的路径：“快速”，指定用于 LLM 快速识别高置信度解决方案的任务，“慢速”，分配给 LLM 认为复杂且对即时解决方案信心较低且需要更多推理路径来验证的任务。在五个流行的推理基准上进行的实验证明了 DynaThink 优于基线。</li>
</ul>

<h3>Title: Development of Cognitive Intelligence in Pre-trained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Raj Sanjay Shah, Khushi Bhardwaj, Sashank Varma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01047">https://arxiv.org/abs/2407.01047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01047">https://arxiv.org/pdf/2407.01047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01047]] Development of Cognitive Intelligence in Pre-trained Language Models(https://arxiv.org/abs/2407.01047)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent studies show evidence for emergent cognitive abilities in Large Pre-trained Language Models (PLMs). The increasing cognitive alignment of these models has made them candidates for cognitive science theories. Prior research into the emergent cognitive abilities of PLMs has largely been path-independent to model training, i.e., has focused on the final model weights and not the intermediate steps. However, building plausible models of human cognition using PLMs would benefit from considering the developmental alignment of their performance during training to the trajectories of children's thinking. Guided by psychometric tests of human intelligence, we choose four sets of tasks to investigate the alignment of ten popular families of PLMs and evaluate their available intermediate and final training steps. These tasks are Numerical ability, Linguistic abilities, Conceptual understanding, and Fluid reasoning. We find a striking regularity: regardless of model size, the developmental trajectories of PLMs consistently exhibit a window of maximal alignment to human cognitive development. Before that window, training appears to endow "blank slate" models with the requisite structure to be poised to rapidly learn from experience. After that window, training appears to serve the engineering goal of reducing loss but not the scientific goal of increasing alignment with human cognition.</li>
<li><strong>摘要：</strong>最近的研究表明，大型预训练语言模型 (PLM) 中存在认知能力的出现。这些模型的认知一致性不断提高，使其成为认知科学理论的候选对象。之前对 PLM 的认知能力出现的研究在很大程度上与模型训练的路径无关，即专注于最终的模型权重而不是中​​间步骤。然而，使用 PLM 构建可信的人类认知模型将受益于考虑其在训练过程中的表现与儿童思维轨迹的发展一致性。在人类智力心理测试的指导下，我们选择了四组任务来研究十个流行的 PLM 系列的一致性，并评估它们可用的中间和最终训练步骤。这些任务是数字能力、语言能力、概念理解和流畅推理。我们发现了一个惊人的规律：无论模型大小如何，PLM 的发展轨迹始终表现出与人类认知发展最大一致的窗口。在此之前，训练似乎赋予了“白板”模型必要的结构，使其能够快速从经验中学习。在此之后，训练似乎服务于减少损失的工程目标，而不是提高与人类认知一致性的科学目标。</li>
</ul>

<h3>Title: Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese</h3>
<ul>
<li><strong>Authors: </strong>Yunqi Xu, Tianchi Cai, Jiyan Jiang, Xierui Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01080">https://arxiv.org/abs/2407.01080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01080">https://arxiv.org/pdf/2407.01080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01080]] Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese(https://arxiv.org/abs/2407.01080)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>The prevailing issue of factual inconsistency errors in conventional Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency Evaluation (FCE). Despite the various FCE methods proposed earlier, these methods are evaluated on datasets generated by specific Large Language Models (LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE methods perform on other LLMs with different error distributions or even unseen error types, as these methods may fail to detect the error types generated by other LLMs. To fill this gap, in this paper, we propose the first comprehensive FCE benchmark \emph{Face4RAG} for RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality inconsistency error and a real-world dataset constructed from six commonly used LLMs, enabling evaluation of FCE methods on specific error types or real-world error distributions. On the proposed benchmark, we discover the failure of existing FCE methods to detect the logical fallacy, which refers to a mismatch of logic structures between the answer and the retrieved reference. To fix this issue, we further propose a new method called \emph{L-Face4RAG} with two novel designs of logic-preserving answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG substantially outperforms previous methods for factual inconsistency detection on a wide range of tasks, notably beyond the RAG task from which it is originally motivated. Both the benchmark and our proposed method are publicly available.\footnote{\url{this https URL}\label{link_face4rag}}</li>
<li><strong>摘要：</strong>传统检索增强生成 (RAG) 中普遍存在的事实不一致错误问题，这推动了事实一致性评估 (FCE) 的研究。尽管之前提出了各种 FCE 方法，但这些方法都是在特定大型语言模型 (LLM) 生成的数据集上进行评估的。如果没有全面的基准，这些 FCE 方法在具有不同错误分布甚至未见错误类型的其他 LLM 上的表现仍未得到探索，因为这些方法可能无法检测到其他 LLM 生成的错误类型。为了填补这一空白，在本文中，我们提出了第一个独立于底层 LLM 的 RAG 综合 FCE 基准 \emph{Face4RAG}。我们的基准包括一个基于精心设计的事实不一致错误类型学的合成数据集和一个由六个常用 LLM 构建的真实世界数据集，可以评估 FCE 方法在特定错误类型或真实世界错误分布上的表现。在提出的基准测试中，我们发现现有的 FCE 方法无法检测逻辑谬误，即答案与检索到的参考之间的逻辑结构不匹配。为了解决这个问题，我们进一步提出了一种名为 \emph{L-Face4RAG} 的新方法，该方法采用了两种新颖的设计，即保留逻辑的答案分解和事实逻辑 FCE。大量实验表明，L-Face4RAG 在各种任务上的事实不一致检测方面的表现都大大优于以前的方法，尤其是在最初激发其灵感的 RAG 任务之外。基准测试和我们提出的方法都是公开可用的。\footnote{\url{此 https URL}\label{link_face4rag}}</li>
</ul>

<h3>Title: Min P Sampling: Balancing Creativity and Coherence at High Temperature</h3>
<ul>
<li><strong>Authors: </strong>Minh Nguyen, Andrew Baker, Andreas Kirsch, Clement Neo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01082">https://arxiv.org/abs/2407.01082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01082">https://arxiv.org/pdf/2407.01082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01082]] Min P Sampling: Balancing Creativity and Coherence at High Temperature(https://arxiv.org/abs/2407.01082)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) generate longform text by successively sampling the next token based on the probability distribution of the token vocabulary at each decoding step. Current popular truncation sampling methods such as top-$p$ sampling, also known as nucleus sampling, often struggle to balance coherence and creativity in generating text, particularly when using higher temperatures. To address this issue, we propose min-$p$, a dynamic truncation sampling method, that establishes a minimum base percentage threshold for tokens, which the scales according to the probability of the top candidate token. Through experiments on several benchmarks, such as GPQA, GSM8K and AlpacaEval Creative Writing, we demonstrate that min-$p$ improves the coherence and quality of generated text even at high temperatures, while also facilitating more creative and diverse outputs compared to top-$p$ and other sampling methods. As of writing, min-$p$ has been adopted by multiple open-source LLM implementations, and have been independently assessed by members of the open-source LLM community, further validating its practical utility and potential.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通过在每个解码步骤中根据标记词汇表的概率分布连续采样下一个标记来生成长文本。当前流行的截断采样方法（例如 top-$p$ 采样，也称为核采样）通常难以在生成文本时平衡连贯性和创造性，尤其是在使用较高温度时。为了解决这个问题，我们提出了 min-$p$，一种动态截断采样方法，它为标记建立最小基本百分比阈值，该阈值根据顶级候选标记的概率进行缩放。通过在 GPQA、GSM8K 和 AlpacaEval Creative Writing 等多个基准上的实验，我们证明 min-$p$ 即使在高温下也能提高生成文本的连贯性和质量，同时与 top-$p$ 和其他采样方法相比，还有助于产生更具创造性和多样性的输出。截至撰写本文时，min-$p$ 已被多个开源 LLM 实现采用，并已由开源 LLM 社区成员进行独立评估，进一步验证了其实用性和潜力。</li>
</ul>

<h3>Title: M2QA: Multi-domain Multilingual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Leon Engländer, Hannah Sterz, Clifton Poth, Jonas Pfeiffer, Ilia Kuznetsov, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01091">https://arxiv.org/abs/2407.01091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01091">https://arxiv.org/pdf/2407.01091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01091]] M2QA: Multi-domain Multilingual Question Answering(https://arxiv.org/abs/2407.01091)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Generalization and robustness to input variation are core desiderata of machine learning research. Language varies along several axes, most importantly, language instance (e.g. French) and domain (e.g. news). While adapting NLP models to new languages within a single domain, or to new domains within a single language, is widely studied, research in joint adaptation is hampered by the lack of evaluation datasets. This prevents the transfer of NLP systems from well-resourced languages and domains to non-dominant language-domain combinations. To address this gap, we introduce M2QA, a multi-domain multilingual question answering benchmark. M2QA includes 13,500 SQuAD 2.0-style question-answer instances in German, Turkish, and Chinese for the domains of product reviews, news, and creative writing. We use M2QA to explore cross-lingual cross-domain performance of fine-tuned models and state-of-the-art LLMs and investigate modular approaches to domain and language adaptation. We witness 1) considerable performance variations across domain-language combinations within model classes and 2) considerable performance drops between source and target language-domain combinations across all model sizes. We demonstrate that M2QA is far from solved, and new methods to effectively transfer both linguistic and domain-specific information are necessary. We make M2QA publicly available at this https URL.</li>
<li><strong>摘要：</strong>泛化和对输入变化的鲁棒性是机器学习研究的核心要求。语言在几个方面有所不同，最重要的是语言实例（例如法语）和领域（例如新闻）。虽然将 NLP 模型适应单个领域内的新语言或单个语言内的新领域已被广泛研究，但由于缺乏评估数据集，联合适应研究受到阻碍。这阻碍了 NLP 系统从资源丰富的语言和领域转移到非主要语言领域组合。为了解决这一差距，我们引入了 M2QA，这是一个多领域多语言问答基准。M2QA 包括 13,500 个 SQuAD 2.0 风格的德语、土耳其语和中文问答实例，适用于产品评论、新闻和创意写作领域。我们使用 M2QA 探索微调模型和最先进的 LLM 的跨语言跨领域性能，并研究领域和语言适应的模块化方法。我们发现 1) 模型类别中不同领域语言组合之间的性能差异很大，2) 所有模型大小中源语言和目标语言领域组合之间的性能下降很大。我们证明 M2QA 问题远未解决，需要采用新方法来有效传输语言和领域特定信息。我们在此 https URL 上公开提供 M2QA。</li>
</ul>

<h3>Title: IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation</h3>
<ul>
<li><strong>Authors: </strong>Senyu Han, Lu Chen, Li-Min Lin, Zhengshan Xu, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01093">https://arxiv.org/abs/2407.01093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01093">https://arxiv.org/pdf/2407.01093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01093]] IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation(https://arxiv.org/abs/2407.01093)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated their capabilities in storyline creation and human-like character role-playing. Current language model agents mainly focus on reasonable behaviors from the level of individuals, and their behaviors might be hard to constraint on the level of the whole storyline. In this paper we introduce IBSEN, a director-actor coordinate agent framework that generates drama scripts and makes the plot played by agents more controllable. The director agent writes plot outlines that the user desires to see, instructs the actor agents to role-play their characters, and reschedules the plot when human players participate in the scenario to ensure the plot is progressing towards the objective. To evaluate the framework, we create a novel drama plot that involves several actor agents and check the interactions between them under the instruction of the director agent. Evaluation results show that our framework could generate complete, diverse drama scripts from only a rough outline of plot objectives, meanwhile maintaining the characteristics of characters in the drama. Our codes and prompts are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型已展示出其在故事情节创作和类似人类的角色扮演方面的能力。当前的语言模型代理主要关注个体层面的合理行为，而其行为在整个故事情节层面可能难以约束。本文介绍了 IBSEN，这是一个导演-演员协调代理框架，它可以生成戏剧脚本并使代理所扮演的情节更加可控。导演代理编写用户希望看到的情节大纲，指示演员代理扮演他们的角色，并在人类玩家参与场景时重新安排情节以确保情节朝着目标发展。为了评估该框架，我们创建了一个涉及多个演员代理的新颖戏剧情节，并在导演代理的指示下检查他们之间的交互。评估结果表明，我们的框架可以仅从情节目标的粗略大纲生成完整、多样的戏剧脚本，同时保持戏剧中人物的特征。我们的代码和提示可在此 https URL 上找到。</li>
</ul>

<h3>Title: Eliminating Position Bias of Language Models: A Mechanistic Approach</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang, Chi Han, Shuiwang Ji, Sham M. Kakade, Hao Peng, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01100">https://arxiv.org/abs/2407.01100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01100">https://arxiv.org/pdf/2407.01100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01100]] Eliminating Position Bias of Language Models: A Mechanistic Approach(https://arxiv.org/abs/2407.01100)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Position bias has proven to be a prevalent issue of modern language models (LMs), where the models prioritize content based on its position within the given context. This bias often leads to unexpected model failures and hurts performance, robustness, and reliability across various applications. Our mechanistic analysis attributes the position bias to two components employed in nearly all state-of-the-art LMs: causal attention and relative positional encodings. Specifically, we find that causal attention generally causes models to favor distant content, while relative positional encodings like RoPE prefer nearby ones based on the analysis of retrieval-augmented question answering (QA). Further, our empirical study on object detection reveals that position bias is also present in vision-language models (VLMs). Based on the above analyses, we propose to ELIMINATE position bias caused by different input segment orders (e.g., options in LM-as-a-judge, retrieved documents in QA) in a TRAINING-FREE ZERO-SHOT manner. Our method changes the causal attention to bidirectional attention between segments and utilizes model attention values to decide the relative orders of segments instead of using the order provided in input prompts, therefore enabling Position-INvariant inferencE (PINE) at the segment level. By eliminating position bias, models achieve better performance and reliability in downstream tasks where position bias widely exists, such as LM-as-a-judge and retrieval-augmented QA. Notably, PINE is especially useful when adapting LMs for evaluating reasoning pairs: it consistently provides 8 to 10 percentage points performance gains in most cases, and makes Llama-3-70B-Instruct perform even better than GPT-4-0125-preview on the RewardBench reasoning subset.</li>
<li><strong>摘要：</strong>位置偏差已被证明是现代语言模型 (LM) 的一个普遍问题，其中模型根据内容在给定上下文中的位置对内容进行优先级排序。这种偏差通​​常会导致意外的模型失败，并损害各种应用程序的性能、稳健性和可靠性。我们的机制分析将位置偏差归因于几乎所有最先进的 LM 中采用的两个组件：因果注意和相对位置编码。具体而言，根据对检索增强型问答 (QA) 的分析，我们发现因果注意通常会导致模型偏爱远处的内容，而像 RoPE 这样的相对位置编码则偏爱近处的内容。此外，我们对物体检测的实证研究表明，位置偏差也存在于视觉语言模型 (VLM) 中。基于上述分析，我们建议以无需训练的零样本方式消除由不同输入段顺序（例如，LM-as-a-judge 中的选项、QA 中检索到的文档）引起的位置偏差。我们的方法将因果注意力转变为段之间的双向注意力，并利用模型注意力值来决定段的相对顺序，而不是使用输入提示中提供的顺序，从而在段级别启用位置不变推理 (PINE)。通过消除位置偏差，模型在位置偏差普遍存在的下游任务（例如 LM-as-a-judge 和检索增强型 QA）中实现了更好的性能和可靠性。值得注意的是，PINE 在调整 LM 以评估推理对时特别有用：它在大多数情况下始终提供 8 到 10 个百分点的性能提升，并使 Llama-3-70B-Instruct 在 RewardBench 推理子集上的表现甚至比 GPT-4-0125-preview 更好。</li>
</ul>

<h3>Title: BERGEN: A Benchmarking Library for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, Stéphane Clinchant</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01102">https://arxiv.org/abs/2407.01102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01102">https://arxiv.org/pdf/2407.01102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01102]] BERGEN: A Benchmarking Library for Retrieval-Augmented Generation(https://arxiv.org/abs/2407.01102)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \url{this https URL}.</li>
<li><strong>摘要：</strong>检索增强生成允许使用外部知识来增强大型语言模型。为了响应最近流行的生成式 LLM，已经提出了许多 RAG 方法，这些方法涉及大量不同的配置，例如评估数据集、集合、指标、检索器和 LLM。不一致的基准测试对比较方法和理解管道中每个组件的影响构成了重大挑战。在这项工作中，我们研究了为 RAG 的系统评估奠定基础的最佳实践，并提出了 BERGEN，这是一个用于标准化 RAG 实验的可重复研究的端到端库。在一项专注于 QA 的广泛研究中，我们对不同的最新检索器、重新排序器和 LLM 进行了基准测试。此外，我们还分析了现有的 RAG 指标和数据集。我们的开源库 BERGEN 可在 \url{this https URL} 下找到。</li>
</ul>

<h3>Title: Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?</h3>
<ul>
<li><strong>Authors: </strong>Guillermo Marco, Julio Gonzalo, Ramón del Castillo, María Teresa Mateo Girona</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01119">https://arxiv.org/abs/2407.01119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01119">https://arxiv.org/pdf/2407.01119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01119]] Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?(https://arxiv.org/abs/2407.01119)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>It has become routine to report research results where Large Language Models (LLMs) outperform average humans in a wide range of language-related tasks, and creative text writing is no exception. It seems natural, then, to raise the bid: Are LLMs ready to compete in creative writing skills with a top (rather than average) novelist? To provide an initial answer for this question, we have carried out a contest between Patricio Pron (an awarded novelist, considered one of the best of his generation) and GPT-4 (one of the top performing LLMs), in the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee Sidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write short stories for both their titles and their opponent's. Then, we prepared an evaluation rubric inspired by Boden's definition of creativity, and we collected 5,400 manual assessments provided by literature critics and scholars. The results of our experimentation indicate that LLMs are still far from challenging a top human creative writer, and that reaching such level of autonomous creative writing skills probably cannot be reached simply with larger language models.</li>
<li><strong>摘要：</strong>在报告研究结果时，大型语言模型 (LLM) 在广泛的语言相关任务中表现优于普通人类已成为常规，而创意文本写作也不例外。因此，提高出价似乎是自然而然的事：LLM 是否准备好与顶级（而非普通）小说家在创意写作技巧上一较高下？为了给这个问题提供初步答案，我们本着人工智能与人类决斗的精神，在 Patricio Pron（获奖小说家，被认为是他这一代中最优秀的小说家之一）和 GPT-4（表现最好的 LLM 之一）之间进行了一场比赛，例如 DeepBlue vs Kasparov 和 AlphaGo vs Lee Sidol。我们要求 Pron 和 GPT-4 各提供 30 本书，然后为他们的书和对手的书写短篇小说。然后，我们准备了一个受 Boden 对创造力定义的评估标准，并收集了文学评论家和学者提供的 5,400 份人工评估。我们的实验结果表明，LLM 还远远不能挑战顶尖的人类创意作家，而仅靠更大的语言模型可能无法达到这种水平的自主创意写作技能。</li>
</ul>

<h3>Title: Calibrated Large Language Models for Binary Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Patrizio Giovannotti, Alexander Gammerman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01122">https://arxiv.org/abs/2407.01122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01122">https://arxiv.org/pdf/2407.01122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01122]] Calibrated Large Language Models for Binary Question Answering(https://arxiv.org/abs/2407.01122)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Quantifying the uncertainty of predictions made by large language models (LLMs) in binary text classification tasks remains a challenge. Calibration, in the context of LLMs, refers to the alignment between the model's predicted probabilities and the actual correctness of its predictions. A well-calibrated model should produce probabilities that accurately reflect the likelihood of its predictions being correct. We propose a novel approach that utilizes the inductive Venn--Abers predictor (IVAP) to calibrate the probabilities associated with the output tokens corresponding to the binary labels. Our experiments on the BoolQ dataset using the Llama 2 model demonstrate that IVAP consistently outperforms the commonly used temperature scaling method for various label token choices, achieving well-calibrated probabilities while maintaining high predictive quality. Our findings contribute to the understanding of calibration techniques for LLMs and provide a practical solution for obtaining reliable uncertainty estimates in binary question answering tasks, enhancing the interpretability and trustworthiness of LLM predictions.</li>
<li><strong>摘要：</strong>量化大型语言模型 (LLM) 在二进制文本分类任务中做出的预测的不确定性仍然是一项挑战。在 LLM 的上下文中，校准是指模型的预测概率与其预测的实际正确性之间的对齐。经过良好校准的模型应产生准确反映其预测正确可能性的概率。我们提出了一种新方法，该方法利用归纳维恩-阿伯斯预测器 (IVAP) 来校准与二进制标签对应的输出标记相关的概率。我们在 BoolQ 数据集上使用 Llama 2 模型进行的实验表明，IVAP 在各种标签标记选择方面始终优于常用的温度缩放方法，在保持高预测质量的同时实现了良好校准的概率。我们的研究结果有助于理解 LLM 的校准技术，并为在二进制问答任务中获得可靠的不确定性估计提供了一种实用的解决方案，从而提高了 LLM 预测的可解释性和可信度。</li>
</ul>

<h3>Title: An Empirical Comparison of Generative Approaches for Product Attribute-Value Identification</h3>
<ul>
<li><strong>Authors: </strong>Kassem Sabeh, Robert Litschko, Mouna Kacimi, Barbara Plank, Johann Gamper</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01137">https://arxiv.org/abs/2407.01137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01137">https://arxiv.org/pdf/2407.01137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01137]] An Empirical Comparison of Generative Approaches for Product Attribute-Value Identification(https://arxiv.org/abs/2407.01137)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Product attributes are crucial for e-commerce platforms, supporting applications like search, recommendation, and question answering. The task of Product Attribute and Value Identification (PAVI) involves identifying both attributes and their values from product information. In this paper, we formulate PAVI as a generation task and provide, to the best of our knowledge, the most comprehensive evaluation of PAVI so far. We compare three different attribute-value generation (AVG) strategies based on fine-tuning encoder-decoder models on three datasets. Experiments show that end-to-end AVG approach, which is computationally efficient, outperforms other strategies. However, there are differences depending on model sizes and the underlying language model. The code to reproduce all experiments is available at: this https URL</li>
<li><strong>摘要：</strong>产品属性对于电子商务平台至关重要，支持搜索、推荐和问答等应用。产品属性和值识别 (PAVI) 的任务涉及从产品信息中识别属性及其值。在本文中，我们将 PAVI 制定为生成任务，并据我们所知提供迄今为止对 PAVI 最全面的评估。我们在三个数据集上比较了基于微调编码器-解码器模型的三种不同的属性值生成 (AVG) 策略。实验表明，计算效率高的端到端 AVG 方法优于其他策略。但是，根据模型大小和底层语言模型，存在差异。重现所有实验的代码可在以下位置获得：此 https URL</li>
</ul>

<h3>Title: Learning to Explore and Select for Coverage-Conditioned Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Takyoung Kim, Kyungjae Lee, Young Rok Jang, Ji Yong Cho, Gangwoo Kim, Minseok Cho, Moontae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01158">https://arxiv.org/abs/2407.01158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01158">https://arxiv.org/pdf/2407.01158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01158]] Learning to Explore and Select for Coverage-Conditioned Retrieval-Augmented Generation(https://arxiv.org/abs/2407.01158)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Interactions with billion-scale large language models typically yield long-form responses due to their extensive parametric capacities, along with retrieval-augmented features. While detailed responses provide insightful viewpoint of a specific subject, they frequently generate redundant and less engaging content that does not meet user interests. In this work, we focus on the role of query outlining (i.e., selected sequence of queries) in scenarios that users request a specific range of information, namely coverage-conditioned ($C^2$) scenarios. For simulating $C^2$ scenarios, we construct QTree, 10K sets of information-seeking queries decomposed with various perspectives on certain topics. By utilizing QTree, we train QPlanner, a 7B language model generating customized query outlines that follow coverage-conditioned queries. We analyze the effectiveness of generated outlines through automatic and human evaluation, targeting on retrieval-augmented generation (RAG). Moreover, the experimental results demonstrate that QPlanner with alignment training can further provide outlines satisfying diverse user interests. Our resources are available at this https URL.</li>
<li><strong>摘要：</strong>与十亿级大型语言模型的交互通常会产生长格式响应，因为它们具有广泛的参数容量以及检索增强功能。虽然详细的响应提供了特定主题的深刻见解，但它们经常生成不符合用户兴趣的冗余且缺乏吸引力的内容。在这项工作中，我们专注于查询大纲（即选定的查询序列）在用户请求特定范围的信息的场景中的作用，即覆盖条件 ($C^2$) 场景。为了模拟 $C^2$ 场景，我们构建了 QTree，即 10K 组信息搜索查询，这些查询以特定主题的各种视角分解。通过利用 QTree，我们训练 QPlanner，这是一个 7B 语言模型，可生成遵循覆盖条件查询的定制查询大纲。我们通过自动和人工评估分析生成的大纲的有效性，目标是检索增强生成 (RAG)。此外，实验结果表明，经过对齐训练的 QPlanner 可以进一步提供满足不同用户兴趣的大纲。我们的资源可在此 https URL 上找到。</li>
</ul>

<h3>Title: $\text{Memory}^3$: Language Modeling with Explicit Memory</h3>
<ul>
<li><strong>Authors: </strong>Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, Weinan E</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] $\text{Memory}^3$: Language Modeling with Explicit Memory(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs with explicit memory, a memory format cheaper than model parameters and text retrieval-augmented generation (RAG). Conceptually, with most of its knowledge externalized to explicit memories, the LLM can enjoy a smaller parameter size, training cost, and inference cost, all proportional to the amount of remaining "abstract knowledge". As a preliminary proof of concept, we train from scratch a 2.4B LLM, which achieves better performance than much larger LLMs as well as RAG models, and maintains higher decoding speed than RAG. The model is named $\text{Memory}^3$, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values). We introduce a memory circuitry theory to support the externalization of knowledge, and present novel techniques including a memory sparsification mechanism that makes storage tractable and a two-stage pretraining scheme that facilitates memory formation.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的训练和推理是一个成本高昂的过程，它将知识从原始数据转移到有意义的计算中。受人类大脑记忆层次结构的启发，我们通过为 LLM 配备显式记忆（一种比模型参数和文本检索增强生成 (RAG) 更便宜的记忆格式）来降低这一成本。从概念上讲，由于其大部分知识都外化为显式记忆，LLM 可以享受更小的参数大小、训练成本和推理成本，所有这些都与剩余的“抽象知识”量成正比。作为概念的初步证明，我们从头开始训练一个 2.4B LLM，它比更大的 LLM 以及 RAG 模型实现了更好的性能，并且保持了比 RAG 更高的解码速度。该模型被命名为 $\text{Memory}^3$，因为显式记忆是 LLM 中继隐式记忆（模型参数）和工作记忆（上下文键值）之后的第三种记忆形式。我们引入了一种记忆回路理论来支持知识的外部化，并提出了一些新技术，包括使存储易于处理的记忆稀疏化机制和促进记忆形成的两阶段预训练方案。</li>
</ul>

<h3>Title: EconNLI: Evaluating Large Language Models on Economics Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yue Guo, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01212">https://arxiv.org/abs/2407.01212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01212">https://arxiv.org/pdf/2407.01212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01212]] EconNLI: Evaluating Large Language Models on Economics Reasoning(https://arxiv.org/abs/2407.01212)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are widely used for writing economic analysis reports or providing financial advice, but their ability to understand economic knowledge and reason about potential results of specific economic events lacks systematic evaluation. To address this gap, we propose a new dataset, natural language inference on economic events (EconNLI), to evaluate LLMs' knowledge and reasoning abilities in the economic domain. We evaluate LLMs on (1) their ability to correctly classify whether a premise event will cause a hypothesis event and (2) their ability to generate reasonable events resulting from a given premise. Our experiments reveal that LLMs are not sophisticated in economic reasoning and may generate wrong or hallucinated answers. Our study raises awareness of the limitations of using LLMs for critical decision-making involving economic reasoning and analysis. The dataset and codes are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 被广泛用于编写经济分析报告或提供财务建议，但它们理解经济知识和推理特定经济事件的潜在结果的能力缺乏系统的评估。为了解决这一差距，我们提出了一个新的数据集，即经济事件的自然语言推理 (EconNLI)，以评估 LLM 在经济领域的知识和推理能力。我们根据 (1) LLM 正确分类前提事件是否会导致假设事件的能力和 (2) LLM 生成由给定前提导致的合理事件的能力来评估它们。我们的实验表明，LLM 在经济推理方面并不复杂，可能会产生错误或幻觉的答案。我们的研究提高了人们对使用 LLM 进行涉及经济推理和分析的关键决策的局限性的认识。数据集和代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Searching for Best Practices in Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01219">https://arxiv.org/abs/2407.01219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01219">https://arxiv.org/pdf/2407.01219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01219]] Searching for Best Practices in Retrieval-Augmented Generation(https://arxiv.org/abs/2407.01219)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a "retrieval as generation" strategy.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 技术已被证明可有效整合最新信息、减轻幻觉并提高响应质量，尤其是在专业领域。虽然已经提出了许多 RAG 方法来通过查询相关检索来增强大型语言模型，但这些方法仍然存在实施复杂和响应时间延长的问题。通常，RAG 工作流程涉及多个处理步骤，每个步骤都可以以各种方式执行。在这里，我们研究现有的 RAG 方法及其潜在组合，以确定最佳 RAG 实践。通过大量实验，我们提出了几种部署 RAG 的策略，以平衡性能和效率。此外，我们证明多模态检索技术可以显著增强有关视觉输入的问答能力，并使用“检索即生成”策略加速多模态内容的生成。</li>
</ul>

<h3>Title: MIRAI: Evaluating LLM Agents for Event Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Chenchen Ye, Ziniu Hu, Yihe Deng, Zijie Huang, Mingyu Derek Ma, Yanqiao Zhu, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01231">https://arxiv.org/abs/2407.01231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01231">https://arxiv.org/pdf/2407.01231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01231]] MIRAI: Evaluating LLM Agents for Event Forecasting(https://arxiv.org/abs/2407.01231)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have empowered LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems. Given this capability, increasing interests have been put into employing LLM agents for predicting international events, which can influence decision-making and shape policy development on an international scale. Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability. To address this gap, we introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events. Our benchmark features an agentic environment with tools for accessing an extensive database of historical, structured events and textual news articles. We refine the GDELT event database with careful cleaning and parsing to curate a series of relational prediction tasks with varying forecasting horizons, assessing LLM agents' abilities from short-term to long-term forecasting. We further implement APIs to enable LLM agents to utilize different tools via a code-based interface. In summary, MIRAI comprehensively evaluates the agents' capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events. Through comprehensive benchmarking, we aim to establish a reliable framework for assessing the capabilities of LLM agents in forecasting international events, thereby contributing to the development of more accurate and trustworthy models for international relation analysis.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展使 LLM 代理能够自主收集世界信息，并据此进行推理以解决复杂问题。鉴于这种能力，人们越来越有兴趣使用 LLM 代理来预测国际事件，这可以影响决策并影响国际范围内的政策发展。尽管人们的兴趣日益浓厚，但缺乏对 LLM 代理的预测能力和可靠性的严格基准。为了解决这一差距，我们引入了 MIRAI，这是一种新颖的基准，旨在系统地评估 LLM 代理作为国际事件背景下的时间预测者。我们的基准具有代理环境，其中包含用于访问大量历史、结构化事件和文本新闻文章数据库的工具。我们通过仔细清理和解析来完善 GDELT 事件数据库，以策划一系列具有不同预测范围的关系预测任务，评估 LLM 代理从短期到长期预测的能力。我们进一步实现了 API，使 LLM 代理能够通过基于代码的界面使用不同的工具。综上所述，MIRAI 从三个维度全面评估了代理的能力：1）自主地从大型全球数据库中获取和整合关键信息；2）使用特定领域的 API 和库编写代码以供工具使用；3）联合推理来自不同格式和时间的历史知识，以准确预测未来事件。通过全面的基准测试，我们旨在建立一个可靠的框架来评估 LLM 代理预测国际事件的能力，从而有助于开发更准确、更可靠的国际关系分析模型。</li>
</ul>

<h3>Title: SignCLIP: Connecting Text and Sign Language by Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Zifan Jiang, Gerard Sant, Amit Moryossef, Mathias Müller, Rico Sennrich, Sarah Ebling</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01264">https://arxiv.org/abs/2407.01264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01264">https://arxiv.org/pdf/2407.01264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01264]] SignCLIP: Connecting Text and Sign Language by Contrastive Learning(https://arxiv.org/abs/2407.01264)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>We present SignCLIP, which re-purposes CLIP (Contrastive Language-Image Pretraining) to project spoken language text and sign language videos, two classes of natural languages of distinct modalities, into the same space. SignCLIP is an efficient method of learning useful visual representations for sign language processing from large-scale, multilingual video-text pairs, without directly optimizing for a specific task or sign language which is often of limited size. We pretrain SignCLIP on Spreadthesign, a prominent sign language dictionary consisting of ~500 thousand video clips in up to 44 sign languages, and evaluate it with various downstream datasets. SignCLIP discerns in-domain signing with notable text-to-video/video-to-text retrieval accuracy. It also performs competitively for out-of-domain downstream tasks such as isolated sign language recognition upon essential few-shot prompting or fine-tuning. We analyze the latent space formed by the spoken language text and sign language poses, which provides additional linguistic insights. Our code and models are openly available.</li>
<li><strong>摘要：</strong>我们提出了 SignCLIP，它重新利用了 CLIP（对比语言-图像预训练）将口语文本和手语视频（两种不同模态的自然语言）投射到同一空间中。SignCLIP 是一种有效的方法，可以从大规模、多语言的视频文本对中学习用于手语处理的有用视觉表示，而无需直接针对特定任务或手语进行优化，因为手语通常规模有限。我们在 Spreadthesign 上对 SignCLIP 进行预训练，Spreadthesign 是一个著名的手语词典，包含多达 44 种手语的约 50 万个视频剪辑，并使用各种下游数据集对其进行评估。SignCLIP 以显著的文本到视频/视频到文本的检索准确度识别域内手语。它在域外下游任务（例如在必要的少量提示或微调后进行单独的手语识别）方面也表现出色。我们分析了口语文本和手语姿势形成的潜在空间，这提供了额外的语言见解。我们的代码和模型是公开的。</li>
</ul>

<h3>Title: The African Woman is Rhythmic and Soulful: Evaluation of Open-ended Generation for Implicit Biases</h3>
<ul>
<li><strong>Authors: </strong>Serene Lim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01270">https://arxiv.org/abs/2407.01270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01270">https://arxiv.org/pdf/2407.01270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01270]] The African Woman is Rhythmic and Soulful: Evaluation of Open-ended Generation for Implicit Biases(https://arxiv.org/abs/2407.01270)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study investigates the subtle and often concealed biases present in Large Language Models (LLMs), which, despite passing explicit bias tests, can still exhibit implicit biases akin to those observed in humans who profess egalitarian beliefs yet demonstrate underlying prejudices. The challenge of measuring such biases is exacerbated as LLMs become increasingly proprietary, restricting access to their internal mechanisms such as embeddings, which are crucial for applying traditional bias measures. To tackle these issues, this study introduces innovative measures of bias inspired by psychological methodologies: the LLM Implicit Association Test (IAT) Bias and the LLM Decision Bias. The LLM IAT Bias is a prompt-based method designed to unearth implicit biases by simulating the well-known psychological IAT but adapted for use with LLMs. The LLM Decision Bias measure is developed to detect subtle discrimination in decision-making tasks, focusing on how LLMs choose between individuals in various scenarios. Open-ended generation is also utilised through thematic analysis of word generations and storytelling. The experiments revealed biases across gender and racial domains, from discriminatory categorisations to exoticisation. Our findings indicate that the prompt-based measure of implicit bias not only correlates with traditional embedding-based methods but also more effectively predicts downstream behaviors, which are crucially measured by the LLM Decision Bias. This relationship underscores the importance of relative, rather than absolute, evaluations in assessing implicit biases, reflecting psychological insights into human bias assessment. This research contributes to the broader understanding of AI ethics and provides suggestions for continually assessing and mitigating biases in advanced AI systems, emphasising the need for more qualitative and downstream focus.</li>
<li><strong>摘要：</strong>本研究调查了大型语言模型 (LLM) 中存在的微妙且通常隐藏的偏见，这些偏见尽管通过了显性偏见测试，但仍可能表现出类似于在宣称平等主义信仰但表现出潜在偏见的人类身上观察到的隐性偏见。随着 LLM 变得越来越专有，限制对其内部机制（例如嵌入）的访问，衡量此类偏见的挑战也随之加剧，而嵌入对于应用传统偏见测量至关重要。为了解决这些问题，本研究引入了受心理学方法启发的创新偏见测量方法：LLM 内隐联想测试 (IAT) 偏见和 LLM 决策偏见。LLM IAT 偏见是一种基于提示的方法，旨在通过模拟众所周知的心理 IAT 来发掘隐性偏见，但适用于 LLM。LLM 决策偏见测量旨在检测决策任务中的微妙歧视，重点关注 LLM 如何在各种情况下在个体之间做出选择。开放式生成也通过对单词生成和讲故事的主题分析来利用。实验揭示了性别和种族领域的偏见，从歧视性分类到异国情调。我们的研究结果表明，基于提示的隐性偏见测量不仅与传统的基于嵌入的方法相关，而且可以更有效地预测下游行为，而下游行为是 LLM 决策偏见的关键衡量标准。这种关系强调了在评估隐性偏见时相对评估而非绝对评估的重要性，反映了对人类偏见评估的心理学见解。这项研究有助于更广泛地理解人工智能伦理，并为持续评估和减轻高级人工智能系统中的偏见提供了建议，强调需要更加定性和下游关注。</li>
</ul>

<h3>Title: First Place Solution of 2023 Global Artificial Intelligence Technology Innovation Competition Track 1</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Wu, Hailiang Zhang, Yang Yang, Jianfeng Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01271">https://arxiv.org/abs/2407.01271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01271">https://arxiv.org/pdf/2407.01271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01271]] First Place Solution of 2023 Global Artificial Intelligence Technology Innovation Competition Track 1(https://arxiv.org/abs/2407.01271)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we present our champion solution to the Global Artificial Intelligence Technology Innovation Competition Track 1: Medical Imaging Diagnosis Report Generation. We select CPT-BASE as our base model for the text generation task. During the pre-training stage, we delete the mask language modeling task of CPT-BASE and instead reconstruct the vocabulary, adopting a span mask strategy and gradually increasing the number of masking ratios to perform the denoising auto-encoder pre-training task. In the fine-tuning stage, we design iterative retrieval augmentation and noise-aware similarity bucket prompt strategies. The retrieval augmentation constructs a mini-knowledge base, enriching the input information of the model, while the similarity bucket further perceives the noise information within the mini-knowledge base, guiding the model to generate higher-quality diagnostic reports based on the similarity prompts. Surprisingly, our single model has achieved a score of 2.321 on leaderboard A, and the multiple model fusion scores are 2.362 and 2.320 on the A and B leaderboards respectively, securing first place in the rankings.</li>
<li><strong>摘要：</strong>本文展示了我们在全球人工智能技术创新大赛赛道1：医学影像诊断报告生成中的冠军方案。我们选择CPT-BASE作为文本生成任务的基础模型。在预训练阶段，我们删除了CPT-BASE的mask语言建模任务，而是重构词汇表，采用span mask策略并逐步增加掩蔽比数目进行去噪自编码器预训练任务。在微调阶段，我们设计了迭代式的检索增强和噪声感知的相似度桶提示策略。检索增强构建微知识库，丰富模型的输入信息，相似度桶进一步感知微知识库中的噪声信息，引导模型根据相似度提示生成更高质量的诊断报告。令人惊喜的是，我们的单一模型在排行榜A上取得了2.321的成绩，多模型融合得分在A、B排行榜上分别为2.362、2.320，稳居排行榜第一名。</li>
</ul>

<h3>Title: Show Less, Instruct More: Enriching Prompts with Definitions and Guidelines for Zero-Shot NER</h3>
<ul>
<li><strong>Authors: </strong>Andrew Zamai, Andrea Zugarini, Leonardo Rigutini, Marco Ernandes, Marco Maggini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01272">https://arxiv.org/abs/2407.01272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01272">https://arxiv.org/pdf/2407.01272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01272]] Show Less, Instruct More: Enriching Prompts with Definitions and Guidelines for Zero-Shot NER(https://arxiv.org/abs/2407.01272)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recently, several specialized instruction-tuned Large Language Models (LLMs) for Named Entity Recognition (NER) have emerged. Compared to traditional NER approaches, these models have strong generalization capabilities. Existing LLMs mainly focus on zero-shot NER in out-of-domain distributions, being fine-tuned on an extensive number of entity classes that often highly or completely overlap with test sets. In this work instead, we propose SLIMER, an approach designed to tackle never-seen-before named entity tags by instructing the model on fewer examples, and by leveraging a prompt enriched with definition and guidelines. Experiments demonstrate that definition and guidelines yield better performance, faster and more robust learning, particularly when labelling unseen Named Entities. Furthermore, SLIMER performs comparably to state-of-the-art approaches in out-of-domain zero-shot NER, while being trained on a reduced tag set.</li>
<li><strong>摘要：</strong>最近，出现了几种专门针对命名实体识别 (NER) 的指令调整大型语言模型 (LLM)。与传统的 NER 方法相比，这些模型具有很强的泛化能力。现有的 LLM 主要侧重于域外分布中的零样本 NER，在大量实体类上进行微调，这些实体类通常与测试集高度或完全重叠。在这项工作中，我们提出了 SLIMER，这种方法旨在通过在更少的示例上指导模型，并利用丰富的定义和指导来处理从未见过的命名实体标签。实验表明，定义和指导可以产生更好的性能、更快和更稳健的学习，特别是在标记未见过的命名实体时。此外，SLIMER 在域外零样本 NER 中的表现与最先进的方法相当，同时在减少的标签集上进行训练。</li>
</ul>

<h3>Title: Collaborative Performance Prediction for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Zhang, Fuyuan Lyu, Xue Liu, Chen Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01300">https://arxiv.org/abs/2407.01300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01300">https://arxiv.org/pdf/2407.01300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01300]] Collaborative Performance Prediction for Large Language Models(https://arxiv.org/abs/2407.01300)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works demonstrated intrinsic similarities within model families and utilized such similarities for performance prediction. However, they tend to overlook the similarities between model families and only consider design factors listed in the original scaling law. To overcome these limitations, we introduce a novel framework, Collaborative Performance Prediction (CPP), which significantly enhances prediction accuracy by leveraging the historical performance of various models on downstream tasks and other design factors for both model and task. We also collect a collaborative data sourced from online platforms containing both historical performance and additional design factors. With the support of the collaborative data, CPP not only surpasses traditional scaling laws in predicting the performance of scaled LLMs but also facilitates a detailed analysis of factor importance, an area previously overlooked.</li>
<li><strong>摘要：</strong>全面理解和准确预测大型语言模型在各种下游任务中的表现已成为 NLP 研究中的一个关键挑战。下游工作的开创性缩放定律展示了模型系列内部的内在相似性，并利用这些相似性进行性能预测。然而，他们往往忽视了模型系列之间的相似性，只考虑了原始缩放定律中列出的设计因素。为了克服这些限制，我们引入了一个新框架，即协作性能预测 (CPP)，它通过利用各种模型在下游任务上的历史表现以及模型和任务的其他设计因素，显著提高了预测准确性。我们还收集了来自在线平台的协作数据，其中包含历史表现和其他设计因素。在协作数据的支持下，CPP 不仅在预测缩放 LLM 的性能方面超越了传统的缩放定律，而且还促进了对因素重要性的详细分析，这是以前被忽视的领域。</li>
</ul>

<h3>Title: Language Portability Strategies for Open-domain Dialogue with Pre-trained Language Models from High to Low Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Njifenjou, Virgile Sucal, Bassam Jabaian, Fabrice Lefèvre</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01315">https://arxiv.org/abs/2407.01315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01315">https://arxiv.org/pdf/2407.01315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01315]] Language Portability Strategies for Open-domain Dialogue with Pre-trained Language Models from High to Low Resource Languages(https://arxiv.org/abs/2407.01315)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper we propose a study of linguistic portability strategies of large pre-trained language models (PLMs) used for open-domain dialogue systems in a high-resource language for this task. In particular the target low-resource language (L_T) will be simulated with French, as it lacks of task-specific resources and allows our human evaluation, when the source language (L_S) is English. For obvious reasons, recent works using such models for open-domain dialogue are mostly developed in English. Yet building specific PLMs for each possible target language supposes collecting new datasets and is costly. For this reason, trying to leverage all existing resources (PLMs and data) in both L_S and L_T , we wish to assess the performance achievable in L_T with different approaches. The first two approaches evaluate the usage of Neural Machine Translation (NMT) at different levels: TrainOnTarget where a L_S dataset is translated before fine-tuning in L_T and TestOnSource where a L_S model is coupled with NMT modules during inference. Then, the advent of BLOOM [2], the world first open-access multilingual large PLM, allow researchers to develop new approaches aiming to leverage not only the model's full accessibility but also its multilingualism and translation abilities. In this context the task is learned in L_S first and adapted to L_T using the MAD-X Adapter architecture [16]. In the two sets of experiments models are evaluated in spoken dialogue conditions with human and the strategies can be compared in terms of perceived interaction quality.</li>
<li><strong>摘要：</strong>在本文中，我们提出研究用于高资源语言开放域对话系统的大型预训练语言模型 (PLM) 的语言可移植性策略。具体来说，目标低资源语言 (L_T) 将用法语模拟，因为它缺乏特定于任务的资源并且当源语言 (L_S) 为英语时允许我们进行人工评估。出于显而易见的原因，最近使用此类模型进行开放域对话的研究大多是用英语开发的。然而，为每种可能的目标语言构建特定的 PLM 意味着收集新数据集，而且成本高昂。因此，为了尝试利用 L_S 和 L_T 中所有现有的资源（PLM 和数据），我们希望使用不同的方法评估在 L_T 中可实现的性能。前两种方法评估了不同级别神经机器翻译 (NMT) 的使用情况：TrainOnTarget，其中在 L_T 中进行微调之前翻译 L_S 数据集，以及 TestOnSource，其中 L_S 模型在推理过程中与 NMT 模块结合。随后，世界上第一个开放获取的多语言大型 PLM BLOOM [2] 的出现，让研究人员能够开发新的方法，不仅要充分利用模型的完全可访问性，还要利用其多语言和翻译能力。在此背景下，首先在 L_S 中学习任务，然后使用 MAD-X 适配器架构 [16] 将其适应到 L_T。在两组实验中，模型在与人类的口语对话条件下进行评估，并且可以在感知交互质量方面比较这些策略。</li>
</ul>

<h3>Title: Protecting Privacy in Classifiers by Token Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Re'em Harel, Yair Elboher, Yuval Pinter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01334">https://arxiv.org/abs/2407.01334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01334">https://arxiv.org/pdf/2407.01334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01334]] Protecting Privacy in Classifiers by Token Manipulation(https://arxiv.org/abs/2407.01334)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Using language models as a remote service entails sending private information to an untrusted provider. In addition, potential eavesdroppers can intercept the messages, thereby exposing the information. In this work, we explore the prospects of avoiding such data exposure at the level of text manipulation. We focus on text classification models, examining various token mapping and contextualized manipulation functions in order to see whether classifier accuracy may be maintained while keeping the original text unrecoverable. We find that although some token mapping functions are easy and straightforward to implement, they heavily influence performance on the downstream task, and via a sophisticated attacker can be reconstructed. In comparison, the contextualized manipulation provides an improvement in performance.</li>
<li><strong>摘要：</strong>使用语言模型作为远程服务需要将私人信息发送给不受信任的提供商。此外，潜在的窃听者可以拦截消息，从而暴露信息。在这项工作中，我们探索了在文本操作层面避免此类数据泄露的前景。我们专注于文本分类模型，检查各种标记映射和上下文操作函数，以查看是否可以在保持原始文本不可恢复的同时保持分类器准确性。我们发现，虽然一些标记映射函数很容易实现，但它们会严重影响下游任务的性能，并且可以通过复杂的攻击者重建。相比之下，上下文操作可以提高性能。</li>
</ul>

<h3>Title: Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaolin Xing, Zhiwei He, Haoyu Xu, Xing Wang, Rui Wang, Yu Hong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01358">https://arxiv.org/abs/2407.01358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01358">https://arxiv.org/pdf/2407.01358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01358]] Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language Models(https://arxiv.org/abs/2407.01358)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>This paper investigates the cross-lingual inconsistencies observed in Large Language Models (LLMs), such as ChatGPT, Llama, and Baichuan, which have shown exceptional performance in various Natural Language Processing (NLP) tasks. Despite their successes, these models often exhibit significant inconsistencies when processing the same concepts across different languages. This study focuses on three primary questions: the existence of cross-lingual inconsistencies in LLMs, the specific aspects in which these inconsistencies manifest, and the correlation between cross-lingual consistency and multilingual capabilities of this http URL address these questions, we propose an innovative evaluation method for Cross-lingual Semantic Consistency (xSC) using the LaBSE model. We further introduce metrics for Cross-lingual Accuracy Consistency (xAC) and Cross-lingual Timeliness Consistency (xTC) to comprehensively assess the models' performance regarding semantic, accuracy, and timeliness inconsistencies. By harmonizing these metrics, we provide a holistic measurement of LLMs' cross-lingual consistency. Our findings aim to enhance the understanding and improvement of multilingual capabilities and interpretability in LLMs, contributing to the development of more robust and reliable multilingual language models.</li>
<li><strong>摘要：</strong>本文研究了大型语言模型 (LLM)（例如 ChatGPT、Llama 和 Baichuan）中观察到的跨语言不一致问题，这些模型在各种自然语言处理 (NLP) 任务中表现出色。尽管这些模型取得了成功，但它们在处理不同语言中的相同概念时通常会表现出明显的不一致。本研究主要关注三个问题：LLM 中是否存在跨语言不一致，这些不一致表现的具体方面，以及跨语言一致性与此 http URL 的多语言能力之间的相关性。为了解决这些问题，我们提出了一种使用 LaBSE 模型的跨语言语义一致性 (xSC) 创新评估方法。我们进一步引入了跨语言准确度一致性 (xAC) 和跨语言及时性一致性 (xTC) 指标，以全面评估模型在语义、准确性和及时性不一致方面的表现。通过协调这些指标，我们提供了 LLM 跨语言一致性的整体测量。我们的研究旨在增进对法学硕士 (LLM) 中的多语言能力和可解释性的理解和提高，有助于开发更为稳健、可靠的多语言语言模型。</li>
</ul>

<h3>Title: Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems</h3>
<ul>
<li><strong>Authors: </strong>Philippe Laban, Alexander R. Fabbri, Caiming Xiong, Chien-Sheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01370">https://arxiv.org/abs/2407.01370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01370">https://arxiv.org/pdf/2407.01370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01370]] Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems(https://arxiv.org/abs/2407.01370)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>LLMs and RAG systems are now capable of handling millions of input tokens or more. However, evaluating the output quality of such systems on long-context tasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity. In this work, we argue that summarization can play a central role in such evaluation. We design a procedure to synthesize Haystacks of documents, ensuring that specific \textit{insights} repeat across documents. The "Summary of a Haystack" (SummHay) task then requires a system to process the Haystack and generate, given a query, a summary that identifies the relevant insights and precisely cites the source documents. Since we have precise knowledge of what insights should appear in a haystack summary and what documents should be cited, we implement a highly reproducible automatic evaluation that can score summaries on two aspects - Coverage and Citation. We generate Haystacks in two domains (conversation, news), and perform a large-scale evaluation of 10 LLMs and corresponding 50 RAG systems. Our findings indicate that SummHay is an open challenge for current systems, as even systems provided with an Oracle signal of document relevance lag our estimate of human performance (56\%) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and Claude 3 Opus score below 20% on SummHay. We show SummHay can also be used to study enterprise RAG systems and position bias in long-context models. We hope future systems can equal and surpass human performance on SummHay.</li>
<li><strong>摘要：</strong>LLM 和 RAG 系统现在能够处理数百万或更多的输入标记。然而，评估此类系统在长上下文任务上的输出质量仍然具有挑战性，因为像 Needle-in-a-Haystack 这样的任务缺乏复杂性。在这项工作中，我们认为摘要可以在这种评估中发挥核心作用。我们设计了一个程序来合成文档的 Haystack，确保特定的 \textit{insights} 在文档之间重复。然后，“Haystack 摘要”(SummHay) 任务需要一个系统来处理 Haystack 并根据查询生成摘要，该摘要可识别相关见解并准确引用源文档。由于我们精确地知道哪些见解应该出现在 Haystack 摘要中以及应该引用哪些文档，因此我们实施了一种高度可重复的自动评估，可以从两个方面对摘要进行评分 - 覆盖率和引用。我们在两个领域（对话、新闻）生成 Haystack，并对 10 个 LLM 和相应的 50 个 RAG 系统进行大规模评估。我们的研究结果表明，SummHay 对当前系统来说是一个开放的挑战，因为即使提供了文档相关性 Oracle 信号的系统，在联合评分上也比我们对人类表现的估计 (56\%) 落后 10 多分。如果没有检索器，GPT-4o 和 Claude 3 Opus 等长上下文 LLM 在 SummHay 上的得分低于 20%。我们表明 SummHay 还可用于研究企业 RAG 系统和长上下文模型中的定位偏差。我们希望未来的系统能够在 SummHay 上与人类表现相当甚至超越人类。</li>
</ul>

<h3>Title: Bridging the Gap: Transfer Learning from English PLMs to Malaysian English</h3>
<ul>
<li><strong>Authors: </strong>Mohan Raj Chanthran, Lay-Ki Soon, Huey Fang Ong, Bhawani Selvaretnam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01374">https://arxiv.org/abs/2407.01374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01374">https://arxiv.org/pdf/2407.01374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01374]] Bridging the Gap: Transfer Learning from English PLMs to Malaysian English(https://arxiv.org/abs/2407.01374)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Malaysian English is a low resource creole language, where it carries the elements of Malay, Chinese, and Tamil languages, in addition to Standard English. Named Entity Recognition (NER) models underperform when capturing entities from Malaysian English text due to its distinctive morphosyntactic adaptations, semantic features and code-switching (mixing English and Malay). Considering these gaps, we introduce MENmBERT and MENBERT, a pre-trained language model with contextual understanding, specifically tailored for Malaysian English. We have fine-tuned MENmBERT and MENBERT using manually annotated entities and relations from the Malaysian English News Article (MEN) Dataset. This fine-tuning process allows the PLM to learn representations that capture the nuances of Malaysian English relevant for NER and RE tasks. MENmBERT achieved a 1.52\% and 26.27\% improvement on NER and RE tasks respectively compared to the bert-base-multilingual-cased model. Although the overall performance of NER does not have a significant improvement, our further analysis shows that there is a significant improvement when evaluated by the 12 entity labels. These findings suggest that pre-training language models on language-specific and geographically-focused corpora can be a promising approach for improving NER performance in low-resource settings. The dataset and code published in this paper provide valuable resources for NLP research work focusing on Malaysian English.</li>
<li><strong>摘要：</strong>马来西亚英语是一种资源匮乏的克里奥尔语，除了标准英语外，还包含马来语、中文和泰米尔语的元素。命名实体识别 (NER) 模型在从马来西亚英语文本中捕获实体时表现不佳，因为它具有独特的形态句法适应性、语义特征和代码转换（混合英语和马来语）。考虑到这些差距，我们推出了 MENmBERT 和 MENBERT，这是一种具有上下文理解功能的预训练语言模型，专门针对马来西亚英语量身定制。我们使用马来西亚英语新闻文章 (MEN) 数据集中手动注释的实体和关系对 MENmBERT 和 MENBERT 进行了微调。此微调过程使 PLM 能够学习捕捉与 NER 和 RE 任务相关的马来西亚英语细微差别的表示。与 bert-base-multilingual-cased 模型相比，MENmBERT 在 NER 和 RE 任务上分别实现了 1.52\% 和 26.27\% 的提高。虽然 NER 的整体性能没有显著提升，但我们进一步分析发现，通过 12 个实体标签进行评估时，NER 的性能有显著提升。这些发现表明，在特定语言和地理语料库上预训练语言模型可以成为在资源匮乏的环境中提高 NER 性能的一种有前途的方法。本文发表的数据集和代码为专注于马来西亚英语的 NLP 研究工作提供了宝贵的资源。</li>
</ul>

<h3>Title: Free-text Rationale Generation under Readability Level Control</h3>
<ul>
<li><strong>Authors: </strong>Yi-Sheng Hsu, Nils Feldhus, Sherzod Hakimov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01384">https://arxiv.org/abs/2407.01384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01384">https://arxiv.org/pdf/2407.01384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01384]] Free-text Rationale Generation under Readability Level Control(https://arxiv.org/abs/2407.01384)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Free-text rationales justify model decisions in natural language and thus become likable and accessible among approaches to explanation across many tasks. However, their effectiveness can be hindered by misinterpretation and hallucination. As a perturbation test, we investigate how large language models (LLMs) perform the task of natural language explanation (NLE) under the effects of readability level control, i.e., being prompted for a rationale targeting a specific expertise level, such as sixth grade or college. We find that explanations are adaptable to such instruction, but the requested readability is often misaligned with the measured text complexity according to traditional readability metrics. Furthermore, the quality assessment shows that LLMs' ratings of rationales across text complexity exhibit a similar pattern of preference as observed in natural language generation (NLG). Finally, our human evaluation suggests a generally satisfactory impression on rationales at all readability levels, with high-school-level readability being most commonly perceived and favored.</li>
<li><strong>摘要：</strong>自由文本原理用自然语言证明了模型决策的合理性，因此在许多任务的解释方法中变得令人喜欢和易于理解。然而，它们的有效性可能会受到误解和幻觉的阻碍。作为扰动测试，我们研究了大型语言模型 (LLM) 在可读性级别控制的影响下如何执行自然语言解释 (NLE) 任务，即被提示针对特定专业水平（如六年级或大学）的原理。我们发现解释可以适应这种指导，但根据传统的可读性指标，所要求的可读性通常与测量的文本复杂度不一致。此外，质量评估表明，LLM 对文本复杂度的原理的评级表现出与自然语言生成 (NLG) 中观察到的类似的偏好模式。最后，我们的人工评估表明，对所有可读性水平的原理的印象普遍令人满意，高中水平的可读性最常见和受欢迎。</li>
</ul>

<h3>Title: Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters</h3>
<ul>
<li><strong>Authors: </strong>Daniil Gurgurov, Mareike Hartmann, Simon Ostermann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01406">https://arxiv.org/abs/2407.01406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01406">https://arxiv.org/pdf/2407.01406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01406]] Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters(https://arxiv.org/abs/2407.01406)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper explores the integration of graph knowledge from linguistic ontologies into multilingual Large Language Models (LLMs) using adapters to improve performance for low-resource languages (LRLs) in sentiment analysis (SA) and named entity recognition (NER). Building upon successful parameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we propose a similar approach for incorporating knowledge from multilingual graphs, connecting concepts in various languages with each other through linguistic relationships, into multilingual LLMs for LRLs. Specifically, we focus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese, Uyghur, Tibetan, and Sinhala -- and employ language-specific adapters fine-tuned on data extracted from the language-specific section of ConceptNet, aiming to enable knowledge transfer across the languages covered by the knowledge graph. We compare various fine-tuning objectives, including standard Masked Language Modeling (MLM), MLM with full-word masking, and MLM with targeted masking, to analyse their effectiveness in learning and integrating the extracted graph data. Through empirical evaluation on language-specific tasks, we assess how structured graph knowledge affects the performance of multilingual LLMs for LRLs in SA and NER, providing insights into the potential benefits of adapting language models for low-resource scenarios.</li>
<li><strong>摘要：</strong>本文探讨了如何使用适配器将语言本体中的图知识集成到多语言大型语言模型 (LLM) 中，以提高低资源语言 (LRL) 在情绪分析 (SA) 和命名实体识别 (NER) 中的性能。基于成功的参数高效微调技术（例如 K-ADAPTER 和 MAD-X），我们提出了一种类似的方法，将多语言图中的知识（通过语言关系将各种语言中的概念相互连接）整合到 LRL 的多语言 LLM 中。具体来说，我们专注于八种 LRL——马耳他语、保加利亚语、印度尼西亚语、尼泊尔语、爪哇语、维吾尔语、藏语和僧伽罗语——并使用针对从 ConceptNet 的语言特定部分提取的数据进行微调的语言特定适配器，旨在实现知识图谱所涵盖的语言之间的知识转移。我们比较了各种微调目标，包括标准掩码语言模型 (MLM)、带全词掩码的 MLM 和带目标掩码的 MLM，以分析它们在学习和集成提取的图形数据方面的有效性。通过对特定语言任务的实证评估，我们评估了结构化图形知识如何影响多语言 LLM 在 SA 和 NER 中对 LRL 的性能，从而深入了解了在资源匮乏的场景中调整语言模型的潜在好处。</li>
</ul>

<h3>Title: Dynamic Few-Shot Learning for Knowledge Graph Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Jacopo D'Abramo, Andrea Zugarini, Paolo Torroni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01409">https://arxiv.org/abs/2407.01409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01409">https://arxiv.org/pdf/2407.01409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01409]] Dynamic Few-Shot Learning for Knowledge Graph Question Answering(https://arxiv.org/abs/2407.01409)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large language models present opportunities for innovative Question Answering over Knowledge Graphs (KGQA). However, they are not inherently designed for query generation. To bridge this gap, solutions have been proposed that rely on fine-tuning or ad-hoc architectures, achieving good results but limited out-of-domain distribution generalization. In this study, we introduce a novel approach called Dynamic Few-Shot Learning (DFSL). DFSL integrates the efficiency of in-context learning and semantic similarity and provides a generally applicable solution for KGQA with state-of-the-art performance. We run an extensive evaluation across multiple benchmark datasets and architecture configurations.</li>
<li><strong>摘要：</strong>大型语言模型为创新的知识图谱问答 (KGQA) 提供了机会。然而，它们本身并不是为查询生成而设计的。为了弥补这一差距，已经提出了依赖于微调或临时架构的解决方案，取得了良好的结果，但域外分布泛化有限。在本研究中，我们介绍了一种称为动态少量学习 (DFSL) 的新方法。DFSL 集成了上下文学习和语义相似性的效率，并为具有最佳性能的 KGQA 提供了普遍适用的解决方案。我们对多个基准数据集和架构配置进行了广泛的评估。</li>
</ul>

<h3>Title: Needle in the Haystack for Memory Based Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Subhajit Chaudhury, Soham Dan, Payel Das, Georgios Kollias, Elliot Nelson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01437">https://arxiv.org/abs/2407.01437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01437">https://arxiv.org/pdf/2407.01437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01437]] Needle in the Haystack for Memory Based Large Language Models(https://arxiv.org/abs/2407.01437)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>In this paper, we demonstrate the benefits of using memory augmented Large Language Model (LLM) architecture in improving the recall abilities of facts from a potentially long context. As a case study we test LARIMAR, a recently proposed LLM architecture which augments a LLM decoder with an external associative memory, on several long-context recall tasks, including passkey and needle-in-the-haystack tests. We demonstrate that the external memory can be adapted at test time to handle contexts much longer than those seen during training, while keeping readouts from the memory recognizable to the trained decoder and without increasing GPU memory footprint. Compared to alternative architectures for long-context recall tasks with models of a comparable parameter count, LARIMAR is able to maintain strong performance without any task-specific training.</li>
<li><strong>摘要：</strong>在本文中，我们展示了使用记忆增强大型语言模型 (LLM) 架构在提高从可能较长的上下文中回忆事实的能力方面的优势。作为案例研究，我们在几个长上下文回忆任务（包括密码和大海捞针测试）上测试了 LARIMAR，这是一种最近提出的 LLM 架构，它使用外部联想记忆增强了 LLM 解码器。我们证明了外部记忆可以在测试时进行调整，以处理比训练期间看到的长得多的上下文，同时保持训练有素的解码器可以识别来自记忆的读数，并且不会增加 GPU 内存占用。与具有可比参数数量模型的长上下文回忆任务的替代架构相比，LARIMAR 能够在没有任何特定于任务的训练的情况下保持强大的性能。</li>
</ul>

<h3>Title: TimeToM: Temporal Space is the Key to Unlocking the Door of Large Language Models' Theory-of-Mind</h3>
<ul>
<li><strong>Authors: </strong>Guiyang Hou, Wenqi Zhang, Yongliang Shen, Linjuan Wu, Weiming Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01455">https://arxiv.org/abs/2407.01455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01455">https://arxiv.org/pdf/2407.01455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01455]] TimeToM: Temporal Space is the Key to Unlocking the Door of Large Language Models' Theory-of-Mind(https://arxiv.org/abs/2407.01455)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Theory of Mind (ToM)-the cognitive ability to reason about mental states of ourselves and others, is the foundation of social interaction. Although ToM comes naturally to humans, it poses a significant challenge to even the most advanced Large Language Models (LLMs). Due to the complex logical chains in ToM reasoning, especially in higher-order ToM questions, simply utilizing reasoning methods like Chain of Thought (CoT) will not improve the ToM capabilities of LLMs. We present TimeToM, which constructs a temporal space and uses it as the foundation to improve the ToM capabilities of LLMs in multiple scenarios. Specifically, within the temporal space, we construct Temporal Belief State Chain (TBSC) for each character and inspired by the cognition perspective of the social world model, we divide TBSC into self-world beliefs and social world beliefs, aligning with first-order ToM (first-order beliefs) and higher-order ToM (higher-order beliefs) questions, respectively. Moreover, we design a novel tool-belief solver that, by considering belief communication between characters in temporal space, can transform a character's higher-order beliefs into another character's first-order beliefs under belief communication period. Experimental results indicate that TimeToM can dramatically improve the reasoning performance of LLMs on ToM questions while taking a big step towards coherent and robust ToM reasoning.</li>
<li><strong>摘要：</strong>心智理论（ToM）是推理自身和他人心理状态的认知能力，是社交的基础。尽管 ToM 对人类来说是自然而然的，但它对即使是最先进的大型语言模型（LLM）也构成了重大挑战。由于 ToM 推理中的逻辑链复杂，尤其是在高阶 ToM 问题中，单纯使用思维链（CoT）等推理方法无法提高 LLM 的 ToM 能力。我们提出了 TimeToM，它构建了一个时间空间并以此为基础在多种场景中提升 LLM 的 ToM 能力。具体而言，在时间空间内，我们为每个角色构建时间信念状态链（TBSC），并受到社会世界模型的认知视角的启发，我们将 TBSC 分为自我世界信念和社会世界信念，分别与一阶 ToM（一阶信念）和高阶 ToM（高阶信念）问题相对应。此外，我们设计了一种新颖的工具信念求解器，通过考虑时间空间中人物之间的信念交流，可以在信念交流期间将一个人物的高阶信念转化为另一个人物的一阶信念。实验结果表明，TimeToM 可以显著提高 LLM 在 ToM 问题上的推理性能，同时向连贯和稳健的 ToM 推理迈出一大步。</li>
</ul>

<h3>Title: Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement</h3>
<ul>
<li><strong>Authors: </strong>Zisu Huang, Xiaohua Wang, Feiran Zhang, Zhibo Xu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01461">https://arxiv.org/abs/2407.01461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01461">https://arxiv.org/pdf/2407.01461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01461]] Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement(https://arxiv.org/abs/2407.01461)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The capacity of large language models (LLMs) to generate honest, harmless, and helpful responses heavily relies on the quality of user prompts. However, these prompts often tend to be brief and vague, thereby significantly limiting the full potential of LLMs. Moreover, harmful prompts can be meticulously crafted and manipulated by adversaries to jailbreak LLMs, inducing them to produce potentially toxic content. To enhance the capabilities of LLMs while maintaining strong robustness against harmful jailbreak inputs, this study proposes a transferable and pluggable framework that refines user prompts before they are input into LLMs. This strategy improves the quality of the queries, empowering LLMs to generate more truthful, benign and useful responses. Specifically, a lightweight query refinement model is introduced and trained using a specially designed reinforcement learning approach that incorporates multiple objectives to enhance particular capabilities of LLMs. Extensive experiments demonstrate that the refinement model not only improves the quality of responses but also strengthens their robustness against jailbreak attacks. Code is available at: this https URL .</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 生成诚实、无害和有用的响应的能力在很大程度上取决于用户提示的质量。然而，这些提示往往简短而模糊，从而严重限制了 LLM 的全部潜力。此外，对手可以精心制作和操纵有害提示来越狱 LLM，诱导它们产生潜在的有毒内容。为了增强 LLM 的功能，同时保持对有害越狱输入的强大鲁棒性，本研究提出了一个可转移和可插入的框架，该框架在将用户提示输入 LLM 之前对其进行细化。此策略提高了查询的质量，使 LLM 能够生成更真实、良性和有用的响应。具体而言，引入了一个轻量级查询细化模型，并使用专门设计的强化学习方法对其进行训练，该方法结合了多个目标来增强 LLM 的特定功能。大量实验表明，细化模型不仅可以提高响应的质量，还可以增强其对越狱攻击的鲁棒性。代码可在以下位置获得：此 https URL 。</li>
</ul>

<h3>Title: Retrieval-augmented generation in multilingual settings</h3>
<ul>
<li><strong>Authors: </strong>Nadezhda Chirkova, David Rau, Hervé Déjean, Thibault Formal, Stéphane Clinchant, Vassilina Nikoulina</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01463">https://arxiv.org/abs/2407.01463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01463">https://arxiv.org/pdf/2407.01463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01463]] Retrieval-augmented generation in multilingual settings(https://arxiv.org/abs/2407.01463)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has recently emerged as a promising solution for incorporating up-to-date or domain-specific knowledge into large language models (LLMs) and improving LLM factuality, but is predominantly studied in English-only settings. In this work, we consider RAG in the multilingual setting (mRAG), i.e. with user queries and the datastore in 13 languages, and investigate which components and with which adjustments are needed to build a well-performing mRAG pipeline, that can be used as a strong baseline in future works. Our findings highlight that despite the availability of high-quality off-the-shelf multilingual retrievers and generators, task-specific prompt engineering is needed to enable generation in user languages. Moreover, current evaluation metrics need adjustments for multilingual setting, to account for variations in spelling named entities. The main limitations to be addressed in future works include frequent code-switching in non-Latin alphabet languages, occasional fluency errors, wrong reading of the provided documents, or irrelevant retrieval. We release the code for the resulting mRAG baseline pipeline at this https URL.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 最近成为一种有前途的解决方案，用于将最新或特定领域的知识整合到大型语言模型 (LLM) 中并提高 LLM 事实性，但主要在英语环境中进行研究。在这项工作中，我们考虑多语言环境 (mRAG) 中的 RAG，即使用 13 种语言的用户查询和数据存储，并研究需要哪些组件和哪些调整来构建性能良好的 mRAG 管道，可用作未来工作的强大基线。我们的研究结果强调，尽管有高质量的现成多语言检索器和生成器，但需要针对任务的提示工程来实现用户语言的生成。此外，当前的评估指标需要针对多语言环境进行调整，以考虑命名实体拼写的变化。未来工作中要解决的主要限制包括非拉丁字母语言中的频繁代码切换、偶尔的流利错误、对提供的文档的错误阅读或不相关的检索。我们在此 https URL 上发布了由此产生的 mRAG 基线管道的代码。</li>
</ul>

<h3>Title: DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Tzu-Han Lin, Chen-An Li, Hung-yi Lee, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01470">https://arxiv.org/abs/2407.01470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01470">https://arxiv.org/pdf/2407.01470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01470]] DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging(https://arxiv.org/abs/2407.01470)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) is a popular strategy for aligning large language models (LLMs) with desired behaviors. Reward modeling is a crucial step in RLHF. However, collecting paired preference data for training reward models is often costly and time-consuming, especially for domain-specific preferences requiring expert annotation. To address this challenge, we propose the \textbf{Do}main knowled\textbf{ge} merged \textbf{R}eward \textbf{M}odel (DogeRM), a novel framework that integrates domain-specific knowledge into a general reward model by model merging. The experiments demonstrate that DogeRM enhances performance across different benchmarks and provide a detailed analysis showcasing the effects of model merging, showing the great potential of facilitating model alignment.</li>
<li><strong>摘要：</strong>从人类反馈中进行强化学习 (RLHF) 是一种将大型语言模型 (LLM) 与所需行为对齐的流行策略。奖励建模是 RLHF 中的关键步骤。然而，收集成对的偏好数据来训练奖励模型通常成本高昂且耗时，尤其是对于需要专家注释的领域特定偏好。为了应对这一挑战，我们提出了 \textbf{Do}main knowled\textbf{ge} 合并 \textbf{R}eward \textbf{M}odel (DogeRM)，这是一个新颖的框架，通过模型合并将领域特定知识集成到通用奖励模型中。实验表明 DogeRM 提高了不同基准上的性能，并提供了展示模型合并效果的详细分析，显示出促进模型对齐的巨大潜力。</li>
</ul>

<h3>Title: LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives</h3>
<ul>
<li><strong>Authors: </strong>Luísa Shimabucoro, Sebastian Ruder, Julia Kreutzer, Marzieh Fadaee, Sara Hooker</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01490">https://arxiv.org/abs/2407.01490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01490">https://arxiv.org/pdf/2407.01490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01490]] LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives(https://arxiv.org/abs/2407.01490)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The widespread adoption of synthetic data raises new questions about how models generating the data can influence other large language models (LLMs) via distilled data. To start, our work exhaustively characterizes the impact of passive inheritance of model properties by systematically studying the consequences of synthetic data integration. We provide one of the most comprehensive studies to-date of how the source of synthetic data shapes models' internal biases, calibration and generations' textual attributes and preferences. We find that models are surprisingly sensitive towards certain attributes even when the synthetic data prompts appear "neutral". which invites the question whether this sensitivity can be exploited for good. Our findings invite the question can we explicitly steer the models towards the properties we want at test time by exploiting the data generation process? This would have historically been considered infeasible due to the cost of collecting data with a specific characteristic or objective in mind. However, improvement in the quality of synthetic data, as well as a shift towards general-purpose models designed to follow a diverse way of instructions, means this question is timely. We propose active inheritance as a term to describe intentionally constraining synthetic data according to a non-differentiable objective. We demonstrate how active inheritance can steer the generation profiles of models towards desirable non-differentiable attributes, e.g. high lexical diversity or low toxicity.</li>
<li><strong>摘要：</strong>合成数据的广泛采用引发了新的问题，即生成数据的模型如何通过提炼数据影响其他大型语言模型 (LLM)。首先，我们的工作通过系统地研究合成数据集成的后果，详尽地描述了模型属性被动继承的影响。我们提供了迄今为止最全面的研究之一，研究合成数据源如何影响模型的内部偏差、校准和生成的文本属性和偏好。我们发现，即使合成数据提示看起来是“中性的”，模型也会对某些属性出奇地敏感。这引出了一个问题，即这种敏感性是否可以得到很好的利用。我们的研究结果引出了一个问题，我们能否通过利用数据生成过程，在测试时明确地引导模型朝着我们想要的属性发展？这在历史上被认为是不可行的，因为收集具有特定特征或目标的数据的成本很高。然而，合成数据质量的提高，以及向遵循多种指令方式的通用模型的转变，意味着这个问题是及时的。我们提出“主动继承”这一术语来描述根据不可微分目标有意限制合成数据。我们展示了主动继承如何引导模型的生成配置文件朝着理想的不可微分属性发展，例如高词汇多样性或低毒性。</li>
</ul>

<h3>Title: RegMix: Data Mixture as Regression for Language Model Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01492">https://arxiv.org/abs/2407.01492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01492">https://arxiv.org/pdf/2407.01492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01492]] RegMix: Data Mixture as Regression for Language Model Pre-training(https://arxiv.org/abs/2407.01492)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10% of the compute budget. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws, and our approach captures the complexity by considering all domains together. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型预训练的数据混合会显著影响性能，但如何确定有效的混合仍不清楚。我们提出使用 RegMix 将其制定为回归任务，以自动识别高性能数据混合。RegMix 涉及使用不同的数据混合训练一组小模型，并拟合回归模型以根据各自的混合预测它们的性能。使用拟合的回归模型，我们模拟排名靠前的混合，并使用它来训练计算量高出几个数量级的大型模型。为了通过经验验证 RegMix，我们针对不同混合的 1B 个标记训练了 512 个具有 1M 个参数的模型，以拟合回归模型并找到最佳混合。使用这种混合，我们为 25B 个标记（即大 1000 倍，长 25 倍）训练了一个 1B 参数模型，我们发现它在 64 个具有其他混合的 1B 参数模型候选中表现最佳。此外，我们的方法与人工选择相比表现出了卓越的性能，并且仅使用 10% 的计算预算就实现了与 DoReMi 相当甚至更好的结果。我们的实验还表明：(1) 数据混合对性能有显著影响，单任务性能变化高达 14.6%；(2) 与被视为高质量的数据（如 Wikipedia）相比，Web 语料库与下游性能的正相关性最强；(3) 域以复杂的方式交互，通常与常识相悖，因此需要像 RegMix 这样的自动化方法；(4) 数据混合效应超越了缩放定律，我们的方法通过同时考虑所有域来捕捉复杂性。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Self-Cognition in Large Language Models: An Exploratory Study</h3>
<ul>
<li><strong>Authors: </strong>Dongping Chen, Jiawen Shi, Yao Wan, Pan Zhou, Neil Zhenqiang Gong, Lichao Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01505">https://arxiv.org/abs/2407.01505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01505">https://arxiv.org/pdf/2407.01505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01505]] Self-Cognition in Large Language Models: An Exploratory Study(https://arxiv.org/abs/2407.01505)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition. In this paper, we perform a pioneering study to explore self-cognition in LLMs. Specifically, we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition and four well-designed principles to quantify LLMs' self-cognition. Our study reveals that 4 of the 48 models on Chatbot Arena--specifically Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable self-cognition. We observe a positive correlation between model size, training data quality, and self-cognition level. Additionally, we also explore the utility and trustworthiness of LLM in the self-cognition state, revealing that the self-cognition state enhances some specific tasks such as creative writing and exaggeration. We believe that our work can serve as an inspiration for further research to study the self-cognition in LLMs.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 在各种应用中取得了显著的成功，但它们也引发了人们对自我认知的担忧。在本文中，我们进行了一项开创性的研究来探索 LLM 中的自我认知。​​具体来说，我们首先构建一个自我认知指导提示池来评估 LLM 在哪里表现出自我认知，并构建四个精心设计的原则来量化 LLM 的自我认知。​​我们的研究表明，Chatbot Arena 上的 48 个模型中有 4 个——特别是 Command R、Claude3-Opus、Llama-3-70b-Instruct 和 Reka-core——表现出一定程度的可检测自我认知。​​我们观察到模型大小、训练数据质量和自我认知水平之间存在正相关关系。此外，我们还探讨了 LLM 在自我认知状态下的实用性和可信度，揭示了自我认知状态增强了一些特定的任务，例如创意写作和夸张。我们相信，我们的工作可以对进一步研究法学硕士中的自我认知提供启发。</li>
</ul>

<h3>Title: KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Yuan, Hongyi Liu, Shaochen (Henry)Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, Xia Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.01527">https://arxiv.org/abs/2407.01527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.01527">https://arxiv.org/pdf/2407.01527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.01527]] KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches(https://arxiv.org/abs/2407.01527)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, prompt</a></li>
<li><strong>Abstract: </strong>Long context capability is a crucial competency for large language models (LLMs) as it mitigates the human struggle to digest long-form texts. This capability enables complex task-solving scenarios such as book summarization, code assistance, and many more tasks that are traditionally manpower-intensive. However, transformer-based LLMs face significant challenges with long context input due to the growing size of the KV cache and the intrinsic complexity of attending to extended inputs; where multiple schools of efficiency-driven approaches -- such as KV cache quantization, token dropping, prompt compression, linear-time sequence models, and hybrid architectures -- have been proposed to produce efficient yet long context-capable models. Despite these advancements, no existing work has comprehensively benchmarked these methods in a reasonably aligned environment. In this work, we fill this gap by providing a taxonomy of current methods and evaluating 10+ state-of-the-art approaches across seven categories of long context tasks. Our work reveals numerous previously unknown phenomena and offers insights -- as well as a friendly workbench -- for the future development of long context-capable LLMs. The source code will be available at this https URL</li>
<li><strong>摘要：</strong>长上下文能力是大型语言模型 (LLM) 的一项关键能力，因为它可以减轻人类消化长篇文本的困难。这种能力可以实现复杂的任务解决场景，例如书籍摘要、代码辅助以及许多其他传统上需要大量人力的任务。然而，由于 KV 缓存的大小不断增加以及处理扩展输入的内在复杂性，基于转换器的 LLM 在长上下文输入方面面临重大挑战；其中提出了多种效率驱动方法——例如 KV 缓存量化、标记删除、提示压缩、线性时间序列模型和混合架构——以生成高效且支持长上下文的模型。尽管取得了这些进展，但目前还没有一项研究在合理一致的环境中对这些方法进行全面的基准测试。在这项工作中，我们通过提供当前方法的分类法并评估七类长上下文任务中的 10 多种最新方法，填补了这一空白。我们的工作揭示了许多以前未知的现象，并为未来开发支持长上下文的 LLM 提供了见解——以及一个友好的工作台。源代码可在此 https URL 上获取</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
