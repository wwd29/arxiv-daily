<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-11</h1>
<h3>Title: Locally Measuring Cross-lingual Lexical Alignment: A Domain and Word Level Perspective</h3>
<ul>
<li><strong>Authors: </strong>Taelin Karidi, Eitan Grossman, Omri Abend</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07239">https://arxiv.org/abs/2410.07239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07239">https://arxiv.org/pdf/2410.07239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07239]] Locally Measuring Cross-lingual Lexical Alignment: A Domain and Word Level Perspective(https://arxiv.org/abs/2410.07239)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>NLP research on aligning lexical representation spaces to one another has so far focused on aligning language spaces in their entirety. However, cognitive science has long focused on a local perspective, investigating whether translation equivalents truly share the same meaning or the extent that cultural and regional influences result in meaning variations. With recent technological advances and the increasing amounts of available data, the longstanding question of cross-lingual lexical alignment can now be approached in a more data-driven manner. However, developing metrics for the task requires some methodology for comparing metric efficacy. We address this gap and present a methodology for analyzing both synthetic validations and a novel naturalistic validation using lexical gaps in the kinship domain. We further propose new metrics, hitherto unexplored on this task, based on contextualized embeddings. Our analysis spans 16 diverse languages, demonstrating that there is substantial room for improvement with the use of newer language models. Our research paves the way for more accurate and nuanced cross-lingual lexical alignment methodologies and evaluation.</li>
<li><strong>摘要：</strong>到目前为止，关于词汇表示空间相互对齐的 NLP 研究主要集中在语言空间的整体对齐上。然而，认知科学长期以来一直专注于局部视角，研究翻译等价物是否真正具有相同的含义，或者文化和区域影响导致含义变化的程度。随着最近的技术进步和可用数据的增加，跨语言词汇对齐这一长期存在的问题现在可以以更数据驱动的方式处理。然而，为该任务制定指标需要一些方法来比较指标效力。我们解决了这一差距，并提出了一种分析合成验证和使用亲属关系领域词汇差距的新型自然验证的方法。我们进一步提出了基于语境化嵌入的新指标，这是迄今为止在该任务上尚未探索过的。我们的分析涵盖了 16 种不同的语言，表明使用较新的语言模型有很大的改进空间。我们的研究为更准确、更细致的跨语言词汇对齐方法和评估铺平了道路。</li>
</ul>

<h3>Title: DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiming Huang, Jianwen Luo, Yan Yu, Yitong Zhang, Fangyu Lei, Yifan Wei, Shizhu He, Lifu Huang, Xiao Liu, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07331">https://arxiv.org/abs/2410.07331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07331">https://arxiv.org/pdf/2410.07331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07331]] DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models(https://arxiv.org/abs/2410.07331)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at [this https URL](this https URL).</li>
<li><strong>摘要：</strong>我们引入了 DA-Code，这是一个专门设计用于评估基于代理的数据科学任务的 LLM 的代码生成基准。该基准具有三个核心要素：首先，DA-Code 中的任务本身就具有挑战性，与传统的代码生成任务不同，需要高级编码技能来打好基础和规划。其次，DA-Code 中的示例全部基于真实而多样的数据，涵盖了广泛的复杂数据整理和分析任务。第三，为了解决这些任务，模型必须使用复杂的数据科学编程语言来执行复杂的数据处理并得出答案。我们在可控且可执行的环境中设置了基准，该环境与现实世界的数据分析场景一致且可扩展。注释者精心设计了评估套件，以确保评估的准确性和稳健性。我们开发了 DA-Agent 基线。实验表明，尽管基线的表现优于其他现有框架，但使用当前最好的 LLM 只能实现 30.5% 的准确率，有很大的改进空间。我们在 [此 https URL](此 https URL) 发布了我们的基准。</li>
</ul>

<h3>Title: Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning</h3>
<ul>
<li><strong>Authors: </strong>Abhinav Bandari, Lu Yin, Cheng-Yu Hsieh, Ajay Kumar Jaiswal, Tianlong Chen, Li Shen, Ranjay Krishna, Shiwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07461">https://arxiv.org/abs/2410.07461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07461">https://arxiv.org/pdf/2410.07461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07461]] Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning(https://arxiv.org/abs/2410.07461)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Network pruning has emerged as a potential solution to make LLMs cheaper to deploy. However, existing LLM pruning approaches universally rely on the C4 dataset as the calibration data for calculating pruning scores, leaving its optimality unexplored. In this study, we evaluate the choice of calibration data on LLM pruning, across a wide range of datasets that are most commonly used in LLM training and evaluation, including four pertaining datasets as well as three categories of downstream tasks encompassing nine datasets. Each downstream dataset is prompted with In-Context Learning (ICL) and Chain-of-Thought (CoT), respectively. Besides the already intriguing observation that the choice of calibration data significantly impacts the performance of pruned LLMs, our results also uncover several subtle and often unexpected findings, summarized as follows: (1) C4 is not the optimal choice for LLM pruning, even among commonly used pre-training datasets; (2) arithmetic datasets, when used as calibration data, performs on par or even better than pre-training datasets; (3) pruning with downstream datasets does not necessarily help the corresponding downstream task, compared to pre-training data; (4) ICL is widely beneficial to all data categories, whereas CoT is only useful on certain tasks. Our findings shed light on the importance of carefully selecting calibration data for LLM pruning and pave the way for more efficient deployment of these powerful models in real-world applications. We release our code at: this https URL.</li>
<li><strong>摘要：</strong>网络剪枝已成为一种降低 LLM 部署成本的潜在解决方案。然而，现有的 LLM 剪枝方法普遍依赖 C4 数据集作为计算剪枝分数的校准数据，而其最优性尚未得到探索。在本研究中，我们评估了校准数据的选择对 LLM 剪枝的影响，涵盖了 LLM 训练和评估中最常用的广泛数据集，包括四个相关数据集以及包含九个数据集的三类下游任务。每个下游数据集分别以上下文学习 (ICL) 和思维链 (CoT) 提示。除了校准数据的选择会显著影响剪枝后 LLM 的性能这个已经很有趣的观察结果之外，我们的结果还揭示了一些微妙且常常出乎意料的发现，总结如下：（1）即使在常用的预训练数据集中，C4 也不是 LLM 剪枝的最佳选择；（2）算术数据集在用作校准数据时，其表现与预训练数据集相当甚至更好； (3) 与预训练数据相比，使用下游数据集进行修剪不一定有助于相应的下游任务；(4) ICL 对所有数据类别都有广泛的好处，而 CoT 仅对某些任务有用。我们的研究结果揭示了仔细选择 LLM 修剪校准数据的重要性，并为在实际应用中更有效地部署这些强大的模型铺平了道路。我们在以下网址发布了我们的代码：这个 https URL。</li>
</ul>

<h3>Title: Localizing Factual Inconsistencies in Attributable Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Arie Cattan, Paul Roit, Shiyue Zhang, David Wan, Roee Aharoni, Idan Szpektor, Mohit Bansal, Ido Dagan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07473">https://arxiv.org/abs/2410.07473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07473">https://arxiv.org/pdf/2410.07473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07473]] Localizing Factual Inconsistencies in Attributable Text Generation(https://arxiv.org/abs/2410.07473)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>There has been an increasing interest in detecting hallucinations in model-generated texts, both manually and automatically, at varying levels of granularity. However, most existing methods fail to precisely pinpoint the errors. In this work, we introduce QASemConsistency, a new formalism for localizing factual inconsistencies in attributable text generation, at a fine-grained level. Drawing inspiration from Neo-Davidsonian formal semantics, we propose decomposing the generated text into minimal predicate-argument level propositions, expressed as simple question-answer (QA) pairs, and assess whether each individual QA pair is supported by a trusted reference text. As each QA pair corresponds to a single semantic relation between a predicate and an argument, QASemConsistency effectively localizes the unsupported information. We first demonstrate the effectiveness of the QASemConsistency methodology for human annotation, by collecting crowdsourced annotations of granular consistency errors, while achieving a substantial inter-annotator agreement ($\kappa > 0.7)$. Then, we implement several methods for automatically detecting localized factual inconsistencies, with both supervised entailment models and open-source LLMs.</li>
<li><strong>摘要：</strong>人们越来越关注以不同粒度级别手动和自动检测模型生成文本中的幻觉。然而，大多数现有方法都无法准确指出错误。在这项工作中，我们引入了 QASemConsistency，这是一种用于在细粒度级别定位可归因文本生成中事实不一致的新形式。从新戴维森形式语义学中汲取灵感，我们建议将生成的文本分解为最小谓词-论元级命题，以简单的问答 (QA) 对表示，并评估每个单独的 QA 对是否由可信参考文本支持。由于每个 QA 对对应于谓词和论元之间的单一语义关系，QASemConsistency 可以有效地定位不受支持的信息。我们首先通过收集粒度一致性错误的众包注释，同时实现相当大的注释者间一致性 ($\kappa > 0.7)$，证明了 QASemConsistency 方法对人工注释的有效性。然后，我们利用监督蕴涵模型和开源 LLM 实现了几种自动检测局部事实不一致的方法。</li>
</ul>

<h3>Title: MoDEM: Mixture of Domain Expert Models</h3>
<ul>
<li><strong>Authors: </strong>Toby Simonds, Kemal Kurniawan, Jey Han Lau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07490">https://arxiv.org/abs/2410.07490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07490">https://arxiv.org/pdf/2410.07490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07490]] MoDEM: Mixture of Domain Expert Models(https://arxiv.org/abs/2410.07490)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We propose a novel approach to enhancing the performance and efficiency of large language models (LLMs) by combining domain prompt routing with domain-specialized models. We introduce a system that utilizes a BERT-based router to direct incoming prompts to the most appropriate domain expert model. These expert models are specifically tuned for domains such as health, mathematics and science. Our research demonstrates that this approach can significantly outperform general-purpose models of comparable size, leading to a superior performance-to-cost ratio across various benchmarks. The implications of this study suggest a potential paradigm shift in LLM development and deployment. Rather than focusing solely on creating increasingly large, general-purpose models, the future of AI may lie in developing ecosystems of smaller, highly specialized models coupled with sophisticated routing systems. This approach could lead to more efficient resource utilization, reduced computational costs, and superior overall performance.</li>
<li><strong>摘要：</strong>我们提出了一种新方法，通过将领域提示路由与领域专业模型相结合来提高大型语言模型 (LLM) 的性能和效率。我们引入了一个系统，该系统利用基于 BERT 的路由器将传入的提示定向到最合适的领域专家模型。这些专家模型专门针对健康、数学和科学等领域进行了调整。我们的研究表明，这种方法可以显著胜过同等规模的通用模型，从而在各种基准测试中实现卓越的性能成本比。这项研究的意义表明 LLM 的开发和部署可能会发生范式转变。人工智能的未来可能在于开发更小、高度专业化的模型生态系统以及复杂的路由系统，而不是仅仅专注于创建越来越大的通用模型。这种方法可以提高资源利用率、降低计算成本并实现卓越的整体性能。</li>
</ul>

<h3>Title: PublicHearingBR: A Brazilian Portuguese Dataset of Public Hearing Transcripts for Summarization of Long Documents</h3>
<ul>
<li><strong>Authors: </strong>Leandro Carísio Fernandes, Guilherme Zeferino Rodrigues Dobins, Roberto Lotufo, Jayr Alencar Pereira</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07495">https://arxiv.org/abs/2410.07495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07495">https://arxiv.org/pdf/2410.07495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07495]] PublicHearingBR: A Brazilian Portuguese Dataset of Public Hearing Transcripts for Summarization of Long Documents(https://arxiv.org/abs/2410.07495)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>This paper introduces PublicHearingBR, a Brazilian Portuguese dataset designed for summarizing long documents. The dataset consists of transcripts of public hearings held by the Brazilian Chamber of Deputies, paired with news articles and structured summaries containing the individuals participating in the hearing and their statements or opinions. The dataset supports the development and evaluation of long document summarization systems in Portuguese. Our contributions include the dataset, a hybrid summarization system to establish a baseline for future studies, and a discussion on evaluation metrics for summarization involving large language models, addressing the challenge of hallucination in the generated summaries. As a result of this discussion, the dataset also provides annotated data that can be used in Natural Language Inference tasks in Portuguese.</li>
<li><strong>摘要：</strong>本文介绍了 PublicHearingBR，这是一个巴西葡萄牙语数据集，用于总结长篇文档。该数据集由巴西众议院举行的公开听证会的记录组成，并附有新闻文章和结构化摘要，其中包含参加听证会的个人及其声明或意见。该数据集支持葡萄牙语长篇文档摘要系统的开发和评估。我们的贡献包括数据集、为未来研究建立基线的混合摘要系统，以及对涉及大型语言模型的摘要评估指标的讨论，解决了生成的摘要中出现幻觉的挑战。作为此次讨论的结果，该数据集还提供了可用于葡萄牙语自然语言推理任务的带注释数据。</li>
</ul>

<h3>Title: Using LLMs to Discover Legal Factors</h3>
<ul>
<li><strong>Authors: </strong>Morgan Gray, Jaromir Savelka, Wesley Oliver, Kevin Ashley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07504">https://arxiv.org/abs/2410.07504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07504">https://arxiv.org/pdf/2410.07504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07504]] Using LLMs to Discover Legal Factors(https://arxiv.org/abs/2410.07504)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Factors are a foundational component of legal analysis and computational models of legal reasoning. These factor-based representations enable lawyers, judges, and AI and Law researchers to reason about legal cases. In this paper, we introduce a methodology that leverages large language models (LLMs) to discover lists of factors that effectively represent a legal domain. Our method takes as input raw court opinions and produces a set of factors and associated definitions. We demonstrate that a semi-automated approach, incorporating minimal human involvement, produces factor representations that can predict case outcomes with moderate success, if not yet as well as expert-defined factors can.</li>
<li><strong>摘要：</strong>因素是法律分析和法律推理计算模型的基础组成部分。这些基于因素的表示使律师、法官以及 AI 和法律研究人员能够推理法律案件。在本文中，我们介绍了一种利用大型语言模型 (LLM) 来发现有效代表法律领域的因素列表的方法。我们的方法以原始法庭意见作为输入，并生成一组因素和相关定义。我们证明，半自动化方法（结合最少的人为参与）可以生成因素表示，这些因素表示可以在一定程度上成功预测案件结果，即使还没有专家定义的因素那么好。</li>
</ul>

<h3>Title: Thought2Text: Text Generation from EEG Signal using Large Language Models (LLMs)</h3>
<ul>
<li><strong>Authors: </strong>Abhijit Mishra, Shreya Shukla, Jose Torres, Jacek Gwizdka, Shounak Roychowdhury</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07507">https://arxiv.org/abs/2410.07507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07507">https://arxiv.org/pdf/2410.07507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07507]] Thought2Text: Text Generation from EEG Signal using Large Language Models (LLMs)(https://arxiv.org/abs/2410.07507)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Decoding and expressing brain activity in a comprehensible form is a challenging frontier in AI. This paper presents Thought2Text, which uses instruction-tuned Large Language Models (LLMs) fine-tuned with EEG data to achieve this goal. The approach involves three stages: (1) training an EEG encoder for visual feature extraction, (2) fine-tuning LLMs on image and text data, enabling multimodal description generation, and (3) further fine-tuning on EEG embeddings to generate text directly from EEG during inference. Experiments on a public EEG dataset collected for six subjects with image stimuli demonstrate the efficacy of multimodal LLMs (LLaMa-v3, Mistral-v0.3, Qwen2.5), validated using traditional language generation evaluation metrics, GPT-4 based assessments, and evaluations by human expert. This approach marks a significant advancement towards portable, low-cost "thoughts-to-text" technology with potential applications in both neuroscience and natural language processing (NLP).</li>
<li><strong>摘要：</strong>以可理解的形式解码和表达大脑活动是人工智能领域的一个具有挑战性的前沿。本文介绍了 Thought2Text，它使用经过指令调整的大型语言模型 (LLM) 并通过 EEG 数据进行微调来实现这一目标。该方法涉及三个阶段：(1) 训练 EEG 编码器以进行视觉特征提取，(2) 在图像和文本数据上微调 LLM，实现多模态描述生成，以及 (3) 在 EEG 嵌入上进一步微调以在推理过程中直接从 EEG 生成文本。在为六名受试者收集的带有图像刺激的公共 EEG 数据集上进行的实验证明了多模态 LLM (LLaMa-v3、Mistral-v0.3、Qwen2.5) 的有效性，并使用传统语言生成评估指标、基于 GPT-4 的评估和人类专家的评估进行了验证。这种方法标志着便携式、低成本“思想到文本”技术的重大进步，在神经科学和自然语言处理 (NLP) 中都有潜在应用。</li>
</ul>

<h3>Title: News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News</h3>
<ul>
<li><strong>Authors: </strong>Tarun Jain, Yufei Gao, Sridhar Vanga, Karan Singla</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07520">https://arxiv.org/abs/2410.07520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07520">https://arxiv.org/pdf/2410.07520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07520]] News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News(https://arxiv.org/abs/2410.07520)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have fast become an essential tools to many conversational chatbots due to their ability to provide coherent answers for varied queries. Datasets used to train these LLMs are often a mix of generic and synthetic samples, thus lacking the verification needed to provide correct and verifiable answers for T.V. News. We collect and share a large collection of QA pairs extracted from transcripts of news recordings from various news-channels across the United States. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM model. Our model surpasses base models of similar size on several open LLM benchmarks. We further integrate and propose a RAG method to improve contextualization of our answers and also point it to a verifiable news recording.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已迅速成为许多对话聊天机器人的必备工具，因为它们能够为各种查询提供连贯的答案。用于训练这些 LLM 的数据集通常是通用样本和合成样本的混合，因此缺乏为电视新闻提供正确且可验证答案所需的验证。我们收集并分享了从美国各个新闻频道的新闻录音记录中提取的大量 QA 对。然后使用生成的 QA 对来微调现成的 LLM 模型。我们的模型在几个开放的 LLM 基准上超越了类似规模的基础模型。我们进一步整合并提出了一种 RAG 方法来改善我们答案的语境化，并将其指向可验证的新闻录音。</li>
</ul>

<h3>Title: DemoShapley: Valuation of Demonstrations for In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Shan Xie, Man Luo, Chadly Daniel Stern, Mengnan Du, Lu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07523">https://arxiv.org/abs/2410.07523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07523">https://arxiv.org/pdf/2410.07523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07523]] DemoShapley: Valuation of Demonstrations for In-Context Learning(https://arxiv.org/abs/2410.07523)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) leveraging in-context learning (ICL) have set new benchmarks in few-shot learning across various tasks without needing task-specific fine-tuning. However, extensive research has demonstrated that the effectiveness of ICL is significantly influenced by the selection and ordering of demonstrations. Considering the critical role of demonstration selection in ICL, we introduce DemoShapley which is inspired by the Data Shapley valuation theorem. This approach assesses the influence of individual demonstration instances, distinguishing between those that contribute positively and those that may hinder performance. Our findings reveal that DemoShapley not only enhances model performance in terms of accuracy and fairness but also generalizes queries from domains distinct from those of the in-context demonstrations, highlighting its versatility and effectiveness in optimizing ICL demonstration selection. Last but not least, DemoShapley demonstrates its ability to aid in identifying noisy data within the demonstration set.</li>
<li><strong>摘要：</strong>利用上下文学习 (ICL) 的大型语言模型 (LLM) 已在各种任务的少量学习中设定了新的基准，而无需针对特定任务进行微调。然而，大量研究表明，ICL 的有效性受到演示的选择和排序的显著影响。考虑到演示选择在 ICL 中的关键作用，我们引入了 DemoShapley，它受到 Data Shapley 估值定理的启发。这种方法评估了单个演示实例的影响，区分了那些有积极贡献的实例和那些可能阻碍性能的实例。我们的研究结果表明，DemoShapley 不仅在准确性和公平性方面提高了模型性能，而且还概括了来自不同于上下文演示领域的查询，突出了其在优化 ICL 演示选择方面的多功能性和有效性。最后但并非最不重要的是，DemoShapley 展示了它帮助识别演示集中的噪声数据的能力。</li>
</ul>

<h3>Title: Upcycling Large Language Models into Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Ethan He, Abhinav Khattar, Ryan Prenger, Vijay Korthikanti, Zijie Yan, Tong Liu, Shiqing Fan, Ashwath Aithal, Mohammad Shoeybi, Bryan Catanzaro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07524">https://arxiv.org/abs/2410.07524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07524">https://arxiv.org/pdf/2410.07524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07524]] Upcycling Large Language Models into Mixture of Experts(https://arxiv.org/abs/2410.07524)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Upcycling pre-trained dense language models into sparse mixture-of-experts (MoE) models is an efficient approach to increase the model capacity of already trained models. However, optimal techniques for upcycling at scale remain unclear. In this work, we conduct an extensive study of upcycling methods and hyperparameters for billion-parameter scale language models. We propose a novel "virtual group" initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures. Through ablations, we find that upcycling outperforms continued dense model training. In addition, we show that softmax-then-topK expert routing improves over topK-then-softmax approach and higher granularity MoEs can help improve accuracy. Finally, we upcycled Nemotron-4 15B on 1T tokens and compared it to a continuously trained version of the same model on the same 1T tokens: the continuous trained model achieved 65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer insights and best practices to effectively leverage upcycling for building MoE language models.</li>
<li><strong>摘要：</strong>将预训练的密集语言模型升级为稀疏混合专家 (MoE) 模型是一种提高已训练模型的模型容量的有效方法。然而，大规模升级的最佳技术仍不清楚。在这项工作中，我们对十亿参数规模语言模型的升级方法和超参数进行了广泛的研究。我们提出了一种新颖的“虚拟组”初始化方案和权重缩放方法，以实现升级为细粒度 MoE 架构。通过消融，我们发现升级优于持续密集模型训练。此外，我们表明 softmax-then-topK 专家路由比 topK-then-softmax 方法有所改进，更高粒度的 MoE 可以帮助提高准确性。最后，我们在 1T 标记上升级了 Nemotron-4 15B，并将其与在相同的 1T 标记上连续训练的相同模型版本进行了比较：连续训练模型实现了 65.3% MMLU，而升级模型实现了 67.6%。我们的研究结果提供了见解和最佳实践，可以有效利用升级再造来构建 MoE 语言模型。</li>
</ul>

<h3>Title: MKGL: Mastery of a Three-Word Language</h3>
<ul>
<li><strong>Authors: </strong>Lingbing Guo, Zhongpu Bo, Zhuo Chen, Yichi Zhang, Jiaoyan Chen, Yarong Lan, Mengshu Sun, Zhiqiang Zhang, Yangyifei Luo, Qian Li, Qiang Zhang, Wen Zhang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07526">https://arxiv.org/abs/2410.07526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07526">https://arxiv.org/pdf/2410.07526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07526]] MKGL: Mastery of a Three-Word Language(https://arxiv.org/abs/2410.07526)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly advanced performance across a spectrum of natural language processing (NLP) tasks. Yet, their application to knowledge graphs (KGs), which describe facts in the form of triplets and allow minimal hallucinations, remains an underexplored frontier. In this paper, we investigate the integration of LLMs with KGs by introducing a specialized KG Language (KGL), where a sentence precisely consists of an entity noun, a relation verb, and ends with another entity noun. Despite KGL's unfamiliar vocabulary to the LLM, we facilitate its learning through a tailored dictionary and illustrative sentences, and enhance context understanding via real-time KG context retrieval and KGL token embedding augmentation. Our results reveal that LLMs can achieve fluency in KGL, drastically reducing errors compared to conventional KG embedding methods on KG completion. Furthermore, our enhanced LLM shows exceptional competence in generating accurate three-word sentences from an initial entity and interpreting new unseen terms out of KGs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在一系列自然语言处理 (NLP) 任务中具有显著的先进性能。然而，它们在知识图谱 (KG) 中的应用仍然是一个尚未充分探索的领域，知识图谱以三元组的形式描述事实并允许最小的幻觉。在本文中，我们通过引入专门的 KG 语言 (KGL) 来研究 LLM 与 KG 的集成，其中句子精确地由实体名词、关系动词组成，并以另一个实体名词结尾。尽管 KGL 的词汇对 LLM 来说并不熟悉，但我们通过量身定制的词典和说明性句子来促进其学习，并通过实时 KG 上下文检索和 KGL 标记嵌入增强来增强上下文理解。我们的结果表明，LLM 可以在 KGL 中实现流畅性，与传统的 KG 嵌入方法相比，大大减少了 KG 完成时的错误。此外，我们增强的 LLM 表现出卓越的能力，能够从初始实体生成准确的三字句子并从 KG 中解释新的未见过的术语。</li>
</ul>

<h3>Title: OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting</h3>
<ul>
<li><strong>Authors: </strong>Xukai Liu, Ye Liu, Kai Zhang, Kehang Wang, Qi Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07549">https://arxiv.org/abs/2410.07549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07549">https://arxiv.org/pdf/2410.07549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07549]] OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting(https://arxiv.org/abs/2410.07549)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Entity Linking (EL) is the process of associating ambiguous textual mentions to specific entities in a knowledge base. Traditional EL methods heavily rely on large datasets to enhance their performance, a dependency that becomes problematic in the context of few-shot entity linking, where only a limited number of examples are available for training. To address this challenge, we present OneNet, an innovative framework that utilizes the few-shot learning capabilities of Large Language Models (LLMs) without the need for fine-tuning. To the best of our knowledge, this marks a pioneering approach to applying LLMs to few-shot entity linking tasks. OneNet is structured around three key components prompted by LLMs: (1) an entity reduction processor that simplifies inputs by summarizing and filtering out irrelevant entities, (2) a dual-perspective entity linker that combines contextual cues and prior knowledge for precise entity linking, and (3) an entity consensus judger that employs a unique consistency algorithm to alleviate the hallucination in the entity linking reasoning. Comprehensive evaluations across seven benchmark datasets reveal that OneNet outperforms current state-of-the-art entity linking methods.</li>
<li><strong>摘要：</strong>实体链接 (EL) 是将模糊的文本提及与知识库中的特定实体相关联的过程。传统的 EL 方法严重依赖大型数据集来提高其性能，这种依赖关系在少数样本实体链接的背景下会成为问题，因为只有有限数量的示例可用于训练。为了应对这一挑战，我们提出了 OneNet，这是一个创新框架，它利用大型语言模型 (LLM) 的少数样本学习能力，而无需进行微调。据我们所知，这标志着将 LLM 应用于少数样本实体链接任务的开创性方法。OneNet 围绕 LLM 提示的三个关键组件构建：(1) 实体减少处理器，通过总结和过滤掉不相关的实体来简化输入，(2) 双视角实体链接器，结合上下文线索和先验知识进行精确的实体链接，以及 (3) 实体共识判断器，采用独特的一致性算法来缓解实体链接推理中的幻觉。对七个基准数据集的综合评估表明，OneNet 的表现优于当前最先进的实体链接方法。</li>
</ul>

<h3>Title: KRAG Framework for Enhancing LLMs in the Legal Domain</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Ha Thanh, Ken Satoh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07551">https://arxiv.org/abs/2410.07551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07551">https://arxiv.org/pdf/2410.07551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07551]] KRAG Framework for Enhancing LLMs in the Legal Domain(https://arxiv.org/abs/2410.07551)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>This paper introduces Knowledge Representation Augmented Generation (KRAG), a novel framework designed to enhance the capabilities of Large Language Models (LLMs) within domain-specific applications. KRAG points to the strategic inclusion of critical knowledge entities and relationships that are typically absent in standard data sets and which LLMs do not inherently learn. In the context of legal applications, we present Soft PROLEG, an implementation model under KRAG, which uses inference graphs to aid LLMs in delivering structured legal reasoning, argumentation, and explanations tailored to user inquiries. The integration of KRAG, either as a standalone framework or in tandem with retrieval augmented generation (RAG), markedly improves the ability of language models to navigate and solve the intricate challenges posed by legal texts and terminologies. This paper details KRAG's methodology, its implementation through Soft PROLEG, and potential broader applications, underscoring its significant role in advancing natural language understanding and processing in specialized knowledge domains.</li>
<li><strong>摘要：</strong>本文介绍了知识表示增强生成 (KRAG)，这是一种新颖的框架，旨在增强大型语言模型 (LLM) 在特定领域应用中的功能。KRAG 指出了战略性地纳入关键知识实体和关系，这些实体和关系通常不存在于标准数据集中，而 LLM 本身不会学习这些实体和关系。在法律应用的背景下，我们介绍了 Soft PROLEG，这是 KRAG 下的一个实现模型，它使用推理图来帮助 LLM 提供针对用户查询的结构化法律推理、论证和解释。KRAG 的集成，无论是作为独立框架还是与检索增强生成 (RAG) 结合使用，都显著提高了语言模型应对和解决法律文本和术语所带来的复杂挑战的能力。本文详细介绍了 KRAG 的方法、通过 Soft PROLEG 实现的方法以及潜在的更广泛应用，强调了其在促进专业知识领域的自然语言理解和处理方面的重要作用。</li>
</ul>

<h3>Title: AI-Press: A Multi-Agent News Generating and Feedback Simulation System Powered by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiawei Liu, Shiyue Yang, Xinnong Zhang, Haoyu Kuang, Libo Sun, Yihang Yang, Siming Chen, Xuanjing Huang, Zhongyu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07561">https://arxiv.org/abs/2410.07561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07561">https://arxiv.org/pdf/2410.07561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07561]] AI-Press: A Multi-Agent News Generating and Feedback Simulation System Powered by Large Language Models(https://arxiv.org/abs/2410.07561)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>The rise of various social platforms has transformed journalism. The growing demand for news content has led to the increased use of large language models (LLMs) in news production due to their speed and cost-effectiveness. However, LLMs still encounter limitations in professionalism and ethical judgment in news generation. Additionally, predicting public feedback is usually difficult before news is released. To tackle these challenges, we introduce AI-Press, an automated news drafting and polishing system based on multi-agent collaboration and Retrieval-Augmented Generation. We develop a feedback simulation system that generates public feedback considering demographic distributions. Through extensive quantitative and qualitative evaluations, our system shows significant improvements in news-generating capabilities and verifies the effectiveness of public feedback simulation.</li>
<li><strong>摘要：</strong>各种社交平台的兴起改变了新闻业。对新闻内容的需求不断增长，导致大型语言模型 (LLM) 在新闻制作中的使用增加，因为它们速度快、成本低。然而，LLM 在新闻生成的专业性和道德判断方面仍然存在局限性。此外，在新闻发布之前预测公众反馈通常很困难。为了应对这些挑战，我们推出了 AI-Press，这是一种基于多智能体协作和检索增强生成的自动新闻起草和润色系统。我们开发了一个反馈模拟系统，该系统会考虑人口分布来生成公众反馈。通过广泛的定量和定性评估，我们的系统在新闻生成能力方面显示出显着的改进，并验证了公众反馈模拟的有效性。</li>
</ul>

<h3>Title: PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency</h3>
<ul>
<li><strong>Authors: </strong>Kenshin Abe, Kaizaburo Chubachi, Yasuhiro Fujita, Yuta Hirokawa, Kentaro Imajo, Toshiki Kataoka, Hiroyoshi Komatsu, Hiroaki Mikami, Tsuguo Mogami, Shogo Murai, Kosuke Nakago, Daisuke Nishino, Toru Ogawa, Daisuke Okanohara, Yoshihiko Ozaki, Shotaro Sano, Shuji Suzuki, Tianqi Xu, Toshihiko Yanase (Preferred Elements, Inc.)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07563">https://arxiv.org/abs/2410.07563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07563">https://arxiv.org/pdf/2410.07563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07563]] PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency(https://arxiv.org/abs/2410.07563)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>We introduce PLaMo-100B, a large-scale language model designed for Japanese proficiency. The model was trained from scratch using 2 trillion tokens, with architecture such as QK Normalization and Z-Loss to ensure training stability during the training process. Post-training techniques, including Supervised Fine-Tuning and Direct Preference Optimization, were applied to refine the model's performance. Benchmark evaluations suggest that PLaMo-100B performs well, particularly in Japanese-specific tasks, achieving results that are competitive with frontier models like GPT-4.</li>
<li><strong>摘要：</strong>我们推出了 PLaMo-100B，这是一款专为日语水平设计的大规模语言模型。该模型使用 2 万亿个 token 从头开始​​训练，并采用 QK Normalization 和 Z-Loss 等架构来确保训练过程中的训练稳定性。我们采用了监督微调和直接偏好优化等训练后技术来优化模型的性能。基准评估表明 PLaMo-100B 表现良好，尤其是在特定于日语的任务中，其结果可与 GPT-4 等前沿模型相媲美。</li>
</ul>

<h3>Title: When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context</h3>
<ul>
<li><strong>Authors: </strong>Enrique Noriega-Atala, Robert Vacareanu, Salena Torres Ashton, Adarsh Pyarelal, Clayton T. Morrison, Mihai Surdeanu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07567">https://arxiv.org/abs/2410.07567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07567">https://arxiv.org/pdf/2410.07567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07567]] When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context(https://arxiv.org/abs/2410.07567)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We introduce a neural architecture finetuned for the task of scenario context generation: The relevant location and time of an event or entity mentioned in text. Contextualizing information extraction helps to scope the validity of automated finings when aggregating them as knowledge graphs. Our approach uses a high-quality curated dataset of time and location annotations in a corpus of epidemiology papers to train an encoder-decoder architecture. We also explored the use of data augmentation techniques during training. Our findings suggest that a relatively small fine-tuned encoder-decoder model performs better than out-of-the-box LLMs and semantic role labeling parsers to accurate predict the relevant scenario information of a particular entity or event.</li>
<li><strong>摘要：</strong>我们引入了一种针对场景上下文生成任务进行微调的神经架构：文本中提到的事件或实体的相关位置和时间。将信息提取置于上下文中有助于在将自动发现聚合为知识图谱时确定其有效性。我们的方法使用流行病学论文语料库中高质量的时间和位置注释数据集来训练编码器-解码器架构。我们还探索了在训练过程中使用数据增强技术。我们的研究结果表明，相对较小的微调编码器-解码器模型比开箱即用的 LLM 和语义角色标记解析器表现更好，可以准确预测特定实体或事件的相关场景信息。</li>
</ul>

<h3>Title: How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Seongyun Lee, Geewook Kim, Jiyeon Kim, Hyunji Lee, Hoyeon Chang, Sue Hyun Park, Minjoon Seo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07571">https://arxiv.org/abs/2410.07571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07571">https://arxiv.org/pdf/2410.07571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07571]] How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?(https://arxiv.org/abs/2410.07571)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Vision-Language adaptation (VL adaptation) transforms Large Language Models (LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this process often compromises the inherent safety capabilities embedded in the original LLMs. Despite potential harmfulness due to weakened safety measures, in-depth analysis on the effects of VL adaptation on safety remains under-explored. This study examines how VL adaptation influences safety and evaluates the impact of safety fine-tuning methods. Our analysis reveals that safety degradation occurs during VL adaptation, even when the training data is safe. While safety tuning techniques like supervised fine-tuning with safety datasets or reinforcement learning from human feedback mitigate some risks, they still lead to safety degradation and a reduction in helpfulness due to over-rejection issues. Further analysis of internal model weights suggests that VL adaptation may impact certain safety-related layers, potentially lowering overall safety levels. Additionally, our findings demonstrate that the objectives of VL adaptation and safety tuning are divergent, which often results in their simultaneous application being suboptimal. To address this, we suggest the weight merging approach as an optimal solution effectively reducing safety degradation while maintaining helpfulness. These insights help guide the development of more reliable and secure LVLMs for real-world applications.</li>
<li><strong>摘要：</strong>视觉语言适应 (VL 适应) 将大型语言模型 (LLM) 转换为大型视觉语言模型 (LVLM)，用于多模态任务，但此过程通常会损害原始 LLM 中嵌入的固有安全功能。尽管由于安全措施减弱而可能造成危害，但对 VL 适应对安全性的影响的深入分析仍未得到充分探索。本研究探讨了 VL 适应如何影响安全性，并评估了安全性微调方法的影响。我们的分析表明，即使训练数据是安全的，在 VL 适应期间也会发生安全性下降。虽然安全调整技术（例如使用安全数据集进行监督微调或从人类反馈中进行强化学习）可以减轻一些风险，但由于过度拒绝问题，它们仍会导致安全性下降和有用性降低。对内部模型权重的进一步分析表明，VL 适应可能会影响某些与安全相关的层，从而可能降低整体安全水平。此外，我们的研究结果表明，VL 适应和安全调整的目标是不同的，这通常导致它们同时应用的效果不佳。为了解决这个问题，我们建议使用权重合并方法作为最佳解决方案，在保持实用性的同时有效降低安全性。这些见解有助于指导为实际应用开发更可靠、更安全的 LVLM。</li>
</ul>

<h3>Title: Detecting Training Data of Large Language Models via Expectation Maximization</h3>
<ul>
<li><strong>Authors: </strong>Gyuwan Kim, Yang Li, Evangelia Spiliopoulou, Jie Ma, Miguel Ballesteros, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07582">https://arxiv.org/abs/2410.07582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07582">https://arxiv.org/pdf/2410.07582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07582]] Detecting Training Data of Large Language Models via Expectation Maximization(https://arxiv.org/abs/2410.07582)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The widespread deployment of large language models (LLMs) has led to impressive advancements, yet information about their training data, a critical factor in their performance, remains undisclosed. Membership inference attacks (MIAs) aim to determine whether a specific instance was part of a target model's training data. MIAs can offer insights into LLM outputs and help detect and address concerns such as data contamination and compliance with privacy and copyright standards. However, applying MIAs to LLMs presents unique challenges due to the massive scale of pre-training data and the ambiguous nature of membership. Additionally, creating appropriate benchmarks to evaluate MIA methods is not straightforward, as training and test data distributions are often unknown. In this paper, we introduce EM-MIA, a novel MIA method for LLMs that iteratively refines membership scores and prefix scores via an expectation-maximization algorithm, leveraging the duality that the estimates of these scores can be improved by each other. Membership scores and prefix scores assess how each instance is likely to be a member and discriminative as a prefix, respectively. Our method achieves state-of-the-art results on the WikiMIA dataset. To further evaluate EM-MIA, we present OLMoMIA, a benchmark built from OLMo resources, which allows us to control the difficulty of MIA tasks with varying degrees of overlap between training and test data distributions. We believe that EM-MIA serves as a robust MIA method for LLMs and that OLMoMIA provides a valuable resource for comprehensively evaluating MIA approaches, thereby driving future research in this critical area.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的广泛部署带来了令人瞩目的进步，但有关其训练数据的信息（其性能的一个关键因素）仍未公开。成员推理攻击 (MIA) 旨在确定特定实例是否是目标模型训练数据的一部分。MIA 可以深入了解 LLM 输出，并帮助检测和解决数据污染和隐私和版权标准合规性等问题。然而，由于预训练数据的规模巨大以及成员资格的模糊性，将 MIA 应用于 LLM 带来了独特的挑战。此外，创建适当的基准来评估 MIA 方法并不是一件容易的事，因为训练和测试数据的分布通常是未知的。在本文中，我们介绍了 EM-MIA，这是一种用于 LLM 的新型 MIA 方法，它通过期望最大化算法迭代地细化成员资格分数和前缀分数，利用这些分数的估计值可以相互改进的二元性。成员资格分数和前缀分数分别评估每个实例成为成员的可能性和作为前缀的判别性。我们的方法在 WikiMIA 数据集上取得了最佳结果。为了进一步评估 EM-MIA，我们提出了 OLMoMIA，这是一个基于 OLMo 资源构建的基准，它使我们能够控制 MIA 任务的难度，训练和测试数据分布之间的重叠程度各不相同。我们相信 EM-MIA 是一种适用于 LLM 的强大 MIA 方法，OLMoMIA 为全面评估 MIA 方法提供了宝贵的资源，从而推动了这一关键领域的未来研究。</li>
</ul>

<h3>Title: StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Minchan Kwon, Gaeun Kim, Jongsuk Kim, Haeil Lee, Junmo Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07652">https://arxiv.org/abs/2410.07652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07652">https://arxiv.org/pdf/2410.07652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07652]] StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models(https://arxiv.org/abs/2410.07652)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Finding appropriate prompts for the specific task has become an important issue as the usage of Large Language Models (LLM) has expanded. Reinforcement Learning (RL) is widely used for prompt tuning, but its inherent instability and environmental dependency make it difficult to use in practice. In this paper, we propose StablePrompt, which strikes a balance between training stability and search space, mitigating the instability of RL and producing high-performance prompts. We formulate prompt tuning as an online RL problem between the agent and target LLM and introduce Adaptive Proximal Policy Optimization (APPO). APPO introduces an LLM anchor model to adaptively adjust the rate of policy updates. This allows for flexible prompt search while preserving the linguistic ability of the pre-trained LLM. StablePrompt outperforms previous methods on various tasks including text classification, question answering, and text generation. Our code can be found in github.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的使用范围不断扩大，为特定任务找到合适的提示已成为一个重要问题。强化学习 (RL) 被广泛用于提示调整，但其固有的不稳定性以及对环境的依赖性使其难以在实践中使用。在本文中，我们提出了 StablePrompt，它在训练稳定性和搜索空间之间取得平衡，减轻了 RL 的不稳定性并产生了高性能的提示。我们将提示调整公式化为代理和目标 LLM 之间的在线 RL 问题，并引入了自适应近端策略优化 (APPO)。APPO 引入了 LLM 锚模型来自适应地调整策略更新率。这允许灵活的提示搜索，同时保留预训练 LLM 的语言能力。StablePrompt 在包括文本分类、问答和文本生成在内的各种任务上都优于以前的方法。我们的代码可以在 github 中找到。</li>
</ul>

<h3>Title: MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yougang Lyu, Lingyong Yan, Zihan Wang, Dawei Yin, Pengjie Ren, Maarten de Rijke, Zhaochun Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07672">https://arxiv.org/abs/2410.07672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07672">https://arxiv.org/pdf/2410.07672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07672]] MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization(https://arxiv.org/abs/2410.07672)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are rapidly advancing and achieving near-human capabilities, aligning them with human values is becoming more urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong alignment problem where we need to effectively align strong student LLMs through weak supervision generated by weak teachers. Existing alignment methods mainly focus on strong-to-weak alignment and self-alignment settings, and it is impractical to adapt them to the much harder weak-to-strong alignment setting. To fill this gap, we propose a multi-agent contrastive preference optimization (MACPO) framework. MACPO facilitates weak teachers and strong students to learn from each other by iteratively reinforcing unfamiliar positive behaviors while penalizing familiar negative ones. To get this, we devise a mutual positive behavior augmentation strategy to encourage weak teachers and strong students to learn from each other's positive behavior and further provide higher quality positive behavior for the next iteration. Additionally, we propose a hard negative behavior construction strategy to induce weak teachers and strong students to generate familiar negative behavior by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human judgments, demonstrate that MACPO simultaneously improves the alignment performance of strong students and weak teachers. Moreover, as the number of weak teachers increases, MACPO achieves better weak-to-strong alignment performance through more iteration optimization rounds.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 迅速发展并实现接近人类的能力，使其与人类价值观保持一致变得更加紧迫。在 LLM 表现优于人类的场景中，我们面临着弱对强对齐问题，我们需要通过弱教师产生的弱监督来有效地对齐强学生 LLM。现有的对齐方法主要侧重于强对弱对齐和自对齐设置，而使其适应更难的弱对强对齐设置是不切实际的。为了填补这一空白，我们提出了一个多智能体对比偏好优化 (MACPO) 框架。MACPO 通过迭代地强化不熟悉的积极行为同时惩罚熟悉的消极行为，促进弱教师和强学生相互学习。为了实现这一点，我们设计了一种相互的积极行为增强策略，鼓励弱教师和强学生学习彼此的积极行为，并为下一次迭代提供更高质量的积极行为。此外，我们提出了一种严格的负面行为构建策略，通过对负面行为数据进行微调，诱导弱教师和强学生产生熟悉的负面行为。在 HH-RLHF 和 PKU-SafeRLHF 数据集上的实验结果（使用自动指标和人工判断进行评估）表明，MACPO 同时提高了强学生和弱教师的对齐性能。此外，随着弱教师数量的增加，MACPO 通过更多的迭代优化轮次实现了更好的弱到强对齐性能。</li>
</ul>

<h3>Title: Smart Audit System Empowered by LLM</h3>
<ul>
<li><strong>Authors: </strong>Xu Yao, Xiaoxu Wu, Xi Li, Huan Xu, Chenlei Li, Ping Huang, Si Li, Xiaoning Ma, Jiulong Shan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07677">https://arxiv.org/abs/2410.07677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07677">https://arxiv.org/pdf/2410.07677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07677]] Smart Audit System Empowered by LLM(https://arxiv.org/abs/2410.07677)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Manufacturing quality audits are pivotal for ensuring high product standards in mass production environments. Traditional auditing processes, however, are labor-intensive and reliant on human expertise, posing challenges in maintaining transparency, accountability, and continuous improvement across complex global supply chains. To address these challenges, we propose a smart audit system empowered by large language models (LLMs). Our approach introduces three innovations: a dynamic risk assessment model that streamlines audit procedures and optimizes resource allocation; a manufacturing compliance copilot that enhances data processing, retrieval, and evaluation for a self-evolving manufacturing knowledge base; and a Re-act framework commonality analysis agent that provides real-time, customized analysis to empower engineers with insights for supplier improvement. These enhancements elevate audit efficiency and effectiveness, with testing scenarios demonstrating an improvement of over 24%.</li>
<li><strong>摘要：</strong>制造质量审核对于确保大规模生产环境中的高产品标准至关重要。然而，传统的审核流程是劳动密集型的，并且依赖于人类的专业知识，这对在复杂的全球供应链中保持透明度、问责制和持续改进构成了挑战。为了应对这些挑战，我们提出了一个由大型语言模型 (LLM) 支持的智能审核系统。我们的方法引入了三项创新：动态风险评估模型，可简化审核程序并优化资源配置；制造合规性副驾驶，可增强自我演进制造知识库的数据处理、检索和评估；以及 Re-act 框架通用性分析代理，可提供实时、定制的分析，使工程师能够洞察供应商改进。这些增强功能提高了审核效率和有效性，测试场景显示改进幅度超过 24%。</li>
</ul>

<h3>Title: Multi-Facet Counterfactual Learning for Content Quality Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Jiasheng Zheng, Hongyu Lin, Boxi Cao, Meng Liao, Yaojie Lu, Xianpei Han, Le Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07693">https://arxiv.org/abs/2410.07693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07693">https://arxiv.org/pdf/2410.07693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07693]] Multi-Facet Counterfactual Learning for Content Quality Evaluation(https://arxiv.org/abs/2410.07693)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Evaluating the quality of documents is essential for filtering valuable content from the current massive amount of information. Conventional approaches typically rely on a single score as a supervision signal for training content quality evaluators, which is inadequate to differentiate documents with quality variations across multiple facets. In this paper, we propose Multi-facet cOunterfactual LEarning (MOLE), a framework for efficiently constructing evaluators that perceive multiple facets of content quality evaluation. Given a specific scenario, we prompt large language models to generate counterfactual content that exhibits variations in critical quality facets compared to the original document. Furthermore, we leverage a joint training strategy based on contrastive learning and supervised learning to enable the evaluator to distinguish between different quality facets, resulting in more accurate predictions of content quality scores. Experimental results on 2 datasets across different scenarios demonstrate that our proposed MOLE framework effectively improves the correlation of document content quality evaluations with human judgments, which serve as a valuable toolkit for effective information acquisition.</li>
<li><strong>摘要：</strong>评估文档质量对于从当前海量的信息中筛选出有价值的内容至关重要。传统方法通常依赖单一分数作为监督信号来训练内容质量评估员，这不足以区分在多个方面质量存在差异的文档。在本文中，我们提出了多方面反事实学习 (MOLE)，这是一个有效构建评估员的框架，可以感知内容质量评估的多个方面。在特定场景下，我们提示大型语言模型生成反事实内容，与原始文档相比，这些内容在关键质量方面存在差异。此外，我们利用基于对比学习和监督学习的联合训练策略，使评估员能够区分不同的质量方面，从而更准确地预测内容质量分数。在不同场景下对 2 个数据集的实验结果表明，我们提出的 MOLE 框架有效地提高了文档内容质量评估与人类判断的相关性，这为有效获取信息提供了宝贵的工具包。</li>
</ul>

<h3>Title: AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, Sujian Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07706">https://arxiv.org/abs/2410.07706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07706">https://arxiv.org/pdf/2410.07706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07706]] AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories(https://arxiv.org/abs/2410.07706)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AgentBank, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories which comprises 16 tasks covering five distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are able to scale the annotated trajectories and generate a trajectory dataset with minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a series of agent models, Samoyed. Our comparative experiments demonstrate the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization.</li>
<li><strong>摘要：</strong>对代理-环境交互轨迹数据进行微调，对于在开源大型语言模型 (LLM) 中实现广义代理功能具有重要意义。在这项工作中，我们引入了 AgentBank，这是迄今为止最大的轨迹调整数据集合，具有超过 50,000 条不同的高质量交互轨迹，包含 16 个任务，涵盖五个不同的代理技能维度。利用新颖的注释管道，我们能够扩展注释轨迹并生成具有最小难度偏差的轨迹数据集。此外，我们在 AgentBank 上微调 LLM 以获得一系列代理模型 Samoyed。我们的比较实验证明了扩展交互轨迹数据以获得广义代理功能的有效性。其他研究还揭示了一些关于轨迹调整和代理技能泛化的关键观察结果。</li>
</ul>

<h3>Title: StepTool: A Step-grained Reinforcement Learning Framework for Tool Learning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuanqing Yu, Zhefan Wang, Weizhi Ma, Zhicheng Guo, Jingtao Zhan, Shuai Wang, Chuhan Wu, Zhiqiang Guo, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07745">https://arxiv.org/abs/2410.07745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07745">https://arxiv.org/pdf/2410.07745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07745]] StepTool: A Step-grained Reinforcement Learning Framework for Tool Learning in LLMs(https://arxiv.org/abs/2410.07745)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite having powerful reasoning and inference capabilities, Large Language Models (LLMs) still need external tools to acquire real-time information retrieval or domain-specific expertise to solve complex tasks, which is referred to as tool learning. Existing tool learning methods primarily rely on tuning with expert trajectories, focusing on token-sequence learning from a linguistic perspective. However, there are several challenges: 1) imitating static trajectories limits their ability to generalize to new tasks. 2) even expert trajectories can be suboptimal, and better solution paths may exist. In this work, we introduce StepTool, a novel step-grained reinforcement learning framework to improve tool learning in LLMs. It consists of two components: Step-grained Reward Shaping, which assigns rewards at each tool interaction based on tool invocation success and its contribution to the task, and Step-grained Optimization, which uses policy gradient methods to optimize the model in a multi-step manner. Experimental results demonstrate that StepTool significantly outperforms existing methods in multi-step, tool-based tasks, providing a robust solution for complex task environments. Codes are available at this https URL.</li>
<li><strong>摘要：</strong>尽管具有强大的推理和推理能力，大型语言模型 (LLM) 仍然需要外部工具来获取实时信息检索或领域特定专业知识来解决复杂任务，这称为工具学习。现有的工具学习方法主要依赖于使用专家轨迹进行调整，侧重于从语言学角度进行标记序列学习。然而，存在几个挑战：1）模仿静态轨迹限制了它们推广到新任务的能力。2）即使是专家轨迹也可能不是最优的，并且可能存在更好的解决方案路径。在这项工作中，我们引入了 StepTool，一种新颖的分步粒度强化学习框架，用于改进 LLM 中的工具学习。它由两个组件组成：分步粒度奖励塑造，它根据工具调用成功率及其对任务的贡献在每次工具交互时分配奖励，以及分步粒度优化，它使用策略梯度方法以多步骤方式优化模型。实验结果表明，StepTool 在多步骤、基于工具的任务中的表现明显优于现有方法，为复杂的任务环境提供了强大的解决方案。代码可在此 https URL 上获取。</li>
</ul>

<h3>Title: GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Umair Nasir, Steven James, Julian Togelius</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07765">https://arxiv.org/abs/2410.07765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07765">https://arxiv.org/pdf/2410.07765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07765]] GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps(https://arxiv.org/abs/2410.07765)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently demonstrated great success in generating and understanding natural language. While they have also shown potential beyond the domain of natural language, it remains an open question as to what extent and in which way these LLMs can plan. We investigate their planning capabilities by proposing GameTraversalBenchmark (GTB), a benchmark consisting of diverse 2D grid-based game maps. An LLM succeeds if it can traverse through given objectives, with a minimum number of steps and a minimum number of generation errors. We evaluate a number of LLMs on GTB and found that GPT-4-Turbo achieved the highest score of 44.97% on GTB\_Score (GTBS), a composite score that combines the three above criteria. Furthermore, we preliminarily test large reasoning models, namely o1, which scores $67.84\%$ on GTBS, indicating that the benchmark remains challenging for current models. Code, data, and documentation are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 最近在生成和理解自然语言方面取得了巨大成功。虽然它们也显示出超越自然语言领域的潜力，但这些 LLM 能够在多大程度上以及以何种方式进行规划仍是一个悬而未决的问题。我们通过提出 GameTraversalBenchmark (GTB) 来研究它们的规划能力，GTB 是一个由各种基于 2D 网格的游戏地图组成的基准。如果 LLM 能够以最少的步骤和最少的生成错误遍历给定的目标，则它成功。我们在 GTB 上评估了许多 LLM，发现 GPT-4-Turbo 在 GTB\_Score (GTBS) 上获得了 44.97% 的最高分，GTBS 是一个结合了上述三个标准的综合分数。此外，我们初步测试了大型推理模型，即 o1，它在 GTBS 上的得分为 $67.84\%$，表明基准对当前模型来说仍然具有挑战性。代码、数据和文档可在此 https URL 上找到。</li>
</ul>

<h3>Title: Dialectical Behavior Therapy Approach to LLM Prompting</h3>
<ul>
<li><strong>Authors: </strong>Oxana Vitman, Nika Amaglobeli, Paul Plachinda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07768">https://arxiv.org/abs/2410.07768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07768">https://arxiv.org/pdf/2410.07768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07768]] Dialectical Behavior Therapy Approach to LLM Prompting(https://arxiv.org/abs/2410.07768)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models demonstrated state-of-the-art results on various reasoning tasks when applying the chain-of-thought (CoT) prompting technique. CoT prompting guides the model into breaking tasks into a few intermediate steps and provides step-by-step demonstrations. However, solving complex reasoning tasks remains a challenge. In this paper, we propose a novel prompting strategy inspired by Dialectical Behavioral Therapy (DBT). DBT, a form of cognitive-behavioral therapy, aims to help individuals cope with stress by developing a system of reasoning. We applied DBT's basic concepts of shaping dialog to construct prompts and conducted experiments on different datasets and LLMs with various numbers of parameters. Our results show that prompts crafted with DBT techniques significantly improve results on smaller models, achieving a 7% increase in accuracy on the StrategyQA, 4.8% on Aqua dataset using 8b parameters model, and a 16.2% increase on the StrategyQA, 5.3% on GSM8K dataset with 14b parameters model.</li>
<li><strong>摘要：</strong>大型语言模型在应用思路链 (CoT) 提示技术时，在各种推理任务上都表现出了最佳效果。CoT 提示引导模型将任务分解为几个中间步骤，并提供分步演示。然而，解决复杂的推理任务仍然是一个挑战。在本文中，我们提出了一种受辩证行为疗法 (DBT) 启发的新型提示策略。DBT 是一种认知行为疗法，旨在通过开发推理系统来帮助个人应对压力。我们应用 DBT 塑造对话的基本概念来构建提示，并在具有不同数量参数的不同数据集和 LLM 上进行了实验。我们的结果表明，使用 DBT 技术制作的提示显著改善了较小模型的结果，在 StrategyQA 上的准确率提高了 7%，在使用 8b 参数模型的 Aqua 数据集上提高了 4.8%，在 StrategyQA 上提高了 16.2%，在具有 14b 参数模型的 GSM8K 数据集上提高了 5.3%。</li>
</ul>

<h3>Title: Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Sweta Agrawal, José G. C. de Souza, Ricardo Rei, António Farinhas, Gonçalo Faria, Patrick Fernandes, Nuno M Guerreiro, Andre Martins</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07779">https://arxiv.org/abs/2410.07779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07779">https://arxiv.org/pdf/2410.07779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07779]] Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation(https://arxiv.org/abs/2410.07779)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Alignment with human preferences is an important step in developing accurate and safe large language models. This is no exception in machine translation (MT), where better handling of language nuances and context-specific variations leads to improved quality. However, preference data based on human feedback can be very expensive to obtain and curate at a large scale. Automatic metrics, on the other hand, can induce preferences, but they might not match human expectations perfectly. In this paper, we propose an approach that leverages the best of both worlds. We first collect sentence-level quality assessments from professional linguists on translations generated by multiple high-quality MT systems and evaluate the ability of current automatic metrics to recover these preferences. We then use this analysis to curate a new dataset, MT-Pref (metric induced translation preference) dataset, which comprises 18k instances covering 18 language directions, using texts sourced from multiple domains post-2022. We show that aligning TOWER models on MT-Pref significantly improves translation quality on WMT23 and FLORES benchmarks.</li>
<li><strong>摘要：</strong>与人类偏好保持一致是开发准确且安全的大型语言模型的重要一步。机器翻译 (MT) 也不例外，更好地处理语言细微差别和上下文特定变化可以提高质量。但是，基于人类反馈的偏好数据的获取和大规模整理成本可能非常高。另一方面，自动指标可以诱导偏好，但它们可能与人类的期望不完全匹配。在本文中，我们提出了一种利用两全其美的方法。我们首先从专业语言学家那里收集由多个高质量 MT 系统生成的翻译的句子级质量评估，并评估当前自动指标恢复这些偏好的能力。然后，我们使用此分析来整理一个新的数据集 MT-Pref（指标诱导翻译偏好）数据集，该数据集包含 18k 个实例，涵盖 18 个语言方向，使用来自 2022 年后多个领域的文本。我们表明，在 MT-Pref 上对齐 TOWER 模型可显著提高 WMT23 和 FLORES 基准的翻译质量。</li>
</ul>

<h3>Title: Rewriting Conversational Utterances with Instructed Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Elnara Galimzhanova, Cristina Ioana Muntean, Franco Maria Nardini, Raffaele Perego, Guido Rocchietti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07797">https://arxiv.org/abs/2410.07797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07797">https://arxiv.org/pdf/2410.07797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07797]] Rewriting Conversational Utterances with Instructed Large Language Models(https://arxiv.org/abs/2410.07797)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Many recent studies have shown the ability of large language models (LLMs) to achieve state-of-the-art performance on many NLP tasks, such as question answering, text summarization, coding, and translation. In some cases, the results provided by LLMs are on par with those of human experts. These models' most disruptive innovation is their ability to perform tasks via zero-shot or few-shot prompting. This capability has been successfully exploited to train instructed LLMs, where reinforcement learning with human feedback is used to guide the model to follow the user's requests directly. In this paper, we investigate the ability of instructed LLMs to improve conversational search effectiveness by rewriting user questions in a conversational setting. We study which prompts provide the most informative rewritten utterances that lead to the best retrieval performance. Reproducible experiments are conducted on publicly-available TREC CAST datasets. The results show that rewriting conversational utterances with instructed LLMs achieves significant improvements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and 11.5% in Recall@500 over state-of-the-art techniques.</li>
<li><strong>摘要：</strong>许多近期研究表明，大型语言模型 (LLM) 能够在问答、文本摘要、编码和翻译等许多 NLP 任务上实现最佳性能。在某些情况下，LLM 提供的结果与人类专家的结果相当。这些模型最具颠覆性的创新是它们能够通过零样本或少样本提示执行任务。此功能已成功用于训练指导式 LLM，其中使用带有人工反馈的强化学习来指导模型直接遵循用户的请求。在本文中，我们研究了指导式 LLM 通过在对话环境中重写用户问题来提高对话搜索效果的能力。我们研究哪些提示提供了最具信息量的重写话语，从而带来最佳的检索性能。在公开的 TREC CAST 数据集上进行了可重复的实验。结果表明，与最先进的技术相比，使用指导的 LLM 重写对话话语可以实现显著的改进，MRR 提高了 25.2%，Precision@1 提高了 31.7%，NDCG@3 提高了 27%，Recall@500 提高了 11.5%。</li>
</ul>

<h3>Title: Linguistically-Informed Multilingual Instruction Tuning: Is There an Optimal Set of Languages to Tune?</h3>
<ul>
<li><strong>Authors: </strong>Gürkan Soykan, Gözde Gül Şahin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07809">https://arxiv.org/abs/2410.07809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07809">https://arxiv.org/pdf/2410.07809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07809]] Linguistically-Informed Multilingual Instruction Tuning: Is There an Optimal Set of Languages to Tune?(https://arxiv.org/abs/2410.07809)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Multilingual language models often perform unevenly across different languages due to limited generalization capabilities for some languages. This issue is significant because of the growing interest in making universal language models that work well for all languages. Instruction tuning with multilingual instruction-response pairs has been used to improve model performance across various languages. However, this approach is challenged by high computational costs, a lack of quality tuning data for all languages, and the "curse of multilinguality" -- the performance drop per language after adding many languages. Recent studies have found that working with datasets with few languages and a smaller number of instances can be beneficial. Yet, there exists no systematic investigation into how choosing different languages affects multilingual instruction tuning. Our study proposes a method to select languages for instruction tuning in a linguistically informed way, aiming to boost model performance across languages and tasks. We use a simple algorithm to choose diverse languages and test their effectiveness on various benchmarks and open-ended questions. Our results show that this careful selection generally leads to better outcomes than choosing languages at random. We suggest a new and simple way of enhancing multilingual models by selecting diverse languages based on linguistic features that could help develop better multilingual systems and guide dataset creation efforts. All resources, including the code for language selection and multilingual instruction tuning, are made available in our official repository at this https URL enabling reproducibility and further research in this area.</li>
<li><strong>摘要：</strong>由于某些语言的泛化能力有限，多语言语言模型在不同语言之间的表现通常不均衡。这个问题很重要，因为人们越来越有兴趣制作适用于所有语言的通用语言模型。使用多语言指令-响应对进行指令调整已被用于提高跨各种语言的模型性能。然而，这种方法面临着高计算成本、缺乏所有语言的高质量调整数据以及“多语言诅咒”——添加多种语言后每种语言的性能下降的挑战。最近的研究发现，使用语言较少、实例数量较少的数据集可能会有所帮助。然而，目前还没有系统地研究选择不同的语言如何影响多语言指令调整。我们的研究提出了一种以语言学的方式选择语言进行指令调整的方法，旨在提高跨语言和任务的模型性能。我们使用一种简单的算法来选择不同的语言，并在各种基准和开放式问题上测试它们的有效性。我们的结果表明，这种仔细的选择通常比随机选择语言产生更好的结果。我们提出了一种增强多语言模型的简单新方法，即根据语言特征选择不同的语言，这有助于开发更好的多语言系统并指导数据集创建工作。所有资源（包括用于语言选择和多语言指令调整的代码）都在我们的官方存储库中提供，网址为 https URL，以便可重复性并进一步研究该领域。</li>
</ul>

<h3>Title: Uncovering Overfitting in Large Language Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, Zhumin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07819">https://arxiv.org/abs/2410.07819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07819">https://arxiv.org/pdf/2410.07819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07819]] Uncovering Overfitting in Large Language Model Editing(https://arxiv.org/abs/2410.07819)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon of Editing Overfit, where edited models assign disproportionately high probabilities to the edit target, hindering the generalization of new knowledge in complex scenarios. We attribute this issue to the current editing paradigm, which places excessive emphasis on the direct correspondence between the input prompt and the edit target for each edit sample. To further explore this issue, we introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge Editing), along with fine-grained evaluation metrics. Through comprehensive experiments and analysis, we demonstrate that Editing Overfit is prevalent in current editing methods and that common overfitting mitigation strategies are of limited effectiveness in knowledge editing. To overcome this, inspired by LLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy called Learn to Inference (LTI), which introduce a Multi-stage Inference Constraint module to guide the edited models in recalling new knowledge similarly to how unedited LLMs leverage knowledge through in-context learning. Extensive experimental results across a wide range of tasks validate the effectiveness of LTI in mitigating Editing Overfit.</li>
<li><strong>摘要：</strong>知识编辑已被提出作为更新和纠正大型语言模型 (LLM) 内部知识的有效方法。然而，现有的编辑方法通常难以处理多跳推理等复杂任务。在本文中，我们识别并研究了编辑过拟合现象，即编辑模型为编辑目标分配不成比例的高概率，阻碍了新知识在复杂场景中的泛化。我们将此问题归因于当前的编辑范式，该范式过分强调每个编辑样本的输入提示和编辑目标之间的直接对应关系。为了进一步探讨这个问题，我们引入了一个新的基准 EVOKE（知识编辑中的编辑过拟合评估）以及细粒度的评估指标。通过全面的实验和分析，我们证明编辑过拟合在当前的编辑方法中普遍存在，并且常见的过拟合缓解策略在知识编辑中效果有限。为了克服这一问题，我们受到 LLM 知识回忆机制的启发，提出了一种名为“学习推理 (LTI)”的即插即用新策略，该策略引入了一个多阶段推理约束模块，以指导编辑后的模型回忆新知识，类似于未编辑的 LLM 通过上下文学习利用知识的方式。广泛任务中的大量实验结果验证了 LTI 在缓解编辑过拟合方面的有效性。</li>
</ul>

<h3>Title: Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Chen, Liang Song, Kun Zhou, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07825">https://arxiv.org/abs/2410.07825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07825">https://arxiv.org/pdf/2410.07825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07825]] Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models(https://arxiv.org/abs/2410.07825)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multi-lingual ability transfer has become increasingly important for the broad application of large language models (LLMs). Existing work highly relies on training with the multi-lingual ability-related data, which may be not available for low-resource languages. To solve it, we propose a Multi-lingual Ability Extraction and Transfer approach, named as MAET. Our key idea is to decompose and extract language-agnostic ability-related weights from LLMs, and transfer them across different languages by simple addition and subtraction operations without training. Specially, our MAET consists of the extraction and transfer stages. In the extraction stage, we firstly locate key neurons that are highly related to specific abilities, and then employ them to extract the transferable ability-specific weights. In the transfer stage, we further select the ability-related parameter tensors, and design the merging strategy based on the linguistic and ability specific weights, to build the multi-lingual ability-enhanced LLM. To demonstrate the effectiveness of our proposed approach, we conduct extensive experiments on mathematical and scientific tasks in both high-resource lingual and low-resource lingual scenarios. Experiment results have shown that MAET can effectively and efficiently extract and transfer the advanced abilities, and outperform training-based baseline methods. Our code and data are available at \url{this https URL}.</li>
<li><strong>摘要：</strong>多语言能力迁移对于大型语言模型 (LLM) 的广泛应用变得越来越重要。现有的工作高度依赖于使用多语言能力相关数据进行训练，而这些数据对于资源较少的语言可能无法获得。为了解决这个问题，我们提出了一种多语言能力提取和迁移方法，称为 MAET。我们的关键思想是从 LLM 中分解和提取与语言无关的能力相关权重，并通过简单的加减运算将它们在不同语言之间迁移，而无需训练。具体来说，我们的 MAET 包括提取和迁移阶段。在提取阶段，我们首先找到与特定能力高度相关的关键神经元，然后使用它们来提取可迁移的能力特定权重。在迁移阶段，我们进一步选择与能力相关的参数张量，并根据语言和能力特定权重设计合并策略，以构建多语言能力增强的 LLM。为了证明我们提出的方法的有效性，我们在高资源语言和低资源语言场景中对数学和科学任务进行了广泛的实验。实验结果表明，MAET 可以有效、高效地提取和迁移高级能力，并且优于基于训练的基线方法。我们的代码和数据可在 \url{此 https URL} 上找到。</li>
</ul>

<h3>Title: Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study of Alignment with Human Responses</h3>
<ul>
<li><strong>Authors: </strong>Pranav Senthilkumar, Visshwa Balasubramanian, Prisha Jain, Aneesa Maity, Jonathan Lu, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07826">https://arxiv.org/abs/2410.07826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07826">https://arxiv.org/pdf/2410.07826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07826]] Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study of Alignment with Human Responses(https://arxiv.org/abs/2410.07826)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Language models often misinterpret human intentions due to their handling of ambiguity, a limitation well-recognized in NLP research. While morally clear scenarios are more discernible to LLMs, greater difficulty is encountered in morally ambiguous contexts. In this investigation, we explored LLM calibration to show that human and LLM judgments are poorly aligned in such scenarios. We used two curated datasets from the Scruples project for evaluation: DILEMMAS, which involves pairs of distinct moral scenarios to assess the model's ability to compare and contrast ethical situations, and ANECDOTES, which presents individual narratives to evaluate the model's skill in drawing out details, interpreting, and analyzing distinct moral scenarios. Model answer probabilities were extracted for all possible choices and compared with human annotations to benchmark the alignment of three models: Llama-3.1-8b, Zephyr-7b-beta, and Mistral-7b. Significant improvements were observed after fine-tuning, with notable enhancements in both cross-entropy and Dirichlet scores, particularly in the latter. Notably, after fine-tuning, the performance of Mistral-7B-Instruct-v0.3 was on par with GPT-4o. However, the experimental models that were examined were all still outperformed by the BERT and RoBERTa models in terms of cross-entropy scores. Our fine-tuning approach, which improves the model's understanding of text distributions in a text-to-text format, effectively enhances performance and alignment in complex decision-making contexts, underscoring the need for further research to refine ethical reasoning techniques and capture human judgment nuances.</li>
<li><strong>摘要：</strong>语言模型经常会因为处理歧义而误解人类的意图，这是 NLP 研究中众所周知的一个限制。虽然 LLM 更容易辨别道德明确的场景，但在道德模糊的背景下会遇到更大的困难。在本次调查中，我们探索了 LLM 校准，以表明人类和 LLM 的判断在这种情况下不一致。我们使用了 Scruples 项目中的两个精选数据集进行评估：DILEMMAS，它涉及成对的不同道德场景，以评估模型比较和对比道德情况的能力；ANECDOTES，它呈现个人叙述，以评估模型提取细节、解释和分析不同道德场景的技能。提取了所有可能选择的模型答案概率，并与人类注释进行比较，以对三个模型的一致性进行基准测试：Llama-3.1-8b、Zephyr-7b-beta 和 Mistral-7b。微调后观察到了显著的改进，交叉熵和狄利克雷得分都有显著增强，尤其是后者。值得注意的是，经过微调后，Mistral-7B-Instruct-v0.3 的性能与 GPT-4o 相当。然而，在交叉熵得分方面，所检查的实验模型仍然都比 BERT 和 RoBERTa 模型表现更好。我们的微调方法提高了模型对文本到文本格式的文本分布的理解，有效地提高了复杂决策环境中的性能和一致性，强调需要进一步研究以改进道德推理技术并捕捉人类判断的细微差别。</li>
</ul>

<h3>Title: Why do objects have many names? A study on word informativeness in language use and lexical systems</h3>
<ul>
<li><strong>Authors: </strong>Eleonora Gualdoni, Gemma Boleda</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07827">https://arxiv.org/abs/2410.07827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07827">https://arxiv.org/pdf/2410.07827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07827]] Why do objects have many names? A study on word informativeness in language use and lexical systems(https://arxiv.org/abs/2410.07827)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Human lexicons contain many different words that speakers can use to refer to the same object, e.g., "purple" or "magenta" for the same shade of color. On the one hand, studies on language use have explored how speakers adapt their referring expressions to successfully communicate in context, without focusing on properties of the lexical system. On the other hand, studies in language evolution have discussed how competing pressures for informativeness and simplicity shape lexical systems, without tackling in-context communication. We aim at bridging the gap between these traditions, and explore why a soft mapping between referents and words is a good solution for communication, by taking into account both in-context communication and the structure of the lexicon. We propose a simple measure of informativeness for words and lexical systems, grounded in a visual space, and analyze color naming data for English and Mandarin Chinese. We conclude that optimal lexical systems are those where multiple words can apply to the same referent, conveying different amounts of information. Such systems allow speakers to maximize communication accuracy and minimize the amount of information they convey when communicating about referents in contexts.</li>
<li><strong>摘要：</strong>人类词汇包含许多不同的词语，说话者可以使用这些词语来指代同一物体，例如，用“紫色”或“洋红色”来指代同一种颜色。一方面，语言使用研究探索了说话者如何调整其指代表达以在上下文中成功交流，而没有关注词汇系统的属性。另一方面，语言进化研究讨论了信息量和简单性的竞争压力如何塑造词汇系统，而没有解决上下文交流问题。我们旨在弥合这些传统之间的差距，并通过考虑上下文交流和词汇结构来探索为什么指称物和词语之间的软映射是交流的好方法。我们提出了一种基于视觉空间的词语和词汇系统的信息量简单测量方法，并分析了英语和普通话的颜色命名数据。我们得出结论，最佳词汇系统是那些可以将多个词应用于同一指称物并传达不同数量信息的词汇系统。这样的系统使得说话者能够最大限度地提高交流的准确性，并最大限度地减少他们在交流上下文中所指事物时所传达的信息量。</li>
</ul>

<h3>Title: NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>William Tan, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07830">https://arxiv.org/abs/2410.07830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07830">https://arxiv.org/pdf/2410.07830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07830]] NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages with Large Language Models(https://arxiv.org/abs/2410.07830)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated exceptional promise in translation tasks for high-resource languages. However, their performance in low-resource languages is limited by the scarcity of both parallel and monolingual corpora, as well as the presence of noise. Consequently, such LLMs suffer with alignment and have lagged behind State-of-The-Art (SoTA) neural machine translation (NMT) models in these settings. This paper introduces NusaMT-7B, an LLM-based machine translation model for low-resource Indonesian languages, starting with Balinese and Minangkabau. Leveraging the pretrained LLaMA2-7B, our approach integrates continued pre-training on monolingual data, Supervised Fine-Tuning (SFT), self-learning, and an LLM-based data cleaner to reduce noise in parallel sentences. In the FLORES-200 multilingual translation benchmark, NusaMT-7B outperforms SoTA models in the spBLEU metric by up to +6.69 spBLEU in translations into Balinese and Minangkabau, but underperforms by up to -3.38 spBLEU in translations into higher-resource languages. Our results show that fine-tuned LLMs can enhance translation quality for low-resource languages, aiding in linguistic preservation and cross-cultural communication.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在高资源语言的翻译任务中表现出色。然而，它们在低资源语言中的表现受到平行和单语语料库稀缺以及噪音存在的限制。因此，此类 LLM 存在对齐问题，在这些环境中落后于最先进的 (SoTA) 神经机器翻译 (NMT) 模型。本文介绍了 NusaMT-7B，这是一种基于 LLM 的机器翻译模型，适用于低资源印尼语，从巴厘语和米南加保语开始。利用预训练的 LLaMA2-7B，我们的方法集成了对单语数据的持续预训练、监督微调 (SFT)、自学习和基于 LLM 的数据清理器，以减少平行句子中的噪音。在 FLORES-200 多语言翻译基准中，NusaMT-7B 在翻译成巴厘语和米南加保语时，其 spBLEU 指标比 SoTA 模型高出多达 +6.69 spBLEU，但在翻译成资源丰富的语言时，其 spBLEU 指标比 SoTA 模型低多达 -3.38 spBLEU。我们的结果表明，经过微调的 LLM 可以提高资源匮乏语言的翻译质量，有助于语言保护和跨文化交流。</li>
</ul>

<h3>Title: Enhancing Language Model Reasoning via Weighted Reasoning in Self-Consistency</h3>
<ul>
<li><strong>Authors: </strong>Tim Knappe, Ryan Li, Ayush Chauhan, Kaylee Chhua, Kevin Zhu, Sean O'Brien</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07839">https://arxiv.org/abs/2410.07839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07839">https://arxiv.org/pdf/2410.07839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07839]] Enhancing Language Model Reasoning via Weighted Reasoning in Self-Consistency(https://arxiv.org/abs/2410.07839)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have rapidly improved their performance on a broad number of tasks, they still often fall short on reasoning tasks. As LLMs become more integrated in diverse real-world tasks, advancing their reasoning capabilities is crucial to their effectiveness in nuanced, complex problems. Wang et al's self-consistency framework reveals that sampling multiple rationales before taking a majority vote reliably improves model performance across various closed-answer reasoning tasks. Standard methods based on this framework aggregate the final decisions of these rationales but fail to utilize the detailed step-by-step reasoning paths applied by these paths. Our work enhances this approach by incorporating and analyzing both the reasoning paths of these rationales in addition to their final decisions before taking a majority vote. These methods not only improve the reliability of reasoning paths but also cause more robust performance on complex reasoning tasks.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 已迅速提高了在大量任务上的性能，但它们在推理任务上仍然常常存在不足。随着 LLM 越来越融入各种现实世界任务，提高其推理能力对于其在细致入微的复杂问题中的有效性至关重要。Wang 等人的自洽框架表明，在进行多数表决之前对多个基本原理进行抽样可以可靠地提高模型在各种封闭式答案推理任务中的性能。基于此框架的标准方法汇总了这些基本原理的最终决策，但未能利用这些路径所应用的详细的逐步推理路径。我们的工作通过在进行多数表决之前合并和分析这些基本原理的推理路径以及它们的最终决策来增强这种方法。这些方法不仅提高了推理路径的可靠性，而且还在复杂的推理任务上带来了更稳健的性能。</li>
</ul>

<h3>Title: Benchmarking Agentic Workflow Generation</h3>
<ul>
<li><strong>Authors: </strong>Shuofei Qiao, Runnan Fang, Zhisong Qiu, Xiaobin Wang, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07869">https://arxiv.org/abs/2410.07869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07869">https://arxiv.org/pdf/2410.07869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07869]] Benchmarking Agentic Workflow Generation(https://arxiv.org/abs/2410.07869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorFBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorFEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference. Code and dataset will be available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 具有处理各种任务的出色能力，推动了推理和规划任务的重大进步，其中将复杂问题分解为可执行的工作流是此过程中的关键步骤。现有的工作流评估框架要么只关注整体性能，要么受到场景覆盖范围有限、工作流结构简单和评估标准松懈等限制。为此，我们引入了 WorFBench，这是一个统一的工作流生成基准，具有多方面的场景和复杂的图形工作流结构。此外，我们提出了 WorFEval，这是一种系统评估协议，利用子序列和子图匹配算法来准确量化 LLM 代理的工作流生成能力。通过对不同类型的 LLM 进行全面评估，我们发现 LLM 代理的序列规划能力和图形规划能力之间存在明显差距，甚至 GPT-4 也表现出约 15% 的差距。我们还训练了两个开源模型并评估它们在保留任务上的泛化能力。此外，我们观察到生成的工作流程可以增强下游任务，使它们能够在推理过程中以更少的时间实现卓越的性能。代码和数据集将在此 https URL 上提供。</li>
</ul>

<h3>Title: Unsupervised Data Validation Methods for Efficient Model Training</h3>
<ul>
<li><strong>Authors: </strong>Yurii Paniv</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07880">https://arxiv.org/abs/2410.07880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07880">https://arxiv.org/pdf/2410.07880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07880]] Unsupervised Data Validation Methods for Efficient Model Training(https://arxiv.org/abs/2410.07880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper investigates the challenges and potential solutions for improving machine learning systems for low-resource languages. State-of-the-art models in natural language processing (NLP), text-to-speech (TTS), speech-to-text (STT), and vision-language models (VLM) rely heavily on large datasets, which are often unavailable for low-resource languages. This research explores key areas such as defining "quality data," developing methods for generating appropriate data and enhancing accessibility to model training. A comprehensive review of current methodologies, including data augmentation, multilingual transfer learning, synthetic data generation, and data selection techniques, highlights both advancements and limitations. Several open research questions are identified, providing a framework for future studies aimed at optimizing data utilization, reducing the required data quantity, and maintaining high-quality model performance. By addressing these challenges, the paper aims to make advanced machine learning models more accessible for low-resource languages, enhancing their utility and impact across various sectors.</li>
<li><strong>摘要：</strong>本文探讨了改进低资源语言机器学习系统的挑战和潜在解决方案。自然语言处理 (NLP)、文本转语音 (TTS)、语音转文本 (STT) 和视觉语言模型 (VLM) 中最先进的模型严重依赖大型数据集，而这些数据集通常不适用于低资源语言。这项研究探索了定义“高质量数据”、开发生成适当数据的方法以及增强模型训练的可访问性等关键领域。全面回顾了当前的方法，包括数据增强、多语言迁移学习、合成数据生成和数据选择技术，突出了进步和局限性。确定了几个开放的研究问题，为旨在优化数据利用率、减少所需数据量和保持高质量模型性能的未来研究提供了框架。通过应对这些挑战，本文旨在使高级机器学习模型更易于用于低资源语言，从而增强其在各个领域的实用性和影响力。</li>
</ul>

<h3>Title: InstructBioMol: Advancing Biomolecule Understanding and Design Following Human Instructions</h3>
<ul>
<li><strong>Authors: </strong>Xiang Zhuang, Keyan Ding, Tianwen Lyu, Yinuo Jiang, Xiaotong Li, Zhuoyi Xiang, Zeyuan Wang, Ming Qin, Kehua Feng, Jike Wang, Qiang Zhang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07919">https://arxiv.org/abs/2410.07919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07919">https://arxiv.org/pdf/2410.07919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07919]] InstructBioMol: Advancing Biomolecule Understanding and Design Following Human Instructions(https://arxiv.org/abs/2410.07919)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Understanding and designing biomolecules, such as proteins and small molecules, is central to advancing drug discovery, synthetic biology, and enzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have revolutionized biomolecular research, achieving remarkable accuracy in biomolecular prediction and design. However, a critical gap remains between AI's computational power and researchers' intuition, using natural language to align molecular complexity with human intentions. Large Language Models (LLMs) have shown potential to interpret human intentions, yet their application to biomolecular research remains nascent due to challenges including specialized knowledge requirements, multimodal data integration, and semantic alignment between natural language and biomolecules. To address these limitations, we present InstructBioMol, a novel LLM designed to bridge natural language and biomolecules through a comprehensive any-to-any alignment of natural language, molecules, and proteins. This model can integrate multimodal biomolecules as input, and enable researchers to articulate design goals in natural language, providing biomolecular outputs that meet precise biological needs. Experimental results demonstrate InstructBioMol can understand and design biomolecules following human instructions. Notably, it can generate drug molecules with a 10% improvement in binding affinity and design enzymes that achieve an ESP Score of 70.4, making it the only method to surpass the enzyme-substrate interaction threshold of 60.0 recommended by the ESP developer. This highlights its potential to transform real-world biomolecular research.</li>
<li><strong>摘要：</strong>理解和设计生物分子（例如蛋白质和小分子）对于推进药物发现、合成生物学和酶工程至关重要。人工智能 (AI) 的最新突破彻底改变了生物分子研究，在生物分子预测和设计方面取得了显着的准确性。然而，人工智能的计算能力与研究人员的直觉之间仍然存在一个关键差距，研究人员使用自然语言将分子复杂性与人类意图联系起来。大型语言模型 (LLM) 已显示出解释人类意图的潜力，但由于专业知识要求、多模态数据集成以及自然语言和生物分子之间的语义对齐等挑战，它们在生物分子研究中的应用仍处于起步阶段。为了解决这些限制，我们提出了 InstructBioMol，这是一种新颖的 LLM，旨在通过对自然语言、分子和蛋白质进行全面的任意对齐来连接自然语言和生物分子。该模型可以整合多模态生物分子作为输入，并使研究人员能够用自然语言表达设计目标，提供满足精确生物需求的生物分子输出。实验结果表明，InstructBioMol 能够理解并按照人类的指令设计生物分子。值得注意的是，它可以生成结合亲和力提高 10% 的药物分子，并设计出 ESP 得分达到 70.4 的酶，使其成为唯一超过 ESP 开发人员推荐的酶-底物相互作用阈值 60.0 的方法。这凸显了其改变现实世界生物分子研究的潜力。</li>
</ul>

<h3>Title: Disease Entity Recognition and Normalization is Improved with Large Language Model Derived Synthetic Normalized Mentions</h3>
<ul>
<li><strong>Authors: </strong>Kuleen Sasse, Shinjitha Vadlakonda, Richard E. Kennedy, John D. Osborne</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07951">https://arxiv.org/abs/2410.07951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07951">https://arxiv.org/pdf/2410.07951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07951]] Disease Entity Recognition and Normalization is Improved with Large Language Model Derived Synthetic Normalized Mentions(https://arxiv.org/abs/2410.07951)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Background: Machine learning methods for clinical named entity recognition and entity normalization systems can utilize both labeled corpora and Knowledge Graphs (KGs) for learning. However, infrequently occurring concepts may have few mentions in training corpora and lack detailed descriptions or synonyms, even in large KGs. For Disease Entity Recognition (DER) and Disease Entity Normalization (DEN), this can result in fewer high quality training examples relative to the number of known diseases. Large Language Model (LLM) generation of synthetic training examples could improve performance in these information extraction tasks. Methods: We fine-tuned a LLaMa-2 13B Chat LLM to generate a synthetic corpus containing normalized mentions of concepts from the Unified Medical Language System (UMLS) Disease Semantic Group. We measured overall and Out of Distribution (OOD) performance for DER and DEN, with and without synthetic data augmentation. We evaluated performance on 3 different disease corpora using 4 different data augmentation strategies, assessed using BioBERT for DER and SapBERT and KrissBERT for DEN. Results: Our synthetic data yielded a substantial improvement for DEN, in all 3 training corpora the top 1 accuracy of both SapBERT and KrissBERT improved by 3-9 points in overall performance and by 20-55 points in OOD data. A small improvement (1-2 points) was also seen for DER in overall performance, but only one dataset showed OOD improvement. Conclusion: LLM generation of normalized disease mentions can improve DEN relative to normalization approaches that do not utilize LLMs to augment data with synthetic mentions. Ablation studies indicate that performance gains for DEN were only partially attributable to improvements in OOD performance. The same approach has only a limited ability to improve DER. We make our software and dataset publicly available.</li>
<li><strong>摘要：</strong>背景：临床命名实体识别和实体规范化系统的机器学习方法可以利用标记语料库和知识图谱 (KG) 进行学习。然而，不常出现的概念在训练语料库中可能很少被提及，并且缺乏详细的描述或同义词，即使在大型 KG 中也是如此。对于疾病实体识别 (DER) 和疾病实体规范化 (DEN)，这可能导致相对于已知疾病的数量而言更少的高质量训练示例。大型语言模型 (LLM) 生成的合成训练示例可以提高这些信息提取任务的性能。方法：我们对 LLaMa-2 13B Chat LLM 进行了微调，以生成一个合成语料库，其中包含来自统一医学语言系统 (UMLS) 疾病语义组的概念的规范化提及。我们测量了 DER 和 DEN 的整体和分布外 (OOD) 性能，包括使用和不使用合成数据增强的情况。我们使用 4 种不同的数据增强策略评估了 3 种不同疾病语料库的性能，其中 DER 使用 BioBERT 进行评估，DEN 使用 SapBERT 和 KrissBERT 进行评估。结果：我们的合成数据使 DEN 得到了显着改善，在所有 3 个训练语料库中，SapBERT 和 KrissBERT 的 Top 1 准确率在整体性能上提高了 3-9 分，在 OOD 数据中提高了 20-55 分。DER 的整体性能也有小幅提升（1-2 分），但只有一个数据集显示 OOD 有所改善。结论：与不利用 LLM 用合成提及增强数据的规范化方法相比，LLM 生成的规范化疾病提及可以改善 DEN。消融研究表明，DEN 的性能提升仅部分归因于 OOD 性能的提高。同样的方法对 DER 的改善能力有限。我们公开我们的软件和数据集。</li>
</ul>

<h3>Title: COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking Suite for the EU Artificial Intelligence Act</h3>
<ul>
<li><strong>Authors: </strong>Philipp Guldimann, Alexander Spiridonov, Robin Staab, Nikola Jovanović, Mark Vero, Velko Vechev, Anna Gueorguieva, Mislav Balunović, Nikola Konstantinov, Pavol Bielik, Petar Tsankov, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07959">https://arxiv.org/abs/2410.07959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07959">https://arxiv.org/pdf/2410.07959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07959]] COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking Suite for the EU Artificial Intelligence Act(https://arxiv.org/abs/2410.07959)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The EU's Artificial Intelligence Act (AI Act) is a significant step towards responsible AI development, but lacks clear technical interpretation, making it difficult to assess models' compliance. This work presents COMPL-AI, a comprehensive framework consisting of (i) the first technical interpretation of the EU AI Act, translating its broad regulatory requirements into measurable technical requirements, with the focus on large language models (LLMs), and (ii) an open-source Act-centered benchmarking suite, based on thorough surveying and implementation of state-of-the-art LLM benchmarks. By evaluating 12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in existing models and benchmarks, particularly in areas like robustness, safety, diversity, and fairness. This work highlights the need for a shift in focus towards these aspects, encouraging balanced development of LLMs and more comprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the first time demonstrates the possibilities and difficulties of bringing the Act's obligations to a more concrete, technical level. As such, our work can serve as a useful first step towards having actionable recommendations for model providers, and contributes to ongoing efforts of the EU to enable application of the Act, such as the drafting of the GPAI Code of Practice.</li>
<li><strong>摘要：</strong>欧盟的《人工智能法案》（AI Act）是朝着负责任的人工智能发展迈出的重要一步，但缺乏明确的技术解释，因此很难评估模型的合规性。这项工作提出了 COMPL-AI，这是一个全面的框架，包括 (i) 对欧盟 AI 法案的第一个技术解释，将其广泛的监管要求转化为可衡量的技术要求，重点是大型语言模型 (LLM)，以及 (ii) 一个以法案为中心的开源基准测试套件，基于对最先进的 LLM 基准的彻底调查和实施。通过在 COMPL-AI 的背景下评估 12 个著名的 LLM，我们揭示了现有模型和基准的缺点，特别是在稳健性、安全性、多样性和公平性等领域。这项工作强调需要将重点转向这些方面，鼓励 LLM 的平衡发展和更全面的监管基准。同时，COMPL-AI 首次展示了将该法案的义务提升到更具体的技术层面的可能性和困难。因此，我们的工作可以作为向模型提供者提供可行建议的有用的第一步，并有助于欧盟为实施该法案而进行的持续努力，例如起草 GPAI 行为准则。</li>
</ul>

<h3>Title: Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, Baobao Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07985">https://arxiv.org/abs/2410.07985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07985">https://arxiv.org/pdf/2410.07985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07985]] Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models(https://arxiv.org/abs/2410.07985)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展已导致数学推理能力取得重大突破。然而，现有的基准测试（如 GSM8K 或 MATH）现在已以高精度被解决（例如，OpenAI o1 在 MATH 数据集上实现了 94.8%），这表明它们不足以真正挑战这些模型。为了弥补这一差距，我们提出了一个全面且具有挑战性的基准测试，专门用于评估 LLM 在奥林匹克级别的数学推理能力。与现有的奥林匹克相关基准测试不同，我们的数据集专注于数学，包含 4428 个竞赛级问题，并经过严格的人工注释。这些问题被精心分为 33 多个子领域，涵盖 10 多个不同的难度级别，从而可以全面评估奥林匹克数学推理中的模型性能。此外，我们基于这个基准测试进行了深入分析。我们的实验结果表明，即使是最先进的模型 OpenAI o1-mini 和 OpenAI o1-preview，在解决极具挑战性的奥林匹克级问题时也会遇到困难，准确率分别为 60.54% 和 52.55%，这凸显了奥林匹克级数学推理面临的重大挑战。</li>
</ul>

<h3>Title: Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets</h3>
<ul>
<li><strong>Authors: </strong>Tommaso Giorgi, Lorenzo Cima, Tiziano Fagni, Marco Avvenuti, Stefano Cresci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.07991">https://arxiv.org/abs/2410.07991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.07991">https://arxiv.org/pdf/2410.07991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.07991]] Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets(https://arxiv.org/abs/2410.07991)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The rise of online platforms exacerbated the spread of hate speech, demanding scalable and effective detection. However, the accuracy of hate speech detection systems heavily relies on human-labeled data, which is inherently susceptible to biases. While previous work has examined the issue, the interplay between the characteristics of the annotator and those of the target of the hate are still unexplored. We fill this gap by leveraging an extensive dataset with rich socio-demographic information of both annotators and targets, uncovering how human biases manifest in relation to the target's attributes. Our analysis surfaces the presence of widespread biases, which we quantitatively describe and characterize based on their intensity and prevalence, revealing marked differences. Furthermore, we compare human biases with those exhibited by persona-based LLMs. Our findings indicate that while persona-based LLMs do exhibit biases, these differ significantly from those of human annotators. Overall, our work offers new and nuanced results on human biases in hate speech annotations, as well as fresh insights into the design of AI-driven hate speech detection systems.</li>
<li><strong>摘要：</strong>在线平台的兴起加剧了仇恨言论的传播，要求可扩展且有效的检测。然而，仇恨言论检测系统的准确性严重依赖于人工标记的数据，而这些数据本质上容易受到偏见的影响。虽然之前的研究已经研究过这个问题，但注释者的特征和仇恨目标的特征之间的相互作用仍未得到探索。我们利用包含注释者和目标的丰富社会人口统计信息的广泛数据集来填补这一空白，揭示了人类偏见如何与目标的属性相关。我们的分析揭示了普遍存在的偏见，我们根据偏见的强度和普遍性对其进行了定量描述和表征，揭示了明显的差异。此外，我们将人类偏见与基于角色的 LLM 所表现出的偏见进行了比较。我们的研究结果表明，虽然基于角色的 LLM 确实表现出偏见，但这些偏见与人类注释者的偏见有很大不同。总的来说，我们的工作为仇恨言论注释中的人为偏见提供了新的、细致入微的结果，并为人工智能驱动的仇恨言论检测系统的设计提供了新的见解。</li>
</ul>

<h3>Title: LLM Cascade with Multi-Objective Optimal Consideration</h3>
<ul>
<li><strong>Authors: </strong>Kai Zhang, Liqian Peng, Congchao Wang, Alec Go, Xiaozhong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08014">https://arxiv.org/abs/2410.08014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08014">https://arxiv.org/pdf/2410.08014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08014]] LLM Cascade with Multi-Objective Optimal Consideration(https://arxiv.org/abs/2410.08014)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated exceptional capabilities in understanding and generating natural language. However, their high deployment costs often pose a barrier to practical applications, especially. Cascading local and server models offers a promising solution to this challenge. While existing studies on LLM cascades have primarily focused on the performance-cost trade-off, real-world scenarios often involve more complex requirements. This paper introduces a novel LLM Cascade strategy with Multi-Objective Optimization, enabling LLM cascades to consider additional objectives (e.g., privacy) and better align with the specific demands of real-world applications while maintaining their original cascading abilities. Extensive experiments on three benchmarks validate the effectiveness and superiority of our approach.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在理解和生成自然语言方面表现出了卓越的能力。然而，它们的高部署成本往往对实际应用构成了障碍。级联本地和服务器模型为这一挑战提供了一个有希望的解决方案。虽然现有的 LLM 级联研究主要集中在性能成本权衡上，但现实世界的场景往往涉及更复杂的要求。本文介绍了一种具有多目标优化的新型 LLM 级联策略，使 LLM 级联能够考虑其他目标（例如隐私），并更好地满足现实世界应用的特定需求，同时保持其原有的级联能力。在三个基准上进行的大量实验验证了我们方法的有效性和优越性。</li>
</ul>

<h3>Title: Private Language Models via Truncated Laplacian Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Huang, Tao Yang, Ivan Habernal, Lijie Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08027">https://arxiv.org/abs/2410.08027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08027">https://arxiv.org/pdf/2410.08027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08027]] Private Language Models via Truncated Laplacian Mechanism(https://arxiv.org/abs/2410.08027)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Deep learning models for NLP tasks are prone to variants of privacy attacks. To prevent privacy leakage, researchers have investigated word-level perturbations, relying on the formal guarantees of differential privacy (DP) in the embedding space. However, many existing approaches either achieve unsatisfactory performance in the high privacy regime when using the Laplacian or Gaussian mechanism, or resort to weaker relaxations of DP that are inferior to the canonical DP in terms of privacy strength. This raises the question of whether a new method for private word embedding can be designed to overcome these limitations. In this paper, we propose a novel private embedding method called the high dimensional truncated Laplacian mechanism. Specifically, we introduce a non-trivial extension of the truncated Laplacian mechanism, which was previously only investigated in one-dimensional space cases. Theoretically, we show that our method has a lower variance compared to the previous private word embedding methods. To further validate its effectiveness, we conduct comprehensive experiments on private embedding and downstream tasks using three datasets. Remarkably, even in the high privacy regime, our approach only incurs a slight decrease in utility compared to the non-private scenario.</li>
<li><strong>摘要：</strong>用于 NLP 任务的深度学习模型容易受到各种隐私攻击。为了防止隐私泄露，研究人员研究了词级扰动，依赖于嵌入空间中差分隐私 (DP) 的正式保证。然而，许多现有方法要么在使用拉普拉斯或高斯机制时在高隐私机制下表现不佳，要么采用比标准 DP 更弱的 DP 松弛，其隐私强度不如标准 DP。这就提出了一个问题：是否可以设计一种新的隐私词嵌入方法来克服这些限制。在本文中，我们提出了一种称为高维截断拉普拉斯机制的新型隐私嵌入方法。具体而言，我们引入了截断拉普拉斯机制的非平凡扩展，该机制以前仅在一维空间情况下进行过研究。从理论上讲，我们表明与以前的隐私词嵌入方法相比，我们的方法具有更低的方差。为了进一步验证其有效性，我们使用三个数据集对隐私嵌入和下游任务进行了全面的实验。值得注意的是，即使在高度隐私的制度下，与非隐私场景相比，我们的方法也只会导致效用略有下降。</li>
</ul>

<h3>Title: The Rise of AI-Generated Content in Wikipedia</h3>
<ul>
<li><strong>Authors: </strong>Creston Brooks, Samuel Eggert, Denis Peskoff</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08044">https://arxiv.org/abs/2410.08044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08044">https://arxiv.org/pdf/2410.08044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08044]] The Rise of AI-Generated Content in Wikipedia(https://arxiv.org/abs/2410.08044)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>The rise of AI-generated content in popular information sources raises significant concerns about accountability, accuracy, and bias amplification. Beyond directly impacting consumers, the widespread presence of this content poses questions for the long-term viability of training language models on vast internet sweeps. We use GPTZero, a proprietary AI detector, and Binoculars, an open-source alternative, to establish lower bounds on the presence of AI-generated content in recently created Wikipedia pages. Both detectors reveal a marked increase in AI-generated content in recent pages compared to those from before the release of GPT-3.5. With thresholds calibrated to achieve a 1% false positive rate on pre-GPT-3.5 articles, detectors flag over 5% of newly created English Wikipedia articles as AI-generated, with lower percentages for German, French, and Italian articles. Flagged Wikipedia articles are typically of lower quality and are often self-promotional or partial towards a specific viewpoint on controversial topics.</li>
<li><strong>摘要：</strong>人工智能生成内容在流行信息源中的兴起引发了人们对责任、准确性和偏见放大的严重担忧。除了直接影响消费者之外，这种内容的广泛存在还对在庞大的互联网搜索中训练语言模型的长期可行性提出了质疑。我们使用专有的人工智能检测器 GPTZero 和开源替代方案 Binoculars 来为最近创建的维基百科页面中人工智能生成内容的存在设定下限。这两个检测器都显示，与 GPT-3.5 发布之前相比，最近页面中人工智能生成的内容显着增加。在阈值校准为在 GPT-3.5 之前的文章上实现 1% 的误报率的情况下，检测器将超过 5% 的新创建的英文维基百科文章标记为人工智能生成的，德语、法语和意大利语文章的百分比较低。被标记的维基百科文章通常质量较低，并且通常是自我宣传或偏向有争议话题的特定观点。</li>
</ul>

<h3>Title: Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hyun Ryu, Gyeongman Kim, Hyemin S. Lee, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08047">https://arxiv.org/abs/2410.08047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08047">https://arxiv.org/pdf/2410.08047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08047]] Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning(https://arxiv.org/abs/2410.08047)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Complex logical reasoning tasks require a long sequence of reasoning, which a large language model (LLM) with chain-of-thought prompting still falls short. To alleviate this issue, neurosymbolic approaches incorporate a symbolic solver. Specifically, an LLM only translates a natural language problem into a satisfiability (SAT) problem that consists of first-order logic formulas, and a sound symbolic solver returns a mathematically correct solution. However, we discover that LLMs have difficulties to capture complex logical semantics hidden in the natural language during translation. To resolve this limitation, we propose a Compositional First-Order Logic Translation. An LLM first parses a natural language sentence into newly defined logical dependency structures that consist of an atomic subsentence and its dependents, then sequentially translate the parsed subsentences. Since multiple logical dependency structures and sequential translations are possible for a single sentence, we also introduce two Verification algorithms to ensure more reliable results. We utilize an SAT solver to rigorously compare semantics of generated first-order logic formulas and select the most probable one. We evaluate the proposed method, dubbed CLOVER, on seven logical reasoning benchmarks and show that it outperforms the previous neurosymbolic approaches and achieves new state-of-the-art results.</li>
<li><strong>摘要：</strong>复杂的逻辑推理任务需要一长串的推理，而具有思路链提示的大型语言模型 (LLM) 仍然无法满足这一要求。为了缓解这一问题，神经符号方法结合了符号求解器。具体而言，LLM 仅将自然语言问题转换为由一阶逻辑公式组成的可满足性 (SAT) 问题，而可靠的符号求解器则返回数学上正确的解决方案。然而，我们发现 LLM 难以在翻译过程中捕捉隐藏在自然语言中的复杂逻辑语义。为了解决这一限制，我们提出了组合一阶逻辑翻译。LLM 首先将自然语言句子解析为由原子子句及其从属项组成的新定义的逻辑依赖结构，然后按顺序翻译解析后的子句。由于单个句子可能存在多个逻辑依赖结构和顺序翻译，我们还引入了两种验证算法以确保更可靠的结果。我们利用 SAT 求解器严格比较生成的一阶逻辑公式的语义，并选择最有可能的那个。我们在七个逻辑推理基准上评估了所提出的方法（称为 CLOVER），并表明它优于以前的神经符号方法并取得了新的最先进的结果。</li>
</ul>

<h3>Title: A Target-Aware Analysis of Data Augmentation for Hate Speech Detection</h3>
<ul>
<li><strong>Authors: </strong>Camilla Casula, Sara Tonelli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08053">https://arxiv.org/abs/2410.08053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08053">https://arxiv.org/pdf/2410.08053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08053]] A Target-Aware Analysis of Data Augmentation for Hate Speech Detection(https://arxiv.org/abs/2410.08053)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Hate speech is one of the main threats posed by the widespread use of social networks, despite efforts to limit it. Although attention has been devoted to this issue, the lack of datasets and case studies centered around scarcely represented phenomena, such as ableism or ageism, can lead to hate speech detection systems that do not perform well on underrepresented identity groups. Given the unpreceded capabilities of LLMs in producing high-quality data, we investigate the possibility of augmenting existing data with generative language models, reducing target imbalance. We experiment with augmenting 1,000 posts from the Measuring Hate Speech corpus, an English dataset annotated with target identity information, adding around 30,000 synthetic examples using both simple data augmentation methods and different types of generative models, comparing autoregressive and sequence-to-sequence approaches. We find traditional DA methods to often be preferable to generative models, but the combination of the two tends to lead to the best results. Indeed, for some hate categories such as origin, religion, and disability, hate speech classification using augmented data for training improves by more than 10% F1 over the no augmentation baseline. This work contributes to the development of systems for hate speech detection that are not only better performing but also fairer and more inclusive towards targets that have been neglected so far.</li>
<li><strong>摘要：</strong>尽管人们努力限制仇恨言论，但仇恨言论仍然是社交网络广泛使用所带来的主要威胁之一。尽管人们已经注意到了这个问题，但缺乏以残疾歧视或年龄歧视等代表性稀缺现象为中心的数据集和案例研究，这可能导致仇恨言论检测系统在代表性不足的身份群体中表现不佳。鉴于 LLM 在生成高质量数据方面具有前所未有的能力，我们研究了使用生成语言模型增强现有数据的可能性，以减少目标不平衡。我们尝试从“测量仇恨言论语料库”（一个带有目标身份信息注释的英语数据集）中增强 1,000 个帖子，使用简单的数据增强方法和不同类型的生成模型添加大约 30,000 个合成示例，并比较自回归和序列到序列方法。我们发现传统的 DA 方法通常比生成模型更可取，但两者的结合往往会产生最好的结果。事实上，对于某些仇恨类别（例如原籍、宗教和残疾），使用增强数据进行训练的仇恨言论分类比无增强基线提高了 10% 以上 F1。这项工作有助于开发仇恨言论检测系统，该系统不仅性能更好，而且对迄今为止被忽视的目标也更加公平和包容。</li>
</ul>

<h3>Title: Closing the Loop: Learning to Generate Writing Feedback via Language Model Simulated Student Revisions</h3>
<ul>
<li><strong>Authors: </strong>Inderjeet Nair, Jiaye Tan, Xiaotian Su, Anne Gere, Xu Wang, Lu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08058">https://arxiv.org/abs/2410.08058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08058">https://arxiv.org/pdf/2410.08058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08058]] Closing the Loop: Learning to Generate Writing Feedback via Language Model Simulated Student Revisions(https://arxiv.org/abs/2410.08058)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Providing feedback is widely recognized as crucial for refining students' writing skills. Recent advances in language models (LMs) have made it possible to automatically generate feedback that is actionable and well-aligned with human-specified attributes. However, it remains unclear whether the feedback generated by these models is truly effective in enhancing the quality of student revisions. Moreover, prompting LMs with a precise set of instructions to generate feedback is nontrivial due to the lack of consensus regarding the specific attributes that can lead to improved revising performance. To address these challenges, we propose PROF that PROduces Feedback via learning from LM simulated student revisions. PROF aims to iteratively optimize the feedback generator by directly maximizing the effectiveness of students' overall revising performance as simulated by LMs. Focusing on an economic essay assignment, we empirically test the efficacy of PROF and observe that our approach not only surpasses a variety of baseline methods in effectiveness of improving students' writing but also demonstrates enhanced pedagogical values, even though it was not explicitly trained for this aspect.</li>
<li><strong>摘要：</strong>提供反馈被广泛认为对于提高学生的写作技能至关重要。语言模型 (LM) 的最新进展使得自动生成可操作且与人类指定属性高度一致的反馈成为可能。然而，尚不清楚这些模型生成的反馈是否真正有效地提高了学生修改的质量。此外，由于缺乏关于可以提高修改表现的具体属性的共识，因此用一组精确的指令提示 LM 生成反馈并非易事。为了应对这些挑战，我们提出了 PROF，它通过从 LM 模拟的学生修改中学习来生成反馈。PROF 旨在通过直接最大化 LM 模拟的学生整体修改表现的有效性来迭代优化反馈生成器。我们专注于一项经济论文作业，通过实证测试了 PROF 的有效性，并观察到我们的方法不仅在提高学生写作水平的有效性方面超越了各种基线方法，而且还表现出增强的教学价值，即使它没有为此方面进行明确的训练。</li>
</ul>

<h3>Title: Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenting Tan, Dongxiao Chen, Jieting Xue, Zihao Wang, Taijie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08068">https://arxiv.org/abs/2410.08068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08068">https://arxiv.org/pdf/2410.08068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08068]] Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models(https://arxiv.org/abs/2410.08068)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit impressive performance across various domains but still struggle with arithmetic reasoning tasks. Recent work shows the effectiveness of prompt design methods in enhancing reasoning capabilities. However, these approaches overlook crucial requirements for prior knowledge of specific concepts, theorems, and tricks to tackle most arithmetic reasoning problems successfully. To address this issue, we propose a novel and effective Teaching-Inspired Integrated Framework, which emulates the instructional process of a teacher guiding students. This method equips LLMs with essential concepts, relevant theorems, and similar problems with analogous solution approaches, facilitating the enhancement of reasoning abilities. Additionally, we introduce two new Chinese datasets, MathMC and MathToF, both with detailed explanations and answers. Experiments are conducted on nine benchmarks which demonstrates that our approach improves the reasoning accuracy of LLMs. With GPT-4 and our framework, we achieve new state-of-the-art performance on four math benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2% (+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各个领域都表现出色，但在算术推理任务上仍然举步维艰。最近的研究表明，快速设计方法在增强推理能力方面是有效的。然而，这些方法忽略了成功解决大多数算术推理问题所需的特定概念、定理和技巧的先验知识的关键要求。为了解决这个问题，我们提出了一个新颖有效的教学启发式集成框架，该框架模拟了教师指导学生的教学过程。这种方法为 LLM 配备了基本概念、相关定理和具有类似解决方法的类似问题，从而有助于提高推理能力。此外，我们引入了两个新的中文数据集 MathMC 和 MathToF，均附有详细的解释和答案。在九个基准上进行的实验表明，我们的方法提高了 LLM 的推理准确性。借助 GPT-4 和我们的框架，我们在四个数学基准（AddSub、SVAMP、Math23K 和 AQuA）上实现了新的最佳性能，准确率分别为 98.2%（+3.3%）、93.9%（+0.2%）、94.3%（+7.2%）和 81.1%（+1.2%）。我们的数据和代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yuan Sui, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08085">https://arxiv.org/abs/2410.08085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08085">https://arxiv.org/pdf/2410.08085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08085]] Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering(https://arxiv.org/abs/2410.08085)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Recent works integrating Knowledge Graphs (KGs) have led to promising improvements in enhancing reasoning accuracy of Large Language Models (LLMs). However, current benchmarks mainly focus on closed tasks, leaving a gap in the assessment of more complex, real-world scenarios. This gap has also obscured the evaluation of KGs' potential to mitigate the problem of hallucination in LLMs. To fill the gap, we introduce OKGQA, a new benchmark specifically designed to assess LLMs enhanced with KGs under open-ended, real-world question answering scenarios. OKGQA is designed to closely reflect the complexities of practical applications using questions from different types, and incorporates specific metrics to measure both the reduction in hallucinations and the enhancement in reasoning capabilities. To consider the scenario in which KGs may have varying levels of mistakes, we further propose another experiment setting OKGQA-P to assess model performance when the semantics and structure of KGs are deliberately perturbed and contaminated. OKGQA aims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on methods and future directions for leveraging KGs to reduce LLMs' hallucination. We believe that this study can facilitate a more complete performance comparison and encourage continuous improvement in integrating KGs with LLMs.</li>
<li><strong>摘要：</strong>最近，整合知识图谱 (KG) 的研究已在提高大型语言模型 (LLM) 的推理准确性方面取得了可喜的进展。然而，目前的基准测试主要侧重于封闭式任务，在更复杂的现实场景的评估方面存在差距。这一差距也掩盖了对 KG 缓解 LLM 幻觉问题的潜力的评估。为了填补这一空白，我们引入了 OKGQA，这是一个专门设计用于在开放式现实问答场景下评估使用 KG 增强的 LLM 的新基准测试。OKGQA 旨在使用不同类型的问题来密切反映实际应用的复杂性，并结合特定指标来衡量幻觉的减少和推理能力的增强。为了考虑 KG 可能存在不同程度错误的场景，我们进一步提出了另一个实验设置 OKGQA-P，以评估在故意扰乱和污染 KG 的语义和结构时模型的性能。 OKGQA 旨在 (1) 探索 KG 能否在开放式环境中提高 LLM 的可信度，以及 (2) 进行比较分析，以阐明利用 KG 减少 LLM 幻觉的方法和未来方向。我们相信这项研究可以促进更完整的性能比较，并鼓励在 KG 与 LLM 的集成方面不断改进。</li>
</ul>

<h3>Title: Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Bai, Ling Yang, Zhen Hao Wong, Jiahui Peng, Xinlin Zhuang, Chi Zhang, Lijun Wu, Qiu Jiantao, Wentao Zhang, Binhang Yuan, Conghui He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08102">https://arxiv.org/abs/2410.08102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08102">https://arxiv.org/pdf/2410.08102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08102]] Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining(https://arxiv.org/abs/2410.08102)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose a novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain of 10.5% across multiple language model benchmarks compared to the state-of-the-art methods.</li>
<li><strong>摘要：</strong>高效的数据选择对于加速大型语言模型 (LLM) 的预训练至关重要。虽然已经提出了各种方法来提高数据效率，但很少有研究解决这些方法之间的内在冲突，以实现 LLM 预训练的最佳数据选择。为了解决这个问题，我们提出了一种新颖的多智能体协作数据选择机制。在这个框架中，每种数据选择方法都充当一个独立的代理，并设计了一个代理控制台来在整个 LLM 训练过程中动态整合来自所有代理的信息。我们进行了广泛的实证研究来评估我们的多智能体框架。实验结果表明，与最先进的方法相比，我们的方法显著提高了数据效率，加速了 LLM 训练的收敛，并在多个语言模型基准测试中实现了 10.5% 的平均性能提升。</li>
</ul>

<h3>Title: What Makes Large Language Models Reason in (Multi-Turn) Code Generation?</h3>
<ul>
<li><strong>Authors: </strong>Kunhao Zheng, Juliette Decugis, Jonas Gehring, Taco Cohen, Benjamin Negrevergne, Gabriel Synnaeve</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08105">https://arxiv.org/abs/2410.08105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08105">https://arxiv.org/pdf/2410.08105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08105]] What Makes Large Language Models Reason in (Multi-Turn) Code Generation?(https://arxiv.org/abs/2410.08105)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Prompting techniques such as chain-of-thought have established themselves as a popular vehicle for improving the outputs of large language models (LLMs). For code generation, however, their exact mechanics and efficacy are under-explored. We thus investigate the effects of a wide range of prompting strategies with a focus on automatic re-prompting over multiple turns and computational requirements. After systematically decomposing reasoning, instruction, and execution feedback prompts, we conduct an extensive grid search on the competitive programming benchmarks CodeContests and TACO for multiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o). Our study reveals strategies that consistently improve performance across all models with small and large sampling budgets. We then show how finetuning with such an optimal configuration allows models to internalize the induced reasoning process and obtain improvements in performance and scalability for multi-turn code generation.</li>
<li><strong>摘要：</strong>诸如思路链之类的提示技术已成为改进大型语言模型 (LLM) 输出的流行工具。然而，对于代码生成，它们的确切机制和功效尚未得到充分探索。因此，我们研究了各种提示策略的效果，重点关注多轮和计算要求下的自动重新提示。在系统地分解推理、指令和执行反馈提示后，我们对多个 LLM 系列和大小 (Llama 3.0 和 3.1、8B、70B、405B 和 GPT-4o) 的竞争性编程基准 CodeContests 和 TACO 进行了广泛的网格搜索。我们的研究揭示了在所有具有小和大采样预算的模型中持续提高性能的策略。然后，我们展示了如何通过这种最佳配置进行微调，使模型能够内化诱导推理过程并获得多轮代码生成的性能和可扩展性的改进。</li>
</ul>

<h3>Title: A Closer Look at Machine Unlearning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08109">https://arxiv.org/abs/2410.08109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08109">https://arxiv.org/pdf/2410.08109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08109]] A Closer Look at Machine Unlearning for Large Language Models(https://arxiv.org/abs/2410.08109)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可能会记住敏感或受版权保护的内容，从而引发隐私和法律问题。由于从头开始重新训练的成本很高，研究人员试图采用机器学习来从 LLM 中删除特定内容，同时保持整体性能。在本文中，我们讨论了 LLM 的机器学习中的几个问题，并提供了对可能方法的见解。为了解决在学习后对模型输出的评估不足的问题，我们引入了三个额外的指标来评估标记多样性、句子语义和事实正确性。然后，我们将学习方法分为非目标和目标，并分别讨论它们的问题。具体而言，非目标学习试图近似的行为是不可预测的，可能涉及幻觉，而现有的正则化不足以进行目标学习。为了缓解这些问题，我们建议使用最大化熵 (ME) 的目标来进行非目标学习，并将答案保留 (AP) 损失作为目标学习的正则化。在三种场景（即虚拟反学习、持续反学习和真实反学习）中的实验结果证明了我们方法的有效性。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System</h3>
<ul>
<li><strong>Authors: </strong>Weize Chen, Jiarui Yuan, Chen Qian, Cheng Yang, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08115">https://arxiv.org/abs/2410.08115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08115">https://arxiv.org/pdf/2410.08115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08115]] Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System(https://arxiv.org/abs/2410.08115)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. Optima employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, Optima shows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10\% tokens on tasks requiring heavy information exchange. Moreover, Optima's efficiency gains open new possibilities for leveraging inference-compute more effectively, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the potential towards scalable, efficient, and effective MAS (this https URL).</li>
<li><strong>摘要：</strong>基于大型语言模型 (LLM) 的多智能体系统 (MAS) 在协作解决问题方面表现出巨大潜力，但它们仍然面临着严峻的挑战：通信效率低、可扩展性差以及缺乏有效的参数更新优化方法。我们提出了 Optima，这是一个新颖的框架，通过 LLM 训练显著提高基于 LLM 的 MAS 中的通信效率和任务效率来解决这些问题。Optima 采用迭代生成、排名、选择和训练范式，奖励函数平衡任务性能、令牌效率和通信可读性。我们探索了各种 RL 算法，包括监督微调、直接偏好优化及其混合方法，深入了解了它们的有效性-效率权衡。我们集成了蒙特卡洛树搜索启发的技术来生成 DPO 数据，将对话轮次视为树节点以探索不同的交互路径。在常见的多智能体任务（包括信息不对称问答和复杂推理）上进行评估后，Optima 显示出与单智能体基线和基于 Llama 3 8B 的 vanilla MAS 相比持续且显著的改进，在需要大量信息交换的任务上以不到 10% 的 token 实现了高达 2.8 倍的性能提升。此外，Optima 的效率提升为更有效地利用推理计算开辟了新的可能性，从而改善了推理时间缩放定律。通过解决基于 LLM 的 MAS 中的基本挑战，Optima 展示了实现可扩展、高效和有效 MAS 的潜力（此 https URL）。</li>
</ul>

<h3>Title: Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks</h3>
<ul>
<li><strong>Authors: </strong>Mathis Pink, Vy A. Vo, Qinyuan Wu, Jianing Mu, Javier S. Turek, Uri Hasson, Kenneth A. Norman, Sebastian Michelmann, Alexander Huth, Mariya Toneva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08133">https://arxiv.org/abs/2410.08133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08133">https://arxiv.org/pdf/2410.08133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08133]] Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks(https://arxiv.org/abs/2410.08133)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Current LLM benchmarks focus on evaluating models' memory of facts and semantic relations, primarily assessing semantic aspects of long-term memory. However, in humans, long-term memory also includes episodic memory, which links memories to their contexts, such as the time and place they occurred. The ability to contextualize memories is crucial for many cognitive tasks and everyday functions. This form of memory has not been evaluated in LLMs with existing benchmarks. To address the gap in evaluating memory in LLMs, we introduce Sequence Order Recall Tasks (SORT), which we adapt from tasks used to study episodic memory in cognitive psychology. SORT requires LLMs to recall the correct order of text segments, and provides a general framework that is both easily extendable and does not require any additional annotations. We present an initial evaluation dataset, Book-SORT, comprising 36k pairs of segments extracted from 9 books recently added to the public domain. Based on a human experiment with 155 participants, we show that humans can recall sequence order based on long-term memory of a book. We find that models can perform the task with high accuracy when relevant text is given in-context during the SORT evaluation. However, when presented with the book text only during training, LLMs' performance on SORT falls short. By allowing to evaluate more aspects of memory, we believe that SORT will aid in the emerging development of memory-augmented models.</li>
<li><strong>摘要：</strong>当前的 LLM 基准侧重于评估模型对事实和语义关系的记忆，主要评估长期记忆的语义方面。然而，在人类中，长期记忆还包括情景记忆，它将记忆与其背景联系起来，例如它们发生的时间和地点。将记忆情境化的能力对于许多认知任务和日常功能至关重要。这种形式的记忆尚未在具有现有基准的 LLM 中进行评估。为了解决在 LLM 中评估记忆的差距，我们引入了序列顺序回忆任务 (SORT)，这是我们从用于研究认知心理学中的情景记忆的任务改编而来的。SORT 要求 LLM 回忆文本段的正确顺序，并提供一个既易于扩展又不需要任何额外注释的通用框架。我们提出了一个初始评估数据集 Book-SORT，其中包含从最近添加到公共领域的 9 本书中提取的 36k 对片段。基于对 155 名参与者的人类实验，我们表明人类可以根据一本书的长期记忆回忆序列顺序。我们发现，在 SORT 评估期间，如果在上下文中给出相关文本，模型可以高精度地执行任务。但是，如果在训练期间只提供书本文本，LLM 在 SORT 上的表现就会欠佳。通过允许评估记忆的更多方面，我们相信 SORT 将有助于新兴的记忆增强模型的发展。</li>
</ul>

<h3>Title: DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory</h3>
<ul>
<li><strong>Authors: </strong>Yutong Wang, Jiali Zeng, Xuebo Liu, Derek F. Wong, Fandong Meng, Jie Zhou, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08143">https://arxiv.org/abs/2410.08143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08143">https://arxiv.org/pdf/2410.08143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08143]] DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory(https://arxiv.org/abs/2410.08143)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved reasonable quality improvements in machine translation (MT). However, most current research on MT-LLMs still faces significant challenges in maintaining translation consistency and accuracy when processing entire documents. In this paper, we introduce DelTA, a Document-levEL Translation Agent designed to overcome these limitations. DelTA features a multi-level memory structure that stores information across various granularities and spans, including Proper Noun Records, Bilingual Summary, Long-Term Memory, and Short-Term Memory, which are continuously retrieved and updated by auxiliary LLM-based components. Experimental results indicate that DelTA significantly outperforms strong baselines in terms of translation consistency and quality across four open/closed-source LLMs and two representative document translation datasets, achieving an increase in consistency scores by up to 4.58 percentage points and in COMET scores by up to 3.16 points on average. DelTA employs a sentence-by-sentence translation strategy, ensuring no sentence omissions and offering a memory-efficient solution compared to the mainstream method. Furthermore, DelTA improves pronoun translation accuracy, and the summary component of the agent also shows promise as a tool for query-based summarization tasks. We release our code and data at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在机器翻译 (MT) 中取得了合理的质量改进。然而，目前大多数关于 MT-LLM 的研究在处理整个文档时，在保持翻译一致性和准确性方面仍然面临重大挑战。在本文中，我们介绍了 DelTA，这是一个旨在克服这些限制的文档级翻译代理。DelTA 具有多级内存结构，可存储各种粒度和跨度的信息，包括专有名词记录、双语摘要、长期记忆和短期记忆，这些信息由基于 LLM 的辅助组件不断检索和更新。实验结果表明，DelTA 在四个开/闭源 LLM 和两个代表性文档翻译数据集上的翻译一致性和质量方面明显优于强基线，一致性得分平均提高了 4.58 个百分点，COMET 得分平均提高了 3.16 个百分点。DelTA 采用逐句翻译策略，确保不遗漏句子，与主流方法相比，它提供了一种内存效率高的解决方案。此外，DelTA 提高了代词翻译的准确性，而代理的摘要组件也显示出作为基于查询的摘要任务工具的潜力。我们在此 https URL 上发布了我们的代码和数据。</li>
</ul>

<h3>Title: Insight Over Sight? Exploring the Vision-Knowledge Conflicts in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyuan Liu, Wenxuan Wang, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Pinjia He, Zhaopeng Tu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08145">https://arxiv.org/abs/2410.08145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08145">https://arxiv.org/pdf/2410.08145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08145]] Insight Over Sight? Exploring the Vision-Knowledge Conflicts in Multimodal LLMs(https://arxiv.org/abs/2410.08145)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper explores the problem of commonsense-level vision-knowledge conflict in Multimodal Large Language Models (MLLMs), where visual information contradicts model's internal commonsense knowledge (see Figure 1). To study this issue, we introduce an automated pipeline, augmented with human-in-the-loop quality control, to establish a benchmark aimed at simulating and assessing the conflicts in MLLMs. Utilizing this pipeline, we have crafted a diagnostic benchmark comprising 374 original images and 1,122 high-quality question-answer (QA) pairs. This benchmark covers two types of conflict target and three question difficulty levels, providing a thorough assessment tool. Through this benchmark, we evaluate the conflict-resolution capabilities of nine representative MLLMs across various model families and find a noticeable over-reliance on textual queries. Drawing on these findings, we propose a novel prompting strategy, "Focus-on-Vision" (FoV), which markedly enhances MLLMs' ability to favor visual data over conflicting textual knowledge. Our detailed analysis and the newly proposed strategy significantly advance the understanding and mitigating of vision-knowledge conflicts in MLLMs. The data and code are made publicly available.</li>
<li><strong>摘要：</strong>本文探讨了多模态大型语言模型 (MLLM) 中常识级视觉知识冲突的问题，其中视觉信息与模型内部的常识知识相矛盾（见图 1）。为了研究这个问题，我们引入了一个自动化管道，并增强了人在环的质量控制，以建立一个旨在模拟和评估 MLLM 中冲突的基准。利用这个管道，我们制作了一个诊断基准，包括 374 张原始图像和 1,122 个高质量问答 (QA) 对。这个基准涵盖了两种类型的冲突目标和三个问题难度级别，提供了一个全面的评估工具。通过这个基准，我们评估了不同模型系列中九个代表性 MLLM 的冲突解决能力，发现对文本查询的过度依赖明显。根据这些发现，我们提出了一种新颖的提示策略“聚焦视觉”(FoV)，这显著增强了 MLLM 偏爱视觉数据而不是冲突的文本知识的能力。我们的详细分析和新提出的策略大大促进了对 MLLM 中视觉知识冲突的理解和缓解。数据​​和代码已公开。</li>
</ul>

<h3>Title: The Effect of Surprisal on Reading Times in Information Seeking and Repeated Reading</h3>
<ul>
<li><strong>Authors: </strong>Keren Gruteke Klein, Yoav Meiri, Omer Shubi, Yevgeni Berzak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08162">https://arxiv.org/abs/2410.08162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08162">https://arxiv.org/pdf/2410.08162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08162]] The Effect of Surprisal on Reading Times in Information Seeking and Repeated Reading(https://arxiv.org/abs/2410.08162)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The effect of surprisal on processing difficulty has been a central topic of investigation in psycholinguistics. Here, we use eyetracking data to examine three language processing regimes that are common in daily life but have not been addressed with respect to this question: information seeking, repeated processing, and the combination of the two. Using standard regime-agnostic surprisal estimates we find that the prediction of surprisal theory regarding the presence of a linear effect of surprisal on processing times, extends to these regimes. However, when using surprisal estimates from regime-specific contexts that match the contexts and tasks given to humans, we find that in information seeking, such estimates do not improve the predictive power of processing times compared to standard surprisals. Further, regime-specific contexts yield near zero surprisal estimates with no predictive power for processing times in repeated reading. These findings point to misalignments of task and memory representations between humans and current language models, and question the extent to which such models can be used for estimating cognitively relevant quantities. We further discuss theoretical challenges posed by these results.</li>
<li><strong>摘要：</strong>意外对处理难度的影响一直是心理语言学研究的核心课题。在这里，我们使用眼动追踪数据来研究日常生活中常见但尚未解决的三种语言处理机制：信息搜索、重复处理以及两者的结合。使用标准的与机制无关的意外估计，我们发现意外理论关于意外对处理时间存在线性影响的预测延伸到这些机制。然而，当使用与人类所处环境和任务相匹配的特定机制环境中的意外估计时，我们发现在信息搜索中，与标准意外相比，此类估计并没有提高处理时间的预测能力。此外，特定机制环境产生的意外估计接近于零，对重复阅读中的处理时间没有预测能力。这些发现指出了人类和当前语言模型之间的任务和记忆表征不一致，并质疑此类模型可用于估计认知相关量的程度。我们进一步讨论了这些结果所带来的理论挑战。</li>
</ul>

<h3>Title: Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qingni Wang, Tiantian Geng, Zhiyuan Wang, Teng Wang, Bo Fu, Feng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08174">https://arxiv.org/abs/2410.08174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08174">https://arxiv.org/pdf/2410.08174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08174]] Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models(https://arxiv.org/abs/2410.08174)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However, these methods typically rely on internal model logits or are restricted to multiple-choice settings, which hampers their generalizability and adaptability in dynamic, open-ended environments. In this paper, we introduce TRON, a two-step framework for risk control and assessment, applicable to any MLLM that supports sampling in both open-ended and closed-ended scenarios. TRON comprises two main components: (1) a novel conformal score to sample response sets of minimum size, and (2) a nonconformity score to identify high-quality responses based on self-consistency theory, controlling the error rates by two specific risk levels. Furthermore, we investigate semantic redundancy in prediction sets within open-ended contexts for the first time, leading to a promising evaluation metric for MLLMs based on average set size. Our comprehensive experiments across four Video Question-Answering (VideoQA) datasets utilizing eight MLLMs show that TRON achieves desired error rates bounded by two user-specified risk levels. Additionally, deduplicated prediction sets maintain adaptiveness while being more efficient and stable for risk assessment under different risk levels.</li>
<li><strong>摘要：</strong>多模态大型语言模型 (MLLM) 在各种任务中都表现出了令人鼓舞的进步，但它们仍然面临重大的可信度问题。先前的研究将分裂共形预测 (SCP) 应用于语言建模，以构建具有统计保证的预测集。然而，这些方法通常依赖于内部模型逻辑或仅限于多项选择设置，这妨碍了它们在动态、开放式环境中的普遍性和适应性。在本文中，我们介绍了 TRON，这是一个用于风险控​​制和评估的两步框架，适用于任何支持在开放式和封闭式场景中抽样的 MLLM。TRON 包含两个主要组成部分：(1) 一种新的共形分数，用于对最小大小的响应集进行抽样，以及 (2) 一种不一致性分数，用于基于自洽理论识别高质量响应，通过两个特定风险级别控制错误率。此外，我们首次研究了开放式上下文中预测集中的语义冗余，从而为基于平均集大小的 MLLM 提出了一种有希望的评估指标。我们对四个视频问答 (VideoQA) 数据集进行了全面的实验，使用了八个 MLLM，结果表明 TRON 可实现由两个用户指定的风险级别所限定的期望错误率。此外，去重后的预测集可保持自适应性，同时在不同风险级别下进行风险评估时更加高效和稳定。</li>
</ul>

<h3>Title: GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, Sumitra Ganesh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08193">https://arxiv.org/abs/2410.08193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08193">https://arxiv.org/pdf/2410.08193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08193]] GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment(https://arxiv.org/abs/2410.08193)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, we introduce GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model--a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, we demonstrate that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 表现出令人印象深刻的功能，但需要仔细与人类偏好对齐。传统的训练时间方法使用人类偏好数据集对 LLM 进行微调，但会产生大量训练成本，并且需要重复训练才能处理不同的用户偏好。测试时间对齐方法通过使用奖励模型 (RM) 来引导冻结的 LLM 而无需重新训练来解决此问题。然而，现有的测试时间方法依赖于旨在评估完整响应的轨迹级 RM，这使得它们不适合需要从部分响应计算下一个标记奖励的自回归文本生成。为了解决这个问题，我们引入了 GenARM，这是一种利用自回归奖励模型的测试时间对齐方法——一种新颖的奖励参数化，旨在预测下一个标记奖励，以实现高效且有效的自回归生成。从理论上讲，我们证明这种参数化可以证明引导冻结的 LLM 达到 KL 正则化强化学习框架内传统 RM 可实现的任何分布。实验结果表明，GenARM 的表现明显优于之前的测试时对齐基线，并且与训练时方法的性能相当。此外，GenARM 能够实现高效的从弱到强的指导，将较大的 LLM 与较小的 RM 对齐，而无需花费大量成本来训练较大的模型。此外，GenARM 支持多目标对齐，允许在偏好维度之间进行实时权衡，并满足不同的用户偏好，而无需重新训练。</li>
</ul>

<h3>Title: MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code</h3>
<ul>
<li><strong>Authors: </strong>Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08196">https://arxiv.org/abs/2410.08196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08196">https://arxiv.org/pdf/2410.08196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08196]] MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code(https://arxiv.org/abs/2410.08196)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline. The code is released at this https URL .</li>
<li><strong>摘要：</strong>代码因其精度和准确性而被证明能有效增强大型语言模型的数学推理能力。先前涉及持续数学预训练的工作通常包括使用数学相关包的代码，这些包主要用于工程、机器学习、信号处理或模块测试等领域，而不是直接关注数学推理。在本文中，我们介绍了一种用于生成持续预训练的数学代码及其相应推理步骤的新方法。我们的方法首先构建高质量的数学持续预训练数据集，结合数学相关的网络数据、使用数学包的代码、数学教科书和合成数据。接下来，我们通过从先前收集的数据中提取 LaTeX 表达式、表达式所需的条件以及表达式的结果来构建推理步骤。基于这些提取的信息，我们生成相应的代码以准确捕捉数学推理过程。将生成的代码附加到每个推理步骤后，将得到由成对的自然语言推理步骤及其相应代码组成的数据。将这些数据与原始数据集相结合，可生成一个包含 192 亿个 token 的高性能数学预训练语料库，我们将其命名为 MathCode-Pile。使用该语料库训练几个流行的基础模型可显著提高它们的数学能力，从而创建 MathCoder2 系列模型。我们所有的数据处理和训练代码都是开源的，确保整个数据收集和训练流程完全透明且易于复制。代码发布在此 https URL 上。</li>
</ul>

<h3>Title: From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions</h3>
<ul>
<li><strong>Authors: </strong>Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08197">https://arxiv.org/abs/2410.08197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08197">https://arxiv.org/pdf/2410.08197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08197]] From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions(https://arxiv.org/abs/2410.08197)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Tool learning enables Large Language Models (LLMs) to interact with external environments by invoking tools, serving as an effective strategy to mitigate the limitations inherent in their pre-training data. In this process, tool documentation plays a crucial role by providing usage instructions for LLMs, thereby facilitating effective tool utilization. This paper concentrates on the critical challenge of bridging the comprehension gap between LLMs and external tools due to the inadequacies and inaccuracies inherent in existing human-centric tool documentation. We propose a novel framework, DRAFT, aimed at Dynamically Refining tool documentation through the Analysis of Feedback and Trails emanating from LLMs' interactions with external tools. This methodology pivots on an innovative trial-and-error approach, consisting of three distinct learning phases: experience gathering, learning from experience, and documentation rewriting, to iteratively enhance the tool documentation. This process is further optimized by implementing a diversity-promoting exploration strategy to ensure explorative diversity and a tool-adaptive termination mechanism to prevent overfitting while enhancing efficiency. Extensive experiments on multiple datasets demonstrate that DRAFT's iterative, feedback-based refinement significantly ameliorates documentation quality, fostering a deeper comprehension and more effective utilization of tools by LLMs. Notably, our analysis reveals that the tool documentation refined via our approach demonstrates robust cross-model generalization capabilities.</li>
<li><strong>摘要：</strong>工具学习使大型语言模型 (LLM) 能够通过调用工具与外部环境进行交互，这是一种有效的策略，可以减轻其预训练数据固有的局限性。在此过程中，工具文档起着至关重要的作用，它为 LLM 提供使用说明，从而促进工具的有效利用。本文集中讨论由于现有以人为本的工具文档固有的不足和不准确性，弥合 LLM 与外部工具之间的理解差距所面临的关键挑战。我们提出了一个新颖的框架 DRAFT，旨在通过分析 LLM 与外部工具交互产生的反馈和轨迹来动态改进工具文档。该方法以创新的反复试验方法为中心，包括三个不同的学习阶段：经验收集、从经验中学习和文档重写，以迭代方式增强工具文档。通过实施促进多样性的探索策略以确保探索性多样性和工具自适应终止机制以防止过度拟合并提高效率，进一步优化了该过程。在多个数据集上进行的大量实验表明，DRAFT 的基于反馈的迭代改进显著改善了文档质量，促进了 LLM 对工具的更深理解和更有效地利用。值得注意的是，我们的分析表明，通过我们的方法改进的工具文档具有强大的跨模型泛化能力。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
