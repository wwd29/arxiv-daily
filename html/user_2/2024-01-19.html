<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-19</h1>
<h3>Title: RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Meiling Tao, Xuechen Liang, Tianyu Shi, Lei Yu, Yiting Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09432">https://arxiv.org/abs/2401.09432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09432">https://arxiv.org/pdf/2401.09432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09432]] RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language  Models(https://arxiv.org/abs/2401.09432)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a significant leap in personalized AI interactions, and paves the way for more authentic and immersive AI-assisted role-playing experiences by enabling more nuanced and emotionally rich dialogues</li>
<li><strong>摘要：</strong>本研究提出了 RoleCraft-GLM，这是一个旨在通过大型语言模型 (LLM) 增强个性化角色扮演的创新框架。 RoleCraft-GLM 解决了对话式 AI 中缺乏个性化交互的关键问题，并提供了具有详细且情感细腻的角色刻画的解决方案。我们贡献了一个独特的对话数据集，从传统的以名人为中心的角色转变为多样化的非名人角色，从而增强了语言建模交互的真实性和复杂性。此外，我们的方法包括细致的角色发展，确保对话既现实又能引起情感共鸣。 RoleCraft-GLM 的有效性通过各种案例研究得到验证，突显了其在不同场景中的多功能性和技能。我们的框架擅长生成准确反映角色个性特征和情感的对话，从而提高用户参与度。总之，RoleCraft-GLM 标志着个性化 AI 交互的重大飞跃，并通过实现更细致、情感丰富的对话，为更真实、更身临其境的 AI 辅助角色扮演体验铺平了道路</li>
</ul>

<h3>Title: Incorporating Riemannian Geometric Features for Learning Coefficient of  Pressure Distributions on Airplane Wings</h3>
<ul>
<li><strong>Authors: </strong>Liwei Hu, Wenyong Wang, Yu Xiang, Stefan Sommer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09452">https://arxiv.org/abs/2401.09452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09452">https://arxiv.org/pdf/2401.09452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09452]] Incorporating Riemannian Geometric Features for Learning Coefficient of  Pressure Distributions on Airplane Wings(https://arxiv.org/abs/2401.09452)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The aerodynamic coefficients of aircrafts are significantly impacted by its geometry, especially when the angle of attack (AoA) is large. In the field of aerodynamics, traditional polynomial-based parameterization uses as few parameters as possible to describe the geometry of an airfoil. However, because the 3D geometry of a wing is more complicated than the 2D airfoil, polynomial-based parameterizations have difficulty in accurately representing the entire shape of a wing in 3D space. Existing deep learning-based methods can extract massive latent neural representations for the shape of 2D airfoils or 2D slices of wings. Recent studies highlight that directly taking geometric features as inputs to the neural networks can improve the accuracy of predicted aerodynamic coefficients. Motivated by geometry theory, we propose to incorporate Riemannian geometric features for learning Coefficient of Pressure (CP) distributions on wing surfaces. Our method calculates geometric features (Riemannian metric, connection, and curvature) and further inputs the geometric features, coordinates and flight conditions into a deep learning model to predict the CP distribution. Experimental results show that our method, compared to state-of-the-art Deep Attention Network (DAN), reduces the predicted mean square error (MSE) of CP by an average of 8.41% for the DLR-F11 aircraft test set.</li>
<li><strong>摘要：</strong>飞机的空气动力学系数受其几何形状的显着影响，特别是当迎角（AoA）较大时。在空气动力学领域，传统的基于多项式的参数化使用尽可能少的参数来描述翼型的几何形状。然而，由于机翼的 3D 几何形状比 2D 翼型更复杂，因此基于多项式的参数化很难在 3D 空间中准确表示机翼的整个形状。现有的基于深度学习的方法可以提取 2D 翼型或 2D 机翼切片形状的大量潜在神经表征。最近的研究强调，直接将几何特征作为神经网络的输入可以提高预测空气动力系数的准确性。受几何理论的启发，我们建议结合黎曼几何特征来学习机翼表面的压力系数（CP）分布。我们的方法计算几何特征（黎曼度量、连接和曲率），并进一步将几何特征、坐标和飞行条件输入到深度学习模型中以预测CP分布。实验结果表明，与最先进的深度注意力网络 (DAN) 相比，我们的方法在 DLR-F11 飞机测试集上平均降低了 CP 的预测均方误差 (MSE) 8.41%。</li>
</ul>

<h3>Title: LoMA: Lossless Compressed Memory Attention</h3>
<ul>
<li><strong>Authors: </strong>Yumeng Wang, Zhenyang Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09486">https://arxiv.org/abs/2401.09486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09486">https://arxiv.org/pdf/2401.09486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09486]] LoMA: Lossless Compressed Memory Attention(https://arxiv.org/abs/2401.09486)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The ability to handle long texts is one of the most important capabilities of Large Language Models (LLMs), but as the text length increases, the consumption of resources also increases dramatically. At present, reducing resource consumption by compressing the KV cache is a common approach. Although there are many existing compression methods, they share a common drawback: the compression is not lossless. That is, information is inevitably lost during the compression process. If the compression rate is high, the probability of losing important information increases dramatically. We propose a new method, Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information into special memory token KV pairs according to a set compression ratio. Our experiments have achieved remarkable results, demonstrating that LoMA can be efficiently trained and has very effective performance.</li>
<li><strong>摘要：</strong>处理长文本的能力是大型语言模型（LLM）最重要的能力之一，但随着文本长度的增加，资源的消耗也急剧增加。目前，通过压缩KV缓存来减少资源消耗是一种常见的做法。尽管现有的压缩方法有很多，但它们都有一个共同的缺点：压缩不是无损的。也就是说，在压缩过程中信息不可避免地会丢失。如果压缩率很高，丢失重要信息的概率就会急剧增加。我们提出了一种新方法，无损压缩内存注意力（LoMA），它允许根据设定的压缩比将信息无损压缩为特殊的内存令牌KV对。我们的实验取得了显着的结果，证明LoMA可以被有效地训练并且具有非常有效的性能。</li>
</ul>

<h3>Title: Memory, Space, and Planning: Multiscale Predictive Representations</h3>
<ul>
<li><strong>Authors: </strong>Ida Momennejad</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09491">https://arxiv.org/abs/2401.09491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09491">https://arxiv.org/pdf/2401.09491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09491]] Memory, Space, and Planning: Multiscale Predictive Representations(https://arxiv.org/abs/2401.09491)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Memory is inherently entangled with prediction and planning. Flexible behavior in biological and artificial agents depends on the interplay of learning from the past and predicting the future in ever-changing environments. This chapter reviews computational, behavioral, and neural evidence suggesting these processes rely on learning the relational structure of experiences, known as cognitive maps, and draws two key takeaways. First, that these memory structures are organized as multiscale, compact predictive representations in hippocampal and prefrontal cortex, or PFC, hierarchies. Second, we argue that such predictive memory structures are crucial to the complementary functions of the hippocampus and PFC, both for enabling a recall of detailed and coherent past episodes as well as generalizing experiences at varying scales for efficient prediction and planning. These insights advance our understanding of memory and planning mechanisms in the brain and hold significant implications for advancing artificial intelligence systems.</li>
<li><strong>摘要：</strong>记忆本质上与预测和计划纠缠在一起。生物和人工智能体的灵活行为取决于在不断变化的环境中学习过去和预测未来的相互作用。本章回顾了计算、行为和神经证据，表明这些过程依赖于学习经验的关系结构（称为认知图），并得出两个关键要点。首先，这些记忆结构在海马和前额皮质（PFC）层次结构中被组织为多尺度、紧凑的预测表示。其次，我们认为这种预测性记忆结构对于海马体和前额皮质的互补功能至关重要，既可以回忆起详细且连贯的过去事件，也可以概括不同尺度的经验以进行有效的预测和规划。这些见解增进了我们对大脑记忆和规划机制的理解，并对推进人工智能系统具有重要意义。</li>
</ul>

<h3>Title: Functional Autoencoder for Smoothing and Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Sidi Wu, Cédric Beaulac, Jiguo Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09499">https://arxiv.org/abs/2401.09499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09499">https://arxiv.org/pdf/2401.09499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09499]] Functional Autoencoder for Smoothing and Representation Learning(https://arxiv.org/abs/2401.09499)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>A common pipeline in functional data analysis is to first convert the discretely observed data to smooth functions, and then represent the functions by a finite-dimensional vector of coefficients summarizing the information. Existing methods for data smoothing and dimensional reduction mainly focus on learning the linear mappings from the data space to the representation space, however, learning only the linear representations may not be sufficient. In this study, we propose to learn the nonlinear representations of functional data using neural network autoencoders designed to process data in the form it is usually collected without the need of preprocessing. We design the encoder to employ a projection layer computing the weighted inner product of the functional data and functional weights over the observed timestamp, and the decoder to apply a recovery layer that maps the finite-dimensional vector extracted from the functional data back to functional space using a set of predetermined basis functions. The developed architecture can accommodate both regularly and irregularly spaced data. Our experiments demonstrate that the proposed method outperforms functional principal component analysis in terms of prediction and classification, and maintains superior smoothing ability and better computational efficiency in comparison to the conventional autoencoders under both linear and nonlinear settings.</li>
<li><strong>摘要：</strong>函数数据分析中的常见流程是首先将离散观察的数据转换为平滑函数，然后通过总结信息的有限维系数向量来表示函数。现有的数据平滑和降维方法主要集中于学习从数据空间到表示空间的线性映射，然而，仅学习线性表示可能是不够的。在本研究中，我们建议使用神经网络自动编码器来学习功能数据的非线性表示，该神经网络自动编码器旨在以通常收集的形式处理数据，而无需预处理。我们设计的编码器采用投影层来计算函数数据和函数权重在观察到的时间戳上的加权内积，而解码器则应用恢复层将从函数数据中提取的有限维向量映射回函数空间使用一组预定的基函数。开发的架构可以容纳规则和不规则间隔的数据。我们的实验表明，所提出的方法在预测和分类方面优于函数主成分分析，并且与线性和非线性设置下的传统自动编码器相比，保持优异的平滑能力和更好的计算效率。</li>
</ul>

<h3>Title: Accelerating Data Generation for Neural Operators via Krylov Subspace  Recycling</h3>
<ul>
<li><strong>Authors: </strong>Hong Wang, Zhongkai Hao, Jie Wang, Zijie Geng, Zhen Wang, Bin Li, Feng Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09516">https://arxiv.org/abs/2401.09516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09516">https://arxiv.org/pdf/2401.09516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09516]] Accelerating Data Generation for Neural Operators via Krylov Subspace  Recycling(https://arxiv.org/abs/2401.09516)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov subspace recycling, a powerful technique for solving a series of interrelated systems by leveraging their inherent similarities. Specifically, SKR employs a sorting algorithm to arrange these systems in a sequence, where adjacent systems exhibit high similarities. Then it equips a solver with Krylov subspace recycling to solve the systems sequentially instead of independently, thus effectively enhancing the solving efficiency. Both theoretical analysis and extensive experiments demonstrate that SKR can significantly accelerate neural operator data generation, achieving a remarkable speedup of up to 13.9 times.</li>
<li><strong>摘要：</strong>学习用于求解偏微分方程（PDE）的神经算子由于其高推理效率而引起了极大的关注。然而，训练此类算子需要生成大量标记数据，即偏微分方程问题及其解决方案。数据生成过程非常耗时，因为它涉及求解大量线性方程组以获得偏微分方程的数值解。许多现有方法独立地求解这些系统，没有考虑它们固有的相似性，导致极其冗余的计算。为了解决这个问题，我们提出了一种新方法，即排序克雷洛夫回收（SKR），以提高解决这些系统的效率，从而显着加速神经算子训练的数据生成。据我们所知，SKR 是解决学习神经算子数据生成耗时问题的首次尝试。 SKR 的核心是 Krylov 子空间回收，这是一种利用系统固有的相似性来解决一系列相互关联系统的强大技术。具体来说，SKR采用排序算法将这些系统按顺序排列，其中相邻系统表现出高度相似性。然后为求解器配备Krylov子空间循环，对系统进行顺序求解而不是独立求解，从而有效提高求解效率。理论分析和大量实验都表明 SKR 可以显着加速神经算子数据生成，实现高达 13.9 倍的显着加速。</li>
</ul>

<h3>Title: BERTologyNavigator: Advanced Question Answering with BERT-based  Semantics</h3>
<ul>
<li><strong>Authors: </strong>Shreya Rajpal (1,2), Ricardo Usbeck (1) ((1) Universität Hamburg, Hamburg, Germany,(2) Vellore Institute of Technology, Vellore, Tamil Nadu, India)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09553">https://arxiv.org/abs/2401.09553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09553">https://arxiv.org/pdf/2401.09553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09553]] BERTologyNavigator: Advanced Question Answering with BERT-based  Semantics(https://arxiv.org/abs/2401.09553)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The development and integration of knowledge graphs and language models has significance in artificial intelligence and natural language processing. In this study, we introduce the BERTologyNavigator -- a two-phased system that combines relation extraction techniques and BERT embeddings to navigate the relationships within the DBLP Knowledge Graph (KG). Our approach focuses on extracting one-hop relations and labelled candidate pairs in the first phases. This is followed by employing BERT's CLS embeddings and additional heuristics for relation selection in the second phase. Our system reaches an F1 score of 0.2175 on the DBLP QuAD Final test dataset for Scholarly QALD and 0.98 F1 score on the subset of the DBLP QuAD test dataset during the QA phase.</li>
<li><strong>摘要：</strong>知识图谱和语言模型的开发和融合对于人工智能和自然语言处理具有重要意义。在本研究中，我们介绍了 BERTologyNavigator——一个两阶段系统，它结合了关系提取技术和 BERT 嵌入来导航 DBLP 知识图 (KG) 内的关系。我们的方法侧重于在第一阶段提取单跳关系和标记候选对。接下来是在第二阶段使用 BERT 的 CLS 嵌入和额外的启发式关系选择。我们的系统在 Scholarly QALD 的 DBLP QuAD 最终测试数据集上达到了 0.2175 的 F1 分数，在 QA 阶段在 DBLP QuAD 测试数据集的子集上达到了 0.98 F1 分数。</li>
</ul>

<h3>Title: Improving Classification Performance With Human Feedback: Label a few,  we label the rest</h3>
<ul>
<li><strong>Authors: </strong>Natan Vidra, Thomas Clifford, Katherine Jijo, Eden Chung, Liang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09555">https://arxiv.org/abs/2401.09555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09555">https://arxiv.org/pdf/2401.09555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09555]] Improving Classification Performance With Human Feedback: Label a few,  we label the rest(https://arxiv.org/abs/2401.09555)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performance. We demonstrate that rather than needing to manually label millions of rows of data, we just need to label a few and the model can effectively predict the rest.</li>
<li><strong>摘要：</strong>在人工智能领域，绝大多数数据都是非结构化的，获取大量标记数据来训练监督机器学习模型构成了重大挑战。为了解决这个问题，我们深入研究了小样本和主动学习，其目标是通过人类对一些标记示例的反馈来改进人工智能模型。本文重点了解连续反馈循环如何完善模型，从而通过增量人工输入提高模型的准确性、召回率和精确度。通过采用 GPT-3.5、BERT 和 SetFit 等大型语言模型 (LLM)，我们的目标是分析使用有限数量的标记示例来大幅提高模型准确性的效果。我们在 Financial Phrasebank、Banking、Craigslist、Trec、Amazon Reviews 数据集上对这种方法进行了基准测试，以证明只需几个带标签的示例，我们就能够超越零样本大型语言模型的准确性，从而提供增强的文本分类性能。我们证明，我们不需要手动标记数百万行数据，而只需要标记一些数据，模型就可以有效地预测其余数据。</li>
</ul>

<h3>Title: Sharing Knowledge in Multi-Task Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Carlo D'Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, Jan Peters</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09561">https://arxiv.org/abs/2401.09561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09561">https://arxiv.org/pdf/2401.09561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09561]] Sharing Knowledge in Multi-Task Deep Reinforcement Learning(https://arxiv.org/abs/2401.09561)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance.</li>
<li><strong>摘要：</strong>我们研究了在任务之间共享表示的好处，以实现在多任务强化学习中有效使用深度神经网络。我们利用这样的假设：从不同的任务中学习，共享共同的属性，有助于概括它们的知识，从而与学习单个任务相比，可以更有效地提取特征。直观上，强化学习算法使用所得的特征集可提供性能优势。我们通过提供理论保证来证明这一点，这些保证强调了方便在任务之间共享表示的条件，将近似值迭代的众所周知的有限时间界限扩展到多任务设置。此外，我们还通过提出三种强化学习算法的多任务扩展来补充我们的分析，我们对广泛使用的强化学习基准进行了实证评估，显示在样本效率和性能方面比单任务算法有显着改进。</li>
</ul>

<h3>Title: Aligning Large Language Models with Counterfactual DPO</h3>
<ul>
<li><strong>Authors: </strong>Bradley Butcher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09566">https://arxiv.org/abs/2401.09566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09566">https://arxiv.org/pdf/2401.09566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09566]] Aligning Large Language Models with Counterfactual DPO(https://arxiv.org/abs/2401.09566)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions. Our findings suggest that counterfactual prompting with DPO presents a low-resource way to fine-tune LLMs to meet the demands for responsible and ethically aligned AI systems.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 的进步在各种应用程序中展示了卓越的功能。这些模型擅长生成上下文连贯的文本补全，并涵盖广泛的主题。然而，训练所需的大量数据集使得在预训练和指令调整阶段调整响应风格具有挑战性。因此，通常采用额外的对齐阶段，其中使用人类偏好数据进一步训练模型，以更好地将其输出与人类期望对齐。虽然此过程本身并未引入新功能，但它确实强调了模型固有的生成样式。本文探讨了在直接偏好优化 (DPO) 框架内利用反事实提示来调整模型的风格，而无需依赖人工干预。我们证明，这种方法有效地灌输了理想的行为，减少了不良行为，并鼓励模型忽略不适当的指令。我们的研究结果表明，DPO 的反事实提示提供了一种资源匮乏的方式来微调法学硕士，以满足对负责任和道德一致的人工智能系统的需求。</li>
</ul>

<h3>Title: Towards Scalable and Robust Model Versioning</h3>
<ul>
<li><strong>Authors: </strong>Wenxin Ding, Arjun Nitin Bhagoji, Ben Y. Zhao, Haitao Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09574">https://arxiv.org/abs/2401.09574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09574">https://arxiv.org/pdf/2401.09574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09574]] Towards Scalable and Robust Model Versioning(https://arxiv.org/abs/2401.09574)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>As the deployment of deep learning models continues to expand across industries, the threat of malicious incursions aimed at gaining access to these deployed models is on the rise. Should an attacker gain access to a deployed model, whether through server breaches, insider attacks, or model inversion techniques, they can then construct white-box adversarial attacks to manipulate the model's classification outcomes, thereby posing significant risks to organizations that rely on these models for critical tasks. Model owners need mechanisms to protect themselves against such losses without the necessity of acquiring fresh training data - a process that typically demands substantial investments in time and capital. In this paper, we explore the feasibility of generating multiple versions of a model that possess different attack properties, without acquiring new training data or changing model architecture. The model owner can deploy one version at a time and replace a leaked version immediately with a new version. The newly deployed model version can resist adversarial attacks generated leveraging white-box access to one or all previously leaked versions. We show theoretically that this can be accomplished by incorporating parameterized hidden distributions into the model training data, forcing the model to learn task-irrelevant features uniquely defined by the chosen data. Additionally, optimal choices of hidden distributions can produce a sequence of model versions capable of resisting compound transferability attacks over time. Leveraging our analytical insights, we design and implement a practical model versioning method for DNN classifiers, which leads to significant robustness improvements over existing methods. We believe our work presents a promising direction for safeguarding DNN services beyond their initial deployment.</li>
<li><strong>摘要：</strong>随着深度学习模型的部署不断在各个行业扩展，旨在获取这些已部署模型的恶意入侵的威胁正在不断增加。如果攻击者获得对已部署模型的访问权限，无论是通过服务器破坏、内部攻击还是模型反转技术，他们都可以构建白盒对抗性攻击来操纵模型的分类结果，从而给依赖这些模型的组织带来重大风险用于关键任务。模型所有者需要机制来保护自己免受此类损失，而无需获取新的训练数据 - 这一过程通常需要大量时间和资本投资。在本文中，我们探讨了在不获取新的训练数据或更改模型架构的情况下生成具有不同攻击属性的模型的多个版本的可行性。模型所有者可以一次部署一个版本，并立即用新版本替换泄漏的版本。新部署的模型版本可以抵御利用白盒访问一个或所有先前泄露的版本而产生的对抗性攻击。我们从理论上证明，这可以通过将参数化隐藏分布合并到模型训练数据中来实现，迫使模型学习由所选数据唯一定义的与任务无关的特征。此外，隐藏分布的最佳选择可以产生一系列模型版本，能够随着时间的推移抵抗复合可转移性攻击。利用我们的分析见解，我们设计并实现了一种适用于 DNN 分类器的实用模型版本控制方法，这使得现有方法的鲁棒性得到了显着提高。我们相信，我们的工作为保护 DNN 服务超出其初始部署提供了一个有希望的方向。</li>
</ul>

<h3>Title: eipy: An Open-Source Python Package for Multi-modal Data Integration  using Heterogeneous Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Jamie J. R. Bennett, Yan Chak Li, Gaurav Pandey</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09582">https://arxiv.org/abs/2401.09582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09582">https://arxiv.org/pdf/2401.09582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09582]] eipy: An Open-Source Python Package for Multi-modal Data Integration  using Heterogeneous Ensembles(https://arxiv.org/abs/2401.09582)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce eipy--an open-source Python package for developing effective, multi-modal heterogeneous ensembles for classification. eipy simultaneously provides both a rigorous, and user-friendly framework for comparing and selecting the best-performing multi-modal data integration and predictive modeling methods by systematically evaluating their performance using nested cross-validation. The package is designed to leverage scikit-learn-like estimators as components to build multi-modal predictive models. An up-to-date user guide, including API reference and tutorials, for eipy is maintained at https://eipy.readthedocs.io . The main repository for this project can be found on GitHub at https://github.com/GauravPandeyLab/eipy .</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 eipy——一个开源 Python 包，用于开发有效的多模态异构集成进行分类。 eipy 同时提供了一个严格且用户友好的框架，用于通过使用嵌套交叉验证系统地评估其性能来比较和选择性能最佳的多模式数据集成和预测建模方法。该软件包旨在利用类似 scikit-learn 的估计器作为构建多模态预测模型的组件。 https://eipy.readthedocs.io 维护着 eipy 的最新用户指南，包括 API 参考和教程。该项目的主要存储库可以在 GitHub 上找到：https://github.com/GauravPandeyLab/eipy。</li>
</ul>

<h3>Title: Learning Shortcuts: On the Misleading Promise of NLU in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Geetanjali Bihani, Julia Taylor Rayz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09615">https://arxiv.org/abs/2401.09615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09615">https://arxiv.org/pdf/2401.09615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09615]] Learning Shortcuts: On the Misleading Promise of NLU in Language Models(https://arxiv.org/abs/2401.09615)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现使得自然语言处理领域的性能显着提升。然而，最近的研究发现，法学硕士在执行任务时经常采取捷径，造成绩效提高的错觉，同时其决策规则缺乏普遍性。这种现象给法学硕士准确评估自然语言理解带来了挑战。我们的论文对该领域的相关研究进行了简明的调查，并提出了快捷学习在语言模型评估中的影响的观点，特别是对于 NLU 任务。本文敦促我们投入更多的研究努力来加深我们对快捷学习的理解，为开发更强大的语言模型做出贡献，并提高现实场景中 NLU 评估的标准。</li>
</ul>

<h3>Title: Physics-Informed Calibration of Aeromagnetic Compensation in Magnetic  Navigation Systems using Liquid Time-Constant Networks</h3>
<ul>
<li><strong>Authors: </strong>Favour Nerrise (1 and 2), Andrew Sosa Sosanya (2), Patrick Neary (2) ((1) Department of Electrical Engineering, Stanford University, CA, USA, (2) SandboxAQ, Palo Alto, CA, USA)</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, physics.comp-ph, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09631">https://arxiv.org/abs/2401.09631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09631">https://arxiv.org/pdf/2401.09631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09631]] Physics-Informed Calibration of Aeromagnetic Compensation in Magnetic  Navigation Systems using Liquid Time-Constant Networks(https://arxiv.org/abs/2401.09631)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Magnetic navigation (MagNav) is a rising alternative to the Global Positioning System (GPS) and has proven useful for aircraft navigation. Traditional aircraft navigation systems, while effective, face limitations in precision and reliability in certain environments and against attacks. Airborne MagNav leverages the Earth's magnetic field to provide accurate positional information. However, external magnetic fields induced by aircraft electronics and Earth's large-scale magnetic fields disrupt the weaker signal of interest. We introduce a physics-informed approach using Tolles-Lawson coefficients for compensation and Liquid Time-Constant Networks (LTCs) to remove complex, noisy signals derived from the aircraft's magnetic sources. Using real flight data with magnetometer measurements and aircraft measurements, we observe up to a 64% reduction in aeromagnetic compensation error (RMSE nT), outperforming conventional models. This significant improvement underscores the potential of a physics-informed, machine learning approach for extracting clean, reliable, and accurate magnetic signals for MagNav positional estimation.</li>
<li><strong>摘要：</strong>磁导航 (MagNav) 是全球定位系统 (GPS) 的新兴替代方案，已被证明对飞机导航非常有用。传统的飞机导航系统虽然有效，但在某些环境下和抵御攻击方面面临精度和可靠性的限制。机载 MagNav 利用地球磁场提供准确的位置信息。然而，飞机电子设备感应的外部磁场和地球的大尺度磁场会扰乱感兴趣的较弱信号。我们引入了一种基于物理的方法，使用 Tolles-Lawson 系数进行补偿，并使用液体时间常数网络 (LTC) 来消除源自飞机磁源的复杂噪声信号。使用真实飞行数据以及磁力计测量和飞机测量，我们观察到航磁补偿误差 (RMSE nT) 降低了 64%，优于传统模型。这一重大改进凸显了基于物理的机器学习方法在提取干净、可靠且准确的磁信号以进行 MagNav 位置估计方面的潜力。</li>
</ul>

<h3>Title: ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on  Climate Change</h3>
<ul>
<li><strong>Authors: </strong>David Thulke, Yingbo Gao, Petrus Pelser, Rein Brune, Rricha Jalota, Floris Fok, Michael Ramos, Ian van Wyk, Abdallah Nasir, Hayden Goldstein, Taylor Tragemann, Katie Nguyen, Ariana Fowler, Andrew Stanco, Jon Gabriel, Jordan Taylor, Dean Moro, Evgenii Tsymbalov, Juliette de Waal, Evgeny Matusov, Mudar Yaghi, Mohammad Shihadah, Hermann Ney, Christian Dugast, Jonathan Dotan, Daniel Erasmus</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09646">https://arxiv.org/abs/2401.09646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09646">https://arxiv.org/pdf/2401.09646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09646]] ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on  Climate Change(https://arxiv.org/abs/2401.09646)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to natively multilingual models while being easier to scale to a large number of languages. Further, to address the intrinsic interdisciplinary aspect of climate change we consider different research perspectives. Therefore, the model can produce in-depth answers focusing on different perspectives in addition to an overall answer. We propose a suite of automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks, ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model while not degrading results on general domain benchmarks. Our human evaluation confirms the trends we saw in our benchmarks. All models were trained and evaluated using renewable energy and are released publicly.</li>
<li><strong>摘要：</strong>本文介绍了 ClimateGPT，这是一个综合了气候变化跨学科研究的特定领域大语言模型的模型系列。我们在 300B 代币的面向科学的数据集上从头开始训练了两个 7B 模型。对于第一个模型，在预训练期间包含 4.2B 领域特定标记，第二个模型在预训练后适应气候领域。此外，ClimateGPT-7B、13B 和 70B 是在 Llama~2 的 4.2B 令牌的特定领域数据集上持续进行预训练的。每个模型都根据与气候科学家密切合作创建的高质量、人工生成的特定领域数据集进行了指令微调。为了减少幻觉的数量，我们优化了检索增强模型并提出了分层检索策略。为了提高我们的模型对非英语使用者的可访问性，我们建议使用级联机器翻译，并表明这种方法的性能可以与本地多语言模型相媲美，同时更容易扩展到大量语言。此外，为了解决气候变化内在的跨学科问题，我们考虑不同的研究视角。因此，该模型除了提供总体答案外，还可以产生针对不同视角的深入答案。我们提出了一套自动气候特定基准来评估法学硕士。在这些基准测试中，ClimateGPT-7B 的性能与大十倍的 Llama-2-70B 聊天模型相当，同时不会降低一般域基准测试的结果。我们的人工评估证实了我们在基准测试中看到的趋势。所有模型均使用可再生能源进行训练和评估，并公开发布。</li>
</ul>

<h3>Title: Convex and Bilevel Optimization for Neuro-Symbolic Inference and  Learning</h3>
<ul>
<li><strong>Authors: </strong>Charles Dickens, Changyu Gao, Connor Pryor, Stephen Wright, Lise Getoor</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09651">https://arxiv.org/abs/2401.09651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09651">https://arxiv.org/pdf/2401.09651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09651]] Convex and Bilevel Optimization for Neuro-Symbolic Inference and  Learning(https://arxiv.org/abs/2401.09651)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.</li>
<li><strong>摘要：</strong>我们通过利用凸优化和双层优化技术来开发用于端到端神经和符号参数学习的基于梯度的通用框架，解决了神经符号（NeSy）系统的关键挑战。我们的框架的适用性通过 NeuPSL（一种最先进的 NeSy 架构）得到了证明。为了实现这一目标，我们提出了 NeuPSL 推理的平滑原始和对偶公式，并表明学习梯度是最佳对偶变量的函数。此外，我们为自然利用热启动的新公式开发了双块坐标下降算法。与当前最好的 NeuPSL 推理方法相比，这使得学习运行时间提高了 100 倍以上。最后，我们对涵盖一系列任务的 8 美元数据集进行了广泛的实证评估，并证明我们的学习框架比其他学习方法实现了高达 16% 的预测性能改进。</li>
</ul>

<h3>Title: Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A  Multi-Leader Multi-Follower Stackelberg Game Approach</h3>
<ul>
<li><strong>Authors: </strong>Jiawen Kang, Yue Zhong, Minrui Xu, Jiangtian Nie, Jinbo Wen, Hongyang Du, Dongdong Ye, Xumin Huang, Dusit Niyato, Shengli Xie</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09680">https://arxiv.org/abs/2401.09680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09680">https://arxiv.org/pdf/2401.09680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09680]] Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A  Multi-Leader Multi-Follower Stackelberg Game Approach(https://arxiv.org/abs/2401.09680)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag, agent</a></li>
<li><strong>Abstract: </strong>The synergy between Unmanned Aerial Vehicles (UAVs) and metaverses is giving rise to an emerging paradigm named UAV metaverses, which create a unified ecosystem that blends physical and virtual spaces, transforming drone interaction and virtual exploration. UAV Twins (UTs), as the digital twins of UAVs that revolutionize UAV applications by making them more immersive, realistic, and informative, are deployed and updated on ground base stations, e.g., RoadSide Units (RSUs), to offer metaverse services for UAV Metaverse Users (UMUs). Due to the dynamic mobility of UAVs and limited communication coverages of RSUs, it is essential to perform real-time UT migration to ensure seamless immersive experiences for UMUs. However, selecting appropriate RSUs and optimizing the required bandwidth is challenging for achieving reliable and efficient UT migration. To address the challenges, we propose a tiny machine learning-based Stackelberg game framework based on pruning techniques for efficient UT migration in UAV metaverses. Specifically, we formulate a multi-leader multi-follower Stackelberg model considering a new immersion metric of UMUs in the utilities of UAVs. Then, we design a Tiny Multi-Agent Deep Reinforcement Learning (Tiny MADRL) algorithm to obtain the tiny networks representing the optimal game solution. Specifically, the actor-critic network leverages the pruning techniques to reduce the number of network parameters and achieve model size and computation reduction, allowing for efficient implementation of Tiny MADRL. Numerical results demonstrate that our proposed schemes have better performance than traditional schemes.</li>
<li><strong>摘要：</strong>无人机 (UAV) 和元宇宙之间的协同作用正在催生一种名为无人机元宇宙的新兴范例，它创建了一个融合物理和虚拟空间的统一生态系统，改变了无人机交互和虚拟探索。无人机双胞胎（UT）作为无人机的数字双胞胎，通过使无人机更加身临其境、更加真实、信息丰富，彻底改变了无人机应用，在路边单元（RSU）等地面基站上进行部署和更新，为无人机提供元宇宙服务Metaverse 用户 (UMU)。由于无人机的动态移动性和RSU的有限通信覆盖范围，必须执行实时UT迁移以确保UMU的无缝沉浸式体验。然而，选择合适的 RSU 并优化所需带宽对于实现可靠且高效的 UT 迁移具有挑战性。为了应对这些挑战，我们提出了一种基于机器学习的微型 Stackelberg 游戏框架，该框架基于修剪技术，可在无人机元宇宙中实现高效的 UT 迁移。具体来说，我们制定了一个多领导者多追随者 Stackelberg 模型，考虑了 UMU 在无人机实用程序中的新沉浸度指标。然后，我们设计了一种微型多智能体深度强化学习（Tiny MADRL）算法来获得代表最佳游戏解决方案的微型网络。具体来说，actor-critic 网络利用剪枝技术来减少网络参数的数量，并实现模型大小和计算量的减少，从而实现 Tiny MADRL 的高效实现。数值结果表明我们提出的方案比传统方案具有更好的性能。</li>
</ul>

<h3>Title: Harnessing Density Ratios for Online Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Philip Amortila, Dylan J. Foster, Nan Jiang, Ayush Sekhari, Tengyang Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09681">https://arxiv.org/abs/2401.09681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09681">https://arxiv.org/pdf/2401.09681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09681]] Harnessing Density Ratios for Online Reinforcement Learning(https://arxiv.org/abs/2401.09681)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of density ratio modeling, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start. In this work we show -- perhaps surprisingly -- that density ratio-based algorithms have online counterparts. Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-efficient online exploration. GLOW addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration. GLOW is computationally inefficient; we complement it with a more efficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2022) wherein online RL is augmented with additional offline data. HyGLOW is derived as a special case of a more general meta-algorithm that provides a provable black-box reduction from hybrid RL to offline RL, which may be of independent interest.</li>
<li><strong>摘要：</strong>离线和在线强化学习的理论尽管是并行发展的，但已经开始显示出统一可能性的迹象，一种设置的算法和分析技术通常在另一种设置中具有自然的对应物。然而，密度比建模的概念（离线强化学习中的一种新兴范式）在在线强化学习中基本上不存在，这或许是有充分理由的：密度比的存在和有界性依赖于对具有良好覆盖范围的探索性数据集的访问，但在线强化学习的核心挑战是在没有数据集的情况下收集这样的数据集。在这项工作中，我们展示了——也许令人惊讶的是——基于密度比的算法有在线对应物。假设仅存在具有良好覆盖率的探索性分布，这种结构条件称为可覆盖性（Xie et al., 2023），我们给出了一种新算法（GLOW），该算法使用密度比可实现性和值函数可实现性来执行样本高效在线勘探。 GLOW 通过谨慎使用截断来解决无限密度比，并将其与乐观主义相结合来指导探索。 GLOW 计算效率低下；我们用更高效的对应物 HyGLOW 来补充它，用于混合 RL 设置（Song 等人，2022），其中在线 RL 通过额外的离线数据进行增强。 HyGLOW 是一种更通用的元算法的特例，它提供了从混合 RL 到离线 RL 的可证明的黑盒简化，这可能具有独立的意义。</li>
</ul>

<h3>Title: Comparative Study on the Performance of Categorical Variable Encoders in  Classification and Regression Tasks</h3>
<ul>
<li><strong>Authors: </strong>Wenbin Zhu, Runwen Qiu, Ying Fu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09682">https://arxiv.org/abs/2401.09682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09682">https://arxiv.org/pdf/2401.09682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09682]] Comparative Study on the Performance of Categorical Variable Encoders in  Classification and Regression Tasks(https://arxiv.org/abs/2401.09682)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Categorical variables often appear in datasets for classification and regression tasks, and they need to be encoded into numerical values before training. Since many encoders have been developed and can significantly impact performance, choosing the appropriate encoder for a task becomes a time-consuming yet important practical issue. This study broadly classifies machine learning models into three categories: 1) ATI models that implicitly perform affine transformations on inputs, such as multi-layer perceptron neural network; 2) Tree-based models that are based on decision trees, such as random forest; and 3) the rest, such as kNN. Theoretically, we prove that the one-hot encoder is the best choice for ATI models in the sense that it can mimic any other encoders by learning suitable weights from the data. We also explain why the target encoder and its variants are the most suitable encoders for tree-based models. This study conducted comprehensive computational experiments to evaluate 14 encoders, including one-hot and target encoders, along with eight common machine-learning models on 28 datasets. The computational results agree with our theoretical analysis. The findings in this study shed light on how to select the suitable encoder for data scientists in fields such as fraud detection, disease diagnosis, etc.</li>
<li><strong>摘要：</strong>分类变量经常出现在分类和回归任务的数据集中，在训练之前需要将它们编码为数值。由于已经开发了许多编码器并且可以显着影响性能，因此为任务选择合适的编码器成为一个耗时但重要的实际问题。本研究将机器学习模型大致分为三类：1）对输入隐式执行仿射变换的ATI模型，例如多层感知器神经网络； 2）Tree-based模型，即基于决策树的模型，例如随机森林； 3）其余的，例如kNN。从理论上讲，我们证明 one-hot 编码器是 ATI 模型的最佳选择，因为它可以通过从数据中学习合适的权重来模仿任何其他编码器。我们还解释了为什么目标编码器及其变体是最适合基于树的模型的编码器。这项研究进行了全面的计算实验，评估了 14 种编码器，包括单热编码器和目标编码器，以及 28 个数据集上的 8 种常见机器学习模型。计算结果与我们的理论分析一致。这项研究的结果揭示了如何为欺诈检测、疾病诊断等领域的数据科学家选择合适的编码器。</li>
</ul>

<h3>Title: Offline Imitation Learning by Controlling the Effective Planning Horizon</h3>
<ul>
<li><strong>Authors: </strong>Hee-Jun Ahn, Seong-Woong Shim, Byung-Jun Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09728">https://arxiv.org/abs/2401.09728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09728">https://arxiv.org/pdf/2401.09728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09728]] Offline Imitation Learning by Controlling the Effective Planning Horizon(https://arxiv.org/abs/2401.09728)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In offline imitation learning (IL), we generally assume only a handful of expert trajectories and a supplementary offline dataset from suboptimal behaviors to learn the expert policy. While it is now common to minimize the divergence between state-action visitation distributions so that the agent also considers the future consequences of an action, a sampling error in an offline dataset may lead to erroneous estimates of state-action visitations in the offline case. In this paper, we investigate the effect of controlling the effective planning horizon (i.e., reducing the discount factor) as opposed to imposing an explicit regularizer, as previously studied. Unfortunately, it turns out that the existing algorithms suffer from magnified approximation errors when the effective planning horizon is shortened, which results in a significant degradation in performance. We analyze the main cause of the problem and provide the right remedies to correct the algorithm. We show that the corrected algorithm improves on popular imitation learning benchmarks by controlling the effective planning horizon rather than an explicit regularization.</li>
<li><strong>摘要：</strong>在离线模仿学习（IL）中，我们通常假设只有少数专家轨迹和来自次优行为的补充离线数据集来学习专家策略。虽然现在通常会最小化状态-动作访问分布之间的差异，以便代理也考虑动作的未来后果，但离线数据集中的采样错误可能会导致离线情况下状态-动作访问的错误估计。在本文中，我们研究了控制有效规划范围（即减少折扣因子）的效果，而不是像之前研究的那样强加显式正则化器。不幸的是，事实证明，当有效规划范围缩短时，现有算法会遭受放大的近似误差，从而导致性能显着下降。我们分析问题的主要原因并提供正确的补救措施来纠正算法。我们表明，修正后的算法通过控制有效规划范围而不是显式正则化来改进流行的模仿学习基准。</li>
</ul>

<h3>Title: Exploration and Anti-Exploration with Distributional Random Network  Distillation</h3>
<ul>
<li><strong>Authors: </strong>Kai Yang, Jian Tao, Jiafei Lyu, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09750">https://arxiv.org/abs/2401.09750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09750">https://arxiv.org/pdf/2401.09750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09750]] Exploration and Anti-Exploration with Distributional Random Network  Distillation(https://arxiv.org/abs/2401.09750)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag, agent</a></li>
<li><strong>Abstract: </strong>Exploration remains a critical issue in deep reinforcement learning for an agent to attain high returns in unknown environments. Although the prevailing exploration Random Network Distillation (RND) algorithm has been demonstrated to be effective in numerous environments, it often needs more discriminative power in bonus allocation. This paper highlights the ``bonus inconsistency'' issue within RND, pinpointing its primary limitation. To address this issue, we introduce the Distributional RND (DRND), a derivative of the RND. DRND enhances the exploration process by distilling a distribution of random networks and implicitly incorporating pseudo counts to improve the precision of bonus allocation. This refinement encourages agents to engage in more extensive exploration. Our method effectively mitigates the inconsistency issue without introducing significant computational overhead. Both theoretical analysis and experimental results demonstrate the superiority of our approach over the original RND algorithm. Our method excels in challenging online exploration scenarios and effectively serves as an anti-exploration mechanism in D4RL offline tasks.</li>
<li><strong>摘要：</strong>探索仍然是深度强化学习中智能体在未知环境中获得高回报的关键问题。尽管流行的探索随机网络蒸馏（RND）算法已被证明在许多环境中都是有效的，但它在奖金分配方面通常需要更多的判别力。本文强调了 RND 中的“奖金不一致”问题，指出了其主要局限性。为了解决这个问题，我们引入了分布式 RND（DRND），它是 RND 的衍生物。 DRND 通过提取随机网络的分布并隐式合并伪计数来增强探索过程，以提高奖金分配的精度。这种改进鼓励代理进行更广泛的探索。我们的方法有效地缓解了不一致问题，而不会引入大量的计算开销。理论分析和实验结果都证明了我们的方法相对于原始 RND 算法的优越性。我们的方法擅长挑战在线探索场景，并有效地充当 D4RL 离线任务中的反探索机制。</li>
</ul>

<h3>Title: Applications of Machine Learning to Optimizing Polyolefin Manufacturing</h3>
<ul>
<li><strong>Authors: </strong>Niket Sharma, Y.A. Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09753">https://arxiv.org/abs/2401.09753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09753">https://arxiv.org/pdf/2401.09753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09753]] Applications of Machine Learning to Optimizing Polyolefin Manufacturing(https://arxiv.org/abs/2401.09753)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This chapter is a preprint from our book by , focusing on leveraging machine learning (ML) in chemical and polyolefin manufacturing optimization. It's crafted for both novices and seasoned professionals keen on the latest ML applications in chemical processes. We trace the evolution of AI and ML in chemical industries, delineate core ML components, and provide resources for ML beginners. A detailed discussion on various ML methods is presented, covering regression, classification, and unsupervised learning techniques, with performance metrics and examples. Ensemble methods, deep learning networks, including MLP, DNNs, RNNs, CNNs, and transformers, are explored for their growing role in chemical applications. Practical workshops guide readers through predictive modeling using advanced ML algorithms. The chapter culminates with insights into science-guided ML, advocating for a hybrid approach that enhances model accuracy. The extensive bibliography offers resources for further research and practical implementation. This chapter aims to be a thorough primer on ML's practical application in chemical engineering, particularly for polyolefin production, and sets the stage for continued learning in subsequent chapters. Please cite the original work [169,170] when referencing.</li>
<li><strong>摘要：</strong>本章是我们书籍的预印本，重点关注在化学和聚烯烃制造优化中利用机器学习 (ML)。它专为热衷于化学过程中最新机器学习应用的新手和经验丰富的专业人士而设计。我们追踪人工智能和机器学习在化学工业中的演变，描述核心机器学习组件，并为机器学习初学者提供资源。详细讨论了各种机器学习方法，涵盖回归、分类和无监督学习技术，以及性能指标和示例。集成方法、深度学习网络（包括 MLP、DNN、RNN、CNN 和 Transformer）因其在化学应用中日益重要的作用而受到探索。实践研讨会指导读者使用先进的机器学习算法进行预测建模。本章最后深入探讨了科学引导的机器学习，提倡采用混合方法来提高模型的准确性。广泛的参考书目为进一步研究和实际实施提供了资源。本章旨在全面介绍机器学习在化学工程（特别是聚烯烃生产）中的实际应用，并为后续章节中的继续学习奠定基础。引用时请注明原文[169,170]。</li>
</ul>

<h3>Title: Resolving Regular Polysemy in Named Entities</h3>
<ul>
<li><strong>Authors: </strong>Shu-Kai Hsieh, Yu-Hsiang Tseng, Hsin-Yu Chou, Ching-Wen Yang, Yu-Yun Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09758">https://arxiv.org/abs/2401.09758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09758">https://arxiv.org/pdf/2401.09758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09758]] Resolving Regular Polysemy in Named Entities(https://arxiv.org/abs/2401.09758)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Word sense disambiguation primarily addresses the lexical ambiguity of common words based on a predefined sense inventory. Conversely, proper names are usually considered to denote an ad-hoc real-world referent. Once the reference is decided, the ambiguity is purportedly resolved. However, proper names also exhibit ambiguities through appellativization, i.e., they act like common words and may denote different aspects of their referents. We proposed to address the ambiguities of proper names through the light of regular polysemy, which we formalized as dot objects. This paper introduces a combined word sense disambiguation (WSD) model for disambiguating common words against Chinese Wordnet (CWN) and proper names as dot objects. The model leverages the flexibility of a gloss-based model architecture, which takes advantage of the glosses and example sentences of CWN. We show that the model achieves competitive results on both common and proper nouns, even on a relatively sparse sense dataset. Aside from being a performant WSD tool, the model further facilitates the future development of the lexical resource.</li>
<li><strong>摘要：</strong>词义消歧主要基于预定义的词义清单来解决常用词的词汇歧义。相反，专有名称通常被认为表示特定的现实世界所指对象。一旦确定了参考，据称歧义就得到了解决。然而，专有名称也通过称谓化表现出歧义，即它们的作用类似于常用词，并且可能表示其所指对象的不同方面。我们建议通过常规多义词来解决专有名称的歧义，我们将其形式化为点对象。本文介绍了一种组合词义消歧（WSD）模型，用于针对中文 Wordnet（CWN）和作为点对象的专有名称来消除常见词的歧义。该模型利用了基于注释的模型架构的灵活性，该架构利用了 CWN 的注释和例句。我们表明，即使在相对稀疏的意义数据集上，该模型在常见名词和专有名词上都取得了有竞争力的结果。除了作为高性能的 WSD 工具之外，该模型还进一步促进了词汇资源的未来开发。</li>
</ul>

<h3>Title: A Comparative Study on Annotation Quality of Crowdsourcing and LLM via  Label Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Jiyi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09760">https://arxiv.org/abs/2401.09760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09760">https://arxiv.org/pdf/2401.09760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09760]] A Comparative Study on Annotation Quality of Crowdsourcing and LLM via  Label Aggregation(https://arxiv.org/abs/2401.09760)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Whether Large Language Models (LLMs) can outperform crowdsourcing on the data annotation task is attracting interest recently. Some works verified this issue with the average performance of individual crowd workers and LLM workers on some specific NLP tasks by collecting new datasets. However, on the one hand, existing datasets for the studies of annotation quality in crowdsourcing are not yet utilized in such evaluations, which potentially provide reliable evaluations from a different viewpoint. On the other hand, the quality of these aggregated labels is crucial because, when utilizing crowdsourcing, the estimated labels aggregated from multiple crowd labels to the same instances are the eventually collected labels. Therefore, in this paper, we first investigate which existing crowdsourcing datasets can be used for a comparative study and create a benchmark. We then compare the quality between individual crowd labels and LLM labels and make the evaluations on the aggregated labels. In addition, we propose a Crowd-LLM hybrid label aggregation method and verify the performance. We find that adding LLM labels from good LLMs to existing crowdsourcing datasets can enhance the quality of the aggregated labels of the datasets, which is also higher than the quality of LLM labels themselves.</li>
<li><strong>摘要：</strong>最近，大型语言模型（LLM）是否能够在数据注释任务上超越众包，引起了人们的兴趣。一些工作通过收集新的数据集，用个体众包工作者和法学硕士工作者在某些特定 NLP 任务上的平均表现来验证这个问题。然而，一方面，用于众包注释质量研究的现有数据集尚未用于此类评估，这可能从不同的角度提供可靠的评估。另一方面，这些聚合标签的质量至关重要，因为在利用众包时，从多个人群标签聚合到同一实例的估计标签是最终收集的标签。因此，在本文中，我们首先调查哪些现有的众包数据集可以用于比较研究并创建基准。然后，我们比较各个人群标签和 LLM 标签之间的质量，并对聚合标签进行评估。此外，我们提出了一种Crowd-LLM混合标签聚合方法并验证了性能。我们发现，将优秀LLM的LLM标签添加到现有众包数据集中可以提高数据集聚合标签的质量，这也高于LLM标签本身的质量。</li>
</ul>

<h3>Title: Leveraging Biases in Large Language Models: "bias-kNN'' for Effective  Few-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Yong Zhang, Hanzhang Li, Zhitao Li, Ning Cheng, Ming Li, Jing Xiao, Jianzong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09783">https://arxiv.org/abs/2401.09783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09783">https://arxiv.org/pdf/2401.09783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09783]] Leveraging Biases in Large Language Models: "bias-kNN'' for Effective  Few-Shot Learning(https://arxiv.org/abs/2401.09783)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown significant promise in various applications, including zero-shot and few-shot learning. However, their performance can be hampered by inherent biases. Instead of traditionally sought methods that aim to minimize or correct these biases, this study introduces a novel methodology named ``bias-kNN''. This approach capitalizes on the biased outputs, harnessing them as primary features for kNN and supplementing with gold labels. Our comprehensive evaluations, spanning diverse domain text classification datasets and different GPT-2 model sizes, indicate the adaptability and efficacy of the ``bias-kNN'' method. Remarkably, this approach not only outperforms conventional in-context learning in few-shot scenarios but also demonstrates robustness across a spectrum of samples, templates and verbalizers. This study, therefore, presents a unique perspective on harnessing biases, transforming them into assets for enhanced model performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种应用中显示出巨大的前景，包括零样本和少样本学习。然而，他们的表现可能会受到固有偏见的阻碍。本研究引入了一种名为“bias-kNN”的新颖方法，而不是传统上寻求的旨在最小化或纠正这些偏差的方法。这种方法利用了有偏差的输出，将它们作为 kNN 的主要特征并用黄金标签进行补充。我们的综合评估涵盖不同领域的文本分类数据集和不同的 GPT-2 模型大小，表明了“bias-kNN”方法的适应性和有效性。值得注意的是，这种方法不仅在少数场景中优于传统的上下文学习，而且在一系列样本、模板和语言器中表现出鲁棒性。因此，这项研究提出了利用偏差、将其转化为资产以增强模型性能的独特视角。</li>
</ul>

<h3>Title: A Semantic Approach for Big Data Exploration in Industry 4.0</h3>
<ul>
<li><strong>Authors: </strong>Idoia Berges, Víctor Julio Ramírez-Durán, Arantza Illarramendi</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09789">https://arxiv.org/abs/2401.09789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09789">https://arxiv.org/pdf/2401.09789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09789]] A Semantic Approach for Big Data Exploration in Industry 4.0(https://arxiv.org/abs/2401.09789)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>The growing trends in automation, Internet of Things, big data and cloud computing technologies have led to the fourth industrial revolution (Industry 4.0), where it is possible to visualize and identify patterns and insights, which results in a better understanding of the data and can improve the manufacturing process. However, many times, the task of data exploration results difficult for manufacturing experts because they might be interested in analyzing also data that does not appear in pre-designed visualizations and therefore they must be assisted by Information Technology experts. In this paper, we present a proposal materialized in a semantic-based visual query system developed for a real Industry 4.0 scenario that allows domain experts to explore and visualize data in a friendly way. The main novelty of the system is the combined use that it makes of captured data that are semantically annotated first, and a 2D customized digital representation of a machine that is also linked with semantic descriptions. Those descriptions are expressed using terms of an ontology, where, among others, the sensors that are used to capture indicators about the performance of a machine that belongs to a Industry 4.0 scenario have been modeled. Moreover, this semantic description allows to: formulate queries at a higher level of abstraction, provide customized graphical visualizations of the results based on the format and nature of the data, and download enriched data enabling further types of analysis.</li>
<li><strong>摘要：</strong>自动化、物联网、大数据和云计算技术的不断发展趋势引发了第四次工业革命（工业4.0），人们可以可视化和识别模式和见解，从而更好地理解数据和可以改进制造工艺。然而，很多时候，数据探索的任务对于制造专家来说很困难，因为他们可能也有兴趣分析预先设计的可视化中没有出现的数据，因此他们必须得到信息技术专家的协助。在本文中，我们提出了一项在为真实工业 4.0 场景开发的基于语义的可视化查询系统中具体化的提案，该系统允许领域专家以友好的方式探索和可视化数据。该系统的主要新颖之处在于它结合使用了首先进行语义注释的捕获数据以及也与语义描述相关联的机器的 2D 定制数字表示。这些描述是使用本体术语来表达的，其中，用于捕获属于工业 4.0 场景的机器性能指标的传感器已被建模。此外，这种语义描述允许：在更高的抽象级别制定查询，根据数据的格式和性质提供结果的定制图形可视化，以及下载丰富的数据以实现更多类型的分析。</li>
</ul>

<h3>Title: PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhijie Zhong, Zhiwen Yu, Yiyuan Yang, Weizheng Wang, Kaixiang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09793">https://arxiv.org/abs/2401.09793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09793">https://arxiv.org/pdf/2401.09793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09793]] PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection(https://arxiv.org/abs/2401.09793)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, code, rag</a></li>
<li><strong>Abstract: </strong>Anomaly detection stands as a crucial aspect of time series analysis, aiming to identify abnormal events in time series samples. The central challenge of this task lies in effectively learning the representations of normal and abnormal patterns in a label-lacking scenario. Previous research mostly relied on reconstruction-based approaches, restricting the representational abilities of the models. In addition, most of the current deep learning-based methods are not lightweight enough, which prompts us to design a more efficient framework for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale patch-based MLP-Mixer architecture that leverages contrastive learning for representational extraction and anomaly detection. Specifically, PatchAD is composed of four distinct MLP Mixers, exclusively utilizing the MLP architecture for high efficiency and lightweight architecture. Additionally, we also innovatively crafted a dual project constraint module to mitigate potential model degradation. Comprehensive experiments demonstrate that PatchAD achieves state-of-the-art results across multiple real-world multivariate time series datasets. Our code is publicly available.\footnote{\url{https://github.com/EmorZz1G/PatchAD}}</li>
<li><strong>摘要：</strong>异常检测是时间序列分析的一个重要方面，旨在识别时间序列样本中的异常事件。这项任务的核心挑战在于有效地学习在缺乏标签的情况下正常和异常模式的表示。以前的研究主要依赖于基于重建的方法，限制了模型的表征能力。此外，目前大多数基于深度学习的方法都不够轻量级，这促使我们设计更高效的异常检测框架。在本研究中，我们介绍了 PatchAD，这是一种新颖的基于多尺度补丁的 MLP-Mixer 架构，它利用对比学习进行表征提取和异常检测。具体来说，PatchAD 由四个不同的 MLP Mixer 组成，专门利用 MLP 架构来实现高效率和轻量级架构。此外，我们还创新地设计了双项目约束模块，以减轻潜在的模型退化。综合实验表明，PatchAD 在多个现实世界的多元时间序列数据集上取得了最先进的结果。我们的代码是公开的。\footnote{\url{https://github.com/EmorZz1G/PatchAD}}</li>
</ul>

<h3>Title: A Fast, Performant, Secure Distributed Training Framework For Large  Language Model</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Yinggui Wang, Anda Cheng, Aihui Zhou, Chaofan Yu, Lei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09796">https://arxiv.org/abs/2401.09796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09796">https://arxiv.org/pdf/2401.09796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09796]] A Fast, Performant, Secure Distributed Training Framework For Large  Language Model(https://arxiv.org/abs/2401.09796)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data. However, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. In this paper, we propose a secure distributed LLM based on model slicing. In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE. Then, secure communication is executed in the TEE and general environments through lightweight encryption. In order to further reduce the equipment cost as well as increase the model performance and accuracy, we propose a split fine-tuning scheme. In particular, we split the LLM by layers and place the latter layers in a server-side TEE (the client does not need a TEE). We then combine the proposed Sparsification Parameter Fine-tuning (SPF) with the LoRA part to improve the accuracy of the downstream task. Numerous experiments have shown that our method guarantees accuracy while maintaining security.</li>
<li><strong>摘要：</strong>分布式（联合）LLM 是使用孤立数据共同训练特定领域 LLM 的重要方法。然而，从服务器或客户端恶意窃取模型参数和数据已成为亟待解决的问题。在本文中，我们提出了一种基于模型切片的安全分布式法学硕士。在这种情况下，我们在客户端和服务器端都部署可信执行环境（TEE），并将微调的结构（LoRA或嵌入P-tuning v2）放入TEE中。然后，通过轻量级加密在TEE和通用环境中执行安全通信。为了进一步降低设备成本并提高模型性能和精度，我们提出了分体微调方案。特别是，我们将LLM按层拆分，并将后面的层放置在服务器端TEE中（客户端不需要TEE）。然后，我们将提出的稀疏化参数微调（SPF）与 LoRA 部分相结合，以提高下游任务的准确性。大量实验表明，我们的方法在保证安全性的同时保证了准确性。</li>
</ul>

<h3>Title: All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks</h3>
<ul>
<li><strong>Authors: </strong>Kazuhiro Takemoto</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09798">https://arxiv.org/abs/2401.09798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09798">https://arxiv.org/pdf/2401.09798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09798]] All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks(https://arxiv.org/abs/2401.09798)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, rag</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose a more serious security threat.</li>
<li><strong>摘要：</strong>像 ChatGPT 这样的大型语言模型 (LLM) 面临着“越狱”挑战，其中的安全措施被绕过，产生道德上有害的提示。本研究引入了一种简单的黑盒方法来有效生成越狱提示，克服了现有方法的高复杂性和计算成本的限制。所提出的技术基于 LLM 可以直接对绕过防护表达式进行采样的假设，使用目标 LLM 本身迭代地将有害提示重写为无害表达式。通过ChatGPT（GPT-3.5和GPT-4）和Gemini-Pro的实验证明，该方法在平均5次迭代内实现了超过80%的攻击成功率，并且尽管模型更新仍然有效。生成的越狱提示措辞自然且简洁，表明它们不易被发现。结果表明，创建有效的越狱提示比之前想象的要简单，而且黑盒越狱攻击会带来更严重的安全威胁。</li>
</ul>

<h3>Title: MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation  Extraction for Material Science Knowledge-base Construction</h3>
<ul>
<li><strong>Authors: </strong>Ankan Mullick, Akash Ghosh, G Sai Chaitanya, Samir Ghui, Tapas Nayak, Seung-Cheol Lee, Satadeep Bhattacharjee, Pawan Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09839">https://arxiv.org/abs/2401.09839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09839">https://arxiv.org/pdf/2401.09839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09839]] MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation  Extraction for Material Science Knowledge-base Construction(https://arxiv.org/abs/2401.09839)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Material science literature is a rich source of factual information about various categories of entities (like materials and compositions) and various relations between these entities, such as conductivity, voltage, etc. Automatically extracting this information to generate a material science knowledge base is a challenging task. In this paper, we propose MatSciRE (Material Science Relation Extractor), a Pointer Network-based encoder-decoder framework, to jointly extract entities and relations from material science articles as a triplet ($entity1, relation, entity2$). Specifically, we target the battery materials and identify five relations to work on - conductivity, coulombic efficiency, capacity, voltage, and energy. Our proposed approach achieved a much better F1-score (0.771) than a previous attempt using ChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown in Fig 1. The material information is extracted from material science literature in the form of entity-relation triplets using MatSciRE.</li>
<li><strong>摘要：</strong>材料科学文献是关于各种类别的实体（例如材料和成分）以及这些实体之间的各种关系（例如电导率、电压等）的事实信息的丰富来源。自动提取这些信息以生成材料科学知识库是一项具有挑战性的工作任务。在本文中，我们提出了MatSciRE（材料科学关系提取器），这是一种基于指针网络的编码器-解码器框架，用于从材料科学文章中联合提取实体和关系作为三元组（$entity1，relation，entity2$）。具体来说，我们针对电池材料并确定了五个需要研究的关系——电导率、库仑效率、容量、电压和能量。我们提出的方法比之前使用 ChemDataExtractor (0.716) 的尝试获得了更好的 F1 分数 (0.771)。 MatSciRE 的整体图形框架如图 1 所示。使用 MatSciRE 以实体关系三元组的形式从材料科学文献中提取材料信息。</li>
</ul>

<h3>Title: Behavioral Simulation: Exploring A Possible Next Paradigm for Science</h3>
<ul>
<li><strong>Authors: </strong>Cheng Wang, Chuwen Wang, Yu Zhao, Shirong Zeng, Wang Zhang, Ronghui Ning</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09851">https://arxiv.org/abs/2401.09851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09851">https://arxiv.org/pdf/2401.09851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09851]] Behavioral Simulation: Exploring A Possible Next Paradigm for Science(https://arxiv.org/abs/2401.09851)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Simulation technologies have been widely utilized in many scientific research fields such as weather forecasting, fluid mechanics and biological populations. It is the best tool to handle problems in complex systems, where closed-form expressions are unavailable and the target distribution in the representation space is too complex to be fully represented by a deep learning (DL) model. We believe that the development of simulation technologies is consistent with scientific paradigms. This paper induces the evolution of scientific paradigms from the perspective of data, algorithms, and computational power. Building upon this perspective, we divide simulation technologies into three stages aligning with the emergence of new paradigms, and find that advanced simulation technologies are typical instances of paradigms integration. Moreover, we propose the concept of behavioral simulation (BS), specifically sophisticated behavioral simulation (SBS), representing a higher degree of paradigms integration based on foundation models to simulate complex social systems involving sophisticated human strategies and behaviors. BS and further SBS are designed to tackle challenges concerning the complex human system that surpasses the capacity of traditional agent-based modeling simulation (ABMS), which can be regarded as a possible next paradigm for science. Through this work, we look forward to more powerful BS and SBS applications in scientific research branches within social science.</li>
<li><strong>摘要：</strong>仿真技术已广泛应用于天气预报、流体力学、生物种群等众多科学研究领域。它是处理复杂系统中问题的最佳工具，在复杂系统中，封闭式表达式不可用，并且表示空间中的目标分布过于复杂，无法通过深度学习 (DL) 模型完全表示。我们相信模拟技术的发展与科学范式是一致的。本文从数据、算法和计算能力的角度归纳了科学范式的演变。在此基础上，我们根据新范式的出现将仿真技术分为三个阶段，并发现先进的仿真技术是范式集成的典型实例。此外，我们提出了行为模拟（BS）的概念，特别是复杂行为模拟（SBS），代表了基于基础模型的更高程度的范式集成，以模拟涉及复杂人类策略和行为的复杂社会系统。 BS 和进一步的 SBS 旨在解决复杂人类系统的挑战，这些挑战超出了传统的基于代理的建模模拟（ABMS）的能力，可以被视为下一个可能的科学范式。通过这项工作，我们期待 BS 和 SBS 在社会科学的科学研究分支中得到更强大的应用。</li>
</ul>

<h3>Title: GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme  Precipitation Nowcasting</h3>
<ul>
<li><strong>Authors: </strong>Eloy Reulen, Siamak Mehrkanoon</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09881">https://arxiv.org/abs/2401.09881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09881">https://arxiv.org/pdf/2401.09881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09881]] GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme  Precipitation Nowcasting(https://arxiv.org/abs/2401.09881)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In recent years, data-driven modeling approaches have gained considerable traction in various meteorological applications, particularly in the realm of weather forecasting. However, these approaches often encounter challenges when dealing with extreme weather conditions. In light of this, we propose GA-SmaAt-GNet, a novel generative adversarial architecture that makes use of two methodologies aimed at enhancing the performance of deep learning models for extreme precipitation nowcasting. Firstly, it uses a novel SmaAt-GNet built upon the successful SmaAt-UNet architecture as generator. This network incorporates precipitation masks (binarized precipitation maps) as an additional data source, leveraging valuable information for improved predictions. Additionally, GA-SmaAt-GNet utilizes an attention-augmented discriminator inspired by the well-established Pix2Pix architecture. Furthermore, we assess the performance of GA-SmaAt-GNet using real-life precipitation dataset from the Netherlands. Our experimental results reveal a notable improvement in both overall performance and for extreme precipitation events. Furthermore, we conduct uncertainty analysis on the proposed GA-SmaAt-GNet model as well as on the precipitation dataset, providing additional insights into the predictive capabilities of the model. Finally, we offer further insights into the predictions of our proposed model using Grad-CAM. This visual explanation technique generates activation heatmaps, illustrating areas of the input that are more activated for various parts of the network.</li>
<li><strong>摘要：</strong>近年来，数据驱动的建模方法在各种气象应用中获得了相当大的关注，特别是在天气预报领域。然而，这些方法在应对极端天气条件时常常遇到挑战。有鉴于此，我们提出了 GA-SmaAt-GNet，这是一种新颖的生成对抗架构，它利用两种方法，旨在增强极端降水临近预报的深度学习模型的性能。首先，它使用基于成功的 SmaAt-UNet 架构构建的新型 SmaAt-GNet 作为生成器。该网络将降水掩模（二值化降水图）作为附加数据源，利用有价值的信息来改进预测。此外，GA-SmaAt-GNet 采用了受完善的 Pix2Pix 架构启发的注意力增强判别器。此外，我们使用荷兰的真实降水数据集评估 GA-SmaAt-GNet 的性能。我们的实验结果表明，整体性能和极端降水事件的性能都有显着改善。此外，我们对所提出的 GA-SmaAt-GNet 模型以及降水数据集进行了不确定性分析，为模型的预测能力提供了更多见解。最后，我们使用 Grad-CAM 对我们提出的模型的预测提供了进一步的见解。这种视觉解释技术生成激活热图，说明网络各个部分更活跃的输入区域。</li>
</ul>

<h3>Title: Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep  Reinforcement Learning in Next-Generation Network</h3>
<ul>
<li><strong>Authors: </strong>Qiong Wu, Wenhua Wang, Pingyi Fan, Qiang Fan, Huiling Zhu, Khaled B. Letaief</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09886">https://arxiv.org/abs/2401.09886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09886">https://arxiv.org/pdf/2401.09886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09886]] Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep  Reinforcement Learning in Next-Generation Network(https://arxiv.org/abs/2401.09886)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, agent</a></li>
<li><strong>Abstract: </strong>Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs. It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information. Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality. Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately. In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents. Hence, it is critical to determine where the popular contents are cached cooperatively. To address these issues, we propose a cooperative edge caching scheme based on elastic federated and multi-agent deep reinforcement learning (CEFMR) to optimize the cost in the network. We first propose an elastic FL algorithm to train the personalized model for each UE, where adversarial autoencoder (AAE) model is adopted for training to improve the prediction accuracy, then {a popular} content prediction algorithm is proposed to predict the popular contents for each SBS based on the trained AAE model. Finally, we propose a multi-agent deep reinforcement learning (MADRL) based algorithm to decide where the predicted popular contents are collaboratively cached among SBSs. Our experimental results demonstrate the superiority of our proposed scheme to existing baseline caching schemes.</li>
<li><strong>摘要：</strong>边缘缓存是下一代网络的一种有前景的解决方案，它通过增强小型基站 (SBS) 中的缓存单元的能力，允许用户设备 (UE) 获取已在 SBS 中预先缓存的用户请求的内容。在保护用户个人信息的同时，通过学习预测准确的热门内容对于SBS来说至关重要。传统的联邦学习（FL）可以保护用户的隐私，但UE之间的数据差异可能导致模型质量下降。因此，有必要为每个UE训练个性化的本地模型，以准确预测热门内容。另外，缓存的内容可以在下一代网络中的相邻SBS之间共享，因此将预测的热门内容缓存在不同的SBS中可能会影响获取内容的成本。因此，协同确定热门内容的缓存位置至关重要。为了解决这些问题，我们提出了一种基于弹性联邦和多智能体深度强化学习（CEFMR）的协作边缘缓存方案，以优化网络成本。我们首先提出一种弹性FL算法来训练每个UE的个性化模型，其中采用对抗性自动编码器（AAE）模型进行训练以提高预测精度，然后提出{一种流行}内容预测算法来预测每个UE的流行内容基于经过训练的 AAE 模型的 SBS。最后，我们提出了一种基于多智能体深度强化学习（MADRL）的算法来决定预测的流行内容在 SBS 之间协作缓存的位置。我们的实验结果证明了我们提出的方案相对于现有基线缓存方案的优越性。</li>
</ul>

<h3>Title: Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes  Through Multimodal Explanations</h3>
<ul>
<li><strong>Authors: </strong>Prince Jha, Krishanu Maity, Raghav Jain, Apoorv Verma, Sriparna Saha, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09899">https://arxiv.org/abs/2401.09899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09899">https://arxiv.org/pdf/2401.09899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09899]] Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes  Through Multimodal Explanations(https://arxiv.org/abs/2401.09899)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Internet memes have gained significant influence in communicating political, psychological, and sociocultural ideas. While memes are often humorous, there has been a rise in the use of memes for trolling and cyberbullying. Although a wide variety of effective deep learning-based models have been developed for detecting offensive multimodal memes, only a few works have been done on explainability aspect. Recent laws like "right to explanations" of General Data Protection Regulation, have spurred research in developing interpretable models rather than only focusing on performance. Motivated by this, we introduce {\em MultiBully-Ex}, the first benchmark dataset for multimodal explanation from code-mixed cyberbullying memes. Here, both visual and textual modalities are highlighted to explain why a given meme is cyberbullying. A Contrastive Language-Image Pretraining (CLIP) projection-based multimodal shared-private multitask approach has been proposed for visual and textual explanation of a meme. Experimental results demonstrate that training with multimodal explanations improves performance in generating textual justifications and more accurately identifying the visual evidence supporting a decision with reliable performance improvements.</li>
<li><strong>摘要：</strong>互联网迷因在传播政治、心理和社会文化思想方面产生了重大影响。虽然模因通常很幽默，但使用模因进行恶搞和网络欺凌的情况有所增加。尽管已经开发了多种有效的基于深度学习的模型来检测攻击性多模态模因，但在可解释性方面只做了很少的工作。最近的法律，例如《通用数据保护条例》的“解释权”，刺激了开发可解释模型的研究，而不仅仅是关注性能。受此启发，我们引入了 {\em MultiBully-Ex}，这是第一个用于代码混合网络欺凌模因多模态解释的基准数据集。在这里，突出显示视觉和文本形式来解释为什么给定的模因是网络欺凌。已经提出了一种基于对比语言图像预训练（CLIP）投影的多模式共享私有多任务方法，用于模因的视觉和文本解释。实验结果表明，使用多模式解释进行训练可以提高生成文本理由的性能，并更准确地识别支持决策的视觉证据，并具有可靠的性能改进。</li>
</ul>

<h3>Title: Enabling On-device Continual Learning with Binary Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Vorabbi, Davide Maltoni, Guido Borghi, Stefano Santi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09916">https://arxiv.org/abs/2401.09916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09916">https://arxiv.org/pdf/2401.09916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09916]] Enabling On-device Continual Learning with Binary Neural Networks(https://arxiv.org/abs/2401.09916)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>On-device learning remains a formidable challenge, especially when dealing with resource-constrained devices that have limited computational capabilities. This challenge is primarily rooted in two key issues: first, the memory available on embedded devices is typically insufficient to accommodate the memory-intensive back-propagation algorithm, which often relies on floating-point precision. Second, the development of learning algorithms on models with extreme quantization levels, such as Binary Neural Networks (BNNs), is critical due to the drastic reduction in bit representation. In this study, we propose a solution that combines recent advancements in the field of Continual Learning (CL) and Binary Neural Networks to enable on-device training while maintaining competitive performance. Specifically, our approach leverages binary latent replay (LR) activations and a novel quantization scheme that significantly reduces the number of bits required for gradient computation. The experimental validation demonstrates a significant accuracy improvement in combination with a noticeable reduction in memory requirement, confirming the suitability of our approach in expanding the practical applications of deep learning in real-world scenarios.</li>
<li><strong>摘要：</strong>设备上学习仍然是一个艰巨的挑战，特别是在处理计算能力有限的资源受限设备时。这一挑战主要源于两个关键问题：首先，嵌入式设备上的可用内存通常不足以容纳内存密集型反向传播算法，该算法通常依赖于浮点精度。其次，由于比特表示的急剧减少，在具有极端量化水平的模型（例如二元神经网络（BNN））上开发学习算法至关重要。在这项研究中，我们提出了一种解决方案，该解决方案结合了持续学习（CL）和二元神经网络领域的最新进展，以实现设备上训练，同时保持竞争性能。具体来说，我们的方法利用二进制潜在重放（LR）激活和一种新颖的量化方案，可显着减少梯度计算所需的位数。实验验证表明，准确性显着提高，并且内存需求显着减少，证实了我们的方法在扩展深度学习在现实场景中的实际应用方面的适用性。</li>
</ul>

<h3>Title: Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance  Sparse Information Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Ruizhe Zhang, Xinke Jiang, Yuchen Fang, Jiayuan Luo, Yongxin Xu, Yichen Zhu, Xu Chu, Junfeng Zhao, Yasha Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09943">https://arxiv.org/abs/2401.09943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09943">https://arxiv.org/pdf/2401.09943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09943]] Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance  Sparse Information Aggregation(https://arxiv.org/abs/2401.09943)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have shown considerable effectiveness in a variety of graph learning tasks, particularly those based on the message-passing approach in recent years. However, their performance is often constrained by a limited receptive field, a challenge that becomes more acute in the presence of sparse graphs. In light of the power series, which possesses infinite expansion capabilities, we propose a novel \underline{G}raph \underline{P}ower \underline{F}ilter \underline{N}eural Network (GPFN) that enhances node classification by employing a power series graph filter to augment the receptive field. Concretely, our GPFN designs a new way to build a graph filter with an infinite receptive field based on the convergence power series, which can be analyzed in the spectral and spatial domains. Besides, we theoretically prove that our GPFN is a general framework that can integrate any power series and capture long-range dependencies. Finally, experimental results on three datasets demonstrate the superiority of our GPFN over state-of-the-art baselines.</li>
<li><strong>摘要：</strong>图神经网络（GNN）在各种图学习任务中表现出了相当大的有效性，特别是近年来基于消息传递方法的任务。然而，它们的性能通常受到有限的感受野的限制，在稀疏图存在的情况下，这一挑战变得更加严峻。鉴于幂级数具有无限扩展能力，我们提出了一种新颖的 \underline{G}raph \underline{P}ower \underline{F}ilter \underline{N}eural Network (GPFN)，它通过以下方式增强节点分类：采用幂级数图滤波器来增强感受野。具体来说，我们的GPFN设计了一种基于收敛幂级数构建具有无限感受野的图滤波器的新方法，可以在谱域和空间域中进行分析。此外，我们从理论上证明了我们的 GPFN 是一个通用框架，可以集成任何幂级数并捕获远程依赖关系。最后，三个数据集的实验结果证明了我们的 GPFN 相对于最先进的基线的优越性。</li>
</ul>

<h3>Title: HGAttack: Transferable Heterogeneous Graph Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>He Zhao, Zhiwei Zeng, Yongwei Wang, Deheng Ye, Chunyan Miao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09945">https://arxiv.org/abs/2401.09945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09945">https://arxiv.org/pdf/2401.09945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09945]] HGAttack: Transferable Heterogeneous Graph Adversarial Attack(https://arxiv.org/abs/2401.09945)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Heterogeneous Graph Neural Networks (HGNNs) are increasingly recognized for their performance in areas like the web and e-commerce, where resilience against adversarial attacks is crucial. However, existing adversarial attack methods, which are primarily designed for homogeneous graphs, fall short when applied to HGNNs due to their limited ability to address the structural and semantic complexity of HGNNs. This paper introduces HGAttack, the first dedicated gray box evasion attack method for heterogeneous graphs. We design a novel surrogate model to closely resemble the behaviors of the target HGNN and utilize gradient-based methods for perturbation generation. Specifically, the proposed surrogate model effectively leverages heterogeneous information by extracting meta-path induced subgraphs and applying GNNs to learn node embeddings with distinct semantics from each subgraph. This approach improves the transferability of generated attacks on the target HGNN and significantly reduces memory costs. For perturbation generation, we introduce a semantics-aware mechanism that leverages subgraph gradient information to autonomously identify vulnerable edges across a wide range of relations within a constrained perturbation budget. We validate HGAttack's efficacy with comprehensive experiments on three datasets, providing empirical analyses of its generated perturbations. Outperforming baseline methods, HGAttack demonstrated significant efficacy in diminishing the performance of target HGNN models, affirming the effectiveness of our approach in evaluating the robustness of HGNNs against adversarial attacks.</li>
<li><strong>摘要：</strong>异构图神经网络 (HGNN) 因其在网络和电子商务等领域的性能而日益受到认可，在这些领域，抵御对抗性攻击的能力至关重要。然而，现有的对抗性攻击方法主要针对同质图而设计，由于其解决 HGNN 结构和语义复杂性的能力有限，因此在应用于 HGNN 时存在不足。本文介绍了HGAttack，这是第一个专用于异构图的灰盒规避攻击方法。我们设计了一种新颖的代理模型，与目标 HGNN 的行为非常相似，并利用基于梯度的方法进行扰动生成。具体来说，所提出的代理模型通过提取元路径引起的子图并应用 GNN 来学习每个子图中具有不同语义的节点嵌入，从而有效地利用异构信息。这种方法提高了对目标 HGNN 生成的攻击的可转移性，并显着降低了内存成本。对于扰动生成，我们引入了一种语义感知机制，该机制利用子图梯度信息在受限扰动预算内自主识别各种关系中的脆弱边缘。我们通过对三个数据集进行综合实验来验证 HGAttack 的功效，并对其生成的扰动提供实证分析。 HGAttack 的性能优于基线方法，在降低目标 HGNN 模型的性能方面表现出显着的功效，证实了我们的方法在评估 HGNN 针对对抗性攻击的鲁棒性方面的有效性。</li>
</ul>

<h3>Title: SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning</h3>
<ul>
<li><strong>Authors: </strong>Ho Fung Tsoi, Vladimir Loncar, Sridhara Dasu, Philip Harris</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ex, physics.ins-det</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09949">https://arxiv.org/abs/2401.09949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09949">https://arxiv.org/pdf/2401.09949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09949]] SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning(https://arxiv.org/abs/2401.09949)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Contrary to the use of genetic programming, the neural network approach to symbolic regression can scale well with high input dimension and leverage gradient methods for faster equation searching. Common ways of constraining expression complexity have relied on multistage pruning methods with fine-tuning, but these often lead to significant performance loss. In this work, we propose SymbolNet, a neural network approach to symbolic regression in a novel framework that enables dynamic pruning of model weights, input features, and mathematical operators in a single training, where both training loss and expression complexity are optimized simultaneously. We introduce a sparsity regularization term per pruning type, which can adaptively adjust its own strength and lead to convergence to a target sparsity level. In contrast to most existing symbolic regression methods that cannot efficiently handle datasets with more than $O$(10) inputs, we demonstrate the effectiveness of our model on the LHC jet tagging task (16 inputs), MNIST (784 inputs), and SVHN (3072 inputs).</li>
<li><strong>摘要：</strong>与使用遗传编程相反，符号回归的神经网络方法可以很好地扩展高输入维度，并利用梯度方法进行更快的方程搜索。限制表达复杂性的常见方法依赖于具有微调的多级修剪方法，但这些通常会导致显着的性能损失。在这项工作中，我们提出了 SymbolNet，这是一种在新颖框架中进行符号回归的神经网络方法，可以在单次训练中动态修剪模型权重、输入特征和数学运算符，同时优化训练损失和表达复杂性。我们为每种剪枝类型引入了一个稀疏正则化项，它可以自适应地调整其自身的强度并导致收敛到目标稀疏水平。与大多数现有的符号回归方法无法有效处理输入超过 $O$(10) 的数据集相比，我们证明了我们的模型在 LHC jet 标记任务（16 个输入）、MNIST（784 个输入）和 SVHN 上的有效性（3072 个输入）。</li>
</ul>

<h3>Title: Towards Generative Abstract Reasoning: Completing Raven's Progressive  Matrix via Rule Abstraction and Selection</h3>
<ul>
<li><strong>Authors: </strong>Fan Shi, Bin Li, Xiangyang Xue</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09966">https://arxiv.org/abs/2401.09966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09966">https://arxiv.org/pdf/2401.09966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09966]] Towards Generative Abstract Reasoning: Completing Raven's Progressive  Matrix via Rule Abstraction and Selection(https://arxiv.org/abs/2401.09966)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Endowing machines with abstract reasoning ability has been a long-term research topic in artificial intelligence. Raven's Progressive Matrix (RPM) is widely used to probe abstract visual reasoning in machine intelligence, where models need to understand the underlying rules and select the missing bottom-right images out of candidate sets to complete image matrices. The participators can display powerful reasoning ability by inferring the underlying attribute-changing rules and imagining the missing images at arbitrary positions. However, existing solvers can hardly manifest such an ability in realistic RPM problems. In this paper, we propose a conditional generative model to solve answer generation problems through Rule AbstractIon and SElection (RAISE) in the latent space. RAISE encodes image attributes as latent concepts and decomposes underlying rules into atomic rules by means of concepts, which are abstracted as global learnable parameters. When generating the answer, RAISE selects proper atomic rules out of the global knowledge set for each concept and composes them into the integrated rule of an RPM. In most configurations, RAISE outperforms the compared generative solvers in tasks of generating bottom-right and arbitrary-position answers. We test RAISE in the odd-one-out task and two held-out configurations to demonstrate how learning decoupled latent concepts and atomic rules helps find the image breaking the underlying rules and handle RPMs with unseen combinations of rules and attributes.</li>
<li><strong>摘要：</strong>赋予机器抽象推理能力一直是人工智能领域长期的研究课题。 Raven 的渐进矩阵 (RPM) 广泛用于探索机器智能中的抽象视觉推理，其中模型需要理解底层规则并从候选集中选择缺失的右下图像来完成图像矩阵。参与者可以通过推断潜在的属性变化规则和想象任意位置缺失的图像来展示强大的推理能力。然而，现有的求解器很难在现实的 RPM 问题中体现出这种能力。在本文中，我们提出了一种条件生成模型，通过潜在空间中的规则抽象离子和选择（RAISE）来解决答案生成问题。 RAISE将图像属性编码为潜在概念，并通过概念将底层规则分解为原子规则，这些原子规则被抽象为全局可学习参数。在生成答案时，RAISE从全局知识集中为每个概念选择合适的原子规则，并将它们组合成RPM的集成规则。在大多数配置中，RAISE 在生成右下角和任意位置答案的任务中优于比较的生成求解器。我们在奇数任务和两个保留配置中测试 RAISE，以演示学习解耦的潜在概念和原子规则如何帮助找到违反底层规则的图像，并处理具有看不见的规则和属性组合的 RPM。</li>
</ul>

<h3>Title: Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language  Models without Logit Access</h3>
<ul>
<li><strong>Authors: </strong>Saibo Geng, Berkay Döner, Chris Wendler, Martin Josifoski, Robert West</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09967">https://arxiv.org/abs/2401.09967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09967">https://arxiv.org/pdf/2401.09967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09967]] Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language  Models without Logit Access(https://arxiv.org/abs/2401.09967)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a novel approach to constrained decoding for blackbox LLMs, which operates without access to the logits of the blackbox LLM. SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a "sketch" for further elaboration. This approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. We demonstrate the efficacy of SGCD through experiments in closed information extraction and constituency parsing, showing how it enhances the utility and flexibility of blackbox LLMs for complex NLP tasks.</li>
<li><strong>摘要：</strong>约束解码是一种对语言模型输出施加约束的技术，提供了一种无需重新训练或架构修改即可控制文本生成的方法。然而，它的应用通常仅限于允许用户访问下一个令牌分布的模型（通常通过 softmax logits），这对黑盒大语言模型 (LLM) 造成了限制。本文介绍了草图引导约束解码 (SGCD)，这是一种针对黑盒 LLM 进行约束解码的新颖方法，其运行无需访问黑盒 LLM 的逻辑。 SGCD 利用本地托管的辅助模型来细化无约束黑盒 LLM 的输出，有效地将初始输出视为“草图”以供进一步阐述。这种方法是对传统的基于逻辑的技术的补充，并且可以在无法获得完全模型透明度的情况下应用约束解码。我们通过封闭信息提取和选区解析实验证明了 SGCD 的功效，展示了它如何增强黑盒 LLM 在复杂 NLP 任务中的实用性和灵活性。</li>
</ul>

<h3>Title: Better Explain Transformers by Illuminating Important Information</h3>
<ul>
<li><strong>Authors: </strong>Linxin Song, Yan Cui, Ao Luo, Freddy Lecue, Irene Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09972">https://arxiv.org/abs/2401.09972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09972">https://arxiv.org/pdf/2401.09972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09972]] Better Explain Transformers by Illuminating Important Information(https://arxiv.org/abs/2401.09972)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Transformer-based models excel in various natural language processing (NLP) tasks, attracting countless efforts to explain their inner workings. Prior methods explain Transformers by focusing on the raw gradient and attention as token attribution scores, where non-relevant information is often considered during explanation computation, resulting in confusing results. In this work, we propose highlighting the important information and eliminating irrelevant information by a refined information flow on top of the layer-wise relevance propagation (LRP) method. Specifically, we consider identifying syntactic and positional heads as important attention heads and focus on the relevance obtained from these important heads. Experimental results demonstrate that irrelevant information does distort output attribution scores and then should be masked during explanation computation. Compared to eight baselines on both classification and question-answering datasets, our method consistently outperforms with over 3\% to 33\% improvement on explanation metrics, providing superior explanation performance. Our anonymous code repository is available at: https://github.com/LinxinS97/Mask-LRP</li>
<li><strong>摘要：</strong>基于 Transformer 的模型在各种自然语言处理 (NLP) 任务中表现出色，吸引了无数的努力来解释其内部工作原理。先前的方法通过关注原始梯度和注意力作为标记归因分数来解释 Transformer，其中在解释计算过程中经常考虑不相关的信息，从而导致令人困惑的结果。在这项工作中，我们建议通过基于分层相关性传播（LRP）方法的细化信息流来突出重要信息并消除不相关信息。具体来说，我们考虑将句法和位置头识别为重要的注意力头，并关注从这些重要头获得的相关性。实验结果表明，不相关的信息确实会扭曲输出归因分数，因此应在解释计算过程中进行屏蔽。与分类和问答数据集的八个基线相比，我们的方法始终表现出色，解释指标提高了 3% 到 33% 以上，提供了卓越的解释性能。我们的匿名代码存储库位于：https://github.com/LinxinS97/Mask-LRP</li>
</ul>

<h3>Title: Gradable ChatGPT Translation Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Hui Jiao, Bei Peng, Lu Zong, Xiaojun Zhang, Xinwei Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09984">https://arxiv.org/abs/2401.09984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09984">https://arxiv.org/pdf/2401.09984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09984]] Gradable ChatGPT Translation Evaluation(https://arxiv.org/abs/2401.09984)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>ChatGPT, as a language model based on large-scale pre-training, has exerted a profound influence on the domain of machine translation. In ChatGPT, a "Prompt" refers to a segment of text or instruction employed to steer the model towards generating a specific category of response. The design of the translation prompt emerges as a key aspect that can wield influence over factors such as the style, precision and accuracy of the translation to a certain extent. However, there is a lack of a common standard and methodology on how to design and select a translation prompt. Accordingly, this paper proposes a generic taxonomy, which defines gradable translation prompts in terms of expression type, translation style, POS information and explicit statement, thus facilitating the construction of prompts endowed with distinct attributes tailored for various translation tasks. Specific experiments and cases are selected to validate and illustrate the effectiveness of the method.</li>
<li><strong>摘要：</strong>ChatGPT作为一种基于大规模预训练的语言模型，对机器翻译领域产生了深远的影响。在 ChatGPT 中，“提示”是指用于引导模型生成特定类别的响应的文本或指令片段。翻译提示的设计成为一个关键环节，在一定程度上影响着翻译的风格、精度、准确性等因素。然而，对于如何设计和选择翻译提示缺乏通用的标准和方法。因此，本文提出了一种通用分类法，从表达类型、翻译风格、词性信息和显性陈述等方面定义了可分级的翻译提示，从而有利于构建针对各种翻译任务而赋予不同属性的提示。选择具体的实验和案例来验证和说明该方法的有效性。</li>
</ul>

<h3>Title: Gender Bias in Machine Translation and The Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eva Vanmassenhove</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10016">https://arxiv.org/abs/2401.10016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10016">https://arxiv.org/pdf/2401.10016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10016]] Gender Bias in Machine Translation and The Era of Large Language Models(https://arxiv.org/abs/2401.10016)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>This chapter examines the role of Machine Translation in perpetuating gender bias, highlighting the challenges posed by cross-linguistic settings and statistical dependencies. A comprehensive overview of relevant existing work related to gender bias in both conventional Neural Machine Translation approaches and Generative Pretrained Transformer models employed as Machine Translation systems is provided. Through an experiment using ChatGPT (based on GPT-3.5) in an English-Italian translation context, we further assess ChatGPT's current capacity to address gender bias. The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.</li>
<li><strong>摘要：</strong>本章探讨了机器翻译在延续性别偏见方面的作用，强调了跨语言环境和统计依赖性带来的挑战。提供了对传统神经机器翻译方法和用作机器翻译系统的生成预训练 Transformer 模型中与性别偏见相关的现有相关工作的全面概述。通过在英语-意大利语翻译环境中使用 ChatGPT（基于 GPT-3.5）的实验，我们进一步评估了 ChatGPT 目前解决性别偏见的能力。研究结果强调了在减少机器翻译系统偏见方面持续取得进步的必要性，并强调了促进语言技术公平性和包容性的重要性。</li>
</ul>

<h3>Title: R-Judge: Benchmarking Safety Risk Awareness for LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, Gongshen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10019">https://arxiv.org/abs/2401.10019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10019">https://arxiv.org/pdf/2401.10019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10019]] R-Judge: Benchmarking Safety Risk Awareness for LLM Agents(https://arxiv.org/abs/2401.10019)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to the human score of 89.38%, showing considerable room for enhancing the risk awareness of LLMs. Notably, leveraging risk descriptions as environment feedback significantly improves model performance, revealing the importance of salient safety risk feedback. Furthermore, we design an effective chain of safety analysis technique to help the judgment of safety risks and conduct an in-depth case study to facilitate future research. R-Judge is publicly available at https://github.com/Lordog/R-Judge.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在跨实际应用程序自主完成任务方面表现出了巨大的潜力。尽管如此，这些 LLM 代理在交互式环境中操作时会带来意想不到的安全风险。这项工作并没有在大多数先前的研究中以法学硕士生成的内容安全为中心，而是解决了在不同环境中对法学硕士代理的行为安全性进行基准测试的迫切需要。我们引入了 R-Judge，这是一个旨在评估法学硕士根据代理交互记录判断安全风险的熟练程度的基准。 R-Judge包含162条座席交互记录，涵盖7个应用类别和10个风险类型中的27个关键风险场景。它结合了人类对安全的共识以及带注释的安全风险标签和高质量的风险描述。利用 R-Judge，我们对 8 位常被用作代理骨干的著名法学硕士进行了综合评估。表现最好的模型 GPT-4 达到了 72.29%，而人类得分为 89.38%，显示出法学硕士风险意识提升的巨大空间。值得注意的是，利用风险描述作为环境反馈显着提高了模型性能，揭示了显着安全风险反馈的重要性。此外，我们设计了有效的安全分析技术链来帮助判断安全风险，并进行深入的案例研究以方便未来的研究。 R-Judge 可在 https://github.com/Lordog/R-Judge 上公开获取。</li>
</ul>

<h3>Title: Self-Rewarding Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10020">https://arxiv.org/abs/2401.10020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10020">https://arxiv.org/pdf/2401.10020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10020]] Self-Rewarding Language Models(https://arxiv.org/abs/2401.10020)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes.</li>
<li><strong>摘要：</strong>我们认为，为了实现超人智能体，未来的模型需要超人反馈才能提供足够的训练信号。目前的方法通常根据人类偏好来训练奖励模型，这可能会受到人类表现水平的瓶颈，其次这些单独的冻结奖励模型无法在 LLM 训练期间学习改进。在这项工作中，我们研究自我奖励语言模型，其中语言模型本身通过法学硕士作为法官来使用，提示在训练期间提供自己的奖励。我们表明，在迭代 DPO 培训期间，不仅提高了指令遵循能力，而且还提高了为自身提供高质量奖励的能力。在我们的方法的三个迭代中对 Llama 2 70B 进行微调，产生的模型优于 AlpacaEval 2.0 排行榜上的许多现有系统，包括 Claude 2、Gemini Pro 和 GPT-4 0613。虽然只是初步研究，但这项工作打开了大门模型可以在两个轴上不断改进的可能性。</li>
</ul>

<h3>Title: Large Language Models for Scientific Information Extraction: An  Empirical Study for Virology</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Shamsabadi, Jennifer D'Souza, Sören Auer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10040">https://arxiv.org/abs/2401.10040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10040">https://arxiv.org/pdf/2401.10040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10040]] Large Language Models for Scientific Information Extraction: An  Empirical Study for Virology(https://arxiv.org/abs/2401.10040)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>In this paper, we champion the use of structured and semantic content representation of discourse-based scholarly communication, inspired by tools like Wikipedia infoboxes or structured Amazon product descriptions. These representations provide users with a concise overview, aiding scientists in navigating the dense academic landscape. Our novel automated approach leverages the robust text generation capabilities of LLMs to produce structured scholarly contribution summaries, offering both a practical solution and insights into LLMs' emergent abilities. For LLMs, the prime focus is on improving their general intelligence as conversational agents. We argue that these models can also be applied effectively in information extraction (IE), specifically in complex IE tasks within terse domains like Science. This paradigm shift replaces the traditional modular, pipelined machine learning approach with a simpler objective expressed through instructions. Our results show that finetuned FLAN-T5 with 1000x fewer parameters than the state-of-the-art GPT-davinci is competitive for the task.</li>
<li><strong>摘要：</strong>在本文中，受维基百科信息框或结构化亚马逊产品描述等工具的启发，我们倡导使用基于话语的学术交流的结构化和语义内容表示。这些表示为用户提供了简洁的概述，帮助科学家在密集的学术领域中导航。我们新颖的自动化方法利用法学硕士强大的文本生成功能来生成结构化的学术贡献摘要，提供实用的解决方案和对法学硕士新兴能力的见解。对于法学硕士来说，首要重点是提高他们作为对话代理的一般智力。我们认为这些模型也可以有效地应用于信息提取（IE），特别是科学等简洁领域内的复杂 IE 任务。这种范式转变用通过指令表达的更简单的目标取代了传统的模块化、流水线机器学习方法。我们的结果表明，经过微调的 FLAN-T5 的参数比最先进的 GPT-davinci 少 1000 倍，在这项任务中具有竞争力。</li>
</ul>

<h3>Title: Antonym vs Synonym Distinction using InterlaCed Encoder NETworks  (ICE-NET)</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Asif Ali, Yan Hu, Jianbin Qin, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10045">https://arxiv.org/abs/2401.10045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10045">https://arxiv.org/pdf/2401.10045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10045]] Antonym vs Synonym Distinction using InterlaCed Encoder NETworks  (ICE-NET)(https://arxiv.org/abs/2401.10045)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Antonyms vs synonyms distinction is a core challenge in lexico-semantic analysis and automated lexical resource construction. These pairs share a similar distributional context which makes it harder to distinguish them. Leading research in this regard attempts to capture the properties of the relation pairs, i.e., symmetry, transitivity, and trans-transitivity. However, the inability of existing research to appropriately model the relation-specific properties limits their end performance. In this paper, we propose InterlaCed Encoder NETworks (i.e., ICE-NET) for antonym vs synonym distinction, that aim to capture and model the relation-specific properties of the antonyms and synonyms pairs in order to perform the classification task in a performance-enhanced manner. Experimental evaluation using the benchmark datasets shows that ICE-NET outperforms the existing research by a relative score of upto 1.8% in F1-measure. We release the codes for ICE-NET at https://github.com/asif6827/ICENET.</li>
<li><strong>摘要：</strong>反义词与同义词的区别是词汇语义分析和自动化词汇资源构建的核心挑战。这些对共享相似的分布环境，这使得区分它们变得更加困难。这方面的领先研究试图捕捉关系对的属性，即对称性、传递性和跨传递性。然而，现有研究无法对关系特定的属性进行适当的建模，限制了它们的最终性能。在本文中，我们提出了用于区分反义词与同义词的 InterlaCed 编码器网络（即 ICE-NET），旨在捕获和建模反义词和同义词对的关系特定属性，以便以性能良好的方式执行分类任务。增强方式。使用基准数据集的实验评估表明，ICE-NET 在 F1 测量中的相对得分高达 1.8%，优于现有研究。我们在 https://github.com/asif6827/ICENET 发布了 ICE-NET 的代码。</li>
</ul>

<h3>Title: Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs</h3>
<ul>
<li><strong>Authors: </strong>Haritz Puerto, Martin Tutek, Somak Aditya, Xiaodan Zhu, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10065">https://arxiv.org/abs/2401.10065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10065">https://arxiv.org/pdf/2401.10065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10065]] Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs(https://arxiv.org/abs/2401.10065)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>Reasoning is a fundamental component for achieving language understanding. Among the multiple types of reasoning, conditional reasoning, the ability to draw different conclusions depending on some condition, has been understudied in large language models (LLMs). Recent prompting methods, such as chain of thought, have significantly improved LLMs on reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs. We hypothesize that code prompts can trigger conditional reasoning in LLMs trained on text and code. We propose a chain of prompts that transforms a natural language problem into code and prompts the LLM with the generated code. Our experiments find that code prompts exhibit a performance boost between 2.6 and 7.7 points on GPT 3.5 across multiple datasets requiring conditional reasoning. We then conduct experiments to discover how code prompts elicit conditional reasoning abilities and through which features. We observe that prompts need to contain natural language text accompanied by high-quality code that closely represents the semantics of the instance text. Furthermore, we show that code prompts are more efficient, requiring fewer demonstrations, and that they trigger superior state tracking of variables or key entities.</li>
<li><strong>摘要：</strong>推理是实现语言理解的基本组成部分。在多种类型的推理中，条件推理（根据某些条件得出不同结论的能力）已在大型语言模型（LLM）中得到充分研究。最近的提示方法，例如思维链，显着提高了法学硕士在推理任务上的水平。尽管如此，人们对法学硕士推理能力的触发因素仍然知之甚少。我们假设代码提示可以触发受过文本和代码训练的法学硕士中的条件推理。我们提出了一系列提示，将自然语言问题转换为代码，并用生成的代码提示法学硕士。我们的实验发现，在需要条件推理的多个数据集上，代码提示在 GPT 3.5 上表现出 2.6 到 7.7 点的性能提升。然后，我们进行实验来发现代码提示如何引发条件推理能力以及通过哪些功能。我们观察到提示需要包含自然语言文本以及紧密代表实例文本语义的高质量代码。此外，我们表明代码提示更有效，需要更少的演示，并且它们会触发变量或关键实体的高级状态跟踪。</li>
</ul>

<h3>Title: Communication-Efficient Personalized Federated Learning for  Speech-to-Text Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yichao Du, Zhirui Zhang, Linan Yue, Xu Huang, Yuqing Zhang, Tong Xu, Linli Xu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10070">https://arxiv.org/abs/2401.10070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10070">https://arxiv.org/pdf/2401.10070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10070]] Communication-Efficient Personalized Federated Learning for  Speech-to-Text Tasks(https://arxiv.org/abs/2401.10070)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>To protect privacy and meet legal regulations, federated learning (FL) has gained significant attention for training speech-to-text (S2T) systems, including automatic speech recognition (ASR) and speech translation (ST). However, the commonly used FL approach (i.e., \textsc{FedAvg}) in S2T tasks typically suffers from extensive communication overhead due to multi-round interactions based on the whole model and performance degradation caused by data heterogeneity among clients.To address these issues, we propose a personalized federated S2T framework that introduces \textsc{FedLoRA}, a lightweight LoRA module for client-side tuning and interaction with the server to minimize communication overhead, and \textsc{FedMem}, a global model equipped with a $k$-nearest-neighbor ($k$NN) classifier that captures client-specific distributional shifts to achieve personalization and overcome data heterogeneity. Extensive experiments based on Conformer and Whisper backbone models on CoVoST and GigaSpeech benchmarks show that our approach significantly reduces the communication overhead on all S2T tasks and effectively personalizes the global model to overcome data heterogeneity.</li>
<li><strong>摘要：</strong>为了保护隐私并满足法律法规的要求，联邦学习 (FL) 在训练语音到文本 (S2T) 系统（包括自动语音识别 (ASR) 和语音翻译 (ST)）方面受到了极大关注。然而，S2T 任务中常用的 FL 方法（即 \textsc{FedAvg}）通常会因基于整个模型的多轮交互而遭受大量的通信开销，以及客户端之间的数据异构性导致的性能下降。 ，我们提出了一个个性化的联合 S2T 框架，它引入了 \textsc{FedLoRA}，一个轻量级 LoRA 模块，用于客户端调整和与服务器交互，以最大限度地减少通信开销，以及 \textsc{FedMem}，一个配备 $k 的全局模型$-最近邻 ($k$NN) 分类器捕获特定于客户的分布变化以实现个性化并克服数据异构性。基于 CoVoST 和 GigaSpeech 基准上的 Conformer 和 Whisper 主干模型的大量实验表明，我们的方法显着降低了所有 S2T 任务的通信开销，并有效地个性化全局模型以克服数据异构性。</li>
</ul>

<h3>Title: Power in Numbers: Robust reading comprehension by finetuning with four  adversarial sentences per example</h3>
<ul>
<li><strong>Authors: </strong>Ariel Marcus</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10091">https://arxiv.org/abs/2401.10091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10091">https://arxiv.org/pdf/2401.10091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10091]] Power in Numbers: Robust reading comprehension by finetuning with four  adversarial sentences per example(https://arxiv.org/abs/2401.10091)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Recent models have achieved human level performance on the Stanford Question Answering Dataset when using F1 scores to evaluate the reading comprehension task. Yet, teaching machines to comprehend text has not been solved in the general case. By appending one adversarial sentence to the context paragraph, past research has shown that the F1 scores from reading comprehension models drop almost in half. In this paper, I replicate past adversarial research with a new model, ELECTRA-Small, and demonstrate that the new model's F1 score drops from 83.9% to 29.2%. To improve ELECTRA-Small's resistance to this attack, I finetune the model on SQuAD v1.1 training examples with one to five adversarial sentences appended to the context paragraph. Like past research, I find that the finetuned model on one adversarial sentence does not generalize well across evaluation datasets. However, when finetuned on four or five adversarial sentences the model attains an F1 score of more than 70% on most evaluation datasets with multiple appended and prepended adversarial sentences. The results suggest that with enough examples we can make models robust to adversarial attacks.</li>
<li><strong>摘要：</strong>最近的模型在使用 F1 分数评估阅读理解任务时，在斯坦福问答数据集上达到了人类水平的表现。然而，在一般情况下，教会机器理解文本的问题尚未得到解决。过去的研究表明，通过在上下文段落中添加一个对抗性句子，阅读理解模型的 F1 分数几乎下降了一半。在本文中，我用新模型 ELECTRA-Small 复制了过去的对抗性研究，并证明新模型的 F1 分数从 83.9% 下降到 29.2%。为了提高 ELECTRA-Small 对这种攻击的抵抗力，我在 SQuAD v1.1 训练示例上对模型进行了微调，在上下文段落中附加了一到五个对抗性句子。与过去的研究一样，我发现一个对抗性句子的微调模型不能很好地泛化到评估数据集。然而，当对四个或五个对抗性句子进行微调时，该模型在大多数具有多个附加和前置对抗性句子的评估数据集上获得了超过 70% 的 F1 分数。结果表明，通过足够的例子，我们可以使模型对对抗性攻击具有鲁棒性。</li>
</ul>

<h3>Title: Marrying Adapters and Mixup to Efficiently Enhance the Adversarial  Robustness of Pre-Trained Language Models for Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Tuc Nguyen, Thai Le</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10111">https://arxiv.org/abs/2401.10111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10111">https://arxiv.org/pdf/2401.10111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10111]] Marrying Adapters and Mixup to Efficiently Enhance the Adversarial  Robustness of Pre-Trained Language Models for Text Classification(https://arxiv.org/abs/2401.10111)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Existing works show that augmenting training data of neural networks using both clean and adversarial examples can enhance their generalizability under adversarial attacks. However, this training approach often leads to performance degradation on clean inputs. Additionally, it requires frequent re-training of the entire model to account for new attack types, resulting in significant and costly computations. Such limitations make adversarial training mechanisms less practical, particularly for complex Pre-trained Language Models (PLMs) with millions or even billions of parameters. To overcome these challenges while still harnessing the theoretical benefits of adversarial training, this study combines two concepts: (1) adapters, which enable parameter-efficient fine-tuning, and (2) Mixup, which train NNs via convex combinations of pairs data pairs. Intuitively, we propose to fine-tune PLMs through convex combinations of non-data pairs of fine-tuned adapters, one trained with clean and another trained with adversarial examples. Our experiments show that the proposed method achieves the best trade-off between training efficiency and predictive performance, both with and without attacks compared to other baselines on a variety of downstream tasks.</li>
<li><strong>摘要：</strong>现有的工作表明，使用干净的示例和对抗性示例来增强神经网络的训练数据可以增强其在对抗性攻击下的通用性。然而，这种训练方法通常会导致干净输入的性能下降。此外，它需要频繁地重新训练整个模型以应对新的攻击类型，从而导致大量且昂贵的计算。这些限制使得对抗性训练机制不太实用，特别是对于具有数百万甚至数十亿参数的复杂预训练语言模型（PLM）。为了克服这些挑战，同时仍然利用对抗性训练的理论优势，本研究结合了两个概念：(1) 适配器，可实现参数高效的微调；(2) 混合，通过对数据对的凸组合来训练神经网络。直观上，我们建议通过微调适配器的非数据对的凸组合来微调 PLM，其中一个适配器使用干净的数据进行训练，另一个使用对抗性示例进行训练。我们的实验表明，与各种下游任务的其他基线相比，所提出的方法在有攻击和无攻击的情况下实现了训练效率和预测性能之间的最佳权衡。</li>
</ul>

<h3>Title: Spatial-Temporal Large Language Model for Traffic Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, Cheng Long, Ziyue Li, Rui Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10134">https://arxiv.org/abs/2401.10134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10134">https://arxiv.org/pdf/2401.10134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10134]] Spatial-Temporal Large Language Model for Traffic Prediction(https://arxiv.org/abs/2401.10134)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we propose a novel partially frozen attention strategy of the LLM, which is designed to capture spatial-temporal dependencies for traffic prediction. Comprehensive experiments on real traffic datasets offer evidence that ST-LLM outperforms state-of-the-art models. Notably, the ST-LLM also exhibits robust performance in both few-shot and zero-shot prediction scenarios.</li>
<li><strong>摘要：</strong>交通预测是智能交通系统的关键组成部分，致力于利用历史数据预测特定位置的未来交通。尽管现有的交通预测模型往往强调开发复杂的神经网络结构，但其准确性并没有相应的提高。最近，大型语言模型（LLM）在时间序列分析方面表现出了出色的能力。与现有模型不同，法学硕士主要通过参数扩展和广泛的预训练来取得进展，同时保持其基本结构。在本文中，我们提出了一种用于流量预测的时空大语言模型（ST-LLM）。具体来说，ST-LLM 将每个位置的时间步重新定义为标记，并结合时空嵌入模块来学习标记的空间位置和全局时间表示。然后将这些表示融合起来，为每个标记提供统一的空间和时间信息。此外，我们提出了一种新颖的法学硕士部分冻结注意力策略，旨在捕获交通预测的时空依赖性。对真实交通数据集的综合实验提供了 ST-LLM 优于最先进模型的证据。值得注意的是，ST-LLM 在少样本和零样本预测场景中也表现出了强大的性能。</li>
</ul>

<h3>Title: Multi-Agent Reinforcement Learning for Maritime Operational Technology  Cyber Security</h3>
<ul>
<li><strong>Authors: </strong>Alec Wilson, Ryan Menzies, Neela Morarji, David Foster, Marco Casassa Mont, Esin Turkbeyler, Lisa Gralewski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10149">https://arxiv.org/abs/2401.10149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10149">https://arxiv.org/pdf/2401.10149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10149]] Multi-Agent Reinforcement Learning for Maritime Operational Technology  Cyber Security(https://arxiv.org/abs/2401.10149)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>This paper demonstrates the potential for autonomous cyber defence to be applied on industrial control systems and provides a baseline environment to further explore Multi-Agent Reinforcement Learning's (MARL) application to this problem domain. It introduces a simulation environment, IPMSRL, of a generic Integrated Platform Management System (IPMS) and explores the use of MARL for autonomous cyber defence decision-making on generic maritime based IPMS Operational Technology (OT). OT cyber defensive actions are less mature than they are for Enterprise IT. This is due to the relatively brittle nature of OT infrastructure originating from the use of legacy systems, design-time engineering assumptions, and lack of full-scale modern security controls. There are many obstacles to be tackled across the cyber landscape due to continually increasing cyber-attack sophistication and the limitations of traditional IT-centric cyber defence solutions. Traditional IT controls are rarely deployed on OT infrastructure, and where they are, some threats aren't fully addressed. In our experiments, a shared critic implementation of Multi Agent Proximal Policy Optimisation (MAPPO) outperformed Independent Proximal Policy Optimisation (IPPO). MAPPO reached an optimal policy (episode outcome mean of 1) after 800K timesteps, whereas IPPO was only able to reach an episode outcome mean of 0.966 after one million timesteps. Hyperparameter tuning greatly improved training performance. Across one million timesteps the tuned hyperparameters reached an optimal policy whereas the default hyperparameters only managed to win sporadically, with most simulations resulting in a draw. We tested a real-world constraint, attack detection alert success, and found that when alert success probability is reduced to 0.75 or 0.9, the MARL defenders were still able to win in over 97.5% or 99.5% of episodes, respectively.</li>
<li><strong>摘要：</strong>本文展示了自主网络防御在工业控制系统中应用的潜力，并提供了一个基线环境来进一步探索多智能体强化学习（MARL）在该问题领域的应用。它介绍了通用集成平台管理系统 (IPMS) 的模拟环境 IPMSRL，并探讨了使用 MARL 在基于通用海事的 IPMS 操作技术 (OT) 上进行自主网络防御决策。 OT 网络防御行动不如企业 IT 成熟。这是由于 OT 基础设施相对脆弱，其原因在于使用遗留系统、设计时工程假设以及缺乏全面的现代安全控制。由于网络攻击的复杂性不断提高以及传统以 IT 为中心的网络防御解决方案的局限性，整个网络环境中存在许多需要解决的障碍。传统 IT 控制很少部署在 OT 基础设施上，而且即使部署在这些基础设施上，一些威胁也没有得到完全解决。在我们的实验中，多代理近端策略优化（MAPPO）的共享批评家实现优于独立近端策略优化（IPPO）。 MAPPO 在 80 万个时间步长后达到最优策略（情节结果平均值为 1），而 IPPO 在 100 万个时间步长后只能达到情节结果平均值 0.966。超参数调整极大地提高了训练性能。经过一百万个时间步长，调整后的超参数达到了最佳策略，而默认超参数只能偶尔获胜，大多数模拟结果都是平局。我们测试了现实世界的约束条件，即攻击检测警报成功率，发现当警报成功概率降低到 0.75 或 0.9 时，MARL 防御者仍然能够分别在超过 97.5% 或 99.5% 的事件中获胜。</li>
</ul>

<h3>Title: Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on  Data-to-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Zdeněk Kasner, Ondřej Dušek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10186">https://arxiv.org/abs/2401.10186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10186">https://arxiv.org/pdf/2401.10186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10186]] Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on  Data-to-Text Generation(https://arxiv.org/abs/2401.10186)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code, rag</a></li>
<li><strong>Abstract: </strong>We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs' in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models' behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs of open LLMs contain a semantic error according to human annotators (91% according to GPT-4). Our code, data, and model outputs are available at https://d2t-llm.github.io.</li>
<li><strong>摘要：</strong>我们研究开放大型语言模型 (LLM) 在多大程度上可以从结构化数据生成连贯且相关的文本。为了防止 LLM 训练数据中泄露的基准产生偏差，我们收集了 Quintd-1：一个针对五个数据到文本 (D2T) 生成任务的临时基准，由从公共 API 收集的标准格式的结构化数据记录组成。我们利用无参考评估指标和法学硕士的情境学习能力，使我们能够在没有人工编写参考的情况下测试模型。我们的评估重点是结合人类注释者和基于 GPT-4 的指标，在​​标记级别注释语义准确性错误。我们对模型跨领域和任务行为的系统检查表明，具有 7B 参数的最先进的开放式法学硕士可以在零样本设置下从各种标准数据格式生成流畅且连贯的文本。然而，我们还表明，输出的语义准确性仍然是一个主要问题：在我们的基准测试中，根据人类注释者的说法，开放法学硕士的输出中有 80% 包含语义错误（根据 GPT-4，91% ）。我们的代码、数据和模型输出可在 https://d2t-llm.github.io 上获取。</li>
</ul>

<h3>Title: Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through  Text Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Qingyun Wang, Zixuan Zhang, Hongxiang Li, Xuan Liu, Jiawei Han, Heng Ji, Huimin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10189">https://arxiv.org/abs/2401.10189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10189">https://arxiv.org/pdf/2401.10189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10189]] Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through  Text Reconstruction(https://arxiv.org/abs/2401.10189)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction process. Finally, we release ChemNER+, a new fine-grained chemical entity extraction dataset that is annotated by domain experts with the ChemNER schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets show that our newly proposed framework has contributed up to 8.26% and 6.84% absolute F1-score gains respectively.</li>
<li><strong>摘要：</strong>化学领域的细粒度几次实体提取面临两个独特的挑战。首先，与一般领域的实体提取任务相比，化学论文中的句子通常包含更多的实体。此外，实体提取模型通常难以提取长尾类型的实体。在本文中，我们提出了 Chem-FINESE，一种新颖的基于序列到序列 (seq2seq) 的少样本实体提取方法，以解决这两个挑战。我们的 Chem-FINESE 有两个组件：一个 seq2seq 实体提取器，用于从输入句子中提取命名实体；以及一个 seq2seq 自验证模块，用于从提取的实体中重建原始输入句子。受良好的实体提取系统需要忠实地提取实体这一事实的启发，我们新的自我验证模块利用实体提取结果来重建原始输入句子。此外，我们设计了一种新的对比损失来减少提取过程中的过度复制。最后，我们发布了 ChemNER+，这是一个新的细粒度化学实体提取数据集，由领域专家使用 ChemNER 模式进行注释。使用 ChemNER+ 和 CHEMET 数据集进行的少样本设置实验表明，我们新提出的框架分别贡献了高达 8.26% 和 6.84% 的绝对 F1 分数增益。</li>
</ul>

<h3>Title: Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt  Tensor Products</h3>
<ul>
<li><strong>Authors: </strong>Shengjie Luo, Tianlang Chen, Aditi S. Krishnapriyan</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, math.GR, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10216">https://arxiv.org/abs/2401.10216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10216">https://arxiv.org/pdf/2401.10216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10216]] Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt  Tensor Products(https://arxiv.org/abs/2401.10216)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications. Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps). However, the computational complexity of such operations increases significantly as higher-order tensors are used. In this work, we propose a systematic approach to substantially accelerate the computation of the tensor products of irreps. We mathematically connect the commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are integrals of products of three spherical harmonics. Through Gaunt coefficients, the tensor product of irreps becomes equivalent to the multiplication between spherical functions represented by spherical harmonics. This perspective further allows us to change the basis for the equivariant operations from spherical harmonics to a 2D Fourier basis. Consequently, the multiplication between spherical functions represented by a 2D Fourier basis can be efficiently computed via the convolution theorem and Fast Fourier Transforms. This transformation reduces the complexity of full tensor products of irreps from $\mathcal{O}(L^6)$ to $\mathcal{O}(L^3)$, where $L$ is the max degree of irreps. Leveraging this approach, we introduce the Gaunt Tensor Product, which serves as a new method to construct efficient equivariant operations across different model architectures. Our experiments on the Open Catalyst Project and 3BPA datasets demonstrate both the increased efficiency and improved performance of our approach.</li>
<li><strong>摘要：</strong>为 E(3) 组开发等变神经网络在跨实际应用的 3D 数据建模中发挥着重要作用。强制这种等变性主要涉及不可约表示（irreps）的张量积。然而，随着使用高阶张量，此类运算的计算复杂度显着增加。在这项工作中，我们提出了一种系统方法来大幅加速不可重复张量积的计算。我们在数学上将常用的 Clebsch-Gordan 系数与 Gaunt 系数联系起来，Gaunt 系数是三个球谐函数乘积的积分。通过 Gaunt 系数，irreps 的张量积就等于球谐函数表示的球函数之间的乘法。这种观点进一步允许我们将等变运算的基础从球谐函数更改为二维傅里叶基础。因此，可以通过卷积定理和快速傅里叶变换有效地计算由二维傅里叶基表示的球面函数之间的乘法。这种变换将不可重复的全张量积的复杂度从 $\mathcal{O}(L^6)$ 降低到 $\mathcal{O}(L^3)$，其中 $L$ 是不可重复的最大程度。利用这种方法，我们引入了 Gaunt 张量积，它是一种跨不同模型架构构建高效等变运算的新方法。我们在 Open Catalyst Project 和 3BPA 数据集上进行的实验证明了我们的方法提高了效率并提高了性能。</li>
</ul>

<h3>Title: ChatQA: Building GPT-4 Level Conversational QA Models</h3>
<ul>
<li><strong>Authors: </strong>Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10225">https://arxiv.org/abs/2401.10225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10225">https://arxiv.org/pdf/2401.10225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10225]] ChatQA: Building GPT-4 Level Conversational QA Models(https://arxiv.org/abs/2401.10225)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, rag</a></li>
<li><strong>Abstract: </strong>In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.</li>
<li><strong>摘要：</strong>在这项工作中，我们介绍了 ChatQA，这是一系列对话式问答 (QA) 模型，可获取 GPT-4 级别的准确度。具体来说，我们提出了一种两阶段指令调整方法，可以显着改善大型语言模型（LLM）的零样本会话 QA 结果。为了处理会话 QA 中的检索，我们在多轮 QA 数据集上微调密集检索器，这提供了与使用最先进的查询重写模型相当的结果，同时大大降低了部署成本。值得注意的是，我们的 ChatQA-70B 在 10 个会话 QA 数据集上的平均得分（54.14 比 53.90）优于 GPT-4，而不依赖于 OpenAI GPT 模型的任何合成数据。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
