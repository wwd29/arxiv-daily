<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-06-17</h1>
<h3>Title: Title:
          Advancing High Resolution Vision-Language Models in Biomedicine</h3>
<ul>
<li><strong>Authors: </strong>Zekai Chen, Arda Pekis, Kevin Brown</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Advancing High Resolution Vision-Language Models in Biomedicine(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, agent</a></li>
<li><strong>Abstract: </strong>Multi-modal learning has significantly advanced generative AI, especially in vision-language modeling. Innovations like GPT-4V and open-source projects such as LLaVA have enabled robust conversational agents capable of zero-shot task completions. However, applying these technologies in the biomedical field presents unique challenges. Recent initiatives like LLaVA-Med have started to adapt instruction-tuning for biomedical contexts using large datasets such as PMC-15M. Our research offers three key contributions: (i) we present a new instruct dataset enriched with medical image-text pairs from Claude3-Opus and LLaMA3 70B, (ii) we propose a novel image encoding strategy using hierarchical representations to improve fine-grained biomedical visual comprehension, and (iii) we develop the Llama3-Med model, which achieves state-of-the-art zero-shot performance on biomedical visual question answering benchmarks, with an average performance improvement of over 10% compared to previous methods. These advancements provide more accurate and reliable tools for medical professionals, bridging gaps in current multi-modal conversational assistants and promoting further innovations in medical AI.</li>
<li><strong>摘要：</strong>多模态学习极大地推动了生成式人工智能的发展，尤其是在视觉语言建模方面。GPT-4V 等创新和 LLaVA 等开源项目已经实现了能够完成零样本任务的强大对话代理。然而，将这些技术应用于生物医学领域带来了独特的挑战。最近的计划，如 LLaVA-Med，已经开始使用 PMC-15M 等大型数据集调整生物医学环境中的指令调整。我们的研究提供了三个关键贡献：(i) 我们提出了一个由来自 Claude3-Opus 和 LLaMA3 70B 的医学图像-文本对组成的新指令数据集，(ii) 我们提出了一种使用分层表示来改进细粒度生物医学视觉理解的新型图像编码策略，以及 (iii) 我们开发了 Llama3-Med 模型，该模型在生物医学视觉问答基准上实现了最先进的零样本性能，与以前的方法相比，平均性能提高了 10% 以上。这些进步为医疗专业人员提供了更准确、更可靠的工具，弥补了当前多模式对话助手的空白，并促进了医疗人工智能的进一步创新。</li>
</ul>

<h3>Title: Title:
          Newswire: A Large-Scale Structured Database of a Century of Historical News</h3>
<ul>
<li><strong>Authors: </strong>Emily Silcock, Abhishek Arora, Luca D'Amico-Wong, Melissa Dell</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Newswire: A Large-Scale Structured Database of a Century of Historical News(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the U.S. historically, local newspapers drew their content largely from newswires like the Associated Press. Historians argue that newswires played a pivotal role in creating a national identity and shared understanding of the world, but there is no comprehensive archive of the content sent over newswires. We reconstruct such an archive by applying a customized deep learning pipeline to hundreds of terabytes of raw image scans from thousands of local newspapers. The resulting dataset contains 2.7 million unique public domain U.S. newswire articles, written between 1878 and 1977. Locations in these articles are georeferenced, topics are tagged using customized neural topic classification, named entities are recognized, and individuals are disambiguated to Wikipedia using a novel entity disambiguation model. To construct the Newswire dataset, we first recognize newspaper layouts and transcribe around 138 millions structured article texts from raw image scans. We then use a customized neural bi-encoder model to de-duplicate reproduced articles, in the presence of considerable abridgement and noise, quantifying how widely each article was reproduced. A text classifier is used to ensure that we only include newswire articles, which historically are in the public domain. The structured data that accompany the texts provide rich information about the who (disambiguated individuals), what (topics), and where (georeferencing) of the news that millions of Americans read over the course of a century. We also include Library of Congress metadata information about the newspapers that ran the articles on their front pages. The Newswire dataset is useful both for large language modeling - expanding training data beyond what is available from modern web texts - and for studying a diversity of questions in computational linguistics, social science, and the digital humanities.</li>
<li><strong>摘要：</strong>在美国，地方报纸的内容主要来自美联社等新闻通讯社。历史学家认为，新闻通讯社在塑造国家认同和对世界的共同理解方面发挥了关键作用，但并没有关于通过新闻通讯社发送的内容的综合档案。我们通过将定制的深度学习管道应用于来自数千家地方报纸的数百 TB 原始图像扫描，重建了这样一个档案。由此产生的数据集包含 270 万篇独特的公共领域美国新闻通讯文章，撰写于 1878 年至 1977 年之间。这些文章中的位置经过地理参考，使用定制的神经主题分类标记主题，识别命名实体，并使用新颖的实体消歧模型将个人消歧到维基百科。为了构建新闻通讯数据集，我们首先识别报纸布局并从原始图像扫描中转录大约 1.38 亿个结构化文章文本。然后，我们使用定制的神经双编码器模型对复制的文章进行去重，在存在大量删节和噪音的情况下，量化每篇文章的复制范围。使用文本分类器来确保我们只包括历史上属于公共领域的新闻通讯文章。文本附带的结构化数据提供了丰富的信息，包括数百万美国人在一个世纪中阅读的新闻的人物（消除歧义的个人）、内容（主题）和地点（地理参考）。我们还包括国会图书馆关于在头版刊登文章的报纸的元数据信息。新闻通讯数据集既可用于大型语言建模 - 将训练数据扩展到现代网络文本之外 - 也可用于研究计算语言学、社会科学和数字人文学科的各种问题。</li>
</ul>

<h3>Title: Title:
          Talking Heads: Understanding Inter-layer Communication in Transformer Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jack Merullo, Carsten Eickhoff, Ellie Pavlick</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Talking Heads: Understanding Inter-layer Communication in Transformer Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Although it is known that transformer language models (LMs) pass features from early layers to later layers, it is not well understood how this information is represented and routed by the model. By analyzing particular mechanism LMs use to accomplish this, we find that it is also used to recall items from a list, and show that this mechanism can explain an otherwise arbitrary-seeming sensitivity of the model to the order of items in the prompt. Specifically, we find that models write into low-rank subspaces of the residual stream to represent features which are then read out by specific later layers, forming low-rank communication channels between layers. By decomposing attention head weight matrices with the Singular Value Decomposition (SVD), we find that previously described interactions between heads separated by one or more layers can be predicted via analysis of their weight matrices. We show that it is possible to manipulate the internal model representations as well as edit model weights based on the mechanism we discover in order to significantly improve performance on our synthetic Laundry List task, which requires recall from a list, often improving task accuracy by over 20%. Our analysis reveals a surprisingly intricate interpretable structure learned from language model pretraining, and helps us understand why sophisticated LMs sometimes fail in simple domains, facilitating future analysis of more complex behaviors.</li>
<li><strong>摘要：</strong>尽管众所周知，Transformer 语言模型 (LM) 会将特征从早期层传递到后期层，但人们并不清楚模型如何表示和路由这些信息。通过分析 LM 用于实现此目的的特定机制，我们发现它也用于从列表中调用项目，并表明这种机制可以解释模型对提示中项目顺序的看似任意的敏感性。具体而言，我们发现模型写入残差流的低秩子空间以表示特征，然后由特定的后期层读出，从而形成层之间的低秩通信通道。通过使用奇异值分解 (SVD) 分解注意头权重矩阵，我们发现可以通过分析它们的权重矩阵来预测先前描述的由一个或多个层分隔的头部之间的相互作用。我们表明，可以根据我们发现的机制操纵内部模型表示以及编辑模型权重，从而显着提高我们的合成洗衣清单任务的性能，该任务需要从列表中调用，通常可将任务准确率提高 20% 以上。我们的分析揭示了从语言模型预训练中学习到的令人惊讶的复杂的可解释结构，并帮助我们理解为什么复杂的 LM 有时会在简单领域失败，从而促进未来对更复杂行为的分析。</li>
</ul>

<h3>Title: Title:
          Decoding the Diversity: A Review of the Indic AI Research Landscape</h3>
<ul>
<li><strong>Authors: </strong>Sankalp KJ, Vinija Jain, Sreyoshi Bhaduri, Tamoghna Roy, Aman Chadha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Decoding the Diversity: A Review of the Indic AI Research Landscape(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This review paper provides a comprehensive overview of large language model (LLM) research directions within Indic languages. Indic languages are those spoken in the Indian subcontinent, including India, Pakistan, Bangladesh, Sri Lanka, Nepal, and Bhutan, among others. These languages have a rich cultural and linguistic heritage and are spoken by over 1.5 billion people worldwide. With the tremendous market potential and growing demand for natural language processing (NLP) based applications in diverse languages, generative applications for Indic languages pose unique challenges and opportunities for research. Our paper deep dives into the recent advancements in Indic generative modeling, contributing with a taxonomy of research directions, tabulating 84 recent publications. Research directions surveyed in this paper include LLM development, fine-tuning existing LLMs, development of corpora, benchmarking and evaluation, as well as publications around specific techniques, tools, and applications. We found that researchers across the publications emphasize the challenges associated with limited data availability, lack of standardization, and the peculiar linguistic complexities of Indic languages. This work aims to serve as a valuable resource for researchers and practitioners working in the field of NLP, particularly those focused on Indic languages, and contributes to the development of more accurate and efficient LLM applications for these languages.</li>
<li><strong>摘要：</strong>本篇综述全面概述了印度语中的大型语言模型 (LLM) 研究方向。印度语是印度次大陆使用的语言，包括印度、巴基斯坦、孟加拉国、斯里兰卡、尼泊尔和不丹等。这些语言具有丰富的文化和语言遗产，全球有超过 15 亿人使用。由于市场潜力巨大，且对基于自然语言处理 (NLP) 的各种语言的应用需求不断增长，印度语的生成应用为研究带来了独特的挑战和机遇。我们的论文深入探讨了印度语生成模型的最新进展，并提出了研究方向的分类，汇总了 84 篇最新出版物。本文调查的研究方向包括 LLM 开发、现有 LLM 的微调、语料库的开发、基准测试和评估，以及围绕特定技术、工具和应用的出版物。我们发现，所有出版物中的研究人员都强调了与数据可用性有限、缺乏标准化以及印度语特殊的语言复杂性相关的挑战。这项工作旨在为 NLP 领域的研究人员和从业人员（特别是专注于印度语的研究人员和从业人员）提供宝贵的资源，并有助于开发针对这些语言的更准确、更高效的 LLM 应用程序。</li>
</ul>

<h3>Title: Title:
          Speech ReaLLM -- Real-time Streaming Speech Recognition with Multimodal LLMs by Teaching the Flow of Time</h3>
<ul>
<li><strong>Authors: </strong>Frank Seide, Morrie Doulaty, Yangyang Shi, Yashesh Gaur, Junteng Jia, Chunyang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Speech ReaLLM -- Real-time Streaming Speech Recognition with Multimodal LLMs by Teaching the Flow of Time(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>We introduce Speech ReaLLM, a new ASR architecture that marries "decoder-only" ASR with the RNN-T to make multimodal LLM architectures capable of real-time streaming. This is the first "decoder-only" ASR architecture designed to handle continuous audio without explicit end-pointing. Speech ReaLLM is a special case of the more general ReaLLM ("real-time LLM") approach, also introduced here for the first time. The idea is inspired by RNN-T: Instead of generating a response only at the end of a user prompt, generate after every input token received in real time (it is often empty). On Librispeech "test", an 80M Speech ReaLLM achieves WERs of 3.0% and 7.4% in real time (without an external LM or auxiliary loss). This is only slightly above a 3x larger Attention-Encoder-Decoder baseline. We also show that this way, an LLM architecture can learn to represent and reproduce the flow of time; and that a pre-trained 7B LLM can be fine-tuned to do reasonably well on this task.</li>
<li><strong>摘要：</strong>我们引入了 Speech ReaLLM，这是一种新的 ASR 架构，它将“仅解码器”ASR 与 RNN-T 结合在一起，使多模态 LLM 架构能够进行实时流式传输。这是第一个设计用于处理连续音频而无需明确端点指向的“仅解码器”ASR 架构。Speech ReaLLM 是更通用的 ReaLLM（“实时 LLM”）方法的一个特例，也是首次在这里介绍。这个想法受到 RNN-T 的启发：不是仅在用户提示结束时生成响应，而是在实时收到每个输入标记后生成响应（它通常是空的）。在 Librispeech“测试”中，80M Speech ReaLLM 实现了 3.0% 和 7.4% 的实时 WER（没有外部 LM 或辅助损失）。这仅略高于 3 倍大的 Attention-Encoder-Decoder 基线。我们还表明，通过这种方式，LLM 架构可以学习表示和重现时间的流动；并且经过预先训练的 7B LLM 可以进行微调，以便相当好地完成这项任务。</li>
</ul>

<h3>Title: Title:
          Multimodal Large Language Models with Fusion Low Rank Adaptation for Device Directed Speech Detection</h3>
<ul>
<li><strong>Authors: </strong>Shruti Palaskar, Oggi Rudovic, Sameer Dharur, Florian Pesce, Gautam Krishna, Aswin Sivaraman, Jack Berkowitz, Ahmed Hussen Abdelaziz, Saurabh Adya, Ahmed Tewfik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Multimodal Large Language Models with Fusion Low Rank Adaptation for Device Directed Speech Detection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) have shown promise for human-like conversations, they are primarily pre-trained on text data. Incorporating audio or video improves performance, but collecting large-scale multimodal data and pre-training multimodal LLMs is challenging. To this end, we propose a Fusion Low Rank Adaptation (FLoRA) technique that efficiently adapts a pre-trained unimodal LLM to consume new, previously unseen modalities via low rank adaptation. For device-directed speech detection, using FLoRA, the multimodal LLM achieves 22% relative reduction in equal error rate (EER) over the text-only approach and attains performance parity with its full fine-tuning (FFT) counterpart while needing to tune only a fraction of its parameters. Furthermore, with the newly introduced adapter dropout, FLoRA is robust to missing data, improving over FFT by 20% lower EER and 56% lower false accept rate. The proposed approach scales well for model sizes from 16M to 3B parameters.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 已显示出实现类人对话的潜力，但它们主要在文本数据上进行预训练。加入音频或视频可以提高性能，但收集大规模多模态数据和预训练多模态 LLM 具有挑战性。为此，我们提出了一种融合低秩自适应 (FLoRA) 技术，该技术可以有效地调整预训练的单模态 LLM，通过低秩自适应来使用新的、以前未见过的模态。对于设备导向的语音检测，使用 FLoRA，多模态 LLM 相对于纯文本方法实现了 22% 的等错误率 (EER) 相对降低，并且与完全微调 (FFT) 对应方法的性能相当，同时只需要调整其参数的一小部分。此外，借助新引入的适配器 dropout，FLoRA 对缺失数据具有很强的鲁棒性，与 FFT 相比，EER 降低了 20%，错误接受率降低了 56%。所提出的方法可以很好地扩展到从 16M 到 3B 参数的模型大小。</li>
</ul>

<h3>Title: Title:
          Multi-Modal Retrieval For Large Language Model Based Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jari Kolehmainen, Aditya Gourav, Prashanth Gurunath Shivakumar, Yile Gu, Ankur Gandhe, Ariya Rastrow, Grant Strimel, Ivan Bulyko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Multi-Modal Retrieval For Large Language Model Based Speech Recognition(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Retrieval is a widely adopted approach for improving language models leveraging external information. As the field moves towards multi-modal large language models, it is important to extend the pure text based methods to incorporate other modalities in retrieval as well for applications across the wide spectrum of machine learning tasks and data types. In this work, we propose multi-modal retrieval with two approaches: kNN-LM and cross-attention techniques. We demonstrate the effectiveness of our retrieval approaches empirically by applying them to automatic speech recognition tasks with access to external information. Under this setting, we show that speech-based multi-modal retrieval outperforms text based retrieval, and yields up to 50 % improvement in word error rate over the multi-modal language model baseline. Furthermore, we achieve state-of-the-art recognition results on the Spoken-Squad question answering dataset.</li>
<li><strong>摘要：</strong>检索是一种广泛采用的利用外部信息改进语言模型的方法。随着该领域向多模态大型语言模型发展，扩展纯文本方法以将其他模态纳入检索中以及将其应用于广泛的机器学习任务和数据类型非常重要。在这项工作中，我们提出了两种方法的多模态检索：kNN-LM 和交叉注意技术。我们通过将检索方法应用于可访问外部信息的自动语音识别任务，从经验上证明了检索方法的有效性。在这种设置下，我们表明基于语音的多模态检索优于基于文本的检索，并且与多模态语言模型基线相比，单词错误率提高了 50%。此外，我们在 Spoken-Squad 问答数据集上取得了最先进的识别结果。</li>
</ul>

<h3>Title: Title:
          FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Zijian Feng, Hanzhang Zhou, Zixiao Zhu, Kezhi Mao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Controllable text generation (CTG) seeks to craft texts adhering to specific attributes, traditionally employing learning-based techniques such as training, fine-tuning, or prefix-tuning with attribute-specific datasets. These approaches, while effective, demand extensive computational and data resources. In contrast, some proposed learning-free alternatives circumvent learning but often yield inferior results, exemplifying the fundamental machine learning trade-off between computational expense and model efficacy. To overcome these limitations, we propose FreeCtrl, a learning-free approach that dynamically adjusts the weights of selected feedforward neural network (FFN) vectors to steer the outputs of large language models (LLMs). FreeCtrl hinges on the principle that the weights of different FFN vectors influence the likelihood of different tokens appearing in the output. By identifying and adaptively adjusting the weights of attribute-related FFN vectors, FreeCtrl can control the output likelihood of attribute keywords in the generated content. Extensive experiments on single- and multi-attribute control reveal that the learning-free FreeCtrl outperforms other learning-free and learning-based methods, successfully resolving the dilemma between learning costs and model performance.</li>
<li><strong>摘要：</strong>可控文本生成 (CTG) 旨在制作符合特定属性的文本，传统上采用基于学习的技术，例如使用特定于属性的数据集进行训练、微调或前缀调整。这些方法虽然有效，但需要大量的计算和数据资源。相比之下，一些提出的无需学习的替代方案绕过了学习，但往往产生较差的结果，体现了机器学习在计算成本和模型功效之间的基本权衡。为了克服这些限制，我们提出了 FreeCtrl，这是一种无需学习的方法，可以动态调整所选前馈神经网络 (FFN) 向量的权重来控制大型语言模型 (LLM) 的输出。FreeCtrl 取决于以下原理：不同 FFN 向量的权重会影响不同标记出现在输出中的可能性。通过识别和自适应调整与属​​性相关的 FFN 向量的权重，FreeCtrl 可以控制生成内容中属性关键字的输出可能性。在单属性和多属性控制上进行的大量实验表明，无需学习的 FreeCtrl 优于其他无需学习和基于学习的方法，成功解决了学习成本和模型性能之间的困境。</li>
</ul>

<h3>Title: Title:
          Detecting Response Generation Not Requiring Factual Judgment</h3>
<ul>
<li><strong>Authors: </strong>Ryohei Kamei, Daiki Shiono, Reina Akama, Jun Suzuki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Detecting Response Generation Not Requiring Factual Judgment(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the remarkable development of large language models (LLMs), ensuring the factuality of output has become a challenge. However, having all the contents of the response with given knowledge or facts is not necessarily a good thing in dialogues. This study aimed to achieve both attractiveness and factuality in a dialogue response for which a task was set to predict sentences that do not require factual correctness judgment such as agreeing, or personal opinions/feelings. We created a dataset, dialogue dataset annotated with fact-check-needed label (DDFC), for this task via crowdsourcing, and classification tasks were performed on several models using this dataset. The model with the highest classification accuracy could yield about 88% accurate classification results.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的飞速发展，确保输出的真实性已成为一项挑战。然而，在对话中，用给定的知识或事实来提供回复的所有内容并不一定是件好事。这项研究旨在实现对话回复中的吸引力和真实性，为此设定了一个任务来预测不需要事实正确性判断的句子，例如同意或个人观点/感受。我们通过众包为这项任务创建了一个数据集，即带有事实核查标签 (DDFC) 注释的对话数据集，并使用该数据集对多个模型执行分类任务。分类准确率最高的模型可以产生大约 88% 的准确分类结果。</li>
</ul>

<h3>Title: Title:
          UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Trinh Pham, Khoi M. Le, Luu Anh Tuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in languages with limited resources. Our approach tackles two essential elements of a language model: the initialization of embeddings and the optimal vocabulary size. Specifically, we propose a novel embedding initialization method that leverages both lexical and semantic alignment for a language. In addition, we present a method for systematically searching for the optimal vocabulary size, ensuring a balance between model complexity and linguistic coverage. Our experiments across multilingual datasets show that our approach greatly improves the F1-Score in several languages. UniBridge is a robust and adaptable solution for cross-lingual systems in various languages, highlighting the significance of initializing embeddings and choosing the right vocabulary size in cross-lingual environments.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 UniBridge（具有优化嵌入和词汇的跨语言迁移学习），这是一种全面的方法，旨在提高跨语言迁移学习的有效性，特别是在资源有限的语言中。我们的方法解决了语言模型的两个基本要素：嵌入的初始化和最佳词汇量。具体来说，我们提出了一种新颖的嵌入初始化方法，该方法利用语言的词汇和语义对齐。此外，我们提出了一种系统地搜索最佳词汇量的方法，确保模型复杂性和语言覆盖率之间的平衡。我们在多语言数据集上的实验表明，我们的方法极大地提高了几种语言的 F1 分数。UniBridge 是一种适用于各种语言的跨语言系统的强大且适应性强的解决方案，突出了在跨语言环境中初始化嵌入和选择正确词汇量的重要性。</li>
</ul>

<h3>Title: Title:
          Self-Knowledge Distillation for Learning Ambiguity</h3>
<ul>
<li><strong>Authors: </strong>Hancheol Park, Soyeong Jeong, Sukmin Cho, Jong C. Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Self-Knowledge Distillation for Learning Ambiguity(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent language models have shown remarkable performance on natural language understanding (NLU) tasks. However, they are often sub-optimal when faced with ambiguous samples that can be interpreted in multiple ways, over-confidently predicting a single label without consideration for its correctness. To address this issue, we propose a novel self-knowledge distillation method that enables models to learn label distributions more accurately by leveraging knowledge distilled from their lower layers. This approach also includes a learning phase that re-calibrates the unnecessarily strengthened confidence for training samples judged as extremely ambiguous based on the distilled distribution knowledge. We validate our method on diverse NLU benchmark datasets and the experimental results demonstrate its effectiveness in producing better label distributions. Particularly, through the process of re-calibrating the confidence for highly ambiguous samples, the issue of over-confidence when predictions for unseen samples do not match with their ground-truth labels has been significantly alleviated. This has been shown to contribute to generating better distributions than the existing state-of-the-art method. Moreover, our method is more efficient in training the models compared to the existing method, as it does not involve additional training processes to refine label distributions.</li>
<li><strong>摘要：</strong>最近的语言模型在自然语言理解 (NLU) 任务中表现出色。然而，当面对可以以多种方式解释的模糊样本时，它们通常不是最优的，过于自信地预测单个标签而不考虑其正确性。为了解决这个问题，我们提出了一种新颖的自我知识蒸馏方法，使模型能够利用从其较低层蒸馏出的知识更准确地学习标签分布。这种方法还包括一个学习阶段，该阶段根据蒸馏出的分布知识重新校准被判断为极其模糊的训练样本的不必要的增强置信度。我们在不同的 NLU 基准数据集上验证了我们的方法，实验结果证明了它在生成更好的标签分布方面的有效性。特别是，通过重新校准高度模糊样本的置信度的过程，当对未见样本的预测与其真实标签不匹配时过度自信的问题得到了显著缓解。事实证明，这有助于生成比现有最先进方法更好的分布。此外，与现有方法相比，我们的方法在训练模型方面更有效率，因为它不涉及额外的训练过程来细化标签分布。</li>
</ul>

<h3>Title: Title:
          Bootstrapping Language Models with DPO Implicit Rewards</h3>
<ul>
<li><strong>Authors: </strong>Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Bootstrapping Language Models with DPO Implicit Rewards(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Human alignment in large language models (LLMs) is an active area of research. A recent groundbreaking work, direct preference optimization (DPO), has greatly simplified the process from past work in reinforcement learning from human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO, after training, provides an implicit reward model. In this work, we make a novel observation that this implicit reward model can by itself be used in a bootstrapping fashion to further align the LLM. Our approach is to use the rewards from a current LLM model to construct a preference dataset, which is then used in subsequent DPO rounds. We incorporate refinements that debias the length of the responses and improve the quality of the preference dataset to further improve our approach. Our approach, named self-alignment with DPO ImpliCit rEwards (DICE), shows great improvements in alignment and achieves superior performance than Gemini Pro on AlpacaEval 2, reaching 27.55% length-controlled win rate against GPT-4 Turbo, but with only 8B parameters and no external feedback. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 中的人类对齐是一个活跃的研究领域。最近的一项开创性工作直接偏好优化 (DPO) 通过绕过 RLHF 中的奖励学习阶段，大大简化了过去人类反馈强化学习 (RLHF) 的工作过程。经过训练后，DPO 提供了一个隐式奖励模型。在这项工作中，我们做出了一个新颖的观察，即这种隐式奖励模型本身可以以引导方式用于进一步对齐 LLM。我们的方法是使用当前 LLM 模型中的奖励来构建偏好数据集，然后在后续的 DPO 轮次中使用。我们结合了消除响应长度偏差和提高偏好数据集质量的改进，以进一步改进我们的方法。我们的方法称为自对齐 DPO ImpliCit rEwards (DICE)，在对齐方面表现出很大的改进，并且在 AlpacaEval 2 上实现了比 Gemini Pro 更好的性能，在对抗 GPT-4 Turbo 时达到了 27.55% 的长度控制胜率，但只有 8B 参数且没有外部反馈。我们的代码可在此 https URL 上获取。</li>
</ul>

<h3>Title: Title:
          Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity</h3>
<ul>
<li><strong>Authors: </strong>Bowen Zhang, Chunping Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Semantic Textual Similarity (STS) constitutes a critical research direction in computational linguistics and serves as a key indicator of the encoding capabilities of embedding models. Driven by advances in pre-trained language models and contrastive learning techniques, leading sentence representation methods can already achieved average Spearman's correlation scores of approximately 86 across seven STS benchmarks in SentEval. However, further improvements have become increasingly marginal, with no existing method attaining an average score higher than 87 on these tasks. This paper conducts an in-depth analysis of this phenomenon and concludes that the upper limit for Spearman's correlation scores using contrastive learning is 87.5. To transcend this ceiling, we propose an innovative approach termed Pcc-tuning, which employs Pearson's correlation coefficient as a loss function to refine model performance beyond contrastive learning. Experimental results demonstrate that Pcc-tuning markedly surpasses previous state-of-the-art strategies, raising the Spearman's correlation score to above 90.</li>
<li><strong>摘要：</strong>语义文本相似度 (STS) 是计算语言学的一个重要研究方向，也是嵌入模型编码能力的关键指标。在预训练语言模型和对比学习技术的推动下，领先的句子表征方法已经可以在 SentEval 的七个 STS 基准测试中实现约 86 的平均 Spearman 相关度得分。然而，进一步的改进已经越来越微不足道，现有的任何方法在这些任务上都无法达到高于 87 的平均分数。本文对这一现象进行了深入分析，并得出结论，使用对比学习的 Spearman 相关度得分的上限是 87.5。为了突破这个上限，我们提出了一种称为 Pcc-tuning 的创新方法，它使用 Pearson 相关系数作为损失函数来改进模型性能，超越对比学习。实验结果表明，Pcc-tuning 明显超越了之前最先进的策略，将 Spearman 相关度得分提高到了 90 以上。</li>
</ul>

<h3>Title: Title:
          Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments</h3>
<ul>
<li><strong>Authors: </strong>Zhenrui Yue, Huimin Zeng, Lanyu Shang, Yifan Liu, Yang Zhang, Dong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>The rapid propagation of misinformation poses substantial risks to public interest. To combat misinformation, large language models (LLMs) are adapted to automatically verify claim credibility. Nevertheless, existing methods heavily rely on the embedded knowledge within LLMs and / or black-box APIs for evidence collection, leading to subpar performance with smaller LLMs or upon unreliable context. In this paper, we propose retrieval augmented fact verification through the synthesis of contrasting arguments (RAFTS). Upon input claims, RAFTS starts with evidence retrieval, where we design a retrieval pipeline to collect and re-rank relevant documents from verifiable sources. Then, RAFTS forms contrastive arguments (i.e., supporting or refuting) conditioned on the retrieved evidence. In addition, RAFTS leverages an embedding model to identify informative demonstrations, followed by in-context prompting to generate the prediction and explanation. Our method effectively retrieves relevant documents as evidence and evaluates arguments from varying perspectives, incorporating nuanced information for fine-grained decision-making. Combined with informative in-context examples as prior, RAFTS achieves significant improvements to supervised and LLM baselines without complex prompts. We demonstrate the effectiveness of our method through extensive experiments, where RAFTS can outperform GPT-based methods with a significantly smaller 7B LLM.</li>
<li><strong>摘要：</strong>虚假信息的快速传播对公众利益构成了重大风险。为了打击虚假信息，大型语言模型 (LLM) 被调整为自动验证声明的可信度。然而，现有方法严重依赖 LLM 中的嵌入知识和/或黑盒 API 来收集证据，导致使用较小的 LLM 或不可靠的上下文时性能不佳。在本文中，我们提出了通过对比论证综合 (RAFTS) 进行检索增强事实验证。在输入声明后，RAFTS 从证据检索开始，我们设计了一个检索管道来收集和重新排序来自可验证来源的相关文档。然后，RAFTS 根据检索到的证据形成对比论证（即支持或反驳）。此外，RAFTS 利用嵌入模型来识别信息丰富的演示，然后根据上下文提示生成预测和解释。我们的方法有效地检索相关文档作为证据，并从不同角度评估论点，结合细微差别的信息进行细粒度决策。结合之前提供的信息丰富的上下文示例，RAFTS 无需复杂提示即可显著改善监督和 LLM 基线。我们通过大量实验证明了我们方法的有效性，其中 RAFTS 可以以明显较小的 7B LLM 超越基于 GPT 的方法。</li>
</ul>

<h3>Title: Title:
          HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning</h3>
<ul>
<li><strong>Authors: </strong>Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, prompt</a></li>
<li><strong>Abstract: </strong>In modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering. However, handling long context sequences with LLMs is prohibitively costly due to the conventional attention mechanism's quadratic time and space complexity, and the context window size is limited by the GPU memory. Although recent works have proposed linear and sparse attention mechanisms to address this issue, their real-world applicability is often limited by the need to re-train pre-trained models. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which simultaneously reduces the training and inference time complexity from $O(T^2)$ to $O(T \log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To this end, we devise a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly. HiP is training-free as it only utilizes the pre-trained attention scores to spot the positions of the top-$k$ most significant elements for each query. Moreover, it ensures that no token is overlooked, unlike the sliding window-based sub-quadratic attention methods, such as StreamingLLM. Extensive experiments on diverse real-world benchmarks demonstrate that HiP significantly reduces prompt (i.e., prefill) and decoding latency and memory usage while maintaining high generation performance with little or no degradation. As HiP allows pretrained LLMs to scale to millions of tokens on commodity GPUs with no additional engineering due to its easy plug-and-play deployment, we believe that our work will have a large practical impact, opening up the possibility to many long-context LLM applications previously infeasible.</li>
<li><strong>摘要：</strong>在现代大型语言模型 (LLM) 中，增加序列长度是增强其在处理复杂任务（例如多模式问答）时的理解力和连贯性的关键挑战。然而，由于传统注意力机制的二次时间和空间复杂度，使用 LLM 处理长上下文序列的成本过高，并且上下文窗口大小受 GPU 内存限制。尽管最近的研究提出了线性和稀疏注意力机制来解决这个问题，但它们在现实世界中的适用性通常受到需要重新训练预训练模型的限制。为此，我们提出了一种新方法，即分层修剪注意力 (HiP)，它同时将训练和推理的时间复杂度从 $O(T^2)$ 降低到 $O(T \log T)$，并将空间复杂度从 $O(T^2)$ 降低到 $O(T)$。为此，我们设计了一种动态稀疏注意力机制，它通过一种新颖的树形搜索算法动态地为给定查询生成注意力掩码。 HiP 无需训练，因为它仅利用预训练的注意力得分来确定每个查询的前 $k$ 个最重要元素的位置。此外，它确保不会忽略任何标记，这与基于滑动窗口的次二次注意力方法（例如 StreamingLLM）不同。在各种实际基准上进行的大量实验表明，HiP 显著减少了提示（即预填充）和解码延迟以及内存使用量，同时保持了高生成性能，几乎没有性能下降。由于 HiP 易于即插即用，无需额外工程，因此允许预训练的 LLM 在商用 GPU 上扩展到数百万个标记，我们相信我们的工作将产生巨大的实际影响，为许多以前不可行长上下文 LLM 应用开辟了可能性。</li>
</ul>

<h3>Title: Title:
          Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Yeza Baihaqi, Angel García Contreras, Seiya Kawano, Koichiro Yoshino</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Rapport is known as a conversational aspect focusing on relationship building, which influences outcomes in collaborative tasks. This study aims to establish human-agent rapport through small talk by using a rapport-building strategy. We implemented this strategy for the virtual agents based on dialogue strategies by prompting a large language model (LLM). In particular, we utilized two dialogue strategies-predefined sequence and free-form-to guide the dialogue generation framework. We conducted analyses based on human evaluations, examining correlations between total turn, utterance characters, rapport score, and user experience variables: naturalness, satisfaction, interest, engagement, and usability. We investigated correlations between rapport score and naturalness, satisfaction, engagement, and conversation flow. Our experimental results also indicated that using free-form to prompt the rapport-building strategy performed the best in subjective scores.</li>
<li><strong>摘要：</strong>融洽关系是会话中注重关系建立的方面，它会影响协作任务的结果。本研究旨在通过使用融洽关系建立策略，通过闲聊建立人机融洽关系。我们通过提示大型语言模型 (LLM)，为基于对话策略的虚拟代理实施了此策略。具体来说，我们使用两种对话策略（预定义序列和自由形式）来指导对话生成框架。我们根据人工评估进行了分析，检查了总轮次、话语特征、融洽关系得分和用户体验变量（自然度、满意度、兴趣、参与度和可用性）之间的相关性。我们研究了融洽关系得分与自然度、满意度、参与度和对话流程之间的相关性。我们的实验结果还表明，使用自由形式来提示融洽关系建立策略在主观得分方面表现最佳。</li>
</ul>

<h3>Title: Title:
          3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position Encoding</h3>
<ul>
<li><strong>Authors: </strong>Xindian Ma, Wenyuan Liu, Peng Zhang, Nan Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position Encoding(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context</a></li>
<li><strong>Abstract: </strong>Inspired by the Bloch Sphere representation, we propose a novel rotary position encoding on a three-dimensional sphere, named 3D Rotary Position Encoding (3D-RPE). 3D-RPE is an advanced version of the widely used 2D Rotary Position Encoding (RoPE), with two major advantages for modeling long contexts: controllable long-term decay and improved position resolution. For controllable long-term decay, 3D-RPE allows for the regulation of long-term decay within the chunk size, ensuring the modeling of relative positional information between tokens at a distant relative position. For enhanced position resolution, 3D-RPE can mitigate the degradation of position resolution caused by position interpolation on RoPE. We have conducted experiments on long-context Natural Language Understanding (NLU) and long-sequence Language Modeling (LM) tasks. From the experimental results, 3D-RPE achieved performance improvements over RoPE, especially in long-context NLU tasks.</li>
<li><strong>摘要：</strong>受 Bloch 球体表示的启发，我们提出了一种新的三维球体旋转位置编码，称为 3D 旋转位置编码 (3D-RPE)。3D-RPE 是广泛使用的 2D 旋转位置编码 (RoPE) 的高级版本，在建模长上下文方面具有两大优势：可控的长期衰减和改进的位置分辨率。对于可控的长期衰减，3D-RPE 允许在块大小内调节长期衰减，确保对远距离相对位置的 token 之间的相对位置信息进行建模。对于增强的位置分辨率，3D-RPE 可以减轻 RoPE 上位置插值导致的位置分辨率下降。我们在长上下文自然语言理解 (NLU) 和长序列语言建模 (LM) 任务上进行了实验。从实验结果来看，3D-RPE 比 RoPE 实现了性能提升，尤其是在长上下文 NLU 任务中。</li>
</ul>

<h3>Title: Title:
          GEB-1.3B: Open Lightweight Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jie Wu, Yufeng Zhu, Lei Shen, Xuqing Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          GEB-1.3B: Open Lightweight Large Language Model(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Recently developed large language models (LLMs) such as ChatGPT, Claude, and Llama have demonstrated impressive abilities, and even surpass human-level performance in several tasks. Despite their success, the resource-intensive demands of these models, requiring significant computational power for both training and inference, limit their deployment to high-performance servers. Additionally, the extensive calculation requirements of the models often lead to increased latency in response times. With the increasing need for LLMs to operate efficiently on CPUs, research about lightweight models that are optimized for CPU inference has emerged. In this work, we introduce GEB-1.3B, a lightweight LLM trained on 550 billion tokens in both Chinese and English languages. We employ novel training techniques, including ROPE, Group-Query-Attention, and FlashAttention-2, to accelerate training while maintaining model performance. Additionally, we fine-tune the model using 10 million samples of instruction data to enhance alignment. GEB-1.3B exhibits outstanding performance on general benchmarks such as MMLU, C-Eval, and CMMLU, outperforming comparative models such as MindLLM-1.3B and TinyLLaMA-1.1B. Notably, the FP32 version of GEB-1.3B achieves commendable inference times on CPUs, with ongoing efforts to further enhance speed through advanced quantization techniques. The release of GEB-1.3B as an open-source model marks a significant contribution to the development of lightweight LLMs, promising to foster further research and innovation in the field.</li>
<li><strong>摘要：</strong>最近开发的大型语言模型 (LLM)，如 ChatGPT、Claude 和 Llama，已经展现出令人印象深刻的能力，甚至在多项任务中超越了人类水平的表现。尽管它们取得了成功，但这些模型对资源的需求（训练和推理都需要大量计算能力）限制了它们只能部署在高性能服务器上。此外，模型的大量计算要求通常会导致响应时间延迟增加。随着 LLM 在 CPU 上高效运行的需求不断增加，针对 CPU 推理优化的轻量级模型的研究也应运而生。在这项工作中，我们介绍了 GEB-1.3B，这是一个轻量级的 LLM，使用 5500 亿个中文和英文标记进行训练。我们采用了新颖的训练技术，包括 ROPE、Group-Query-Attention 和 FlashAttention-2，以加速训练，同时保持模型性能。此外，我们使用 1000 万个指令数据样本对模型进行微调，以增强对齐能力。 GEB-1.3B 在 MMLU、C-Eval 和 CMMLU 等通用基准测试中表现出色，优于 MindLLM-1.3B 和 TinyLLaMA-1.1B 等比较模型。值得注意的是，GEB-1.3B 的 FP32 版本在 CPU 上实现了值得称赞的推理时间，并且正在通过先进的量化技术进一步提高速度。GEB-1.3B 作为开源模型的发布标志着对轻量级 LLM 开发的重大贡献，有望促进该领域的进一步研究和创新。</li>
</ul>

<h3>Title: Title:
          Knowledge Editing in Language Models via Adapted Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Knowledge Editing in Language Models via Adapted Direct Preference Optimization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can become outdated over time as they may lack updated world knowledge, leading to factual knowledge errors and gaps. Knowledge Editing (KE) aims to overcome this challenge using weight updates that do not require expensive retraining. We propose treating KE as an LLM alignment problem. Toward this goal, we introduce Knowledge Direct Preference Optimization (KDPO), a variation of the Direct Preference Optimization (DPO) that is more effective for knowledge modifications. Our method is based on an online approach that continually updates the knowledge stored in the model. We use the current knowledge as a negative sample and the new knowledge we want to introduce as a positive sample in a process called DPO. We also use teacher-forcing for negative sample generation and optimize using the positive sample, which helps maintain localized changes. We tested our KE method on various datasets and models, comparing it to several cutting-edge methods, with 100 and 500 sequential edits. Additionally, we conducted an ablation study comparing our method to the standard DPO approach. Our experimental results show that our modified DPO method allows for more refined KE, achieving similar or better performance compared to previous methods.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可能会随着时间的推移而过时，因为它们可能缺乏更新的世界知识，从而导致事实知识错误和差距。知识编辑 (KE) 旨在使用不需要昂贵再训练的权重更新来克服这一挑战。我们建议将 KE 视为 LLM 对齐问题。为了实现这一目标，我们引入了知识直接偏好优化 (KDPO)，这是直接偏好优化 (DPO) 的一种变体，对知识修改更有效。我们的方法基于一种在线方法，该方法不断更新存储在模型中的知识。我们将当前知识用作负样本，将我们想要引入的新知识用作正样本，这个过程称为 DPO。我们还使用教师强制来生成负样本并使用正样本进行优化，这有助于保持局部变化。我们在各种数据集和模型上测试了我们的 KE 方法，将其与几种前沿方法进行了比较，并进行了 100 和 500 次连续编辑。此外，我们还进行了一项消融研究，将我们的方法与标准 DPO 方法进行了比较。我们的实验结果表明，我们改进的 DPO 方法可以实现更精细的 KE，实现与以前的方法相似或更好的性能。</li>
</ul>

<h3>Title: Title:
          CliBench: Multifaceted Evaluation of Large Language Models in Clinical Decisions on Diagnoses, Procedures, Lab Tests Orders and Prescriptions</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Derek Ma, Chenchen Ye, Yu Yan, Xiaoxuan Wang, Peipei Ping, Timothy S Chang, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CliBench: Multifaceted Evaluation of Large Language Models in Clinical Decisions on Diagnoses, Procedures, Lab Tests Orders and Prescriptions(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The integration of Artificial Intelligence (AI), especially Large Language Models (LLMs), into the clinical diagnosis process offers significant potential to improve the efficiency and accessibility of medical care. While LLMs have shown some promise in the medical domain, their application in clinical diagnosis remains underexplored, especially in real-world clinical practice, where highly sophisticated, patient-specific decisions need to be made. Current evaluations of LLMs in this field are often narrow in scope, focusing on specific diseases or specialties and employing simplified diagnostic tasks. To bridge this gap, we introduce CliBench, a novel benchmark developed from the MIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs' capabilities in clinical diagnosis. This benchmark not only covers diagnoses from a diverse range of medical cases across various specialties but also incorporates tasks of clinical significance: treatment procedure identification, lab test ordering and medication prescriptions. Supported by structured output ontologies, CliBench enables a precise and multi-granular evaluation, offering an in-depth understanding of LLM's capability on diverse clinical tasks of desired granularity. We conduct a zero-shot evaluation of leading LLMs to assess their proficiency in clinical decision-making. Our preliminary results shed light on the potential and limitations of current LLMs in clinical settings, providing valuable insights for future advancements in LLM-powered healthcare.</li>
<li><strong>摘要：</strong>将人工智能 (AI)，尤其是大型语言模型 (LLM)，整合到临床诊断过程中，为提高医疗效率和可及性提供了巨大的潜力。虽然 LLM 在医学领域显示出一些前景，但它们在临床诊断中的应用仍未得到充分探索，尤其是在现实世界的临床实践中，需要做出高度复杂的、针对患者的决策。目前对该领域 LLM 的评估范围往往很窄，侧重于特定疾病或专业，并采用简化的诊断任务。为了弥补这一差距，我们引入了 CliBench，这是一种从 MIMIC IV 数据集开发的新基准，可对 LLM 在临床诊断方面的能力进行全面而现实的评估。该基准不仅涵盖了来自不同专业的各种医疗案例的诊断，还包含具有临床意义的任务：治疗程序识别、实验室测试订购和药物处方。在结构化输出本体的支持下，CliBench 可以进行精确和多粒度的评估，深入了解 LLM 在所需粒度的各种临床任务上的能力。我们对领先的 LLM 进行零样本评估，以评估他们在临床决策方面的能力。我们的初步结果揭示了当前 LLM 在临床环境中的潜力和局限性，为 LLM 驱动的医疗保健的未来发展提供了宝贵的见解。</li>
</ul>

<h3>Title: Title:
          Experiments in News Bias Detection with Pre-Trained Neural Transformers</h3>
<ul>
<li><strong>Authors: </strong>Tim Menzner, Jochen L. Leidner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Experiments in News Bias Detection with Pre-Trained Neural Transformers(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The World Wide Web provides unrivalled access to information globally, including factual news reporting and commentary. However, state actors and commercial players increasingly spread biased (distorted) or fake (non-factual) information to promote their agendas. We compare several large, pre-trained language models on the task of sentence-level news bias detection and sub-type classification, providing quantitative and qualitative results.</li>
<li><strong>摘要：</strong>万维网提供了无与伦比的全球信息访问渠道，包括事实新闻报道和评论。然而，国家行为者和商业参与者越来越多地传播有偏见（扭曲）或虚假（非事实）的信息来推广他们的议程。我们在句子级新闻偏见检测和子类型分类任务上比较了几个大型预训练语言模型，提供了定量和定性结果。</li>
</ul>

<h3>Title: Title:
          BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages</h3>
<ul>
<li><strong>Authors: </strong>Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew Ali Ayele, Víctor Gutiérrez-Basulto, Yazmín Ibáñez-García, Hwaran Lee, Shamsuddeen Hassan Muhammad, Kiwoong Park, Anar Sabuhi Rzayev, Nina White, Seid Muhie Yimam, Mohammad Taher Pilehvar, Nedjma Ousidhoum, Jose Camacho-Collados, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often lack culture-specific knowledge of daily life, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs' cultural sensitivities are limited to a single language or collected from online sources such as Wikipedia, which do not reflect the mundane everyday lifestyles of diverse regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play, or the sports they practice in school is common cultural knowledge but uncommon in easily collected online sources, especially for underrepresented cultures. To address this issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate LLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijani, Hausa, and Sundanese. We construct the benchmark to include two formats of questions: short-answer and multiple-choice. We show that LLMs perform better for cultures that are highly represented online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format. For cultures represented by mid-to-high-resource languages, LLMs perform better in their local languages, but for cultures represented by low-resource languages, LLMs perform better in English than the local languages. We make our dataset publicly available at: this https URL.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常缺乏针对日常生活的文化知识，尤其是在不同地区和非英语语言中。现有的用于评估 LLM 文化敏感性的基准仅限于单一语言或从 Wikipedia 等在线资源收集，这些资源无法反映不同地区平凡的日常生活方式。也就是说，关于人们生日庆祝时吃的食物、他们通常使用的香料、年轻人演奏的乐器或他们在学校练习的运动的信息是常见的文化知识，但在容易收集的在线资源中并不常见，尤其是对于代表性不足的文化。为了解决这个问题，我们引入了 BLEnD，这是一个手工制作的基准，旨在评估 LLM 在不同文化和语言中的日常知识。BLEnD 包含来自 16 个国家/地区的 52.6k 个问答对，使用 13 种不同的语言，包括阿姆哈拉语、阿萨姆语、阿塞拜疆语、豪萨语和巽他语等资源匮乏的语言。我们构建了基准，包括两种问题格式：简答题和多项选择题。我们表明，LLM 在网上高度代表性的文化中表现更好，在简答题格式中，表现最佳的模型 GPT-4 的最大差异为 57.34%。对于由中高资源语言代表的文化，LLM 在其当地语言中表现更好，但对于由低资源语言代表的文化，LLM 在英语中的表现优于当地语言。我们将我们的数据集公开发布在此：此 https URL。</li>
</ul>

<h3>Title: Title:
          A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization</h3>
<ul>
<li><strong>Authors: </strong>KuanChao Chu, Yi-Pei Chen, Hideki Nakayama</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>This research investigates prompt designs of evaluating generated texts using large language models (LLMs). While LLMs are increasingly used for scoring various inputs, creating effective prompts for open-ended text evaluation remains challenging due to model sensitivity and subjectivity in evaluation of text generation. Our study experimented with different prompt structures, altering the sequence of output instructions and including explanatory reasons. We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a different level of rule understanding in the prompt. An additional optimization may enhance scoring alignment if sufficient data is available. This insight is crucial for improving the accuracy and consistency of LLM-based evaluations.</li>
<li><strong>摘要：</strong>本研究调查了使用大型语言模型 (LLM) 评估生成文本的提示设计。虽然 LLM 越来越多地用于对各种输入进行评分，但由于模型敏感性和文本生成评估的主观性，创建有效的开放式文本评估提示仍然具有挑战性。我们的研究尝试了不同的提示结构，改变了输出指令的顺序并加入了解释性原因。我们发现，呈现原因和分数的顺序会显著影响 LLM 的评分，提示中的规则理解程度也不同。如果有足够的数据，额外的优化可能会增强评分一致性。这一见解对于提高基于 LLM 的评估的准确性和一致性至关重要。</li>
</ul>

<h3>Title: Title:
          HIRO: Hierarchical Information Retrieval Optimization</h3>
<ul>
<li><strong>Authors: </strong>Krish Goel, Mahek Chandak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          HIRO: Hierarchical Information Retrieval Optimization(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in natural language tasks but face limitations due to static training datasets, resulting in outdated or contextually shallow responses. Retrieval-Augmented Generation (RAG) addresses this by integrating real-time external knowledge, enhancing model accuracy and credibility, especially for knowledge-intensive tasks. However, RAG-enhanced LLMs struggle with long contexts, causing them to "choke" on information overload, compromising response quality. Recent RAG applications use hierarchical data structures for storing documents, organized at various levels of summarization and information density. In this context, we introduce HIRO (Hierarchical Information Retrieval Optimization), a novel querying approach for RAG applications using hierarchical structures for storing documents. HIRO employs DFS-based recursive similarity score calculation and branch pruning to minimize the context returned to the LLM without informational loss. HIRO outperforms existing querying mechanisms on the NarrativeQA dataset by an absolute performance gain of 10.85%.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在自然语言任务中表现出色，但由于静态训练数据集而受到限制，导致响应过时或上下文浅薄。检索增强生成 (RAG) 通过集成实时外部知识来解决此问题，提高模型准确性和可信度，尤其是对于知识密集型任务。然而，RAG 增强型 LLM 难以处理长上下文，导致它们因信息过载而“窒息”，从而影响响应质量。最近的 RAG 应用程序使用分层数据结构来存储文档，这些文档按不同级别的摘要和信息密度组织。在此背景下，我们引入了 HIRO（分层信息检索优化），这是一种使用分层结构存储文档的 RAG 应用程序的新型查询方法。HIRO 采用基于 DFS 的递归相似度得分计算和分支修剪来最小化返回到 LLM 的上下文，而不会丢失信息。HIRO 在 NarrativeQA 数据集上的表现优于现有的查询机制，绝对性能提升了 10.85%。</li>
</ul>

<h3>Title: Title:
          Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models</h3>
<ul>
<li><strong>Authors: </strong>Manas Jhalani, Annervaz K M, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In the realm of multimodal tasks, Visual Question Answering (VQA) plays a crucial role by addressing natural language questions grounded in visual content. Knowledge-Based Visual Question Answering (KBVQA) advances this concept by adding external knowledge along with images to respond to questions. We introduce an approach for KBVQA, augmenting the existing vision-language transformer encoder-decoder (OFA) model. Our main contribution involves enhancing questions by incorporating relevant external knowledge extracted from knowledge graphs, using a dynamic triple extraction method. We supply a flexible number of triples from the knowledge graph as context, tailored to meet the requirements for answering the question. Our model, enriched with knowledge, demonstrates an average improvement of 4.75\% in Exact Match Score over the state-of-the-art on three different KBVQA datasets. Through experiments and analysis, we demonstrate that furnishing variable triples for each question improves the reasoning capabilities of the language model in contrast to supplying a fixed number of triples. This is illustrated even for recent large language models. Additionally, we highlight the model's generalization capability by showcasing its SOTA-beating performance on a small dataset, achieved through straightforward fine-tuning.</li>
<li><strong>摘要：</strong>在多模态任务领域，视觉问答 (VQA) 通过解决基于视觉内容的自然语言问题发挥着至关重要的作用。基于知识的视觉问答 (KBVQA) 通过添加外部知识和图像来回答问题，推进了这一概念。我们介绍了一种 KBVQA 方法，增强了现有的视觉语言转换器编码器-解码器 (OFA) 模型。我们的主要贡献是通过使用动态三元组提取方法，结合从知识图谱中提取的相关外部知识来增强问题。我们从知识图谱中提供灵活数量的三元组作为上下文，以满足回答问题的要求。我们的模型富含知识，在三个不同的 KBVQA 数据集上，精确匹配得分比最先进的模型平均提高了 4.75%。通过实验和分析，我们证明，与提供固定数量的三元组相比，为每个问题提供可变的三元组可以提高语言模型的推理能力。即使是最近的大型语言模型也证明了这一点。此外，我们通过简单的微调展示模型在小型数据集上的 SOTA 超越性能，突出了模型的泛化能力。</li>
</ul>

<h3>Title: Title:
          FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain</h3>
<ul>
<li><strong>Authors: </strong>Jin Liu, Steffen Thoma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>This paper describes the inference system of FZI-WIM at the SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system utilizes the chain of thought (CoT) paradigm to tackle this complex reasoning problem and further improves the CoT performance with self-consistency. Instead of greedy decoding, we sample multiple reasoning chains with the same prompt and make the final verification with majority voting. The self-consistent CoT system achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90 (3rd), and consistency score of 0.73 (12th). We release the code and data publicly this https URL.</li>
<li><strong>摘要：</strong>本文介绍了 SemEval-2024 任务 2：临床试验的安全生物医学自然语言推理中的 FZI-WIM 推理系统。我们的系统利用思路链 (CoT) 范式来解决这个复杂的推理问题，并通过自洽性进一步提高了 CoT 性能。我们没有使用贪婪解码，而是对具有相同提示的多个推理链进行采样，并通过多数表决进行最终验证。自洽的 CoT 系统实现了 0.80（第 1 名）的基线 F1 得分、0.90（第 3 名）的忠诚度得分和 0.73（第 12 名）的一致性得分。我们通过此 https URL 公开发布了代码和数据。</li>
</ul>

<h3>Title: Title:
          Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoman Wang, Claudio Fantinuoli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Assessing the performance of interpreting services is a complex task, given the nuanced nature of spoken language translation, the strategies that interpreters apply, and the diverse expectations of users. The complexity of this task become even more pronounced when automated evaluation methods are applied. This is particularly true because interpreted texts exhibit less linearity between the source and target languages due to the strategies employed by the interpreter. This study aims to assess the reliability of automatic metrics in evaluating simultaneous interpretations by analyzing their correlation with human evaluations. We focus on a particular feature of interpretation quality, namely translation accuracy or faithfulness. As a benchmark we use human assessments performed by language experts, and evaluate how well sentence embeddings and Large Language Models correlate with them. We quantify semantic similarity between the source and translated texts without relying on a reference translation. The results suggest GPT models, particularly GPT-3.5 with direct prompting, demonstrate the strongest correlation with human judgment in terms of semantic similarity between source and target texts, even when evaluating short textual segments. Additionally, the study reveals that the size of the context window has a notable impact on this correlation.</li>
<li><strong>摘要：</strong>鉴于口译的微妙性、口译员所采用的策略以及用户的不同期望，评估口译服务的表现是一项复杂的任务。当应用自动评估方法时，这项任务的复杂性变得更加明显。由于口译员所采用的策略，口译文本在源语言和目标语言之间表现出较少的线性，因此尤其如此。本研究旨在通过分析自动指标与人工评估的相关性来评估自动指标在评估同声传译方面的可靠性。我们关注口译质量的一个特定特征，即翻译准确性或忠实度。作为基准，我们使用语言专家进行的人工评估，并评估句子嵌入和大型语言模型与它们的相关性。我们在不依赖参考翻译的情况下量化源文本和翻译文本之间的语义相似性。结果表明，GPT 模型（尤其是具有直接提示的 GPT-3.5）在源文本和目标文本之间的语义相似性方面表现出与人类判断最强的相关性，即使在评估短文本片段时也是如此。此外，研究表明，上下文窗口的大小对这种相关性有显着影响。</li>
</ul>

<h3>Title: Title:
          Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Li, Yixuan Tang, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities across various tasks but still face challenges such as hallucinations. One potential reason for hallucinations is the lack of relevant knowledge or context. Thus, a promising solution to mitigate this issue involves instructing LLMs to respond with "I do not know" when a question falls outside their knowledge domain or the provided context. However, in this work, we observed that LLMs struggle to admit their lack of knowledge, primarily due to existing instruction datasets designed to encourage specific answers. To improve large language models' capability to recognize the boundaries of their knowledge, we propose a novel approach called uncertainty-sensitive tuning. This method involves two-stage training designed for uncertainty recognition and prompt-sensitive activation. In the first stage, we guide the LLM to reject unknown questions. In the second stage, we recover the decreased performance in QA tasks by incorporating designed causal instructions. By leveraging this method, we aim to enhance the model's ability to identify areas of uncertainty. The experimental results demonstrate that our proposed uncertainty-sensitive tuning method significantly improves the performance of the Llama2-chat-7B model. Specifically, it achieves a substantial 34.7% improvement in handling questions involving knowledge gaps compared to the original model. Moreover, our approach outperforms GPT-4, exhibiting a 9.4% increase in overall performance. We open-source the model and code on GitHub.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种任务中都表现出了卓越的能力，但仍然面临着幻觉等挑战。幻觉的一个潜在原因是缺乏相关知识或背景。因此，缓解此问题的一个有希望的解决方案是指示 LLM 在问题超出其知识领域或提供的上下文时回答“我不知道”。然而，在这项工作中，我们观察到 LLM 很难承认他们缺乏知识，这主要是由于现有的指令数据集旨在鼓励具体答案。为了提高大型语言模型识别知识边界的能力，我们提出了一种称为不确定性敏感调整的新方法。该方法涉及两阶段训练，旨在识别不确定性和提示敏感激活。在第一阶段，我们指导 LLM 拒绝未知问题。在第二阶段，我们通过结合设计的因果指令来恢复 QA 任务中下降的性能。通过利用这种方法，我们旨在增强模型识别不确定性领域的能力。实验结果表明，我们提出的不确定性敏感调整方法显着提高了 Llama2-chat-7B 模型的性能。具体来说，与原始模型相比，它在处理涉及知识差距的问题方面取得了 34.7% 的显著提升。此外，我们的方法优于 GPT-4，整体性能提高了 9.4%。我们在 GitHub 上开源了该模型和代码。</li>
</ul>

<h3>Title: Title:
          The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yan Liu, Yu Liu, Xiaokang Chen, Pin-Yu Chen, Daoguang Zan, Min-Yen Kan, Tsung-Yi Ho</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Pre-trained Language models (PLMs) have been acknowledged to contain harmful information, such as social biases, which may cause negative social impacts or even bring catastrophic results in application. Previous works on this problem mainly focused on using black-box methods such as probing to detect and quantify social biases in PLMs by observing model outputs. As a result, previous debiasing methods mainly finetune or even pre-train language models on newly constructed anti-stereotypical datasets, which are high-cost. In this work, we try to unveil the mystery of social bias inside language models by introducing the concept of {\sc Social Bias Neurons}. Specifically, we propose {\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e., neurons) in a language model that can be attributed to undesirable behavior, such as social bias. By formalizing undesirable behavior as a distributional property of language, we employ sentiment-bearing prompts to elicit classes of sensitive words (demographics) correlated with such sentiments. Our IG$^2$ thus attributes the uneven distribution for different demographics to specific Social Bias Neurons, which track the trail of unwanted behavior inside PLM units to achieve interoperability. Moreover, derived from our interpretable technique, {\sc Bias Neuron Suppression (BNS)} is further proposed to mitigate social biases. By studying BERT, RoBERTa, and their attributable differences from debiased FairBERTa, IG$^2$ allows us to locate and suppress identified neurons, and further mitigate undesired behaviors. As measured by prior metrics from StereoSet, our model achieves a higher degree of fairness while maintaining language modeling ability with low cost.</li>
<li><strong>摘要：</strong>人们已经认识到，预训练语言模型 (PLM) 包含有害信息，例如社会偏见，这可能会在应用中造成负面的社会影响，甚至带来灾难性的后果。之前关于这个问题的研究主要集中在使用黑盒方法（例如探测）通过观察模型输出来检测和量化 PLM 中的社会偏见。因此，以前的去偏方法主要在新建的反刻板数据集上微调甚至预训练语言模型，这些方法成本很高。在这项工作中，我们试图通过引入“社会偏见神经元”的概念来揭开语言模型内部社会偏见的神秘面纱。具体来说，我们提出了“集成间隙梯度 (IG$^2$)”来准确定位语言模型中可归因于不良行为（例如社会偏见）的单元（即神经元）。通过将不良行为形式化为语言的分布属性，我们使用带有情绪的提示来引出与此类情绪相关的敏感词类别（人口统计数据）。因此，我们的 IG$^2$ 将不同人口统计数据的不均匀分布归因于特定的社会偏见神经元，这些神经元会跟踪 PLM 单元内部不良行为的踪迹，以实现互操作性。此外，从我们的可解释技术中衍生出来的偏见神经元抑制 (BNS) 被进一步提出来以减轻社会偏见。通过研究 BERT、RoBERTa 及其与去偏 FairBERTa 的可归因差异，IG$^2$ 使我们能够定位和抑制已识别的神经元，并进一步减轻不良行为。通过 StereoSet 先前的指标衡量，我们的模型实现了更高的公平性，同时以低成本保持了语言建模能力。</li>
</ul>

<h3>Title: Title:
          Evaluation of Large Language Models: STEM education and Gender Stereotypes</h3>
<ul>
<li><strong>Authors: </strong>Smilla Due, Sneha Das, Marianne Andersen, Berta Plandolit López, Sniff Andersen Nexø, Line Clemmensen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Evaluation of Large Language Models: STEM education and Gender Stereotypes(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have an increasing impact on our lives with use cases such as chatbots, study support, coding support, ideation, writing assistance, and more. Previous studies have revealed linguistic biases in pronouns used to describe professions or adjectives used to describe men vs women. These issues have to some degree been addressed in updated LLM versions, at least to pass existing tests. However, biases may still be present in the models, and repeated use of gender stereotypical language may reinforce the underlying assumptions and are therefore important to examine further. This paper investigates gender biases in LLMs in relation to educational choices through an open-ended, true to user-case experimental design and a quantitative analysis. We investigate the biases in the context of four different cultures, languages, and educational systems (English/US/UK, Danish/DK, Catalan/ES, and Hindi/IN) for ages ranging from 10 to 16 years, corresponding to important educational transition points in the different countries. We find that there are significant and large differences in the ratio of STEM to non-STEM suggested education paths provided by chatGPT when using typical girl vs boy names to prompt lists of suggested things to become. There are generally fewer STEM suggestions in the Danish, Spanish, and Indian context compared to the English. We also find subtle differences in the suggested professions, which we categorise and report.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 对我们的生活产生了越来越大的影响，其用例包括聊天机器人、学习支持、编码支持、构思、写作辅助等。先前的研究表明，用于描述职业的代词或用于描述男性与女性的形容词存在语言偏见。这些问题在更新的 LLM 版本中得到了一定程度的解决，至少通过了现有的测试。然而，模型中可能仍然存在偏见，反复使用性别刻板语言可能会强化潜在的假设，因此有必要进一步研究。本文通过开放式、忠实于用户案例的实验设计和定量分析，研究了 LLM 中与教育选择相关的性别偏见。我们研究了四种不同文化、语言和教育体系（英语/美国/英国、丹麦/丹麦、加泰罗尼亚语/西班牙和印地语/印度）背景下的偏见，年龄范围从 10 岁到 16 岁，对应于不同国家的重要教育转折点。我们发现，当使用典型的女孩和男孩名字来提示建议成为的职业列表时，chatGPT 提供的 STEM 与非 STEM 建议教育路径的比例存在显著差异。与英语相比，丹麦语、西班牙语和印度语环境中的 STEM 建议通常较少。我们还发现建议的职业存在细微差异，我们对此进行了分类和报告。</li>
</ul>

<h3>Title: Title:
          BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack</h3>
<ul>
<li><strong>Authors: </strong>Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20\% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60\% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers, enabling the processing of lengths up to 11 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 1 million token lengths.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 的输入上下文大小急剧增加。然而，现有的评估方法却没有跟上步伐，无法全面评估模型处理长上下文的效率。为了弥补这一差距，我们引入了 BABILong 基准，旨在测试语言模型对分布在极长文档中的事实进行推理的能力。BABILong 包括一组 20 个不同的推理任务，包括事实链、简单归纳、演绎、计数和处理列表/集合。这些任务本身就很有挑战性，当所需事实分散在长自然文本中时，难度就更大了。我们的评估表明，流行的 LLM 仅有效利用了 10-20\% 的上下文，并且随着推理复杂性的增加，其性能急剧下降。在上下文推理的替代方案中，检索增强生成方法在单事实问答中实现了 60\% 的适度准确率，与上下文长度无关。在上下文扩展方法中，循环记忆转换器的性能最高，能够处理长度高达 1100 万个标记。BABILong 基准可扩展到任意长度，以支持评估即将推出的、功能更强大的新模型，并且我们提供长度高达 100 万个标记的分割。</li>
</ul>

<h3>Title: Title:
          Datasets for Multilingual Answer Sentence Selection</h3>
<ul>
<li><strong>Authors: </strong>Matteo Gabburo, Stefano Campese, Federico Agostini, Alessandro Moschitti</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Datasets for Multilingual Answer Sentence Selection(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Answer Sentence Selection (AS2) is a critical task for designing effective retrieval-based Question Answering (QA) systems. Most advancements in AS2 focus on English due to the scarcity of annotated datasets for other languages. This lack of resources prevents the training of effective AS2 models in different languages, creating a performance gap between QA systems in English and other locales. In this paper, we introduce new high-quality datasets for AS2 in five European languages (French, German, Italian, Portuguese, and Spanish), obtained through supervised Automatic Machine Translation (AMT) of existing English AS2 datasets such as ASNQ, WikiQA, and TREC-QA using a Large Language Model (LLM). We evaluated our approach and the quality of the translated datasets through multiple experiments with different Transformer architectures. The results indicate that our datasets are pivotal in producing robust and powerful multilingual AS2 models, significantly contributing to closing the performance gap between English and other languages.</li>
<li><strong>摘要：</strong>答案句选择 (AS2) 是设计有效的基于检索的问答 (QA) 系统的关键任务。由于其他语言的带注释数据集稀缺，AS2 的大多数进步都集中在英语上。这种资源的缺乏阻碍了对不同语言的有效 AS2 模型进行训练，导致英语和其他语言环境的 QA 系统之间存在性能差距。在本文中，我们引入了五种欧洲语言（法语、德语、意大利语、葡萄牙语和西班牙语）的 AS2 新高质量数据集，这些数据集是通过使用大型语言模型 (LLM) 对现有英语 AS2 数据集（如 ASNQ、WikiQA 和 TREC-QA）进行监督自动机器翻译 (AMT) 获得的。我们通过使用不同的 Transformer 架构进行多次实验来评估我们的方法和翻译数据集的质量。结果表明，我们的数据集对于生成强大而强大的多语言 AS2 模型至关重要，对缩小英语与其他语言之间的性能差距做出了重大贡献。</li>
</ul>

<h3>Title: Title:
          IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Ding, Weiqi Wang, Sze Heng Douglas Kwok, Minghao Liu, Tianqing Fang, Jiaxin Bai, Junxian He, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Enhancing Language Models' (LMs) ability to understand purchase intentions in E-commerce scenarios is crucial for their effective assistance in various downstream tasks. However, previous approaches that distill intentions from LMs often fail to generate meaningful and human-centric intentions applicable in real-world E-commerce contexts. This raises concerns about the true comprehension and utilization of purchase intentions by LMs. In this paper, we present IntentionQA, a double-task multiple-choice question answering benchmark to evaluate LMs' comprehension of purchase intentions in E-commerce. Specifically, LMs are tasked to infer intentions based on purchased products and utilize them to predict additional purchases. IntentionQA consists of 4,360 carefully curated problems across three difficulty levels, constructed using an automated pipeline to ensure scalability on large E-commerce platforms. Human evaluations demonstrate the high quality and low false-negative rate of our benchmark. Extensive experiments across 19 language models show that they still struggle with certain scenarios, such as understanding products and intentions accurately, jointly reasoning with products and intentions, and more, in which they fall far behind human performances. Our code and data are publicly available at this https URL.</li>
<li><strong>摘要：</strong>增强语言模型 (LM) 理解电子商务场景中购买意图的能力对于其有效协助各种下游任务至关重要。然而，以前从 LM 中提取意图的方法往往无法生成适用于现实世界电子商务环境的有意义的、以人为本的意图。这引发了人们对 LM 对购买意图的真正理解和利用的担忧。在本文中，我们提出了 IntentionQA，这是一个双任务多项选择题回答基准，用于评估 LM 对电子商务中购买意图的理解。具体来说，LM 的任务是根据购买的产品推断意图并利用它们来预测额外的购买。IntentionQA 包含 4,360 个精心策划的问题，涵盖三个难度级别，使用自动化管道构建，以确保在大型电子商务平台上的可扩展性。人工评估证明了我们的基准的高质量和低假阴性率。对 19 种语言模型进行的大量实验表明，它们在某些场景下仍存在困难，例如准确理解产品和意图、联合推理产品和意图等，在这些场景下它们的表现远远落后于人类。我们的代码和数据在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Title:
          Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for Beat-Aligned Poetry Generation</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Elzohbi, Richard Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for Beat-Aligned Poetry Generation(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The intersection between poetry and music provides an interesting case for computational creativity, yet remains relatively unexplored. This paper explores the integration of poetry and music through the lens of beat patterns, investigating whether a byte-based language model can generate words that fit specific beat patterns within the context of poetry. Drawing on earlier studies, we developed a method to train a byte-based transformer model, ByT5, to align poems with beat patterns. The results demonstrate a high level of beat alignment while maintaining semantic coherence. Future work will aim to improve the model's ability to create complete beat-aligned poems.</li>
<li><strong>摘要：</strong>诗歌和音乐之间的交集为计算创造力提供了一个有趣的案例，但仍相对未被探索。本文通过节拍模式的视角探索了诗歌和音乐的融合，研究了基于字节的语言模型是否可以生成符合诗歌语境中特定节拍模式的单词。借鉴早期研究，我们开发了一种方法来训练基于字节的转换器模型 ByT5，以将诗歌与节拍模式对齐。结果显示，在保持语义连贯性的同时，节拍对齐水平很高。未来的工作将旨在提高模型创作完整节拍对齐诗歌的能力。</li>
</ul>

<h3>Title: Title:
          CHIRON: Rich Character Representations in Long-Form Narratives</h3>
<ul>
<li><strong>Authors: </strong>Alexander Gurung, Mirella Lapata</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          CHIRON: Rich Character Representations in Long-Form Narratives(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Characters are integral to long-form narratives, but are poorly understood by existing story analysis and generation systems. While prior work has simplified characters via graph-based methods and brief character descriptions, we aim to better tackle the problem of representing complex characters by taking inspiration from advice given to professional writers. We propose CHIRON, a new `character sheet' based representation that organizes and filters textual information about characters. We construct CHIRON sheets in two steps: a Generation Module that prompts an LLM for character information via question-answering and a Validation Module that uses automated reasoning and a domain-specific entailment model to eliminate false facts about a character. We validate CHIRON via the downstream task of masked-character prediction, where our experiments show CHIRON is better and more flexible than comparable summary-based baselines. We also show that metrics derived from CHIRON can be used to automatically infer character-centricity in stories, and that these metrics align with human judgments.</li>
<li><strong>摘要：</strong>人物是长篇叙事不可或缺的一部分，但现有的故事分析和生成系统对人物的理解却很差。虽然先前的工作已经通过基于图形的方法和简短的人物描述简化了人物，但我们的目标是通过从专业作家的建议中汲取灵感，更好地解决复杂人物的表示问题。我们提出了 CHIRON，这是一种基于“人物表”的新表示，可以组织和过滤有关人物的文本信息。我们分两步构建 CHIRON 表：生成模块通过问答提示 LLM 提供人物信息；验证模块使用自动推理和领域特定蕴涵模型来消除有关人物的错误事实。我们通过下游任务屏蔽人物预测来验证 CHIRON，我们的实验表明 CHIRON 比同类基于摘要的基线更好、更灵活。我们还表明，从 CHIRON 得出的指标可用于自动推断故事中的人物中心性，并且这些指标与人类判断一致。</li>
</ul>

<h3>Title: Title:
          A Fundamental Trade-off in Aligned Language Models and its Relation to Sampling Adaptors</h3>
<ul>
<li><strong>Authors: </strong>Naaman Tan, Josef Valvoda, Anej Svete, Tianyu Liu, Yanxia Qin, Kan Min-Yen, Ryan Cotterell</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          A Fundamental Trade-off in Aligned Language Models and its Relation to Sampling Adaptors(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The relationship between the quality of a string and its probability $p(\boldsymbol{y})$ under a language model has been influential in the development of techniques to build good text generation systems. For example, several decoding algorithms have been motivated to manipulate $p(\boldsymbol{y})$ to produce higher-quality text. In this work, we examine the probability--quality relationship in language models explicitly aligned to human preferences, e.g., through Reinforcement Learning through Human Feedback (RLHF). We find that, given a general language model and its aligned version, for corpora sampled from an aligned language model, there exists a trade-off between the average reward and average log-likelihood of the strings under the general language model. We provide a formal treatment of this issue and demonstrate how a choice of sampling adaptor allows for a selection of how much likelihood we exchange for the reward.</li>
<li><strong>摘要：</strong>字符串的质量与其在语言模型下的概率 $p(\boldsymbol{y})$ 之间的关系对构建良好文本生成系统的技术开发具有重要影响。例如，有几种解码算法被激励去操纵 $p(\boldsymbol{y})$ 以生成更高质量的文本。在这项工作中，我们研究了与人类偏好明确一致的语言模型中的概率-质量关系，例如通过人类反馈强化学习 (RLHF)。我们发现，给定一个通用语言模型及其对齐版本，对于从对齐语言模型中采样的语料库，在通用语言模型下，字符串的平均奖励和平均对数似然之间存在权衡。我们对这个问题进行了正式处理，并展示了如何通过选择采样适配器来选择我们用多少似然来交换奖励。</li>
</ul>

<h3>Title: Title:
          Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs</h3>
<ul>
<li><strong>Authors: </strong>Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, Tom Goldstein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, a randomly sampled subset of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale Llama-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks.</li>
<li><strong>摘要：</strong>大型语言模型可以记忆和重复其训练数据，从而造成隐私和版权风险。为了减轻记忆，我们对下一个标记训练目标进行了细微的修改，我们称之为金鱼损失。在训练期间，随机抽取的标记子集被排除在损失计算之外。这些被丢弃的标记不会被模型记忆，从而阻止逐字复制训练集中的完整标记链。我们对十亿级 Llama-2 模型进行了广泛的实验，包括预训练和从头开始训练的模型，并证明可提取记忆显著减少，而对下游基准几乎没有影响。</li>
</ul>

<h3>Title: Title:
          DevBench: A multimodal developmental benchmark for language learning</h3>
<ul>
<li><strong>Authors: </strong>Alvin Wei Ming Tan, Sunny Yu, Bria Long, Wanjing Anya Ma, Tonya Murray, Rebecca D. Silverman, Jason D. Yeatman, Michael C. Frank</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          DevBench: A multimodal developmental benchmark for language learning(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>How (dis)similar are the learning trajectories of vision-language models and children? Recent modeling work has attempted to understand the gap between models' and humans' data efficiency by constructing models trained on less data, especially multimodal naturalistic data. However, such models are often evaluated on adult-level benchmarks, with limited breadth in language abilities tested, and without direct comparison to behavioral data. We introduce DevBench, a multimodal benchmark comprising seven language evaluation tasks spanning the domains of lexical, syntactic, and semantic ability, with behavioral data from both children and adults. We evaluate a set of vision-language models on these tasks, comparing models and humans not only on accuracy but on their response patterns. Across tasks, models exhibit variation in their closeness to human response patterns, and models that perform better on a task also more closely resemble human behavioral responses. We also examine the developmental trajectory of OpenCLIP over training, finding that greater training results in closer approximations to adult response patterns. DevBench thus provides a benchmark for comparing models to human language development. These comparisons highlight ways in which model and human language learning processes diverge, providing insight into entry points for improving language models.</li>
<li><strong>摘要：</strong>视觉语言模型和儿童的学习轨迹有多相似（不同）？最近的建模工作试图通过构建在较少数据（尤其是多模态自然数据）上训练的模型来了解模型和人类数据效率之间的差距。然而，这种模型通常是在成人水平的基准上进行评估的，测试的语言能力范围有限，并且没有直接与行为数据进行比较。我们引入了 DevBench，这是一个多模态基准，包含七个语言评估任务，涵盖词汇、句法和语义能力领域，行为数据来自儿童和成人。我们在这些任务上评估了一组视觉语言模型，不仅在准确性上比较了模型和人类，而且在响应模式上也比较了它们。在各个任务中，模型在接近人类响应模式方面表现出差异，在任务上表现更好的模型也更接近人类的行为响应。我们还研究了 OpenCLIP 在训练过程中的发展轨迹，发现训练越多，结果越接近成人响应模式。因此，DevBench 为将模型与人类语言发展进行比较提供了一个基准。这些比较突出了模型和人类语言学习过程的不同之处，为改进语言模型的切入点提供了见解。</li>
</ul>

<h3>Title: Title:
          Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] Title:
          Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Reward models trained on human preference data have been proven to be effective for aligning Large Language Models (LLMs) with human intent within the reinforcement learning from human feedback (RLHF) framework. However, the generalization capabilities of current reward models to unseen prompts and responses are limited. This limitation can lead to an unexpected phenomenon known as reward over-optimization, where excessive optimization of rewards results in a decline in actual performance. While previous research has advocated for constraining policy optimization, our study proposes a novel approach to enhance the reward model's generalization ability against distribution shifts by regularizing the hidden states. Specifically, we retain the base model's language model head and incorporate a suite of text-generation losses to preserve the hidden states' text generation capabilities, while concurrently learning a reward head behind the same hidden states. Our experimental results demonstrate that the introduced regularization technique markedly improves the accuracy of learned reward models across a variety of out-of-distribution (OOD) tasks and effectively alleviate the over-optimization issue in RLHF, offering a more reliable and robust preference learning paradigm.</li>
<li><strong>摘要：</strong>事实证明，在强化学习人类反馈 (RLHF) 框架内，基于人类偏好数据训练的奖励模型可有效将大型语言模型 (LLM) 与人类意图对齐。然而，当前奖励模型对未见过的提示和响应的泛化能力有限。这种限制可能导致一种称为奖励过度优化的意外现象，即过度优化奖励会导致实际性能下降。虽然先前的研究主张限制策略优化，但我们的研究提出了一种新方法，通过规范隐藏状态来增强奖励模型对分布变化的泛化能力。具体来说，我们保留了基础模型的语言模型头，并结合了一套文本生成损失来保留隐藏状态的文本生成能力，同时在相同的隐藏状态后面学习奖励头。我们的实验结果表明，引入的正则化技术显着提高了学习奖励模型在各种分布外 (OOD) 任务中的准确性，并有效缓解了 RLHF 中的过度优化问题，从而提供了更可靠、更强大的偏好学习范式。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
