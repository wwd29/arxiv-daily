<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-19</h1>
<h3>Title: Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>William Guo, Adaku Uchendu, Ana Smith</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13722">https://arxiv.org/abs/2511.13722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13722">https://arxiv.org/pdf/2511.13722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13722]] Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models(https://arxiv.org/abs/2511.13722)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.</li>
<li><strong>摘要：</strong>为了减轻大型语言模型（LLM）生成文本的潜在危害，研究人员提出了水印，这是一种在文本中嵌入可检测信号的过程。通过水印，我们始终可以准确地检测 LLM 生成的文本。然而，最近的研究结果表明，这些技术通常会对生成文本的质量产生负面影响，并且对抗性攻击可以剥离水印信号，导致文本可能逃避检测。这些发现对法学硕士创建者广泛采用水印产生了阻力。最后，为了鼓励采用，我们通过比较释义和反向翻译（即英语 $\to$ 另一种语言 $\to$ English）攻击来评估几种水印技术对对抗性攻击的鲁棒性；以及他们通过使用语言度量来捕获文本的质量和写作风格来保持无水印文本的质量和写作风格的能力。我们的结果表明，这些水印技术保留了语义，偏离了无水印文本的书写风格，并且容易受到对抗性攻击，尤其是反向翻译攻击。</li>
</ul>

<h3>Title: Can QE-informed (Re)Translation lead to Error Correction?</h3>
<ul>
<li><strong>Authors: </strong>Govardhan Padmanabhan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13884">https://arxiv.org/abs/2511.13884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13884">https://arxiv.org/pdf/2511.13884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13884]] Can QE-informed (Re)Translation lead to Error Correction?(https://arxiv.org/abs/2511.13884)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.</li>
<li><strong>摘要：</strong>本文介绍了提交给 WMT 2025 自动翻译质量评估系统任务 3 - 质量估计 (QE) 的分段级纠错的两种方法。虽然联合训练 QE 系统和自动后期编辑 (APE) 已显示出这两项任务的性能有所提高，但 APE 系统仍然会过度校正机器翻译 (MT) 的输出，从而导致性能下降。我们研究了一种简单的免训练方法 - QE 通知重翻译，并将其与相同免训练范式中的另一种方法进行比较。我们的获胜方法是从不同法学硕士生成的多个候选者中选择最高质量的翻译。第二种方法更类似于 APE，指示 LLM 替换所提供的 QE 解释中指定的错误子字符串。采用条件启发式来最小化编辑次数，目的是最大化增益编辑比。这两种提出的方​​法分别获得了 0.0201 和 -0.0108 的 Delta COMET 分数，领先第一种方法在子任务排行榜上取得获胜位置。</li>
</ul>

<h3>Title: What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations</h3>
<ul>
<li><strong>Authors: </strong>Mihir Gupte, Eshan Dixit, Muhammad Tayyab, Arun Adiththan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13900">https://arxiv.org/abs/2511.13900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13900">https://arxiv.org/pdf/2511.13900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13900]] What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations(https://arxiv.org/abs/2511.13900)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 有效利用远程上下文的能力不断减弱，即“迷失在中间”现象，这对基于检索的 LLM 应用提出了重大挑战。为了研究这种现象在实际应用环境中的影响，我们引入了 GM-Extract，这是一个精心设计的新颖基准数据集，用于评估 LLM 在控制变量检索方面的性能。为了准确诊断故障模式，我们提出了一种简单而优雅的评估系统，使用两个不同的指标：一个用于空间检索能力（文档指标），另一个用于语义检索能力（变量提取指标）。我们对两个多文档任务（键值提取和问答）的 7-8B 参数模型进行了系统评估，证明只需改变数据在上下文窗口中的表示方式即可显着改变检索性能。虽然没有一致地观察到明显的 U 形曲线，但我们的分析揭示了跨模型的清晰表现模式，我们进一步将其与困惑度分数相关联。此外，我们对缓解方法进行了文献调查，将其分为两种不同的方法：黑盒方法和白盒方法。然后，我们将这些技术应用于我们的基准测试，发现它们的功效非常微妙。我们的评估重点介绍了这些策略成功提高绩效的场景，以及它们导致负面影响的令人惊讶的案例，从而全面了解它们在实际环境中的效用。</li>
</ul>

<h3>Title: Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Yilun Zhu, Nikhita Vedula, Shervin Malmasi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.13994">https://arxiv.org/abs/2511.13994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.13994">https://arxiv.org/pdf/2511.13994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.13994]] Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition(https://arxiv.org/abs/2511.13994)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.</li>
<li><strong>摘要：</strong>具有最高级（例如，最佳、最受欢迎）的搜索查询需要跨多个维度比较候选者，要求语言理解和领域知识。我们表明，法学硕士可以通过提取结构化解释或提示的框架来揭示电子商务查询中这些表达背后的潜在意图。我们的方法将查询分解为与检索同时生成的属性值提示，从而能够有效地集成到排名管道中。与基线相比，我们的方法在 MAP 中将搜索性能提高了 10.9 点，在 MRR 中排名提高了 5.9 点。由于基于 LLM 的直接重新排名面临着令人望而却步的延迟，因此我们开发了一种有效的方法，将最高级的解释转移到轻量级模型。我们的研究结果提供了关于如何在模型之间表示和转移最高级语义的见解，推进检索系统中的语言解释，同时解决实际部署限制。</li>
</ul>

<h3>Title: Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports</h3>
<ul>
<li><strong>Authors: </strong>Chenchen Kuai, Zihao Li, Braden Rosen, Stephanie Paan, Navid Jafari, Jean-Louis Briaud, Yunlong Zhang, Youssef M. A. Hashash, Yang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14010">https://arxiv.org/abs/2511.14010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14010">https://arxiv.org/pdf/2511.14010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14010]] Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports(https://arxiv.org/abs/2511.14010)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, agent</a></li>
<li><strong>Abstract: </strong>Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.</li>
<li><strong>摘要：</strong>灾后勘察报告包含理解多种灾害相互作用的关键证据，但其非结构化叙述使系统性知识转移变得困难。大型语言模型 (LLM) 为分析这些报告提供了新的潜力，但当缺乏领域基础时，通常会生成不可靠或幻觉的输出。本研究引入了混合检索代理 RAG (MoRA-RAG)，这是一种以知识为基础的法学硕士框架，可将侦察报告转化为多危险推理的结构化基础。该框架集成了混合检索机制，该机制可以在特定于危险的数据库之间动态路由查询，同时使用代理分块来在检索过程中保持上下文连贯性。它还包括一个验证循环，用于评估证据充分性、优化查询并在信息不完整时启动有针对性的搜索。我们通过从 GEER 勘察报告中派生问答对来构建 HazardRecQA，该报告记录了 7 种主要灾害类型的 90 个全球事件。 MoRA-RAG 的准确率高达 94.5%，比零样本 LLM 提高了 30%，比最先进的 RAG 系统提高了 10%，同时减少了不同 LLM 架构中的幻觉。 MoRA-RAG 还使开放权重法学硕士能够实现与专有模型相当的性能。它建立了一个新的范式，将灾后记录转化为可操作的、值得信赖的灾害抵御能力情报。</li>
</ul>

<h3>Title: HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection</h3>
<ul>
<li><strong>Authors: </strong>Junjie Wu, Yumeng Fu, Nan Yu, Guohong Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14027">https://arxiv.org/abs/2511.14027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14027">https://arxiv.org/pdf/2511.14027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14027]] HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection(https://arxiv.org/abs/2511.14027)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.</li>
<li><strong>摘要：</strong>多模式脱离上下文（OOC）错误信息检测的最新进展在检查支持或反驳图像文本对的不同模式之间的一致性方面取得了显着进展。然而，现有的OOC错误信息检测方法往往强调内部一致性的作用，忽视了图文对与外部证据之间外部一致性的重要性。在本文中，我们提出了 HiEAG，一种新颖的分层证据增强生成框架，通过利用多模态大语言模型（MLLM）的广泛知识来完善外部一致性检查。我们的方法将外部一致性检查分解为一个全面的引擎管道，除了检索之外，它还集成了重新排序和重写。证据重排序模块利用自动证据选择提示（AESP）从证据检索的产品中获取相关的证据项。随后，证据重写模块利用自动证据生成提示（AEGP）来改进基于 MLLM 的 OOC 错误信息检测器的任务适应性。此外，我们的方法可以解释判断，并通过指令调整实现令人印象深刻的性能。不同基准数据集上的实验结果表明，我们提出的 HiEAG 在所有样本的准确性上都超过了之前最先进的 (SOTA) 方法。</li>
</ul>

<h3>Title: Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Zijin Su, Huanzhu Lv, Yuren Niu, Yiming Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14073">https://arxiv.org/abs/2511.14073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14073">https://arxiv.org/pdf/2511.14073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14073]] Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement(https://arxiv.org/abs/2511.14073)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.</li>
<li><strong>摘要：</strong>多标签情感分类通过检测单个文本中的多种情感，在自然语言处理中发挥着至关重要的作用。然而，像 GoEmotions 这样的现有数据集经常遭受严重的类别不平衡，这会影响模型的性能，特别是对于代表性不足的情绪。为了解决这个问题，我们通过集成原始 GoEmotions 数据、使用 RoBERTa-base-GoEmotions 模型来自 Sentiment140 的情感标记样本以及 GPT-4 mini 生成的手动注释文本，构建了一个平衡的多标签情感数据集。我们的数据平衡策略确保了 28 个情绪类别的均匀分布。基于该数据集，我们开发了一个增强的多标签分类模型，该模型结合了预训练的 FastText 嵌入、用于局部特征提取的卷积层、用于上下文学习的双向 LSTM 以及用于突出情感相关单词的注意机制。 sigmoid 激活的输出层可实现多标签预测，混合精度训练可提高计算效率。实验结果表明，与在不平衡数据上训练的模型相比，准确率、精确度、召回率、F1 分数和 AUC 都有显着提高，凸显了我们方法的有效性。</li>
</ul>

<h3>Title: Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT</h3>
<ul>
<li><strong>Authors: </strong>Le Yu, Zhengyue Zhao, Yawen Zheng, Yunhao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14106">https://arxiv.org/abs/2511.14106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14106">https://arxiv.org/pdf/2511.14106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14106]] Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT(https://arxiv.org/abs/2511.14106)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}</li>
<li><strong>摘要：</strong>推理增强视觉语言模型 (RVLM) 依靠安全对齐来防止有害行为，但其暴露的思想链 (CoT) 痕迹引入了新的攻击面。在这项工作中，我们发现 RVLM 的安全对齐可以很容易地突破一种称为 \textbf{Stealth Fine-Tuning} 的新颖攻击方法。我们的方法通过 \textbf{段级干扰} 引出有害的推理痕迹，并将自生成的输出重新用作监督微调数据。通过基于回合的加权损失设计，产生了一种轻量级的、分布一致的微调方法。在我们的实验中，在单个 A100 (QLoRA) 上仅使用 499 个样本且耗时不到 3 小时，Stealth Fine-Tuning 的 ASR 性能比 IDEATOR 高出 38.52\%，同时保留了一般推理能力，因为调整后的模型保留了原始表示分布。 AdvBench 和几个通用基准测试上的实验表明，Stealth Fine-Tuning 是绕过对齐防御的一种低成本且高效的方法。 \textcolor{red}{\textbf{免责声明：本文包含可能令人不安或冒犯的内容。}}</li>
</ul>

<h3>Title: Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding</h3>
<ul>
<li><strong>Authors: </strong>Truong Vo, Weiyi Wu, Kaize Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14112">https://arxiv.org/abs/2511.14112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14112">https://arxiv.org/pdf/2511.14112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14112]] Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding(https://arxiv.org/abs/2511.14112)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.</li>
<li><strong>摘要：</strong>从临床文本自动进行 ICD 编码是医学 NLP 中的一项关键任务，但仍然受到诊断代码的极端长尾分布的阻碍。数以千计的稀有和零样本 ICD 代码在 MIMIC-III 等数据集中的代表性严重不足，导致宏观 F1 分数较低。在这项工作中，我们提出了一个以数据为中心的框架，可以生成高质量的综合放电摘要，以减轻这种不平衡。我们的方法通过利用现实世界的共现模式、ICD 描述、同义词、分类法和类似的临床记录，构建基于稀有代码的真实多标签代码集。使用这些结构化提示，我们生成了 90,000 个综合注释，涵盖 7,902 个 ICD 代码，显着扩大了培训分布。我们在原始数据集和扩展数据集上对两种最先进的基于 Transformer 的模型 PLM-ICD 和 GKI-ICD 进行了微调。实验表明，我们的方法适度改进了宏观 F1，同时保持了强大的微观 F1，优于之前的 SOTA。虽然相对于计算成本来说，收益似乎微乎其微，但我们的结果表明，精心设计的合成数据可以增强长尾 ICD 代码预测的公平性。</li>
</ul>

<h3>Title: SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA</h3>
<ul>
<li><strong>Authors: </strong>Naveen Lamba, Sanju Tiwari, Manas Gaur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14172">https://arxiv.org/abs/2511.14172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14172">https://arxiv.org/pdf/2511.14172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14172]] SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA(https://arxiv.org/abs/2511.14172)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.</li>
<li><strong>摘要：</strong>法学硕士仍然在与幻觉作斗争，尤其是在面对修饰语、否定、数字、例外和命名实体等符号触发因素时。然而，我们对这些象征性幻觉的起源缺乏清晰的了解，因此系统地处理此类触发因素并在模型内定位幻觉的出现至关重要。虽然之前的工作使用 LSC 和激活方差分析等统计技术探索本地化，但这些方法平等对待所有标记，并忽略了符号语言知识在触发幻觉中所起的作用。到目前为止，还没有方法研究符号元素如何专门驱动跨模型层的幻觉失败，也没有使用符号语言知识作为本地化框架的基础。我们提出了第一个符号本地化框架，利用符号语言和语义知识来有意义地追踪所有模型层中幻觉的发展。通过关注模型如何处理符号触发器，我们使用 HaluEval 和 TruthfulQA 分析了五个模型。我们的符号知识方法揭示了这些语言元素的注意力方差在早期层（2-4）中爆炸性地变得严重不稳定，否定触发了灾难性的方差水平，这表明符号语义处理从一开始就崩溃了。从符号语言知识的角度来看，尽管模型规模较大，但幻觉率仍然很高（Gemma 变体为 78.3%-83.7%），并且对整个更深层次的符号语义触发的注意力急剧下降。我们的研究结果表明，幻觉从根本上来说是一种符号语言处理失败，而不是一般的生成问题，这表明符号语义知识为理解和定位法学硕士的幻觉机制提供了关键。</li>
</ul>

<h3>Title: Harnessing Deep LLM Participation for Robust Entity Linking</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Hou, Chenyu Zhang, Rui Meng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14181">https://arxiv.org/abs/2511.14181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14181">https://arxiv.org/pdf/2511.14181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14181]] Harnessing Deep LLM Participation for Robust Entity Linking(https://arxiv.org/abs/2511.14181)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process. In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence. Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.</li>
<li><strong>摘要：</strong>实体链接（EL）是将文本实体映射到知识库中相应条目的任务，构成了自然语言理解的基本组成部分。大型语言模型 (LLM) 的最新进展展示了增强 EL 性能的巨大潜力。先前的研究已利用法学硕士来改善实体消歧和输入表示，从而在准确性和稳健性方面取得显着收益。然而，这些方法通常将法学硕士应用于 EL 任务的孤立阶段，未能在整个过程中充分整合其能力。在这项工作中，我们引入了 DeepEL，这是一个将 LLM 纳入实体链接任务的每个阶段的综合框架。此外，我们发现单独消除实体歧义不足以实现最佳性能。为了解决这一限制，我们提出了一种新颖的自我验证机制，该机制利用全局上下文信息，使法学硕士能够纠正自己的预测，并更好地识别同一句子中实体之间的衔接关系。对 10 个基准数据集的广泛实证评估表明，DeepEL 大大优于现有的最先进方法，总体 F1 分数平均提高 2.6%，在域外数据集上显着提高 4%。这些结果强调了法学硕士深度整合在推进实体链接最先进水平方面的功效。</li>
</ul>

<h3>Title: MuCPT: Music-related Natural Language Model Continued Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Kai Tian, Yirong Mao, Wendong Bi, Hanjie Wang, Que Wenhui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14245">https://arxiv.org/abs/2511.14245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14245">https://arxiv.org/pdf/2511.14245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14245]] MuCPT: Music-related Natural Language Model Continued Pretraining(https://arxiv.org/abs/2511.14245)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.</li>
<li><strong>摘要：</strong>大型语言模型在一般任务上表现强劲，但在音乐等专业环境中仍然受到限制，特别是在音乐娱乐领域，其中语料库规模、纯度以及数据与训练目标之间的匹配至关重要。我们通过构建一个大型的、与音乐相关的自然语言语料库（40B 代币）来解决这个问题，该语料库结合了开源和内部数据，并实现了域优先的数据管道：轻量级分类器对域内文本进行过滤和加权，然后进行多阶段清理、重复数据删除和隐私保护屏蔽。我们进一步将多源音乐文本与相关元数据集成，以形成更广泛、结构更好的领域知识基础。在训练方面，我们引入了基于参考模型（RM）的令牌级软评分来进行质量控制：统一的损失比标准用于数据选择和优化过程中的动态权重降低，减少噪声梯度并放大任务对齐信号，从而实现更有效的音乐领域持续预训练和对齐。为了评估事实性，我们设计了 MusicSimpleQA 基准，该基准采用简短的单一答案提示和自动一致性评分。除了基准设计之外，我们还沿着数据构成的轴进行系统比较。总的来说，这项工作推进了正确的语料库和正确的目标，提供了一个可扩展的数据训练框架和一个可重用的评估工具，用于在音乐领域构建领域法学硕士。</li>
</ul>

<h3>Title: Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning</h3>
<ul>
<li><strong>Authors: </strong>Rui Liu, Yuan Zhao, Zhenqi Jia</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14249">https://arxiv.org/abs/2511.14249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14249">https://arxiv.org/pdf/2511.14249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14249]] Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning(https://arxiv.org/abs/2511.14249)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at this https URL.</li>
<li><strong>摘要：</strong>自动电影配音模型根据给定的脚本生成生动的语音，根据简短的音色提示复制说话者的音色，同时确保与无声视频的口型同步。现有的方法模拟了一个简化的工作流程，演员在没有准备的情况下直接配音，忽略了关键的导演与演员的互动。相比之下，真实的工作流程涉及动态协作：导演积极与演员互动，引导他们在表演前内化背景线索，特别是情感。为了解决这个问题，我们提出了一种新的检索增强导演与演员交互学习方案来实现真实的电影配音，称为Authentic-Dubber，它包含三个新颖的机制：（1）我们构建了一个多模态参考镜头库来模拟导演提供的学习镜头。请注意，我们集成了大型语言模型（LLM）来实现对多模态信号的情感表征的深入理解。 （2）为了模仿演员在配音过程中如何有效、全面地内化导演提供的镜头，我们提出了一种基于情感相似性的检索增强策略。该策略检索与目标无声视频最相关的多模态信息。 （3）我们开发了一种基于渐进图的语音生成方法，该方法逐步合并检索到的多模态情感知识，从而模拟演员的最终配音过程。上述机制使得Authentic-Dubber能够忠实地复制原汁原味的配音流程，实现情感表现力的全面提升。 V2C动画基准数据集的主观和客观评估验证了有效性。代码和演示可在此 https URL 获取。</li>
</ul>

<h3>Title: AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR</h3>
<ul>
<li><strong>Authors: </strong>Gabrial Zencha Ashungafac, Mardhiyah Sanni, Busayo Awobade, Alex Gichamba, Tobi Olatunji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14255">https://arxiv.org/abs/2511.14255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14255">https://arxiv.org/pdf/2511.14255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14255]] AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR(https://arxiv.org/abs/2511.14255)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, hallucination</a></li>
<li><strong>Abstract: </strong>Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.</li>
<li><strong>摘要：</strong>语音人工智能的最新进展，包括 Google 的 NotebookLM 和 OpenAI 的语音到语音 API，正在推动全球对语音接口的广泛兴趣。尽管有这种势头，但还没有针对非洲语言多样性的公开可用的特定应用模型评估。我们推出了 AfriSpeech-MultiBench，这是第一个针对 10 多个国家和七个应用领域的 100 多种非洲英语口音的特定领域评估套件：金融、法律、医疗、一般对话、呼叫中心、命名实体和幻觉鲁棒性。我们使用从各种开放非洲口音英语语音数据集中提取的自发和非自发语音对话，对各种开放式、封闭式、单模态 ASR 和基于多模态 LLM 的语音识别系统进行基准测试。我们的实证分析揭示了系统性差异：开源 ASR 模型在自发语音环境中表现出色，但在嘈杂的非母语对话中表现较差；多模式法学硕士的口音更加稳健，但与特定领域的命名实体存在困难；专有模型可提供高精度的干净语音，但因国家和领域而异。针对非洲英语进行微调的模型以较低的延迟实现了有竞争力的准确性，这是部署的实际优势，但幻觉仍然是大多数 SOTA 模型的一个大问题。通过发布这一综合基准，我们使从业者和研究人员能够选择适合非洲用例的语音技术，为服务不足的社区培育包容性语音应用。</li>
</ul>

<h3>Title: Entropy-Guided Reasoning Compression</h3>
<ul>
<li><strong>Authors: </strong>Hourun Zhu, Yang Gao, Wenlong Fei, Jiawei Li, Huashan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14258">https://arxiv.org/abs/2511.14258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14258">https://arxiv.org/pdf/2511.14258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14258]] Entropy-Guided Reasoning Compression(https://arxiv.org/abs/2511.14258)</code><input type="text"></li>
<li><strong>Keywords: </strong>chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.</li>
<li><strong>摘要：</strong>大型推理模型在复杂推理任务上表现出了卓越的性能，但由于计算成本高和可部署性差，其思想链输出过长仍然是主要的实际瓶颈。现有的压缩方法取得了部分成功，但忽略了训练过程中的一个关键现象——熵冲突。在压缩训练过程中，熵会减少，导致推理时间缩短但探索受到限制，而以准确性为导向的目标会增加熵，从而延长推理链。这可能会导致模型陷入局部困境。我们的分析进一步揭示了熵冲突的根源：许多高熵令牌是逻辑连接器，它们接收更大的梯度并在性能目标下受到鼓励，而压缩目标同时惩罚这些潜在的冗余连接器。这种相反的压力造成了熵冲突的直接来源。为了解决这些问题，我们采用熵引导的训练框架。随着熵的下降，通过鼓励简洁的思维步骤来引导模型进行有效的推理；随着熵的增加，在紧凑推理模式下加强探索以提高鲁棒性。对六个数学基准的实验表明，我们的方法将推理长度压缩到原始的 20%，同时保持甚至超过基线精度。代码和模型将公开发布。</li>
</ul>

<h3>Title: Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space</h3>
<ul>
<li><strong>Authors: </strong>Ante Wang, Weizhi Ma, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14275">https://arxiv.org/abs/2511.14275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14275">https://arxiv.org/pdf/2511.14275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14275]] Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space(https://arxiv.org/abs/2511.14275)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.</li>
<li><strong>摘要：</strong>了解模型响应的可靠性在应用中至关重要。凭借法学硕士强大的生成能力，研究重点集中在生成言语信心上。通过结合思想链推理进一步增强了这一点，提供了逻辑和透明的估计。然而，推理策略如何影响估计的置信度仍有待探索。在这项工作中，我们证明预测语言化概率分布可以有效地鼓励对置信度估计进行深入推理。直观上，它要求法学硕士考虑答案空间内的所有候选人，而不是基于单一猜测，并仔细分配置信度分数以满足分布的要求。无论答案空间是否已知，该方法在不同模型和各种任务中都显示出优势。即使在强化学习之后，它的优势仍然保持不变，并且进一步分析表明它的推理模式符合人类的期望。</li>
</ul>

<h3>Title: AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Zbib, Hasan Abed Al Kader Hammoud, Sina Mukalled, Nadine Rizk, Fatima Karnib, Issam Lakkis, Ammar Mohanna, Bernard Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14295">https://arxiv.org/abs/2511.14295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14295">https://arxiv.org/pdf/2511.14295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14295]] AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models(https://arxiv.org/abs/2511.14295)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.</li>
<li><strong>摘要：</strong>我们推出 AraLingBench：一个完全由人类注释的基准，用于评估大型语言模型 (LLM) 的阿拉伯语语言能力。该基准涵盖五个核心类别：语法、词法、拼写、阅读理解和句法，通过 150 个专家设计的多项选择题来直接评估结构语言理解。对 35 名阿拉伯语和双语法学硕士的评估表明，当前模型表现出很强的表面能力，但在更深层次的语法和句法推理方面存在困难。 AraLingBench 强调了基于知识的基准测试的高分与真正的语言掌握之间持续存在的差距，表明许多模型通过记忆或模式识别而不是真实的理解来取得成功。通过分离和衡量基本语言技能，AraLingBench 为培养阿拉伯语法学硕士提供了一个诊断框架。完整的评估代码可在 GitHub 上公开获取。</li>
</ul>

<h3>Title: ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions</h3>
<ul>
<li><strong>Authors: </strong>Xingwei He, Qianru Zhang, Pengfei Chen, Guanhua Chen, Linlin Yu, Yuan Yuan, Siu-Ming Yiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14342">https://arxiv.org/abs/2511.14342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14342">https://arxiv.org/pdf/2511.14342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14342]] ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions(https://arxiv.org/abs/2511.14342)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.</li>
<li><strong>摘要：</strong>遵循指令是大型语言模型 (LLM) 的一项关键功能。虽然现有的工作主要侧重于评估法学硕士遵守用户指令的程度，但它们经常忽略指令包含冲突约束的情况——这在复杂的提示中很常见。法学硕士在这种条件下的行为仍有待探索。为了弥补这一差距，我们引入了 ConInstruct，这是一个专门为评估法学硕士检测和解决用户指令中冲突的能力而设计的基准。使用该数据集，我们评估法学硕士的冲突检测性能并分析其冲突解决行为。我们的实验揭示了两个关键发现：（1）大多数专有 LLM 表现出强大的冲突检测能力，而在开源模型中，只有 DeepSeek-R1 表现出类似的强大性能。 DeepSeek-R1 和 Claude-4.5-Sonnet 的平均 F1 分数最高，分别为 91.5% 和 87.3%，总体排名第一和第二。 (2) 尽管法学硕士具有很强的冲突检测能力，但在面临冲突的约束时很少明确通知用户冲突或要求澄清。这些结果强调了当前法学硕士的一个关键缺点，并强调了设计遵循指令的法学硕士时未来改进的一个重要领域。</li>
</ul>

<h3>Title: The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Prathamesh Kalamkar, Ned Letcher, Meissane Chami, Sahger Lad, Shayan Mohanty, Prasanna Pendse</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14365">https://arxiv.org/abs/2511.14365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14365">https://arxiv.org/pdf/2511.14365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14365]] The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models(https://arxiv.org/abs/2511.14365)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The application of large language models (LLMs) to chemistry is frequently hampered by a "tokenization bottleneck", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 在化学中的应用经常受到“标记化瓶颈”的阻碍，其中针对通用域文本进行调整的标记化器往往会将化学表示（例如 SMILES）分割成语义上无信息的子标记。本文介绍了一种原则方法，通过在单个模型中统一自然语言和分子结构的表示来解决这一瓶颈。我们的方法涉及有针对性的词汇扩展——使用化学显着标记来增强预训练的法学硕士的词汇，然后继续对化学领域文本进行预训练以整合这些新知识。我们提供了该策略有效性的实证证明，表明我们的方法可以在一系列下游化学任务中带来卓越的性能。</li>
</ul>

<h3>Title: ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hongwei Liu, Junnan Liu, Shudong Liu, Haodong Duan, Yuqiang Li, Mao Su, Xiaohong Liu, Guangtao Zhai, Xinyu Fang, Qianhong Ma, Taolin Zhang, Zihan Ma, Yufeng Zhao, Peiheng Zhou, Linchen Xiao, Wenlong Zhang, Shijie Zhou, Xingjian Ma, Siqi Sun, Jiaye Ge, Meng Li, Yuhong Liu, Jianxin Dong, Jiaying Li, Hui Wu, Hanwen Liang, Jintai Lin, Yanting Wang, Jie Dong, Tong Zhu, Tianfan Fu, Conghui He, Qi Zhang, Songyang Zhang, Lei Bai, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14366">https://arxiv.org/abs/2511.14366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14366">https://arxiv.org/pdf/2511.14366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14366]] ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning(https://arxiv.org/abs/2511.14366)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的快速发展导致许多既定基准的性能饱和，质疑它们区分前沿模型的能力。与此同时，现有的高难度基准往往存在学科焦点狭窄、答案格式过于简单化以及容易受到数据污染的问题，从而与现实世界的科学探究产生了保真度差距。为了应对这些挑战，我们引入了ATLAS（面向AGI的科学逻辑应用测试床），这是一个由大约800个原始问题组成的大规模、高难度、跨学科的评估套件。 ATLAS由领域专家（博士级及以上）开发，涵盖数学、物理、化学、生物学、计算机科学、地球科学和材料科学七个核心科学领域。其主要特点包括：（1）原创性高、抗污染性强，所有试题均为全新设计或大幅改编，防止测试数据泄露； （2）跨学科焦点，旨在评估模型跨科学领域整合知识和推理的能力； (3) 高保真答案，优先考虑涉及多步骤推理和 LaTeX 格式表达式的复杂、开放式答案，而不是简单的多项选择题； （4）严格的质量控制，采用专家同行评审和对抗性测试的多阶段过程，确保问题的难度、科学价值和正确性。我们还提出了一个强大的评估范例，使用法学硕士评委小组对复杂的答案进行自动化、细致的评估。领先模型的初步结果证明了 ATLAS 在区分其先进科学推理能力方面的有效性。我们计划将ATLAS发展成为一个长期、开放、社区驱动的平台，为通用人工智能的进步提供可靠的“统治者”。</li>
</ul>

<h3>Title: Mitigating Label Length Bias in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mario Sanz-Guerrero, Katharina von der Wense</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14385">https://arxiv.org/abs/2511.14385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14385">https://arxiv.org/pdf/2511.14385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14385]] Mitigating Label Length Bias in Large Language Models(https://arxiv.org/abs/2511.14385)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是强大的零样本和少样本学习器。然而，在预测一组候选选项时，法学硕士会受到标签偏差的影响，而现有的校准方法会忽略多标记类别标签产生的偏差。我们解决了一个称为标签长度偏差的问题，即即使在标准长度标准化之后，不同长度的标签的处理也不一致。为了缓解这个问题，我们提出了标准化上下文校准（NCC），这是一种在全标签级别标准化和校准预测的有效方法。 NCC 在多个数据集和模型上比之前的方法取得了统计上的显着改进，F1 增益高达 10%。此外，NCC 将偏见缓解扩展到更广泛的任务，例如多项选择题回答。我们的分析表明，当与上下文学习相结合时，NCC 对少数样本选择不太敏感，需要更少的样本来获得竞争性能，并产生更可靠的置信估计。这些发现强调了减轻全标签偏差以提高基于 LLM 的方法的性能和稳健性的重要性，特别是在类标签自然包含多个标记的实际应用中。</li>
</ul>

<h3>Title: Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education</h3>
<ul>
<li><strong>Authors: </strong>Xin Yi, Yue Li, Dongsheng Shi, Linlin Wang, Xiaoling Wang, Liang He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14423">https://arxiv.org/abs/2511.14423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14423">https://arxiv.org/pdf/2511.14423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14423]] Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education(https://arxiv.org/abs/2511.14423)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地集成到教育应用程序中。然而，它们仍然容易受到越狱和微调攻击，这可能会损害安全性并导致有害的输出。现有研究主要集中于一般安全评估，对教育场景独特的安全要求关注有限。为了解决这一差距，我们构建了 EduHarm，这是一个基准，包含跨越五个代表性教育场景的安全-不安全教学对，从而能够对教育法学硕士进行系统的安全评估。此外，我们为教育法学硕士提出了一个三阶段防护框架（TSSF），可以同时缓解越狱和微调攻击。首先，安全意识的注意力重新调整将注意力转向关键的不安全令牌，从而恢复区分不安全和安全输入的危害性特征。其次，分层安全判断通过聚合多个层的安全线索来检测不安全指令，从而识别有害特征。最后，防御驱动的双路由将安全和不安全的查询分开，确保对良性输入的正常处理和对有害输入的保护响应。跨八种越狱攻击策略的大量实验表明，TSSF 有效增强了安全性，同时防止过度拒绝良性查询。对三个微调攻击数据集的评估进一步表明，它始终能够对有害查询实现强大的防御，同时保持良性微调的效用收益。</li>
</ul>

<h3>Title: MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents</h3>
<ul>
<li><strong>Authors: </strong>Jinru Ding, Lu Lu, Chao Ding, Mouxiao Bian, Jiayuan Chen, Renjie Lu, Wenrao Pang, Xiaoqin Wu, Zhiqiang Liu, Luyi Jiang, Bing Han, Yunqiu Wang, Jie Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14439">https://arxiv.org/abs/2511.14439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14439">https://arxiv.org/pdf/2511.14439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14439]] MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents(https://arxiv.org/abs/2511.14439)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.</li>
<li><strong>摘要：</strong>医学大语言模型 (LLM)、多模式模型和代理的最新进展需要反映真实临床工作流程和安全约束的评估框架。我们推出了 MedBench v4，这是一个全国性的、基于云的基准测试基础设施，包含超过 700,000 个专家策划的任务，涵盖 24 个主要专业和 91 个二级专业，并专门针对法学硕士、多模式模型和代理。项目经过来自 500 多家机构的临床医生的多阶段细化和多轮审查，开放式回答由法学硕士作为法官根据人类评分进行评分。我们评估了 15 个前沿模型。基础法学硕士的平均总分达到 54.1/100（最好：Claude Sonnet 4.5, 62.5/100），但安全性和道德仍然很低（18.4/100）。多模态模型总体表现较差（平均 47.5/100；最好：GPT-5，54.9/100），感知扎实，但跨模态推理较弱。基于相同骨干网络构建的代理显着提高了端到端性能（平均 79.8/100），基于 Claude Sonnet 4.5 的代理总体得分高达 85.3/100，安全任务得分高达 88.9/100。因此，MedBench v4 揭示了基础模型在多模式推理和安全性方面持续存在的差距，同时表明具有治理意识的代理编排可以在不牺牲能力的情况下显着增强基准临床准备情况。通过将任务与中国临床指南和监管重点相结合，该平台为审计医疗人工智能的医院、开发商和政策制定者提供了实用参考。</li>
</ul>

<h3>Title: Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning</h3>
<ul>
<li><strong>Authors: </strong>Trishala Jayesh Ahalpara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14445">https://arxiv.org/abs/2511.14445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14445">https://arxiv.org/pdf/2511.14445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14445]] Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning(https://arxiv.org/abs/2511.14445)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.</li>
<li><strong>摘要：</strong>我们推出了 Tell Me，这是一个心理健康系统，它利用大型语言模型的进步为用户和研究人员提供可访问的、上下文感知的支持。该系统集成了三个组件：（i）检索增强生成（RAG）助手，用于个性化、基于知识的对话； (ii) 以客户资料为条件的综合客户-治疗师对话生成器，以促进治疗语言和数据增强的研究； (iii) 一个由 CrewAI 实施的 Well-being AI 团队，每周制作自我护理计划和引导冥想音频。该系统被设计为情感处理的反思空间，而不是专业治疗的替代品。它说明了会话助理如何降低支持障碍、补充现有护理并扩大获得心理健康资源的机会。为了解决保密治疗数据的短缺问题，我们引入了根据客户资料生成的综合客户-治疗师对话。最后，规划器展示了一种创新的代理工作流程，用于动态自适应、个性化的自我护理，弥补了静态健康工具的局限性。我们描述了该架构，展示了其功能，并使用基于 LLM 的自动判断和人类用户研究报告了 RAG 助手在策划的福祉场景中的评估。这项工作强调了 NLP 研究人员和心理健康专业人士之间跨学科合作的机会，以推进人类与人工智能互动的负责任创新，以实现福祉。</li>
</ul>

<h3>Title: Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingyue Cheng, Jie Ouyang, Shuo Yu, Ruiran Yan, Yucong Luo, Zirui Liu, Daoyu Wang, Qi Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14460">https://arxiv.org/abs/2511.14460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14460">https://arxiv.org/pdf/2511.14460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14460]] Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning(https://arxiv.org/abs/2511.14460)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.</li>
<li><strong>摘要：</strong>人们越来越多地探索大型语言模型（LLM）来构建能够主动环境交互（例如，通过工具使用）来解决复杂问题的智能体。强化学习（RL）被认为是一项关键技术，在训练此类智能体方面具有巨大潜力；然而，强化学习在法学硕士代理人中的有效应用仍处于起步阶段，面临着相当大的挑战。目前，这个新兴领域缺乏对专门为 LLM Agent 环境量身定制的 RL 方法的深入探索，同时也缺乏为此目的而设计的灵活且易于扩展的培训框架。为了帮助推进这一领域，本文首先通过系统地扩展马尔可夫决策过程（MDP）框架来全面定义 LLM Agent 的关键组件，重新审视并阐明了 LLM Agent 的强化学习方法。其次，我们介绍 Agent-R1，这是一个模块化、灵活且用户友好的训练框架，用于基于 RL 的 LLM Agent，旨在直接适应不同的任务场景和交互环境。我们对 Multihop QA 基准任务进行了实验，为我们提出的方法和框架的有效性提供了初步验证。</li>
</ul>

<h3>Title: LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation</h3>
<ul>
<li><strong>Authors: </strong>David Carmel, Simone Filice, Guy Horowitz, Yoelle Maarek, Alex Shtoff, Oren Somekh, Ran Tavory</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14531">https://arxiv.org/abs/2511.14531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14531">https://arxiv.org/pdf/2511.14531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14531]] LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation(https://arxiv.org/abs/2511.14531)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.</li>
<li><strong>摘要：</strong>随着检索增强生成（RAG）在生成人工智能解决方案中变得越来越重要，越来越需要系统地评估其有效性。我们引入了 LiveRAG 基准，这是一个包含 895 个综合问题和答案的公开数据集，旨在支持基于 RAG 的问答系统的系统评估。这一综合基准源自 SIGIR'2025 LiveRAG 挑战赛期间使用的基准，在严格的时间限制下对参赛者进行了评估。它增加了挑战赛期间未向参赛者提供的信息，例如真实答案，以及用于评估参赛者答案的相关支持声明。此外，每个问题都与估计的难度和可辨别性分数相关联，这些分数是通过将项目响应理论模型应用于竞争对手的回答而得出的。我们的分析强调了基准测试问题的多样性、难度级别的广泛性以及它们在区分系统功能方面的有用性。 LiveRAG 基准有望帮助社区推进 RAG 研究、进行系统评估并开发更强大的问答系统。</li>
</ul>

<h3>Title: Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shreya Adrita Banik, Niaz Nafi Rahman, Tahsina Moiukh, Farig Sadeque</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14606">https://arxiv.org/abs/2511.14606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14606">https://arxiv.org/pdf/2511.14606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14606]] Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models(https://arxiv.org/abs/2511.14606)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.</li>
<li><strong>摘要：</strong>检测新闻媒体中的政治偏见是一项复杂的任务，需要解释微妙的语言和语境线索。尽管自然语言处理 (NLP) 的最新进展已经实现了自动偏差分类，但大型语言模型 (LLM) 与人类判断的一致程度仍然相对未得到充分探索，也尚未得到充分理解。本研究旨在提出一个比较框架，用于评估人类注释和多个法学硕士（包括 GPT、BERT、RoBERTa 和 FLAN）的政治偏见检测。我们构建了手动注释的新闻文章数据集，并评估注释一致性、偏见极性和模型间一致性，以量化人类和模型对偏见的看法之间的差异。实验结果表明，在传统的基于 Transformer 的模型中，RoBERTa 与人类标签实现了最高的一致性，而 GPT 等生成模型在零样本设置中表现出与人类注释的最强整体一致性。在所有基于 Transformer 的基线中，我们经过微调的 RoBERTa 模型获得了最高的准确度以及与人工注释标签的最强一致性。我们的研究结果强调了人类和法学硕士在感知政治倾向方面的系统性差异，强调了在自动媒体偏见检测中将人类可解释性与模型可扩展性结合起来的混合评估框架的需要。</li>
</ul>

<h3>Title: Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Kahaan Gandhi, Boris Bolliet, Inigo Zubeldia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14631">https://arxiv.org/abs/2511.14631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14631">https://arxiv.org/pdf/2511.14631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14631]] Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities(https://arxiv.org/abs/2511.14631)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: this https URL</li>
<li><strong>摘要：</strong>我们证明，由视觉语言模型（VLM）引导的多智能体系统可以改善端到端的自主科学发现。通过将绘图视为可验证的检查点，VLM 作为法官根据动态生成的特定领域的规则来评估数字，使代理能够纠正自己的错误并实时引导探索性数据分析。宇宙学和天体化学的案例研究表明，无需人工干预即可从错误的推理路径中恢复并适应新的数据集。在数据驱动发现的 10 项任务基准上，VLM 增强系统以 0.7-0.8 分的 1 分通过，而纯代码基线为 0.2-0.3，代码和文本基线为 0.4-0.5，同时还提供了可审计的推理跟踪，提高了可解释性。此处提供代码：此 https URL</li>
</ul>

<h3>Title: A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases</h3>
<ul>
<li><strong>Authors: </strong>Tao Yang, Dandan Huang, Yunting Lin, Pengfei Wu, Zhikun Wu, Gangyuan Ma, Yulan Lu, Xinran Dong, Dingpeng Li, Junshuang Ge, Zhiyan Zhang, Xuanzhao Huang, Wenyan Nong, Yao Zhou, Hui Tang, Hongxi Yang, Shijie Zhang, Juan Li, Xiaojun Cao, Lin Yang, Xia Gao, Kaishou Xu, Xiaoqiong Gu, Wen Zhang, Huimin Xia, Li Liu, Wenhao Zhou, Mulin Jun Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14638">https://arxiv.org/abs/2511.14638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14638">https://arxiv.org/pdf/2511.14638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14638]] A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases(https://arxiv.org/abs/2511.14638)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.</li>
<li><strong>摘要：</strong>罕见疾病影响着全世界数亿人，但诊断往往需要数年时间。对流管道将嘈杂的证据提取与下游推理诊断分离开来，普通/医学大语言模型 (LLM) 面临着稀缺的现实世界电子健康记录 (EHR)、过时的领域知识和幻觉。我们组装了一个大型的、领域专业的临床语料库和临床医生验证的推理集，并通过分阶段指令调整、思想链学习和基于图形的检索来开发 RareSeek R1。在多中心 EHR 叙述和公共基准中，RareSeek R1 实现了最先进的准确性、强大的泛化性以及在嘈杂或重叠表型下的稳定性。当叙述与优先变体配对时，通过解决歧义并使候选者与机制保持一致，增强检索会产生最大的收益。人体研究表明，其性能与经验丰富的医生相当，并且在辅助使用方面取得了持续的进步。值得注意的是，透明推理强调了决定性的非表型证据（中位数为 23.1%，例如影像学、干预措施、功能测试），这些证据支撑着许多正确的诊断。这项工作提出了一种叙事优先、知识整合的推理范式，可以缩短诊断过程并实现可审计、可临床翻译的决策支持。</li>
</ul>

<h3>Title: Graded strength of comparative illusions is explained by Bayesian inference</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Zhang, Erxiao Wang, Cory Shain</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14642">https://arxiv.org/abs/2511.14642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14642">https://arxiv.org/pdf/2511.14642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14642]] Graded strength of comparative illusions is explained by Bayesian inference(https://arxiv.org/abs/2511.14642)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.</li>
<li><strong>摘要：</strong>与视觉处理一样，语言处理也容易产生错觉，导致人们系统性地错误感知刺激。在一种这样的情况下——比较错觉（CI），例如，更多的学生去过俄罗斯比我有——理解者倾向于判断这个句子是可以接受的，尽管它潜在的无意义的比较。先前的研究认为，这种现象可以解释为噪声通道上的贝叶斯推理：句子解释的后验概率与该解释的先验概率以及损坏观察到的 (CI) 句子的可能性成正比。最初的行为研究通过评估一组狭义的 CI 句子的替代解释来支持这一说法，并表明理解者更喜欢那些更有可能被腐蚀成虚幻句子的解释。在这项研究中，我们通过使用合理解释的后验概率的定量模型直接预测幻觉的强度，复制并大大超越了这项早期的工作，这是我们通过统计语言模型与人类行为数据的新颖综合得出的。我们的模型不仅解释了 CI 效应强度的精细分级，还解释了由代词与完整名词短语比从句主语引起的先前无法解释的效应。这些发现支持了句子理解的噪声通道理论，证明该理论对经验上证实的比较错觉做出了新颖的预测。这一结果与虚幻和非虚幻上下文中噪声通道处理的相关证据相结合，支持噪声通道推理作为不同语言处理现象的统一计算级理论。</li>
</ul>

<h3>Title: Bias in, Bias out: Annotation Bias in Multilingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xia Cui, Ziyi Huang, Naeemeh Adel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14662">https://arxiv.org/abs/2511.14662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14662">https://arxiv.org/pdf/2511.14662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14662]] Bias in, Bias out: Annotation Bias in Multilingual Large Language Models(https://arxiv.org/abs/2511.14662)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.</li>
<li><strong>摘要：</strong>NLP 数据集中的注释偏差仍然是开发多语言大语言模型 (LLM) 的主要挑战，特别是在文化多元化的环境中。任务框架、注释者主观性和文化不匹配带来的偏差可能会扭曲模型输出并加剧社会危害。我们提出了一个全面的框架来理解注释偏差、区分指令偏差、注释者偏差以及上下文和文化偏差。我们回顾了检测方法（包括注释者间一致性、模型分歧和元数据分析），并重点介绍了多语言模型分歧和文化推理等新兴技术。我们进一步概述了主动和被动的缓解策略，包括多样化的注释者招募、迭代指南细化和事后模型调整。我们的贡献包括：（1）注释偏差的类型学； (2) 检测指标的综合； （3）适用于多语言环境的基于集成的偏差缓解方法，以及（4）注释过程的伦理分析。这些见解共同旨在为法学硕士提供更加公平和基于文化的注释渠道。</li>
</ul>

<h3>Title: Streamlining Industrial Contract Management with Retrieval-Augmented LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kristi Topollai, Tolga Dimlioglu, Anna Choromanska, Simon Odie, Reginald Hui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14671">https://arxiv.org/abs/2511.14671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14671">https://arxiv.org/pdf/2511.14671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14671]] Streamlining Industrial Contract Management with Retrieval-Augmented LLMs(https://arxiv.org/abs/2511.14671)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.</li>
<li><strong>摘要：</strong>合同管理涉及审查和谈判定义权利、义务和协议条款的规定、个别条款。在此过程中，对条款的修改被提出并迭代完善，其中一些可能存在问题或不可接受。由于标记数据的稀缺和非结构化遗留合同的大量存在，自动化该工作流程具有挑战性。在本文中，我们提出了一个模块化框架，旨在通过检索增强生成（RAG）管道简化合同管理。我们的系统集成了合成数据生成、语义子句检索、可接受性分类和基于奖励的对齐，以标记有问题的修订并生成改进的替代方案。我们的系统与行业合作伙伴合作开发和评估，在识别和优化有问题的修订方面实现了 80% 以上的准确率，在现实世界、资源匮乏的条件下展示了强大的性能，并提供了加速合同修订工作流程的实用方法。</li>
</ul>

<h3>Title: SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Biaojie Zeng, Min Zhang, Juan Zhou, Fengrui Liu, Ruiyang Huang, Xin Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14684">https://arxiv.org/abs/2511.14684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14684">https://arxiv.org/pdf/2511.14684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14684]] SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction(https://arxiv.org/abs/2511.14684)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在解决数学问题时经常会出现推理错误，如何自动检测并纠正这些错误已成为重要的研究方向。然而，现有的方法\textit{主要侧重于模型内的自我纠正}，缺乏教育环境中所需的“教师式”纠正，\textit{即系统地指导和修改学生解决问题的过程。为了解决这一差距，我们提出了 \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection})，这是一种将法学硕士与学生推理结合起来的新颖方法。具体来说，\texttt{SMRC} 将学生推理表述为多步骤顺序决策问题，并引入蒙特卡罗树搜索（MCTS）来探索最佳校正路径。为了降低注释过程级奖励的成本，我们利用 LLM 和最终答案评估引导的广度优先搜索（BFS）来生成奖励信号，然后通过反向传播机制将其分布在中间推理步骤中，从而实现细粒度的过程监督。此外，我们还构建了高中数学基准，MSEB（多解错误基准），由 158 个实例组成，其中包括问题陈述、学生解决方案和正确推理步骤。我们进一步提出了一种以 \textbf{解决方案准确性} 和 \textbf{正确步骤保留} 为中心的双重评估协议，提供了教育适用性的综合衡量标准。实验表明，在有效性和整体性能方面，\texttt{SMRC} 在两个公共数据集（ProcessBench 和 MR-GSM8K）以及我们的 MSEB 上显着优于现有方法。代码和数据可从此 https URL 获取。</li>
</ul>

<h3>Title: Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries</h3>
<ul>
<li><strong>Authors: </strong>Kiera McCormick, Rafael Martínez-Galarza</a></li>
<li><strong>Subjects: </strong>cs.CL, astro-ph.IM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14685">https://arxiv.org/abs/2511.14685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14685">https://arxiv.org/pdf/2511.14685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14685]] Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries(https://arxiv.org/abs/2511.14685)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.</li>
<li><strong>摘要：</strong>大型语言模型已经证明了在跨领域、跨模式的多个层面上良好泛化的能力，甚至还表现出了上下文学习能力。这使得研究问题涉及如何使用它们来编码通常只能从科学测量中获得的物理信息，并松散地编码在文本描述中。使用天体物理学作为测试平台，我们研究了 LLM 嵌入是否可以通过两个主要问题对从科学测量中获得的物理汇总统计数据进行编码：1）提示是否对 LLM 如何编码这些量发挥作用？ 2）语言的哪些方面在编码测量所代表的物理过程中最重要？我们使用稀疏自动编码器来研究这一点，该编码器从文本中提取可解释的特征。</li>
</ul>

<h3>Title: Ground Truth Generation for Multilingual Historical NLP using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Clovis Gladstone, Zhao Fang, Spencer Dean Stewart</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14688">https://arxiv.org/abs/2511.14688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14688">https://arxiv.org/pdf/2511.14688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14688]] Ground Truth Generation for Multilingual Historical NLP using LLMs(https://arxiv.org/abs/2511.14688)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.</li>
<li><strong>摘要：</strong>由于注释数据有限以及与现代网络语料库的领域不匹配，历史和低资源 NLP 仍然具有挑战性。本文概述了我们使用大型语言模型 (LLM) 为历史法语（16-20 世纪）和中文（1900-1950）文本创建真实注释的工作。通过在我们的语料库子集上利用 LLM 生成的真实事实，我们能够对 spaCy 进行微调，以在词性 (POS) 注释、词形还原和命名实体识别 (NER) 的特定时期测试中取得显着成果。我们的结果强调了特定领域模型的重要性，并证明即使相对有限的合成数据也可以改进计算人文研究中资源不足的语料库的 NLP 工具。</li>
</ul>

<h3>Title: Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances</h3>
<ul>
<li><strong>Authors: </strong>Rishu Kumar Singh, Navneet Shreya, Sarmistha Das, Apoorva Singh, Sriparna Saha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14693">https://arxiv.org/abs/2511.14693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14693">https://arxiv.org/pdf/2511.14693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14693]] Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances(https://arxiv.org/abs/2511.14693)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: this https URL</li>
<li><strong>摘要：</strong>现有的投诉分析方法很大程度上依赖于单模态、简短的内容，例如推文或产品评论。这项工作通过利用多模式、多回合的客户支持对话来推动该领域的发展，用户经常共享文本投诉和视觉证据（例如屏幕截图、产品照片），以实现对投诉方面和严重程度的细粒度分类。我们推出了 VALOR，一种具有专家路由功能的验证感知学习器，专为这种多模式设置而定制。它采用多专家推理设置，使用大规模生成模型和思想链 (CoT) 提示细致入微的决策。为了确保模态之间的一致性，计算语义对齐分数并通过元融合策略将其集成到最终分类中。为了与联合国可持续发展目标 (UN SDG) 保持一致，拟议的框架通过推进人工智能驱动的工具来实现稳健、可扩展和情境感知的服务基础设施，从而支持 SDG 9（工业、创新和基础设施）。此外，通过对投诉叙述和视觉背景进行结构化分析，它可以促进更具响应性的产品设计和改进消费者服务的问责制，从而为 SDG 12（负责任的消费和生产）做出贡献。我们在用细粒度方面和严重性标签注释的精心策划的多模式投诉数据集上评估 VALOR，表明它始终优于基线模型，特别是在信息跨文本和图像分布的复杂投诉场景中。这项研究强调了多模式交互和专家验证在实际投诉理解系统中的价值。与数据和代码相关的资源可在此处获得：此 https URL</li>
</ul>

<h3>Title: Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance</h3>
<ul>
<li><strong>Authors: </strong>Raha Aghaei, Ali A. Kiaei, Mahnaz Boush, Mahan Rofoosheh, Mohammad Zavvar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.14709">https://arxiv.org/abs/2511.14709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.14709">https://arxiv.org/pdf/2511.14709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.14709]] Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance(https://arxiv.org/abs/2511.14709)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.</li>
<li><strong>摘要：</strong>本研究分析了大型语言模型 (LLM) 在变革研发 (R&D) 流程中的多种功能。通过自动化知识发现、促进假设创建、整合跨学科见解以及促进创新生态系统内的合作，法学硕士极大地提高了研究过程的效率和有效性。通过对科学文献、专利数据库和实验数据的广泛分析，这些模型可以实现更灵活、更明智的研发工作流程，最终加快创新周期并缩短突破性想法的上市时间。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
